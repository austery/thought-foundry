---
author: 硅谷101
date: '2025-09-06'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=uLYNDk-QnKg
speaker: 硅谷101
tags:
  - t-literature-note
  - '101'
  - google-ai
  - multimodal-ai
  - nano-banana
  - generative-ai
  - image-generation
title: 谷歌AI多模态五大主线布局：从Nano Banana看其生成式AI战略
summary: 本文深入探讨谷歌最新文生图模型Nano Banana的突破性能力，如惊人的一致性和多图融合。同时，系统梳理谷歌在文生图、文生视频、交互世界生成等五大多模态产品线，分析其在生成式AI领域的战略布局与市场潜力。
insight: ''
draft: true
series: ''
category: ''
area: ''
project: ''
status: evergreen
---
### Nano Banana：文生图领域的“炸裂”新星

近期，一款代号为**Nano Banana**的模型在评测平台上悄然登场，它没有官方公告或文档，却凭借惊人的图像质量和角色一致性，迅速超越了众多老牌模型，在AI社区引发轰动。最初，人们猜测其可能是**OpenAI**（一家美国人工智能研究公司，以开发ChatGPT等模型闻名）的秘密实验品，或是独立研究团队的“黑马之作”。几天后，谜底揭晓：**Google**（谷歌，全球知名科技公司）正式认领，**Nano Banana**正是其最新发布的文生图模型——**Gemini 2.5 Flash Image**。

作为**Gemini 2.0 Flash**（谷歌Gemini系列模型的一个版本）的升级版，**Nano Banana**更贴近真实工作流，是一个功能强大的**AI编辑器**（Artificial Intelligence Editor: 基于人工智能技术，辅助或自动完成编辑任务的工具）。它不仅能在多次编辑中保持角色和画面的高度一致，还支持用户通过自然语言完成精细的局部修改和多图合成。与过去大多数模型追求“生成一张好图”的目标不同，**Nano Banana**更像一个随时待命的设计助手，能够帮助用户不断迭代、调整、优化和创造。许多网友测试后表示，这可能预示着**Photoshop**（Adobe公司开发的图像处理软件）时代的终结。

在文生图模型赛道竞争日益激烈的当下，**Nano Banana**为何能再次掀起狂潮？相较于**OpenAI**、**Flux**（一个专注于AI艺术创作和3D生成的平台）等强劲对手，它有何特别之处？其真实效果如何？以及，谷歌的多模态能力究竟发展到了何种程度？

### 硅谷101年度科技大会预告

欢迎来到硅谷101，我是陈茜。在深入探讨**Nano Banana**之前，我们很高兴宣布硅谷101的年度科技大会又回来了！过去一年，我们关注的前沿科技领域，如AI、机器人、自动驾驶、脑机接口、生命科学、Web3等都发展迅猛。为了回馈大家的支持，我们诚挚邀请您参加10月5日的**Alignment 2025 硅谷101科技大会**。

作为内容团队，我们的大会不仅邀请大咖分享科技干货，更是一场有料、有故事、有梗的前沿聚会。去年的线下大会早早售罄，因此今年大会的规模更大，话题更有趣，嘉宾阵容更强，包括AI大模型、Agent（智能体）、多模态、机器人、健康医疗、AI教育、投资、Web3等领域的顶级研究员、投资人和一线从业者。

此外，今年大会还将新增“全球创业挑战赛”环节，从全球数百支报名初创团队中评选六支队伍参加总决赛，获得包括**Founders Fund**（一家硅谷顶级风险投资公司）合伙人级别投资人的展示机会及当场投资意向。这是我们第二次将技术干货与故事带到线下，致力于用最好的内容让大家亲身感受前沿科技的酷炫和温度。10月5日，硅谷，期待与各位见面，从这里驶向未来。

### Nano Banana的匿名亮相与谷歌认领

在被谷歌正式认领之前，**Nano Banana**匿名登场于目前全球最火、最权威的大模型测评平台**LMArena**（Large Model Arena: 一个以社区投票为主导的AI模型竞技场，用户“盲选”更满意的结果进行匿名对战，网站根据投票对模型排名）。该平台主要通过两个模型匿名对战，用户“盲选”出更满意的结果，再根据社区投票和算法对各模型进行排名。

大约在8月中旬，**LMArena**的文生图和图片编辑榜单上突然出现了神秘的**Nano Banana**代号。在随后的几天内，它凭借极其稳定和惊艳的输出，迅速蹿升并稳坐榜首，引发了广泛关注和讨论。大家都在猜测这个神秘模型的开发者。

当关于**Nano Banana**的讨论进入白热化时，8月25日前后，包括**DeepMind**（谷歌旗下的人工智能研究公司）首席执行官**Demis Hassabis**（DeepMind的联合创始人兼CEO）在内的谷歌工程高管，开始在社交平台上“暗戳戳”地发布带有香蕉元素的帖子，逐渐揭示悬念。在**Gemini 2.5 Flash Image**被正式官宣发布前，谷歌首席执行官**Pichai**（Sundar Pichai，谷歌及其母公司Alphabet的CEO）更是连发三根香蕉，宣示了对**Nano Banana**的“主权”。上一次文生图模型如此热闹的场面，还要追溯到几个月前**GPT-4o**（OpenAI于2024年发布的旗舰多模态模型）引发的吉卜力热潮。

### Nano Banana的三大突破性能力

**Nano Banana**究竟好在哪里？我们采访了一些开发者，他们普遍表示，**Nano Banana**最大的突破在于其“一致性”能力。

开发者表示：“我觉得最惊艳的就是，它在角色的一致性上做得效果非常好，比之前的模型效果上应该是做得最好的一个。”
另一位开发者也提到：“**Nano Banana**让我觉得比较震撼的地方，就是它的一次生成成功，保持这种一致性，包括它的可编辑性是让我很惊讶的地方。”

过去，许多模型在对图片进行反复修改时，最常见的问题是“换了衣服人也变了脸”。例如，当用户想改变照片里外套的颜色时，系统可能会同时扭曲五官，这种“不够像”的小偏差使得AI难以成为可靠的创作工具。**Nano Banana**的改进之处在于，它能在多轮编辑中牢牢锁住人物或物体的核心特征，无论是调整姿势、更换服装，还是将狗狗放入新的背景中，主体都能始终保持不变。

第二个大的突破在于“多图融合”。过去，将两张完全不同的照片合成在一起，常见问题包括不同图像间的不协调、空间扭曲、细节丢失或变形，导致人物在场景中看起来像是被“贴上去的”。而**Nano Banana**在多图合成时，能够自动处理风格和逻辑一致性，使画面看上去浑然一体。

第三个亮点是“自然语言驱动的精准修改”。以前修改照片往往需要自己画蒙版或使用专业工具反复擦拭。现在，用户只需简单描述“换一个背景”、“从照片中移除整个人物”或“改变人物的姿势”等，**Nano Banana**就能在其他部分保持不变的前提下，精准执行用户的要求，将图片编辑的操作门槛几乎降到零。甚至，用户无需使用语言，随手画个简笔画也能实现修改。

此外，它还加入了多轮对话式编辑和风格混配功能。用户可以先指示模型将房间刷成薄荷绿，再添加书架、更换地毯，模型会一步步记住上下文，不会推翻之前的成果。用户甚至可以要求它将花瓣的纹理应用到鞋子上，或将蝴蝶翅膀的图案变成一条裙子，生成全新的创意风格。

安全性也得到了优先考虑。谷歌为所有**Nano Banana**生成的图片添加了可见水印，同时还有肉眼不可见的数字水印**SynthID**（Google开发的一种数字水印技术，用于识别和追溯AI生成的内容），以确保未来能识别和追溯AI作品。

### Nano Banana背后的技术与愿景

在**Nano Banana**正式发布后，其背后的**DeepMind**团队首次走到台前，讲述了这款模型的研发故事。团队介绍，**Nano Banana**最核心的突破是使用了一种叫做“交替生成”的图像生成新范式。它会将用户的复杂指令拆分成多个步骤，每一步只进行一个小调整，例如先换衣服，再改背景，然后添加宠物。这种方式可以使AI编辑器不再“失忆”性地一次性乱改，而是带着每一轮修改的“记忆”，从而保持主体的一致性。

研发人员还透露，**Nano Banana**之所以能在创意场景中表现得更自然，是因为它充分利用了**Gemini**（谷歌开发的通用多模态基础模型）的“世界知识”。**Gemini**团队与**Imagen**（Google Research开发的文生图模型系列）团队强强联合，**Gemini**团队提供了语言理解和世界知识的能力，使模型能听懂复杂的指令；而**Imagen**团队则提供了高质量图像生成和风格控制的经验。两者的结合使**Nano Banana**不仅能绘图，还能理解逻辑和语义，在“理解—创造—理解”的循环中表现全面。

对于**Nano Banana**未来的发展方向，**DeepMind**的研究员表示，他们希望**Nano Banana**不仅仅是一个“生成图片”的模型，而是能够成为一个可靠的、能够陪伴用户进行思考和创作的智能体。

研究员指出：“我认为高度的智能化是我们不断推进的方向，同时保证或增强视觉质量。”

### 性能表现与用户体验：成本低、速度快、可玩性高

根据**LMArena**的匿名测评结果以及谷歌公布的测试数据，此次的**Gemini 2.5 Flash Image**基本上全方位超越了**ChatGPT 4o**、**FLUX Kontext**（Flux AI的一个模型）、**QWEN Image Edit**（阿里通义千问的图像编辑模型）等竞争对手。其生成成本更是惊人，单张图像的生成成本仅需0.039美元，即不到3毛人民币。

那么，**Nano Banana**的真实效果真的如此出色吗？目前，普通用户可以在**Google Gemini应用程序**（谷歌Gemini模型的移动应用）、**Google AI Studio**（谷歌提供给开发者进行AI模型开发和部署的平台）中直接调用**Nano Banana**，也可以通过**Gemini API**（Gemini模型的应用程序编程接口）和**Vertex AI平台**（谷歌云提供的一套机器学习平台）使用。**Adobe**（全球知名多媒体和创意软件公司）、**Lovart**（一家AI创意工具平台）等平台也陆续宣布将其集成到创意工具中。这些渠道的开放使得普通用户、专业设计人士和开发者都能轻松访问。

尤其值得注意的是，用户不仅可以免费使用，而且与之前许多模型龟速出图不同，**Nano Banana**的生成速度非常快，输入指令后大约几秒钟就能完成出图或修改。这种便捷高效的操作让网友们“玩疯了”。

### 惊艳的“人物一致性”与多图融合演示

首先，大家几乎都对**Nano Banana**的“人物一致性效果”感到惊艳。例如，给一张普通的游客照更换背景、更换衣服，假装置身球赛现场，只需动动手指，几秒钟即可完成。影棚里的侧面照也能变成正脸证件照，发型、造型的更换更是轻轻松松。以前品牌方需要花费大量经费进行棚拍、置景、造型，现在只需输入几行文字，即可零成本出片。此外，还能用一张卡通人物图片生成各式各样的人物表情和动作，或随意改变自家宠物的毛色或品种。

我自己也忍不住上手测试。首先，尝试将家里的萨摩耶小D换个颜色，结果萨摩耶瞬间变成了藏獒。再换个品种，哈士奇的效果也不错。

接着，我上传了一张在后院抱着宝宝的照片，让**Nano Banana**将我们“瞬移”到马尔代夫、巴黎、北京故宫，看起来环游世界毫无难度。然后，我尝试让它把我怀里的宝宝变成一只猩猩宝宝。效果非常自然，在我的主体保持不变的情况下，猩猩宝宝的墨镜、表情和动作都保留了原片。

我决定增加难度，让它将我的表情从微笑变为惊讶，并将姿态从看向镜头转为惊讶地看向宝宝。结果人物的一致性依然保持得非常好，更令人惊叹的是，我侧头之后，墨镜中的反光竟然变成了沙滩的镜像，逻辑和细节处理得非常出色。

虽然谷歌目前尚未发布**Nano Banana**相关的技术报告，但亚马逊**AGI部门 Applied Scientist**（Applied Scientist: 应用科学家，负责将前沿AI研究应用于实际产品和解决方案）**张宋扬**猜测，这次**Nano Banana**一致性控制能力之所以得到很大提升，可能是在数据上投入了大量精力。

张宋扬表示：“他们有一些自己的用户数据，需要做一些数据的清洗，因为并不是所有的数据直接拿过来用就能做到想要的效果。你需要针对，比如说有些数据你需要进行一些筛选，把一些高质量数据，包括一些我觉得比较重要，比如像人脸这种比较难做的，这种你需要增加它的比例。数据的清理是一个很大的工作要做，一个是数据来源，一个是数据清理，主要是这两点。”

### 多图融合的专业应用与局限性

除了超稳定的人物一致性之外，**Nano Banana**的“多图融合”功能也已达到出神入化的程度。想要让人物跨时空会面？它生成的照片几乎能以假乱真，从人物表情到光线对焦都毫无违和感。甚至上传几个食材，它就能“做”出一道色香味俱全的菜。

我再次尝试，首先让“老冤家”**马斯克**（Elon Musk，特斯拉和SpaceX创始人）和**Altman**（Sam Altman，OpenAI CEO）“攒个局”，感觉他们聊得还不错。接着，我让他们都穿上香蕉服装，效果有点可爱。

加点难度，让它把“路人”**Pichai**（Sundar Pichai，谷歌CEO）和**扎克伯格**（Mark Zuckerberg，Meta Platforms CEO）加入合影。结果却出现了问题：马斯克变成了小扎，Pichai也不见了。虽然人物总算回来了，但这显然不是马斯克，也不是Pichai。当我向它确认最右边这位是否是Pichai时，它居然斩钉截铁地告诉我“是”！看来，它连自家老板都不认识。我尝试再次引导它纠正，但最终以失败告终。

尽管**Nano Banana**在某些复杂场景下仍存在bug，但对于一般的合影需求，其处理效果依然非常流畅和出神入化。比起娱乐和玩梗，更重要的是，多图融合能力已显现出专业化替代的潜力。例如，有网友一次性输入了包括模特照片、产品、布景元素在内的十几张图片，让**Nano Banana**进行融合设计，最终效果令人惊艳，几乎可以媲美甚至超越广告设计公司。

对于服装品牌而言，**Nano Banana**几乎可以帮助他们省去寻找模特拍摄产品图的工作。例如，让**Taylor Swift**（美国流行音乐歌手）换身西装，只需上传一张衣服的平面照片即可搞定，甚至还能调整各种姿势、光影，连续生成各种角度、各种姿势的模特上身效果。

此外，拥有“世界知识”的**Nano Banana**对抽象指令的理解能力也大幅提升。网友随手画的简笔画，它都能准确结合上传的人物图片，进行姿势改变与创意设计，例如让**Sam Altman**（OpenAI CEO）表演鞍马，一张图即可搞定。再如，在平面地图上随意画一根线，它就能展示从这根线的视角能看到的实际风景。

官方介绍中的多轮对话式编辑和风格混配能力也并未夸大。例如，将其用于室内设计和绘图渲染，或让它用不同的花朵纹理设计新衣服。即便对初次生成的样式不满意，再次调整也能得到更好的效果。

### AI图片编辑达到文字编辑水平

针对目前**Nano Banana**所展现出来的能力，**Nathan**表示，在某种程度上，我们对图片的编辑能力已经开始达到了文字的类似水平。

Nathan解释道：“它现在实现的功能相当于什么？相当于我们在文字的这个阶段，你把其中一段话划出来，说你帮我改一下，或者这段话的语义不对，你帮我重新描述一下。那现在它能够在图片做到相同文字编辑那样的效果。比如说把某个窗帘的颜色换成红色、换成蓝色，或者说你能够把这个物件给去掉，从一个图片中加这个物件。其实你看就很像我们在编辑文字式的，去在很细微、细枝末节的地方进行一定程度的优化和改变。我觉得它是真正地在多模态的这个模型中做到了单一的语言模型可以达到的效果，我觉得这一点是它们很大的一个突破。”

### 拓展：与多平台结合实现视频创作

我们以上展示的还只是**Nano Banana**能力的冰山一角。自发布以来，网友们不断挖掘出各种新鲜玩法，例如已火爆全网的“手办模型”制作。除了“手办”等静态操作，另一个爆火趋势是将**Nano Banana**的能力与其他平台结合，创作出效果惊艳的视频。

例如，有人将**Nano Banana**与**Seedance**（一个专注于视频生成和编辑的AI平台）、**Kling**（一个专注于视频生成的AI模型）结合使用，实现了让**梵高**（Vincent van Gogh，荷兰著名画家）、**蒙娜丽莎**（Mona Lisa，达芬奇的著名画作）从油画中走出来，变成真人在中央公园聊天的效果。有人使用**Nano Banana**和**Seedance**，仅用不到两小时就制作出了一个动画短片。还有人使用**Nano Banana**与**Weavy**（一个专注于3D内容创作的AI平台）制作出细节满满的3D产品介绍。

### 与竞品对比及当前局限

那么，与其它模型相比，**Nano Banana**的能力是否真的强大很多？我们自己也进行了几个简单的测试。

在同样改变照片背景的指令下：
*   **GPT-5**（OpenAI正在开发的下一代GPT模型）生成的人物几乎变了样。
*   **FLUX**生成的人物虽然被抠出，但头发少了一块，人像像是被贴上去的。

在照片融合进行合影的指令下：
*   **GPT**不仅不能一次性理解并完成指令，生成出来的照片也完全不可用，人物像被贴上去，甚至主体都变形了。
*   我们用同样的**Prompt**（提示词，用于引导AI模型生成内容的文本指令）在**GPT**上尝试创建手办，人物比例、五官、背景等细节也出现了不同程度的瑕疵。

对比使用下来，最大的感受是目前其他模型在生成速度上不仅比**Nano Banana**慢了几倍，效果和输出稳定性方面也逊色不少。

当然，**Nano Banana**目前也并非没有翻车的时候。首先，在中文能力方面，它依然没有出现质的突破，生成的图片仍存在文字乱码、乱读的现象，对编辑指令的理解和执行也会出现偏差。例如，当我要求为“手办”加上一双腿时，它直接加到了盒子上，背景图片也不翼而飞。在多轮对话编辑中，它在面对复杂指令时可能突然“凌乱”。比如，在前文对话中我尝试继续让它多图融合，生成一张马斯克、扎克伯格、皮柴一起围观我怀里猩猩宝宝的照片，结果不仅人物比例和表情极其不协调，连我这个照片主体都变成了另一个人。

此外，网友对其不满意的地方还包括图片分辨率不高、对提示词的审查过于严格、艺术性和审美上比不上**Midjourney**（一个领先的AI艺术生成工具）和**Imagen**等。但总体来说，大家对**Nano Banana**的评价主要还是以积极、正面为主，几乎都认为这是文生图领域的又一里程碑。

### 谷歌的多模态五大主线布局

如果从更长的时间维度来看**Nano Banana**，你会发现这并非谷歌的一次“偶然爆发”。事实上，在过去一年多的时间里，谷歌几乎以“密集轰炸”的节奏，将多模态产品一口气推到台前，各种模型、各种迭代甚至可以用眼花缭乱来形容。

那么，目前谷歌到底有哪些多模态产品线呢？谷歌的多模态产品目前已经形成了一个完整的矩阵，大致可以分成五条主线：

#### 1. 文生图：Imagen系列

**Imagen**系列最早可追溯到2022年5月，当时**Google Research**（谷歌的研究部门）首次提出这一文生图模型。其特点是结合大语言模型理解提示，再用**扩散模型**（Diffusion Model: 一种生成式AI模型，通过逐步去除噪声来生成高质量图像）生成图像。在当时，它被认为是超越**DALL-E 2**（OpenAI于2022年发布的文生图模型）的新一代方案。然而，由于安全和版权风险，**Imagen**最初并未开放给公众使用。直到2024年**I/O大会**（Google I/O: 谷歌年度开发者大会），谷歌才正式推出**Imagen 3**，真正走向产品化。2025年5月，**Imagen 4**发布，进一步强化了光影表现和细节质量，朝着“接近真实摄影”的方向迈进。

#### 2. 文生视频：Veo系列

2024年1月，谷歌研究院首次发布了**Lumiere**（谷歌开发的文生视频模型），采用“时空一致性扩散”的方式直接生成整段视频，使动作和背景更加连贯自然。随后在2024年5月**I/O大会**上，**Veo 1**（谷歌开发的文生视频模型系列）正式亮相，能够生成1080p高清视频。同年12月，**Veo 2**升级至4K，并首次接入**Vertex AI平台**。而在2025年5月**I/O大会**上，谷歌再次亮相**Veo 3**，不仅能生成视频，还能同步生成音乐和旁白，将文生视频真正推进到影视级创作阶段。

#### 3. 交互世界生成：Genie系列

**Genie**系列，即“交互世界生成”，也被称为“世界模型”。与文生视频模型不同，**Genie**的目标不是生成一段“看”的视频，而是直接生成一个“能玩”的虚拟世界。**Genie 1**于2024年初首次亮相，作为第一个能够根据图像生成可玩2D游戏环境的模型，展示了AI创造互动世界的潜力。紧随其后，**Genie 2**在2024年底发布，在一代基础上取得了巨大进步，能生成更长、更复杂的3D互动世界，将AI生成的虚拟环境从二维平面扩展到三维空间。最新的**Genie 3**在今年8月5日推出，能力再次提升到新高度，能够从文本或图像提示生成动态、可导航的3D世界，并首次支持实时交互和“提示性世界事件”，允许用户在生成环境中实时修改物体或天气，使其成为真正意义上的“世界模型”。换句话说，它不仅能帮助用户生成一段画面，还能让用户真正地“走进去”玩和体验。这使得**Genie**成为谷歌多模态矩阵中一个特别的分支，它不仅仅是单纯的视频生成，而是文生视频和虚拟交互的结合，预示着谷歌的多模态探索正在触碰“沉浸式体验”和“虚拟世界构建”的边界。

#### 4. 面向创作者的工具集

2024年5月，谷歌在**I/O大会**上同时推出了**ImageFX**（谷歌的文生图实验性工具）和**VideoFX**（谷歌的文生视频实验性工具），让用户可以直接在Labs中体验文生图与文生视频。2025年5月，谷歌又发布了**Flow**（一个专为影视叙事设计的工具），它将**Veo**和**Imagen**的能力整合到分镜、镜头、叙事风格的工作流中。

#### 5. 多模态底座：Gemini系列

**Gemini多模态底座**是谷歌的通用多模态基础模型，是整个系统的“大脑”。它的核心能力在于理解、推理和处理文本、图像、音频、视频等各种信息。**Gemini**扮演着通用智能体的角色，为其他更专业的模型提供强大的基础支持和世界知识。

2023年底，**Gemini 1.0**发布，确立了**Ultra、Pro、Nano**（Gemini模型家族的不同尺寸，分别针对最复杂的任务、广泛应用和设备端应用）三个不同尺寸的模型家族形态。2024年2月，**Gemini 1.5**发布，带来了革命性进步，尤其是其突破性的**长上下文窗口**（Long Context Window: 指AI模型能够一次性处理和理解的文本或数据长度，长度越长，模型能处理的信息越多），使其能够一次性处理海量的文本、代码、图像甚至视频，在理解复杂、冗长的文档或视频方面具有前所未有的能力。2025年2月，**Gemini 2.0**系列登场，推出了**Flash和Flash-Lite**（Gemini模型家族中针对低延迟和大规模部署优化的版本），以更好地应对需要低延迟和大规模部署的应用场景。2025年8月，**Gemini 2.5 Flash Image**，也就是大家熟知的**Nano Banana**，正式现身，将“AI修图”直接变成了人人可用的体验。

### 谷歌多模态战略的清晰路径与未来展望

盘点下来，谷歌的多模态战略已逐渐清晰成型：文生图的**Imagen**、文生视频的**Veo**、探索交互世界的**Genie**，再通过**Flow**、**ImageFX**、**VideoFX**将这些能力嵌入创作工作流，而背后支撑这一切的是快速迭代的**Gemini多模态底座**。

在采访中，Nathan谈到，谷歌拥有顶尖的人才、基建和数据底座，这是其在当前激烈的大模型竞争中的优势所在。

Nathan表示：“在这看似复杂、庞大的产品线背后，谷歌其实也有着比较清晰的产品路线。它每一次发布不同的模型，其背后都有一个消费场景，或者一定的**user case**（用户案例: 用户在使用产品或系统时完成特定任务的情景），其实是针对一定的**user profile**（用户画像: 对目标用户特征的描述）。因为大家在做产品的第一件事，就是要去了解你的客户人群是谁，你才会去做产品，而不是说做了产品再去找客户人群。所以在我看来，谷歌的产品主线是根据客户人群、应用场景去开发和迭代它的模型，思路其实是比较清晰的。”

与此同时，谷歌也正在渐进式地朝着“大而全”的智能体方向推进。

研究员指出：“现在很多大公司都希望做一个大而全的模型，能够支持不同的模态，是一个**ending to ending**（端到端: 指从输入到输出的整个过程都由一个系统或模型完成）的生成模型，包括语音、图片、视频、文字，甚至代码，都希望做一个大而全的，因为这应该更符合大家对智能的认知。但是这是一个很大的框架，但在这个框架之下，我们可能需要针对每一种任务去进行研究，比如先研究怎么去生成图片，再研究怎么生成视频。所以你会发现，它们的模型是不同的团队在做，它们得先把某一个任务先突破了，然后发布了一个产品，然后另外一个突破了再发一个产品。但我相信作为一个用户来说，我们肯定也是希望它能够把这些模型融合在一起，这样使用界面也更简洁。”

对于未来谷歌多模态的发展，大家目前普遍猜测，谷歌或许会把更多的模型能力向**Gemini**融合，进而面向普通用户打造一个多模态的超级流量入口。而如**Imagen**、**Veo**、**Genie**等模型，未来则将继续向纵深发展，主要为专业级的开发提供服务。

从**Nano Banana**到一整套多模态矩阵，我们看到了谷歌过去一年多的加速爆发。在这场生成式AI的竞赛里，谷歌曾被质疑掉队，但现在，无论是图像、视频，还是虚拟世界和创作工作流，谷歌几乎把所有环节都重新补齐。这种“连环拳”式的产品发布，似乎在向外界释放一个信号：谷歌不只是在追赶，而是在试图用一个完整矩阵去重新定义生成式AI的边界。

但问题是，这样的爆发能不能真正转化为市场优势？在这场速度与创新的较量中，**Nano Banana**又能领先多久呢？欢迎在评论区告诉我们，你觉得谷歌的这波多模态大爆发如何？**Nano Banana**到底好不好用？