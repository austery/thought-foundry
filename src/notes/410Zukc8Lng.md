---
author: 硅谷101播客
date: '2025-10-24'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=410Zukc8Lng
speaker: 硅谷101播客
tags:
  - 3d-digital-human
  - ai-rendering
  - real-time-interaction
  - robotics-control
  - generative-ai
title: 柴金祥教授：Sora 2之后，3D数字人的技术壁垒与商业未来
summary: 本期内容探讨了在Sora 2等文生视频模型冲击下，3D数字人技术的核心差异与挑战。柴金祥教授详细解析了从2D视频到3D实时交互的技术壁垒，分享了其团队如何通过AI渲染与解算技术，大幅降低成本并摆脱对昂贵GPU的依赖。文章追溯了3D动画与机器人研究的深厚渊源，并展望了3D数字人作为未来人机交互新范式，以及其模型反哺机器人控制的巨大潜力，探讨了从“白盒”控制到“黑盒”学习的行业范式转移。
insight: ''
draft: true
series: ''
category: technology
area: tech-insights
project:
  - ai-impact-analysis
  - systems-thinking
  - historical-insights
people:
  - 柴金祥
  - Sam Altman
  - Tilly Norwood
  - Jensen Huang
  - Jessica Hodgins
  - Marc Raibert
  - Sergey Levine
  - Karen Liu
companies_orgs:
  - 魔珐科技
  - OpenAI
  - 卡内基梅隆大学
  - NVIDIA
  - Boston Dynamics
  - Pi
  - Stanford University
  - Unreal Engine
  - Epic Games
  - 瑞芯微
  - 本田
products_models:
  - Sora 2
  - Genie3
  - ChatGPT
  - 星云平台
  - RK3566
  - VLA (视觉语言动作模型)
  - ASIMO
media_books:
  - 《硅谷101》
status: evergreen
---
### 引言：当AI成为内容生产者

近期，Sora 2的发布引起了广泛关注，它能将一句话迅速转化为一段10秒的短视频。与此同时，好莱坞完全由AI创作的演员Tilly Norwood，在诞生6个多月后就吸引了6.5万粉丝，她会发自拍、代言品牌，却从未在现实世界中真实存在过。这些现象揭示了一个趋势：数字人正在成为新的内容生产者。

然而，从屏幕上生成一段2D视频，到实现一个能够稳定、实时互动的3D数字人，这中间仍存在巨大的技术壁垒。本期嘉宾柴金祥教授，早在2000年便进入卡内基梅隆大学研究机器人。由于机器人应用落地困难，他的团队反而成为全球最早用AI进行3D动画创作的团队之一。在长达二十多年的研究生涯中，他见证了技术从机器人到好莱坞AI动画，再到如今3D数字人模型反向驱动机器人的演进。这既像一种轮回，也是一个新的开始。

本期节目将深入探讨此轮人工智能浪潮如何改变3D数字人和机器人领域，以及在好莱坞与游戏产业中，谁将成为这项技术的真正受益者。

### Sora 2 的冲击：2D 视频生成与 3D 数字人的核心差异

Sora 2相较于Sora 1在画质和视频形态上取得了显著进步，其核心能力在于**文生视频**（Text-to-Video: 指通过输入文字描述直接生成视频内容的技术）。它的视频形态主要以人为中心，可以让视频里的人完成跳舞、交流、吃东西等各种动作。

尽管Sora 2的效果令人惊艳，但它仍存在一些局限性。首先，生成的视频时长依然被限制在10秒以内。其次，物理一致性问题依然存在。许多用户在实际使用中发现，虽然一些刷屏的演示效果很好，但自己生成时仍会遇到瑕疵，例如视频背景中的文字出现乱码。

更重要的是，目前Sora 2缺乏精细化的控制和编辑能力。用户可以通过提示词（prompt）生成视频，但无法精确控制人物的动作、表情，也无法在生成后对不满意的部分进行修改。

相比之下，我们团队专注于3D数字人的研究与产品开发。我们认为，要创造一个能像真人一样交流、跳舞、娱乐的AI，最终的大模型需要结合2D和3D的训练数据。我们希望生成的数字人不仅没有时长限制，还要保证物理上的准确性，没有瑕疵，并且能够被精确控制，实现实时交互且成本低廉。

目前，我们已经开始将大量的视频数据与我们已有的3D训练数据相结合，用于训练3D数字人的大模型。Sora 2的出现，第一次让人们看到了大模型驱动人物做出各种动作的可能性，而我们正在3D的维度上将这一可能性推向更深的层次。

### 2D 与 3D 的本质区别

Sora 2本质上是文生2D视频，而3D数字人则是在三维空间中构建。一个关键区别在于，3D模型可以被放置在VR/AR环境中，像现实世界一样进行360度观察。

更重要的是，3D模型天然具备可控性。你可以像控制一个木偶一样，让它精确地执行任何动作。而在2D的像素层面，要实现对人物动作和表情的精准控制则非常困难。

当我们在展厅的屏幕上看到一个立体的数字人时，它扮演的是人机交互的载体。这种交互必须是实时的，通常要求端到端的延迟小于1.5秒，这与Sora动辄数分钟的生成时间完全不同。此外，作为服务或展示人员，数字人的动作和表情不能有任何瑕疵，比如手指突然多出一根，或者肢体衔接不上等问题。这种对物理准确性的要求远高于目前文生视频所能达到的水平。

### 3D 数字人技术的“三座大山”：成本、延迟与质量

要将3D数字人规模化部署到终端，成本是一个决定性因素。传统的文生视频技术由于处理的是非结构化的像素信息，其推理和生成过程的成本极高，无法实现商业扩展。

我们之所以能大幅降低成本，核心在于2D与3D的区别。描述一个人的动作表情，3D模型只需要几百个参数就足够了，这类似于控制人体的几百块肌肉。我们的模型从文本生成这些参数，然后传输到终端。终端利用我们开发的AI**渲染**（Rendering: 将3D模型数据转化为2D图像的过程）和AI**物理引擎/解算**（Physics Simulation: 模拟物理规律，如重力、碰撞、布料和毛发动态等，使虚拟世界更真实的技术）技术，将这些参数实时转化为视频。

这一流程中最关键的突破在于，我们用AI替代了传统依赖昂贵GPU的游戏引擎进行渲染和解算。这使得我们的成本主要集中在生成那几百个参数上，其量级与大模型生成Token类似，成本极低。目前，我们的成本甚至比主流的语音合成大模型还要低几十分之一。这项技术让我们的3D数字人可以在几百元人民币的国产芯片（如瑞芯微RK3566）上流畅运行。

在交互流程上，我们通常需要两个模型协同工作：
1.  一个类似ChatGPT的多模态输入到文本输出的大模型，负责理解和决策。
2.  我们自研的“文生3D多模态大模型”，负责将文本指令转化为包含语音、表情、动作、手势的3D多模态输出。

我们自己的模型专注于后者，同时可以与千问、DeepSeek、豆包等主流大模型无缝对接，从而形成端到端、像真人一样交流的完整体验。

### 平台的诞生：从服务公司到“星云”生态

基于上述技术突破，我们即将发布“星云平台”，将我们的文生3D多模态能力开放给所有开发者。这标志着我们从一家3D数字人内容制作公司，转型为一家平台型公司。我们希望开发者不必重复造轮子，能够轻松地将实时、低成本、高质量的3D数字人交互能力集成到他们的应用中。

### 好莱坞与游戏产业的变革：数据与算法的鸿沟

传统的3D内容生产，无论是好莱坞电影（如《阿凡达》）还是3A级游戏，都依赖于一个极其昂贵且耗时的流程。首先，通过相机阵列扫描演员，进行精细的3D建模和绑定（建立骨骼肌肉系统）。然后，演员穿上带有标记点的**动捕**（Motion Capture: 记录并处理真实物体或人的动作，并将其应用于数字模型的技术）服，由另一套相机系统捕捉其动作，再驱动数字模型。最后，通过强大的渲染引擎输出视频。整个过程成本高昂，仅制作一个高精度数字人模型的成本就可能高达10万美元。

在大模型时代，虽然这些行业都希望拥抱AI，但面临着一个核心困境：3D内容的AI化取决于高质量的3D数据和顶尖的AI算法，而这两个要素恰好分散在两个没有交集的行业中。影视和游戏公司拥有海量的美术资产和高质量的3D内容，但普遍缺乏AI研发能力；而AI公司算法能力虽强，却苦于没有高质量的3D训练数据。

我们从2018年成立之初，就为游戏、影视等B端客户提供结合AI与美术的3D内容制作服务。在这个过程中，我们积累了超过1000小时的高质量3D动画数据。这些数据制作成本极高，在国内，一秒钟高质量的人脸、手势动画数据的成本至少在1000元人民币左右。正是这些珍贵的数据，构成了我们训练大模型最核心的壁垒。

### 从机器人到动画的轮回：3D 数字人的技术溯源

我于2000年前往卡内基梅隆大学机器人研究所攻读博士，当时的博士论文课题就是如何创建一个可交互的3D数字人，并用AI制作动画。我们的团队可以说是世界上最早用AI做动画的，因为运动捕捉技术在当时刚刚出现，为AI训练提供了数据。

我的博士导师是Jessica Hodgins，她主要研究人形机器人和3D数字动画。而她的导师，正是大名鼎鼎的波士顿动力（Boston Dynamics）创始人Marc Raibert。这揭示了一条有趣的技术传承脉络。

Jessica Hodgins在博士期间研究的是单腿人形机器人，因为双足平衡在当时是极难解决的问题。她开创性地将用于控制机器人走、跑、跳的物理动力学方法，应用到了虚拟世界，成为全球第一个用物理运动控制来制作数字人动画的学者。

2000年，随着动捕数据的出现，她开始带领我们用AI方法制作动画。有趣的是，当AI动画技术成熟后，她又开始思考，这些技术是否能反过来驱动机器人（Robotics）。如今，许多顶尖的机器人专家，例如Pi的联合创始人、伯克利教授Sergey Levine，在博士期间都是研究动画的。他们将物理仿真和运动控制的方法从虚拟世界带回了物理世界。

这两个领域之所以高度相通，是因为它们都在处理3D空间中的运动驱动问题，一个在虚拟世界，一个在物理世界。动画研究相对简单，因为它不受硬件和物理环境的严格限制，因此成为了机器人研究的理想“试验田”。

### 未来的交互范式：当每个屏幕都有一个“数字人”

心理学研究表明，人与人之间的沟通，视觉信号（动作、表情）占比超过50%，而语言内容本身仅占7%。因此，未来的人机交互必然会超越目前的文本和语音模式。我们相信，未来的交互方式将是“人与数字人”的交互，就像人与人之间一样。

我们的目标，就是将具备语音、动作、表情和姿态的3D数字人带到每一个屏幕上——从展厅大屏、电视、电脑、手机、车机，到桌面的迷你全息屏。当大模型无处不在时，用户与之交互的界面，不应再是冰冷的文本框，而是一个生动、自然的数字形象。

### 殊途同归：3D 模型如何反哺机器人

我们开发的3D数字人技术能够直接用于驱动机器人。当数字人理解了对话内容，并生成了相应的语音、动作、表情和姿态参数后，这套参数同样可以驱动一个机器人做出相应的动作和手势。

目前，我们已经与一些机器人公司展开合作。我们提供的3D动作数据，可以作为机器人的训练素材。例如，我们将发布一个3D动作大模型，用户只需输入“往前走五步，趴在地上，然后爬起来跑”，模型就能自动生成相应的3D动作数据。这些数据可以极大地丰富机器人的训练集，帮助它们学习爬楼梯等复杂动作，而无需依赖昂贵的物理捕捉。

这涉及到机器人控制的两个核心概念：**运动学**（Kinematics: 研究物体运动几何性质，不考虑力）和**动力学**（Dynamics: 研究力与运动之间的关系）。我们的模型首先解决运动学问题，即规划出“应该怎么动”；机器人公司则可以此为基础，通过**强化学习**（Reinforcement Learning: 一种机器学习方法，智能体通过与环境互动，根据获得的奖励或惩罚来学习如何做出最优决策）等方式，解决动力学问题，即“需要用多大的力去完成这个动作”。

### 机器人产业的现状与未来：从“白盒”到“黑盒”的范式转移

20年前，机器人研究的核心是平衡与抓取，尤其对于双足机器人而言，防止跌倒是一个巨大的挑战。当时的研究主要依赖于工程师手动调试参数的“控制器”模式，这是一种“白盒”方法，泛化能力极差，换个环境就可能失效。

如今，机器人领域取得了巨大进步。这得益于数据的积累、强化学习等AI方法的引入，以及NVIDIA等公司提供的强大仿真环境。现在的机器人研究范式，已经从传统的“白盒”控制论，转向了端到端的“黑盒”学习模型。我们不再需要精确计算每一个动作的力，而是像人学习一样，通过大量的尝试和反馈（Reward Function）来掌握技能。

尽管如此，机器人领域仍面临巨大挑战。目前我们看到的许多演示，如机器人开可乐，仍然是在高度受控的特定场景下完成的，泛化能力非常有限。无论是平衡、抓取还是导航，要在非结构化的真实世界中做到鲁棒和泛化，依然是一条漫长的道路。

至于机器人领域何时能迎来像语言模型那样的“GPT-3时刻”，我认为还需要很长时间，或许十年内有希望。这条新的技术路径虽然前景光明，但中间必然会经历起伏和挑战。从商业化角度看，人形机器人在“白领”工作（如服务、陪伴）上的落地，可能会比在“蓝领”工作（如工业、物流）上更快。