---
title: DeepSeek V3.1与国产芯片：UE8M0 FP8技术深度解析及其行业影响
summary: 深度解析DeepSeek V3.1模型采用的UE8M0 FP8 Scale参数精度，探讨其技术逻辑、对国产芯片突围的潜在影响，以及在算力、生态和市场方面面临的挑战。文章还对寒武纪等相关概念股的投资风险进行了冷静分析。
area: tech-insights
category: technology
project:
- ai-impact-analysis
- investment-strategy
- market-cycles
tags:
- ai-chips
- cambricon-stock
- deep-learning-acceleration
- domestic-gpu
- fp8-quantization
people: []
companies_orgs: []
products_models: []
media_books: []
date: '2025-08-26'
author: 最佳拍档
speaker: 最佳拍档
draft: true
guest: ''
insight: ''
layout: post.njk
series: ''
source: https://www.youtube.com/watch?v=zTdUzh-KVbE
status: evergreen
---
### 引言：DeepSeek V3.1引爆国产算力芯片热潮

近期，国产算力芯片领域因一个词汇——**UE8M0 FP8**而备受关注。这股热潮源于DeepSeek在8月21日发布的**V3.1版本模型**（DeepSeek发布的最新大语言模型版本），其最大的亮点在于模型训练采用了UE8M0 FP8 Scale的参数精度。DeepSeek特别提到，UE8M0 FP8是“针对即将发布的下一代国产芯片设计”的。正是这句话，迅速引起了资本市场的强烈反应，A股和港股中的“国产芯片、FP8概念股”短线大涨。本文将深入探讨UE8M0 FP8背后的技术逻辑、其能否成为国产芯片突围的关键，以及它对行业可能产生的影响。

### 理解FP8：从二进制到浮点数基础

为了全面理解UE8M0 FP8这一概念，我们将它拆分为“UE8M0”和“FP8”两部分进行解释，并从计算机的基础知识讲起。

众所周知，计算机中的数据都以**二进制**（Binary: 只使用0和1两个数字，遵循“逢二进一”规则）形式存储。其中，“1”表示通电、开、有信号，“0”表示断电、关、无信号。二进制只有这两种状态，因此不容易出错，读写可靠。举个例子，数字2用二进制表示为10，数字3表示为11。在**十进制**（Decimal: 以10为基数的计数系统）中，每一位代表10的不同次方，例如321 = 3×10² + 2×10¹ + 1×10⁰。而在二进制中，每一位代表2的不同次方，例如二进制的1101 = 1×2³ + 1×2² + 0×2¹ + 1×2⁰ = 8 + 4 + 0 + 1，相当于十进制的13。需要注意的是，从右到左，每一位的权重是2的幂次方，最右边是2⁰，然后是2¹、2²、2³。

简单来说，计算机存储数字有两种基本形式：一个是**整数类型**（INT: Integer，计算机储存不带小数部分的数字的形式），一个是小数点部分，即**浮点数**（FP: Floating Point，用于表示带有小数部分的数字）。整数类型没有小数部分，例如1、2、3。**INT4**指用4位二进制数来表示一个整数，最多能表示2的4次方，即16个不同的数；如果是有符号数，范围通常是-8到7；如果是无符号数，范围则是0到15。同理，**INT8**最多能表示2的8次方，即256个数；有符号数范围是-128到127，无符号数范围是0到255。

**FP**，全称为Floating Point，即浮点数，用于表示带有小数部分的数。浮点数本质上就是把一个数字拆分成三部分：分别是符号位、指数和尾数。其中，符号位表示数值的符号，只占用1比特（0表示正数，1表示负数）；指数部分决定了数的范围；而尾数部分表示数值的底数部分，决定了数的精度和小数部分的具体值。

### 浮点数精度与量化技术

举个例子，**FP32**（Full Precision Floating Point: 一种标准的32位浮点数格式）是一种标准的32位浮点数，它有1位符号位、8位指数位和23位尾数位，可以表示从1.2x10⁻³⁸到3.4x10³⁸的范围，精度可达到小数点后6位左右。

而**FP16**（Half Precision Floating Point: 一种半精度浮点数格式）是一种半精度浮点数，它占用的空间是FP32的一半，只有16位。它有1位符号位、5位表示大小的指数位和10位表示精度的尾数位。FP16的表示范围从大约6.1x10⁻⁵到6.5x10⁴，精度相对较低，只能达到小数点后3位左右。这就好比把FP32的尺子刻度变粗一些，虽然精度降低了，但是使用起来更节省空间和时间。

而**FP8**（8-bit Floating Point: 一种新兴的低精度8位浮点数格式）是一种新兴的低精度浮点数格式，也是Nvidia、Arm、Intel联合推出的8位浮点数据格式，用来加速深度学习的训练和推理。目前已经在Nvidia Hopper和Ada Lovelace等GPU上提供了支持。在硬件中，FP8有两种不同的表示形式：**E4M3**（Exponent 4 bits, Mantissa 3 bits: 1位符号位、4位指数位、3位尾数位组成）和**E5M2**（Exponent 5 bits, Mantissa 2 bits: 1位符号位、5位指数位、2位尾数位组成）。E4M3可以存储+/-448之间的值和NaN（Not a Number），而E5M2可以存储+/-57344之间的值、NaN或者无穷大，它的好处是增加了动态范围，代价是降低了存储值的精度。相比起FP32和FP16，FP8的表示范围和精度进一步降低了，但是它的优势在于占用的存储空间更小，计算速度也更快。这就好比用一个非常简单的计数器来代替一把精确的尺子一样，虽然只能表示有限的数字，但是速度更快，更适合在资源受限的环境中使用。

我们再来讲几个概念：
*   **多精度计算**（Multi-precision Computing: 指用不同精度进行计算，在需要使用高精度计算的部分使用双精度FP64，其他部分使用半精度FP16或者单精度FP32计算）。
*   **混合精度计算**（Mixed-precision Computing: 指在单个操作中使用不同的精度级别，从而在不牺牲精度的情况下提高计算效率，减少运行所需的内存、时间和功耗）。
*   **量化精度**（Quantization Precision: 类似把一幅高精度的画变成一幅低精度的画，但是尽量让它看起来还差不多，这个“压缩”的过程就是“量化”。例如，把FP32转换成FP16或者INT8）。一般情况下，精度越低，模型的尺寸和推理内存就占用的越少。像FP32会占用4个字节，量化为INT8之后，只需要1个字节。

再比如我们经常听到的**MXFP8**（MicroScaling FP8: 一种量化精度，通过微缩放将张量分割成小数据块并为每个块计算独立缩放因子），就可以视为一种量化精度。它通过**微缩放**（MicroScaling: 一种量化技术，将张量分割成许多个小数据块，并为每个块计算一个独立的缩放因子）将张量分割成许多个小数据块，比如每32个元素一个块，并且为每个块计算一个独立的缩放因子来解决这个问题。由于每个1x32的块会共享一个缩放因子，所以每个块内的数据都能被有效缩放到FP8的可表示范围内，从而在保留精度的同时，享受到低精度计算所带来的性能优势。

### UE8M0：缩放因子的特殊格式

了解完前面这些内容，我们再来说**UE8M0**（Unsigned Exponent 8 bits Mantissa 0 bits: 无符号指数位占8位，尾数位为0位）就很好理解了。首先，UE8M0的全称是Unsigned Exponent 8 bits Mantissa 0 bits，也就是无符号指数位占8位，尾数位为0位。其中，“U”表示无符号（Unsigned），与有符号（Signed）相对应，也就是数字不带正负号。这种数据格式无法表示负数，但是可以用同样的数据长度来表示更多的正数。“E”（Exponent），即指数，也就是科学计数法的“次方”。“M”（Mantissa），即尾数，也就是科学计数法中的“头”或者“有效数字”。所以加起来就是，UE8M0是无符号，只能取0或者正数，用8位数字来表达指数，用0位数字来表达尾数（此时的尾数默认为1）。也就是说这种数字格式只能表示从2的0次方到2的255次方。

不过，我们需要区分的是，UE8M0 FP8并不是像E4M3、E5M2这样的另一种FP8格式，而是指的是**MXFP8**（MicroScaling FP8: 一种量化精度，通过微缩放将张量分割成小数据块并为每个块计算独立缩放因子）里缩放因子的格式。**MX**（MicroScaling: 开放计算项目（OCP）组织发布的一种数据格式）这是**OCP**（Open Compute Project: 开放计算项目，一个致力于开放硬件设计的组织）组织在2023年发布的一种数据格式。简单来说，它就是把一个张量按照固定的小块（比如K=32）进行切分，每个块共享一个以幂次的形式存放的“缩放因子X”，块内元素用低位宽格式（比如FP8）来存储。这样既保留了8比特的低带宽优势，又能够靠更细颗粒的定标获得更大的可用动态范围与更稳的数值。其中，MX的块尺度与元素的格式是相互独立的，所以说，UE8M0其实指的就是那个缩放因子X的格式——无符号（U）、8位指数（E8）、0位尾数（M0），也就是只有指数，没有符号和尾数。而元素的格式依然是E4M3或者E5M2的FP8。

### UE8M0的优势与国产芯片的渐进式演进

UE8M0最大的好处是它只能表示2的整数幂。因此硬件在解码的时候，只需要进行位移，不需要进行浮点乘法、规格化或者舍入操作。硬件的关键路径更短，带宽和能耗也更加友好。

通过这种方式，2025年英伟达的**Blackwell**（英伟达下一代GPU架构）把MXFP8/6/4做成了张量核的原生数据类型，在硬件里原生支持了UE8M0，从而将MXFP8训练端到端的吞吐提升到了BF16精度的2倍，性能却依然相近。其次，由于每块的缩放元数据从FP32的32比特降为了UE8M0的8比特，导致元数据部分的流量下降了75%，虽然这不是总的流量，但是依然有所利好。最重要的是，对于大多数已量产的、仍然以FP16/BF16 + INT8通路为主的国产GPU来说，它们很难做到对完整**FP8 FMA**（Fused Multiply-Add: 融合乘加指令，一种常见的硬件计算指令）的硬件栈支持。而UE8M0的位移解码 + 块级FP8存算实现难度和代价更低，更加符合国产芯片阶段性的演进路径。在带宽和容量限制方面，FP8+块缩放也能够显著降低**HBM/DDR**（High Bandwidth Memory/Double Data Rate: 高带宽内存/双倍数据速率，均为计算机内存技术）的压力，正符合国产芯片最希望看到的，用算法和格式把算力的水分挤出来的方向。

### 国产芯片FP8支持现状与DeepSeek的协同作用

根据国内媒体与机构的报道：
*   **华为昇腾910D**（华为昇腾系列AI芯片）的下一代芯片将支持FP8精度，预计第四季度送测，2026年第一季度量产。
*   深圳AI第一股云天励飞旗下的**NPU Nova500**（云天励飞旗下的神经网络处理器）也宣称实现了对FP8的硬件原生支持，是**中芯南方14nm**（中芯国际南方厂的14纳米工艺）唯一量产的推理芯片。
*   **沐曦曦云C500**/**C600**（沐曦（MetaX）研发的AI通用GPU产品）可以同时支持多精度混合算力，比如FP32、FP16、INT8等等，配备64GB HBM2e显存。今年7月发布的曦云C600也将支持FP8精度计算。
*   **燧原科技L600**（燧原科技的AI训练芯片）芯片采用了训推一体的架构，也原生支持FP8低精度。
*   **摩尔线程MUSA**（MUSA: 摩尔线程统一系统架构，其GPU的软件/硬件架构）架构宣称原生FP8张量加速，并且点名能很好支持UE8M0 FP8 Scale。
*   **芯原VIP9000 NPU**（芯原（VeriSilicon）的神经网络处理器IP）在采访中提到增加了FP8（E4M3/E5M2）的支持，强调与主流框架/工具链的易部署性。
*   股价一路暴涨的寒武纪，其旗下**MLU370-S4**、**思元590**及最新**690**系列芯片（寒武纪（Cambricon）研发的AI训练/推理芯片）均支持FP8计算，据传690已经送测，字节跳动等头部企业正在测试，可能会获得大规模算力采购订单。

截止到上周末，寒武纪市值突破5200亿元，A股排名仅次于茅台。可以说，DeepSeek V3.1这次带火的UE8M0 FP8，把训练/权重格式对齐到了MX的标准，也对齐了软件端与国产硬件的最佳工作点，从而有可能构建软硬协同的一致坐标系，降低生态的碎片化成本。

### 狂欢后的冷静：国产芯片与英伟达的差距

但是在国内芯片行业狂欢的时候，我们依然需要保持冷静。很多人会觉得，有了UE8M0 FP8（MX）格式，是不是国产芯片立刻就要赶超英伟达的GPU了呢？答案是还差得远。因为两者之间的差距往往不在格式本身，而在于算子、内核、内存与网络互联、框架与工具链生态、以及对标准细节实现的一致性等等方面。

首先从数值与算法的角度来看，标准的一致性还没有完全对齐。在OCP的规范中，MX的定义是块大小K=32，块内元素用FP8/FP6/FP4精度，每个块共享UE8M0，只编码2的幂次等等。问题是，如何取整到2的幂次这件事，不同的实现并不完全一致。像NVIDIA的MXFP8训练配方里，明确把尺度取整改为向上取整（ceil(log2)），并且给出消融实验证明，因为按照OCP v1.0建议的“向下取整”，在大规模预训练里会更加容易溢出和发散。如果国产硬件和软件仍然按照v1.0的规范来做，那么训练的稳定性就可能对不上。在FP8的格式选择上，NVIDIA的结论是权重、激活和激活梯度都用E4M3格式，这和很多FP8等于梯度用E5M2的传统经验不一样。正所谓差之毫厘，谬以千里。

其次，在算子与内核方面，如果不能做到原生支持MX，就会带来很多的隐形开销。MX需要在张量核里处理很多“每块一次”的尺度。如果在软件里频繁来处理这些缩放，就会非常昂贵。相比之下，Blackwell在硬件层面把尺度取整与量化塞进了张量核的指令路径，才把这笔开销吃掉。所以如果没有这条硬件上的捷径，想要在别家芯片上用MX，内核层面的额外读、改、写等操作就会吞掉所有的收益。此外还有转置的问题。Blackwell的MX要求“沿归约维的块数据连续”，但是训练时得前后反向传播会频繁的更换归约维，导致普通FP8的转置是重排，而MX的转置则是要“重量化”，这在没有做专门硬件和内核优化的时候，代价会非常高。另外，为了同时服务行/列两条归约轴，训练框架通常需要给每个张量保留两份的MX量化版本，这不仅占用大量的显存，也会增加数据搬运的代价。

第三，在内存与网络互联方面，**NVLink**/**NVSwitch**（英伟达（NVIDIA）用于GPU之间高速互联和数据交换的技术）的规模化优势明显。Blackwell GPU把NVLink的带宽拉到了每秒1.8 TB，并且通过NVLink Switch把72个GPU拉进一个可以保持每秒1.8 TB的NVLink域，还能跨机柜进行扩展。这直接决定了FP8/MX的带宽红利能否真正的转化成集群的吞吐量。如果替代平台只有**PCIe**（Peripheral Component Interconnect Express: 外围组件互连标准）或者传统的**以太网**（Ethernet: 局域网技术）和**InfiniBand**（InfiniBand: 高速计算机网络通信标准），通信就会相对吃紧。同样的MX/FP8算力优势，也会被**All-Reduce**（分布式计算中所有进程之间聚合数据的操作）和**张量并行**（将大型模型张量分割到多个设备上进行并行计算的技术）的通信所抵消掉。

第四，生态与通用性的缺失。不仅像**dtype**（Data Type: 数据类型，编程中指变量或数据存储的格式）这样的框架与编译工具支持仍不成熟，就连**PyTorch**（一个开源机器学习库）核心对MX的基础类型的支持也仍在推进中。没有一线框架的原生支持，通用性就会打折。此外，各家厂商对FP8的支持在细节上也并不一致。比如**AMD MI300**（AMD公司推出的一系列加速计算芯片）的文档里就明确写到MI300的FP8编码与H100不同。**Intel Gaudi**（英特尔公司推出的AI加速器）的官方虽然公开了FP8的训练和推理教程，但是并没有宣称支持MX原生的块缩放。如果再叠加MX的尺度取整差异，想要在多家硬件之间迁移同一个FP8 MX模型，可能需要进行重新的转换和校准才能够确保稳定。

### 国产芯片的渐进式演进路径

不过，即便如此，DeepSeek在模型端明确采用UE8M0的块缩放范式，对于夹缝中生存的国内芯片来说，这也是算是一种不多的求变模式。至少有机会实现一种渐进式的增量演进，先把存取与带宽的红利吃干净，再逐步把计算路径FP8化。

比如，第一步可以先在推理端实现权重采用FP8精度，激活采用BF16/FP16精度，累加采用FP32精度，从而大幅降低显存与权重的带宽，改善延迟和吞吐。

第二步再在部分训练链路实现FP8化，比如说在**GEMM**（General Matrix Multiply: 通用矩阵乘法，深度学习中的核心运算）的主干上采用MXFP8精度，而归一化和**Softmax**（一种将任意实数向量转换为概率分布的函数）等过程依然保持高精度，等训练先跑稳定了再来扩展规模。

第三步等待硬件的代际升级，从而再实现原生的MX/FP8张量核。无论如何，未来依然是任重道远。

### 关于寒武纪的投资警示

最后，为了回应观众关于寒武纪的提问，简单说三个点：
1.  它的市盈率如今已经达到了4000倍。这意味着如果要靠利润来回本，得等上四千多年。
2.  它虽然账面上开始盈利了，但是公司2025年一季度的经营现金流还是负的13个亿。这就好比一个人工资条上写着月入过万，但是实际上钱包里只剩几百块了，可能连房租都要交不起了。
3.  如果仔细看它的客户结构，会发现八成多的收入都靠一个大单子撑着的。2024年84.2%的营收都来自第四季度某个大客户一次性订单，并且前五大客户贡献了94.63%销售额。万一，我是说万一，这个大客户明年不续约了呢？

所以，对于寒武纪这样的“妖股”，建议是：可以关注，但别太投入；可以期待，但是别太当真。历史经验告诉我们，每一次A股造神的背后，都是无数散户沉重的代价。2021年，贵州茅台冲破2400元/股，市值登顶3.27万亿，“茅王”诞生，随后股价一路向下，至今股价跌幅超40%。2022年，宁德时代冲破690元/股，市值突破1.6万亿，“宁王”诞生，随后股价一度跌超60%。如今，“寒王”诞生，最终它又将何去何从呢？总而言之，股市有风险，投资需谨慎。