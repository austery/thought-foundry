---
author: 最佳拍档
date: '2025-10-15'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=roAgE4glhqM
speaker: 最佳拍档
tags:
  - t-literature-note
  - ai-agent
  - reinforcement-learning
  - implicit-world-modeling
  - self-reflection
  - imitation-learning
title: AI Agent迈向经验时代：早期经验范式助力自主学习
summary: 本文探讨AI Agent迈向经验时代。重点介绍META“早期经验”范式，通过隐式世界建模和自我反思，让Agent在无外部奖励下从自身试错中学习。该方法有效解决了模仿学习局限，并为强化学习提供优质初始化，助力Agent自主提升。
insight: ''
draft: true
series: ''
category: ''
area: ''
project: ''
status: evergreen
---
### 经验时代的到来与挑战

图灵奖得主Richard Sutton与谷歌RL大佬David Silver合作撰写的文章《欢迎来到经验时代（Welcome to the Era of Experience）》，引发了广泛关注。文章指出，人类数据已接近极限，**AI Agent**（人工智能智能体: 能够感知环境、做出决策并执行行动的人工智能实体）若想突破现有瓶颈，必须像人类和动物一样，通过与环境的持续互动生成经验流，并通过**强化学习**（Reinforcement Learning, RL: 一种机器学习方法，通过与环境互动，根据奖励和惩罚来学习最优行为策略）实现自主提升，预示着AI Agent将迎来经验时代，这是一个重大的范式转变。

### 当前AI Agent训练的局限性

然而，在许多环境中，基于经验数据使用强化学习训练Agent仍面临诸多挑战。一方面，这些环境常缺乏可验证或密集的奖励信号，尤其是在开放式场景中，如大多数网页环境通常不会返回明确的任务反馈。另一方面，Agent可能需要在长时间跨度内进行低效的探索与泛化，例如多轮工具使用或复杂交互流程。目前，大多数语言Agent采用**监督微调**（Supervised Fine-Tuning, SFT: 一种机器学习技术，使用带有标签的专家数据对预训练模型进行进一步训练，使其适应特定任务），从专家示范中学习，以避免过度依赖奖励信号。尽管此方法训练效率高，但其缺乏环境交互，无法从失败中学习或主动探索，同时对高质量专家数据的依赖过强、成本过高，泛化性也有限。因此，一个关键问题浮出水面：如何在没有外部奖励的情况下，让Agent从自身的经验中学习成长？

### 早期经验范式的核心思想

针对上述问题，META超级智能实验室MSL、FAIR和俄亥俄州立大学的研究团队提出了一种名为“早期经验”的中间路线。其核心思想是让Agent在训练时，既从人类专家数据中学习，也从自身的试错中学习。具体而言，Agent在环境中提出替代行动，收集这些行动带来的**未来状态**（Future States: 执行某个行动后环境将达到的状态），并将其直接转化为监督信号，无需等待外部奖励，也无需完全依赖专家。Agent自身行动的后果，即是最好的“老师”。

### 理论基础：马尔可夫决策过程

为了理解这一范式，研究团队将语言Agent的决策问题形式化为一个**马尔可夫决策过程**（Markov Decision Process, MDP: 决策理论中的数学框架，用于对序列决策问题进行建模，其中结果部分随机且受决策者控制）。一个MDP可用一个元组 M=(S, A, T, R, γ, ρ₀) 来定义。
*   **S**表示**状态空间**（State Space: 在马尔可夫决策过程中，所有可能的状态集合）。在语言Agent场景中，S是Agent能感知到的环境信息，如网页内容、工具输出、环境的文本描述。
*   **A**表示**行动空间**（Action Space: 在马尔可夫决策过程中，所有可能的行动集合）。在语言Agent场景中，A是Agent能做的离散行动，如点击网页按钮、调用搜索工具、生成文本回复等。
*   **T**表示**转移函数**（Transition Function: 描述在特定状态下执行特定行动后，转移到下一个状态的概率），它描述在状态s下执行行动a，转移到下一个状态s'的概率。例如，点击提交按钮后，页面可能跳转到成功页面，也可能会停留在原来的页面。
*   **R**是**奖励函数**（Reward Function: 定义在特定状态下执行特定行动后，Agent获得的即时奖励），但在许多场景下R是未知或不可验证的，这也是早期经验要解决的问题。
*   **γ**是**折扣因子**（Discount Factor, γ: 用于衡量未来奖励相对于当前奖励价值的参数，数值介于0到1之间），用于权衡当前奖励和未来奖励的重要性。
*   **ρ₀**是**初始状态分布**（Initial State Distribution, ρ₀: 描述Agent开始任务时环境可能处于的各种状态及其概率），即Agent刚开始任务时环境可能处于的状态集合。
Agent的核心是**策略**（Policy, π_θ: 定义了Agent在给定状态下选择行动的规则或概率分布），用π_θ表示，其中θ是模型参数。其作用是将状态s映射到行动a的概率分布。例如，在网页购物场景中，给定当前页面显示红色衬衫、预算20美元这个状态，策略会给出点击蓝色衬衫、点击价格筛选等行动的概率，这便是Agent的决策逻辑。

### 传统模仿学习的问题

在无奖励场景下，传统模仿学习的训练目标是最小化**模仿损失**（Imitation Loss: 衡量Agent模仿专家行为能力的损失函数）。这里的(s_i, a_i)来自专家数据集D_expert，即人类专家在状态s_i下执行的正确行动a_i。这个损失函数旨在让Agent在状态s_i下尽可能选择专家行动a_i。然而，它存在两个致命问题：
1.  **分布偏移**（Distribution Shift: 训练数据和实际部署数据分布不一致的现象，可能导致模型性能下降）。Agent在实际部署时，其策略π_θ必然会与专家策略存在偏差。例如，专家从未点击无效按钮，但Agent可能会。这导致Agent在遇到训练数据中没有的状态时，错误会不断累积。罗斯等人在2011年的研究指出，这种偏移会使模仿学习的性能越用越差。
2.  缺乏行动后果的认知。Agent只见过专家行动和专家未来状态的配对，从未见过自己行动和自己未来状态的结果。它不知道如果点错按钮会出现错误提示，也不知道填错日期会导致表单提交失败。因此，遇到错误时无法修正，更无法理解专家选择某个行动而非其他行动的原因。罗斯和巴涅尔在2010年的研究强调，这种后果正是模仿学习泛化能力弱的核心原因。

### 构建D_rollout数据集：行动与后果的经验

早期经验范式旨在解决上述两个问题。其第一步是构建D_rollout数据集，让Agent获得从行动到后果的经验。具体流程如下：
1.  从专家数据集D_expert中取出每个状态s_i。
2.  让Agent的初始策略π_θ生成一个候选行动集A_i，其中包含K个替代行动，并包含专家行动a_i进行对比。这里的K是**超参数**（Hyperparameter: 在模型训练前设定的参数，而不是通过训练数据学习的参数，例如学习率、批次大小等），例如K=4意味着每个状态会生成4个替代行动。
3.  执行这些行动以收集未来状态。执行专家行动a_i会得到专家的未来状态s_{i+1}；执行每个替代行动a_i^j会根据环境的转移函数T(s_i, a_i^j)得到对应的未来状态s_i^j。这些未来状态是具体的环境反馈，例如点击错误按钮后出现的无效操作文本，或填错日期后页面显示的“请输入正确日期格式”的提示，这些都是Agent能感知到的后果。
4.  将这些状态、替代行动、未来状态的三元组收集起来，构成D_rollout数据集。该数据集的关键在于它不需要任何外部奖励，未来状态本身就包含了行动质量的隐含信息。错误提示意味着行动无效，页面正常跳转意味着行动有效，这些信息足以供Agent学习。

### 训练策略一：隐式世界建模 (IWM)

有了rollout数据集，研究团队提出了两种具体的训练策略，它们如同Agent学习的两条腿：一条负责感知环境规律，一条负责思考行动合理性。
第一条腿是**隐式世界建模**（Implicit World Modeling, IWM: 一种让Agent通过预测未来状态来内化环境动态的学习方法，无需单独构建模拟器）。其核心思路是让Agent将学习环境动态视为一个辅助的预测任务，通过预测下一个状态，在自己的策略中内化环境的运行规律，而非单独建立外部模拟器。由于语言Agent的状态均以自然语言表示（如网页内容、工具输出、环境反馈），预测下一个状态可直接转化为语言模型最擅长的预测下一个token的任务。例如，在网页订机票场景中，Agent输入“当前状态，机票查询页面，出发地北京，目的地上海；行动，输入无效日期2025-02-30”，那么下一个状态就是页面显示的“日期无效，请选择正确日期”。隐式世界建模就是让Agent学习从状态+行动的组合中预测出这段下一个状态的文本。
其**损失函数**（Loss Function: 衡量模型预测值与真实值之间差异的函数，训练目标是最小化此函数）定义为：`L_IWM = - Σ log p_θ(s_i^j | s_i, a_i^j)`。这里的p_θ是语言模型的输出分布，意味着让Agent在给定s_i和a_i^j的情况下，尽可能准确地预测出s_i^j。特别重要的一点是，用于预测下一个状态的模型参数θ与Agent执行行动时的策略参数θ是同一个，这意味着Agent在学习环境如何反应的同时，也在优化自己如何行动，两者深度绑定，无需额外训练一个独立的世界模型模拟器。
在实际训练时，研究团队采用了两阶段流水线：第一阶段，用IWM的损失函数训练一个周期，让Agent先摸清环境的“脾气”，例如点击提交按钮可能出现哪几种反馈，输入错误信息会得到什么提示；第二阶段，再用专家数据集D_expert做监督微调，即最小化模仿学习的损失。这样做的好处是，Agent在学习专家行动前，已对环境有了基础认知，能够理解专家选择该行动是因为它会带来好的下个状态，而非简单死记硬背行动配对。此外，rollout数据集的规模通常会比专家数据集大一个数量级，能够给Agent提供更丰富的试错经验。

### 训练策略二：自我反思 (SR)

如果说隐式世界建模是让Agent感知环境规律，那么**自我反思**（Self-Reflection, SR: Agent通过对比自身行动后果与专家行动后果，生成解释并优化策略的学习方法）就是让Agent思考行动的合理性。通过对比自己的替代行动和专家行动的后果，生成为什么专家行动更好的解释，再用这些解释来优化策略。其实现流程比隐式世界建模多了一步生成解释：
1.  从专家状态s_i出发，执行专家行动a_i，得到专家下一步状态s_{i+1}；执行替代行动a_i^j，得到替代状态s_i^j。这一步与隐式世界建模相同。
2.  核心步骤是生成反思。研究团队使用大语言模型生成一段**思维链**（Chain-of-Thought, CoT: 一种提示技术，引导大型语言模型逐步推理，生成中间步骤的解释）c_i^j，解释为什么专家行动a_i比替代行动a_i^j更优，解释的依据是s_{i+1}和s_i^j的差异。研究团队设计了严格的提示词模板，要求解释必须包含任务目标、分析行动、对比专家行动合理性以及论证约束条件强调四个部分，以确保反思的逻辑性和实用性。
3.  构建自我反思数据集D_refl，将状态s_i、替代行动a_i^j以及反思c_i^j的三元组收集起来，形成D_refl。
4.  训练策略。Agent需要在给定s_i的情况下，同时预测出反思c_i^j和专家行动a_i。对应的损失函数是：`L_SR = - Σ log p_θ(c_i^j | s_i, a_i^j) - Σ log p_θ(a_i | s_i, c_i^j)`。这里的p_θ是语言模型的输出分布，意味着让Agent在看到s_i时，先想清楚为什么专家行动更好，再做出专家行动。
在实际训练中，研究团队会将D_refl和专家数据集D_expert混合起来。如果专家数据中本身就有专家写的行动解释，也会保留下来一起训练。这样做能够平衡专家示范的准确性和自我反思的**泛化能力**（Generalization Ability: 模型在未见过的数据上表现良好的能力），让Agent既不会偏离专家路线，又能理解行动背后的逻辑。

### 实验验证：有效性、泛化性与RL兼容性

研究团队进行了一套全面的实验，覆盖8个环境、3种模型，从有效性、泛化性、**RL兼容性**（RL Compatibility: 指某种方法或模型能够与强化学习算法有效结合的能力）三个维度验证了早期经验的价值。
为了确保结果的可靠性，研究团队选择了8个差异极大的场景，包括具身家庭环境ALFWorld、科学实验室环境ScienceWorld、长序列旅行规划环境TravelPlanner、开放域多跳问答SearchQA、多轮工具调用环境BFCLv3、客户服务场景Tau-Bench、电商购物场景WebShop以及多领域网页导航WebArena-Lite。模型方面，研究团队选择了3种不同规模、不同家族的指令微调模型，以验证方法的模型无关性。训练和评估流程也经过严格控制，首先为模仿学习基线找到每个环境的最优训练步数，同时为早期经验的两种策略使用与基线完全相同的总步数，最后在评估时使用每个环境的原生指标，遵循官方评估标准，确保结果的可比较性。

实验结果显示：
1.  **有效性：** 早期经验在领域内任务上的表现。两种策略在所有8个环境中都显著优于模仿学习基线，平均提升+9.6%。其中WebShop环境的隐式世界建模提升最明显，TravelPlanner环境的自我反思提升最明显（+15.0%）。
2.  **领域泛化能力：** 早期经验在分布外（OOD）数据上的表现。针对ALFWorld、BFCLv3、SearchQA三个环境设计了相应的分布外场景。结果显示，虽然所有方法在分布外场景下性能都会下降，但早期经验能够挽回更多的损失。更关键的是，部分场景下的分布外增益超过了领域内得分，这说明早期经验的自身试错能够覆盖专家数据未包含的场景，泛化性更强。
3.  **RL兼容性：** 验证早期经验模型能否作为RL的优质初始化，进一步提升性能。研究团队选择了三个有可验证奖励的环境，采用主流的**GRPO算法**（Generalized Proximal Policy Optimization, GRPO: 一种强化学习算法，属于策略梯度方法，用于优化Agent的策略）进行RL训练，对比模仿学习初始化和早期经验初始化的效果。结果明确，早期经验初始化的RL模型最终性能远高于模仿学习初始化，且早期经验的优势会持续到RL训练结束，全程保持差距甚至扩大。这说明早期经验不仅能自己提升性能，还能为后续的RL打牢基础，是连接模仿学习和强化学习的桥梁。

### 消融实验与基线对比

除了上述核心维度，研究团队还进行了三个关键的**消融实验**（Ablation Study: 一种实验方法，通过移除或禁用模型或系统的某个组件来评估该组件的贡献），进一步验证了早期经验的价值。
1.  **专家数据量的影响：** 在WebShop和ALFWorld中减少专家数据量。结果显示，WebShop中只用1/8的专家数据就超过了使用100%专家数据的模仿学习；同样在ALFWorld中，只用1/2的专家数据就超过了使用100%专家数据的模仿学习。这说明早期经验能用更少的专家数据达到更好的效果，大幅降低数据成本。
2.  **分支因子K的影响：** 测试了K=1、2、4、8四种替代行动数量的情况。结果显示，隐式世界建模的性能会随K增大而稳步提升，因为更多替代行动能让Agent学习更全面的环境动态；而自我反思的性能在K=2-4时最优，K=8时略有下降，因为太多替代行动会让对比解释变得复杂，Agent难以聚焦核心差异。
3.  **模型规模的扩展：** 使用Llama-3.3-70B模型在WebArena-Lite环境进行测试，验证模型从3B升级到70B时早期经验的有效性。结果显示，隐式世界建模和自我反思的成功率均高于模仿学习，而且即使采用了**LoRA**（Low-Rank Adaptation: 一种参数高效的微调技术，通过在预训练模型中注入小的可训练矩阵来适应新任务，同时保持大部分原始模型参数不变），早期经验依然能带来稳定的增益，说明它在大模型、有限计算资源下也适用。

研究团队还对比了早期经验与两种常见的基线方法：
1.  **长思维链**（Long CoT: 一种通过生成更长的逐步推理过程来提升大语言模型决策能力的技术）：结果显示，Long CoT只能让模仿学习的性能提升一小部分，且在复杂环境中会适得其反，因为长思维链可能导致行动偏离专家路线。而早期经验的提升幅度较大，且无性能下降风险。
2.  **STaR风格数据**（Self-Taught Reasoner, STaR-style data: 一种通过让模型生成专家行动的解释来辅助学习的方法，通常不依赖替代行动或未来状态的对比）：结果显示，STaR风格数据的解释匹配率很低，且解释不够“接地气”，导致模仿学习的性能下降。而早期经验的解释基于真实的行动后果，匹配率更高，能够稳定提升性能。

### 总结：早期经验的价值、局限与未来方向

**早期经验范式**的核心价值有三点：
1.  **解决了无奖励学习难题：** 直接以行动-未来状态为监督信号，无需外部奖励，可适配多数现实环境。
2.  **降低了数据依赖：** 用更少的专家数据就能达到甚至超过全量模仿学习的性能，大幅降低了数据成本。
3.  **连接了两大范式：** 既是模仿学习的增强版，又是强化学习的优质初始化，为大语言Agent从人类数据时代过渡到经验时代提供了可行路径。

当然，目前的研究也存在一定的局限性。首先，当前方法主要针对短交互序列，如单步点击、单轮工具调用。对于几十步的长序列任务，如何分配早期经验的学习权重仍需进一步研究。其次，如果环境存在危险行动，如删除重要文件，Agent的试错可能带来风险。如何在安全约束下生成替代行动也是未来需要解决的问题。

对于未来的研究方向，研究团队给出了一些建议，包括：
*   **结合自监督目标：** 将行动一致性、预测状态相似度计算等自监督任务融入早期经验，进一步提升环境的感知能力。
*   **跨环境迁移：** 让Agent在A环境学到的早期经验能够迁移到B环境，减少重复训练。
*   **大规模真实世界部署：** 通过在真实产品中收集自然的交互数据，用早期经验实现持续学习，从而让Agent越用越好。

这篇论文让我们看到，Agent在迈向自主学习的目标上更近了一步。未来的AI助手也许无需人类一遍遍地教导，就能通过自身的试错和反思变得更加聪明。或许，Sutton等人提出的经验时代真的会在某个时间点到来。