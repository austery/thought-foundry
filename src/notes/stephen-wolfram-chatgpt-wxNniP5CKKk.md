---
author: 北美王路飞
date: '2025-09-11'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=wxNniP5CKKk
speaker: 北美王路飞
tags:
  - t-literature-note
  - chatgpt
  - stephen-wolfram
  - artificial-intelligence
  - neural-networks
  - computational-irreducibility
title: Stephen Wolfram拆解ChatGPT本质：幻觉智能与文字接龙的奥秘
summary: 基于Stephen Wolfram的著作，本文深入剖析ChatGPT的运作机制，揭示其“智能”实为强大文字接龙的幻觉。探讨AI成功要素、神经网络原理、词嵌入与注意力机制，并提出结合符号计算AI以克服其局限性。
insight: ''
draft: true
series: ''
category: ''
area: ''
project: ''
status: evergreen
---
### ChatGPT本质的深度解析

本期节目将深入探讨**ChatGPT**（Generative Pre-trained Transformer: 一种基于深度学习的语言模型）的本质。内容基于Stephen Wolfram于2023年发表的著作《ChatGPT在做什么以及它为什么能够成功》。主持人此前也曾制作过关于此书的节目，但因流量较低而未能广泛传播。现在重新制作，希望能为听众带来更深刻的理解。

近期关于Gary Marcus批评ChatGPT的视频引发了许多讨论，有观众认为Gary Marcus不懂AI，即便他曾创办AI公司并被Uber收购。主持人呼吁大家对不同观点保持宽容和开放的心态，因为深入了解后可能会发现其观点具有一定道理。例如，主持人曾对Peter Thiel持有刻板印象，但在深入了解后发现其独特之处，并计划制作系列节目分享新视角。

Stephen Wolfram对GPT的看法与Gary Marcus有相似之处，他认为我们所感受到的智能、理解和创造力，实际上是一种彻头彻尾的幻觉。我们将直接进入主题。

### 幻觉智能：ChatGPT的文字接龙本质

在Stephen Wolfram看来，ChatGPT能够写诗、写代码、谈天说地，通过各种考试，看似无所不知，但其本质上只是一个无比强大的**文字接龙工具**。Stephen Wolfram在他的书中对GPT内核的神经网络做了非常深入的解释。他认为，我们人类感知到的智能、理解和创造力，是一种彻头彻尾的幻觉。我们总觉得它能做到这一切背后，一定有一个复杂的、类似于人脑的思考过程。然而，Stephen Wolfram指出，ChatGPT生成每一个词，都是在做一个极其简单的数学题：根据前面内容，下一个词最大概率是什么。接下来的节目将借助Stephen Wolfram的这本小册子，拆解ChatGPT的底层逻辑。

### 介绍Stephen Wolfram：一位跨界通才的视角

在介绍这本书之前，必须隆重介绍作者**Stephen Wolfram**（史蒂芬·沃尔夫勒姆: 英国裔美国数学家、物理学家、计算机科学家、商业家）。他绝非普通的程序员或科技评论家，而是一位跨越物理学、数学、计算机科学与哲学的通才。他之所以能写出如此透彻解析GPT的小册子，是因为他过去40多年的人生几乎都在为理解复杂系统做准备。

他的履历令人惊叹：
*   15岁时，发表了第一篇关于粒子物理的科研论文。
*   20岁时，获得了加州理工学院的理论物理学博士学位，并荣获麦克阿瑟天才奖（MacArthur Fellowship: 每年授予20至30位在各自领域内有突出贡献的美国公民的奖项），成为那个时代最耀眼的学术明星之一。
*   早在1983年，他就亲手编写了一个神经网络程序，那时Windows操作系统都尚未诞生。这表明他对这项技术的理解拥有长达40年的历史纵深。
*   他人生最重要的项目是花费40多年时间，几乎以一己之力打造了一门全新的语言，名为**Wolfram Language**（沃尔夫勒姆语言: 一种多范式编程语言，设计用于计算科学和数学），也是**Mathematica**（数学计算软件: 基于Wolfram Language的综合性技术计算系统）背后的语言。Mathematica在量子物理等课程中常用于解偏微分方程，它拥有强大的计算引擎。
*   他的野心在于创造一种能够精确描述和计算世界万物的符号语言，从程序、化学分子到金融数据，都希望用统一的可计算框架去表示。
*   他还是知识引擎**Wolfram Alpha**（沃尔夫勒姆阿尔法: 基于Wolfram Language的计算知识引擎，能直接回答问题而非提供网页链接）的发明者。Siri的部分技术也间接使用了他的成果。Wolfram Alpha代表了与ChatGPT完全不同的另一条AI路线。
*   他一生出版了近300万字的著作，过去30年写了1500万字的邮件，总共打了约5000万字，连自己写了多少邮件都统计得清清楚楚，这展现了他对万物量化和记录的极致追求。

当这样一位人物来解读ChatGPT时，他的视角是极其独特的。他既是亲手实践过神经网络的圈内人，又是创造了另一条技术路线的旁观者。他能从最底层的代码和数学原理出发，又能够上升到计算物理以及宇宙的哲学高度。他并非猜测ChatGPT在做什么，而是用自己构建了半辈子的庞大计算和知识理论体系，去框定和解释ChatGPT这个新物种。因此，这本小册子非常值得推荐。

### AI技术“核爆”的深层原因

ChatGPT背后的核心技术——神经网络，其实一点都不新。Stephen Wolfram提到，早在1983年他就亲手编写过神经网络程序，但当时的电脑速度缓慢，程序也无法实现任何有趣的功能。这引出了一个疑问：一个40年前的技术，为何在今天“核爆”？

这就像做菜一样，光有食谱（即神经网络的理论）是没用的，还需要顶级的食材和给力的厨具。在过去40年间，发生了三件大事，才凑齐了AI这盘“满汉全席”：
1.  **算力爆炸：** 今天的电脑比40年前快了何止100万倍，过去需要跑几年的计算，现在可能几秒钟就能完成，这为训练一个巨大无比的神经网络提供了可能。
2.  **数据海洋：** 互联网的诞生带来了数十亿计的网页、数字化图书和各种文本资料，相当于为AI提供了几乎无限的“食材”，去学习和品尝人类语言的各种“味道”。
3.  **工程上的各种创新：** 一系列算法和架构上的创新，让顶尖的“大厨们”琢磨出各种烹饪技巧，能让同样的食材做出米其林三星级的“味道”。

因此，ChatGPT的成功并非凭空冒出的魔法，而是一场酝酿了40年的技术风暴。当一个比Stephen Wolfram当年那台大了十几亿倍的神经网络，被喂进了整个互联网的知识之后，一个奇迹发生了：它真的学会了像人一样说话。

然而，正是这种“像人一样说话”恰恰是让我们最困惑的地方。我们人类说话背后是意义在驱动，我们想表达一个意思，然后组织语言说出来。而机器呢，它懂得什么是意义吗？这就是我们要解开的核心谜题。ChatGPT这个“黑箱”到底是怎么运转的，它生成那些看似充满意义的文字，内部发生了什么？它为什么能够做到，而且做得那么好？Wolfram的这本书就像一把手术刀，精准地剖开了这个黑箱。他没有用太多花哨的术语，而是用近乎**第一性原理**（First Principles: 指从最基本、最原始的假设出发，而不是依赖于现有的知识或观念进行推导和解决问题）的方式，带我们从最基本的单元开始，一步步搭建对ChatGPT的理解。

### 深入ChatGPT大脑：文字接龙与概率预测

现在正式进入ChatGPT的大脑。记住开头的“暴论”：它是在玩文字接龙。

举个例子：假设我们输入“AI最好的地方在于它能力去”，接下来ChatGPT要做的不是去思考AI到底有什么能力，而是去它的“记忆库”中搜索。这个记忆库是它读过数十亿人类撰写的网页和书籍。它会统计在人类语言中，当出现“AI最好的地方在于它的能力去”这句话之后，最可能出现的词是什么。它可能出现一个概率列表，例如“学习”的概率可能是最高的，然后是“预测”、“创造”或“理解”。但它并没有真正理解，它只是在做统计和预测。

接下来怎么选择呢？最简单的想法是每次都选择频率最高的词。但是，Wolfram称之为“一点巫术的开始”。如果每次都选择最“正确”的答案，写出来的文章会非常无聊、死板，甚至不断重复。因此，为了让文章显得更有趣、更有创造力，ChatGPT会引入一个随机性，它偶尔会选择一个概率没那么高的词。这种随机性的程度由一个叫做**温度**（Temperature: 在语言模型中控制输出随机性的参数，值越高，输出越随机）的参数来控制。温度越高，它越可能“不走寻常路”，选出一些更意外的词。这就是为什么你用同一个问题问它好几次，都会得到不同答案的原因。

所以现在明白了：一篇洋洋洒洒的千字长文，在ChatGPT看来，就是不断重复一个动作——“根据我已经写出来的所有文字，下一个最合理的词是什么？”然后加上一个词，再重复，又是一个词。就是这么简单粗暴。

### 从统计到模型：语言依赖的挑战

听到这里，你肯定会问：这些概率是从哪里来的呢？难道是硬生生统计所有的词语组合吗？

我们先从一个更简单的模型说起。如果我们不用ChatGPT这么复杂的模型，只是用最原始的统计方法，能生成像样的句子吗？
*   例如，我们统计英语单个单词出现的频率，然后像抽奖一样随机往外蹦词，结果肯定是一堆胡言乱语，比如“程序过度被研究”，这根本就无法看懂。
*   然后我们再升级一下，统计两个词连在一起的概率，这叫做**2-gram**（二元语法: 一种统计语言模型，通过统计相邻两个词的出现频率来预测下一个词）。例如，"cat"后面很有可能跟的是"is"或"that"。这样生成的句子会稍微通顺一点，但依然是胡说八道。

问题在于，语言的依赖关系是长程的。一句话的开头可能会影响到十几个甚至几十个词的结尾。要统计所有20个词组合的概率，这个数字比宇宙中的粒子总数还要多，历史上所写过的文字加起来，总共一个零头都凑不够。因此，靠死记硬背的统计是行不通的。

那怎么办呢？这就得引出故事里最关键的主角——**模型**。简单来说，模型就是这种举一反三的工具。我们不需要看过所有的句子，我们只需要给它喂足够多的例子，让它自己去总结出一套生成概率的规则。这样，哪怕遇到一个它从来没有见过的句子，根据这套规则，也能够估算出下一个词的概率。ChatGPT用的就是这种模型，也就是我们前面所提到的**神经网络**（Neural Network: 模仿生物神经元之间连接方式的计算模型，通过学习数据中的模式来解决问题）。

### 神经网络的运作机制

神经网络这个词听起来很高大上，但拆开来看，其实没那么玄乎。你可以把它看作一个超级复杂的函数，我们都知道函数，给一个输入X，它给出一个输出Y。神经网络也是这样，只不过它的输入和输出都超级复杂。

我们以一个简单的例子——识别手写数字来理解一下。你看到一张手写着数字“2”的图片，对于电脑来说，它就是一堆像素点，每个点有一个灰度值。我们可以把所有这些像素点的灰度值拉长成一长串的数字，作为神经网络的输入。然后，这串数字就在网络里开始它的“奇幻漂流”了。

这个网络有很多层的人工神经元组成。每一层神经元都会对上一层传过来的数字进行一次**加权求和**（Weighted Sum: 将每个输入乘以一个权重后再相加），然后，再套上一个简单的**激活函数**（Activation Function: 在神经网络中引入非线性，决定神经元是否被激活以及其输出强度），比如说小于0就归0，大于0就不变。算出来的结果再传递给下一层。就这样一层层地传达下去，像多米诺骨牌一样。最后在输出层，大概有10个神经元，分别代表了数字0-9。经过计算之后，哪个神经元输出的数值最大，网络就会认为那张图片就是哪个数字。比如在这里，代表“2”的那个神经元的数值会是最高的。是不是很神奇？一个由成千上万极其简单的数学运算组成的庞大系统，竟然能够识别这么智能的任务。

那么，这些神经元之间的连接权重（就是那个加权求和的权）是怎么来的呢？是天上掉下来的吗？当然不是，是**训练**（Training: 神经网络通过学习大量数据来调整其内部参数，以达到特定任务目标的过程）出来的，或者说，是**调教**出来的。我们给它看过成千上万张已经标注好答案的图片，比如说给它一张“2”的图片，如果它识别错了，我们就告诉它错了，正确答案是“2”。然后用一个**反向传播算法**（Backpropagation: 一种用于训练神经网络的算法，通过计算损失函数对权重的梯度来调整权重）微调网络里成千上万个权重，那下次再看到这张图时，犯错的可能性就会小一点点。

这个过程，Wolfram打了个比方：在一个有着无数山谷和山峰的复杂地形上找最低点。每一次调整权重，相当于我们朝这个山坡最陡峭的方向挪一小步，也就是所谓的**梯度下降**（Gradient Descent: 一种优化算法，通过迭代地沿着损失函数梯度的反方向移动来找到函数的局部最小值）。经过几万次、几亿次的挪动，我们最后就能找到一个足够低的山谷，这时候网络的权重基本上就调教好了，它学会了识别数字。

### 词嵌入：连接文字与数字的桥梁

我们知道了神经网络，也知道它怎么被训练的，但有一个关键问题还没解决：怎么把文字这个东西喂给只懂数字的神经网络呢？总不能简单地给“猫”编号为1，“狗”编号为2吧？这样机器没有办法完全理解“猫”和“狗”在意义上比“猫”和“椅子”更接近。

于是，一个天才般的想法诞生了，叫做**词嵌入**（Word Embedding: 将词语映射到低维向量空间的技术，使得语义相似的词语在空间中距离更近）。简单来说，我们不要用一个数字来代表一个词，而是用一个**向量**（Vector: 在数学中，表示大小和方向的量，在词嵌入中指代表词语语义的多维数值）——也就是一长串数字来表示一个词。这个词可以看作是在一个几百甚至上千维度（我们无法想象）的意义空间的坐标。

这个坐标怎么算出来的呢？还是靠训练。比如说，一个神经网络的任务是预测句子中间的词。经过海量文本的训练，网络会发现“鳄鱼”（alligator）和“短吻鳄”（crocodile）在出现的上下文环境总是很相似，于是它就会在那个高维度的意义空间里，把这两个词的坐标放得特别近。而“萝卜”和“老鹰”这两个词的上下文天差地别，它们的坐标就会离得特别远。

你猜怎么着？当这个意义空间构建好之后，一些奇妙的事情就发生了。人们发现，在这个空间里，词语之间的关系竟然可以用向量的加减法来表示。最经典的例子就是：国王的向量减去男人的向量，加上女人的向量，等于王后的向量。这就很神奇了，这意味着在神经网络的**无监督学习**（Unsupervised Learning: 一种机器学习范式，模型在没有标签的数据中发现模式和结构）中，自己领悟了人类语言中蕴含的复杂语义关系，并且把它们编码到了这个高维空间。

现在，我们就可以把整个ChatGPT的工作流程串起来了。它拿到你输入的文字，先把每个词或者词根转化为它在意义空间中的坐标向量。然后，这个巨大的神经网络（也就是**Transformer**）就开始对这些向量进行一些极其复杂的计算，最后预测出下一个最可能的词的坐标向量，然后再把这个向量翻译成我们能够看懂的词。所以，ChatGPT生成文本的过程，本质就是在我们看不见的一个高维的意义空间里头，顺着一条语义上最通顺的轨迹散步，每一步都迈向下一个概率最高的点。

### Transformer与注意力机制：长文本理解的关键

我们刚刚提到了一个关键角色——**Transformer**（转换器: 一种基于自注意力机制的深度学习模型架构，广泛应用于自然语言处理），也就是ChatGPT所用的神经网络架构的名字，也是它效果如此惊人的核心工程之一。

Transformer最厉害的一点在于它发明了一种叫做**注意力机制**（Attention Mechanism: 深度学习模型中的一种机制，允许模型在处理序列数据时，动态地关注输入序列中不同部分的重要性）的东西。这是什么意思呢？打个比方，我们人在读一个长句子的时候，注意力也不是平均分配的。比如这个句子：“我今天在公园里看到一只非常可爱的毛茸茸的白色小猫它正在草坪上懒洋洋地晒着太阳，并且时不时地摇着尾巴。”当你读到“它”的时候，你大脑会立即把注意力放到前面的“小猫”身上，而不是“公园”或者“草地”。

早期的语言模型没有这个能力，它看每一个词都差不多，所以处理长句的时候很容易忘掉前面的内容。而Transformer的注意力机制就很完美地模拟了这一点。在决定下一个词是什么的时候，它会给前面所有出现的词都分配一个**注意力权重**（Attention Weight: 注意力机制中用于衡量输入序列中各个部分对当前输出重要性的数值），哪些词对预测下一个词更重要，权重就更高。这让ChatGPT拥有超长的长文本理解和生成能力。它能记住几十页前你跟它聊过的内容，并且在你写长篇大论的开头和结尾之间建立联系，让整个文章的逻辑看起来非常连贯。这就是注意力机制的功劳。当然，这也是谷歌当时撰写的跨时代论文“Attention Is All You Need”发表之后，才让OpenAI能够开发出ChatGPT。

### 人类反馈强化学习：塑造AI价值观

光靠网上读书，还不足以打造出我们今天看到的彬彬有礼、乐于助人、而且三观很正的ChatGPT。因为网上内容鱼龙混杂，什么都有。如果放任自流地学习，AI可能会学到很多偏见、错误的信息，甚至出现一些攻击性的言论，比如像Grok之前就曾出现过纳粹言论。

所以，OpenAI的工程师给它最后加了一道也非常重要的工序：**基于人类反馈的强化学习**（Reinforcement Learning from Human Feedback, RLHF: 一种机器学习技术，通过人类对模型输出的偏好反馈来训练奖励模型，进而优化语言模型）。

这个过程简单来说，就是请人类当老师。他们先让最初的模型对各种问题生成好几个不同的答案，然后雇佣一批标注员，像老师改作业一样，给这些回答排序：哪个最好、哪个次之、哪个最差。接着，他们就用这些人类偏好的数据，又训练另一个**奖励模型**（Reward Model: 在强化学习中，根据环境状态和智能体行为给出奖励信号的模型）。这个模型的作用就是去模仿人类老师的“品味”，去给AI的任何回答打分。最后，ChatGPT去跟这个奖励模型玩，ChatGPT不断生成新的回答，而奖励模型则不断地给它打分。ChatGPT的目标就是想办法让自己生成的回答能在这个内置老师那里拿到尽可能高的分数。通过这个过程，ChatGPT学会了如何生成更符合人类期望、更安全、更有帮助的回答。就像一个孩子，不仅读了很多书，还有一位耐心的老师在旁边时刻纠正他、引导他，最终才成长成一个优秀的学生。

### ChatGPT的局限性：语言模型而非计算引擎

现在我们已经把ChatGPT的底层原理和训练过程都扒了一遍了。理论上看，它看起来很完美，但实践中，它真的是无所不能吗？当然不是。记住它的本质：它是一个**概率模型**。它追求的是听起来最合理，而不是事实最正确。这导致了它会经常“一本正经地胡说八道”，这种现象被称为**幻觉**（Hallucination: 语言模型生成的内容看似合理但与事实不符或完全虚构的现象）。

例如，你问它一个精确计算的问题。Wolfram在书里就举了个例子，问它芝加哥到东京有多远。ChatGPT会给你一个看起来很自信的答案，甚至还贴心地换算了公里，但是那个数字是错的（当然这是Wolfram当时使用的ChatGPT版本）。为什么呢？它并不是在计算，它只是在它的记忆库中找到了无数个描述距离的文本，然后模仿这些文本的风格，生成一个看起来“那么回事”的数字。

再比如数学题，你让它算个微积分，它可以给出非常详细的解题步骤，格式工整，术语专业，但是十有八九是错的。更搞笑的是，它编造这些解题步骤时犯的错误，和人类学生犯的几乎是一模一样，因为它学习材料里包含了大量学生在网上问的错题和错误的解法。这告诉大家一个极其重要的原理：**ChatGPT是一个语言模型，不是一个计算引擎。**

### 混合AI：ChatGPT与Wolfram Alpha的完美结合

那么，怎么解决ChatGPT事实不靠谱这个问题呢？难道就没救了吗？当然有救。Wolfram在这本书的第二部分给出了一个非常绝妙的解决方案：把两种AI结合起来。

第一种，就是以ChatGPT为代表的，基于统计处理非结构化语言的AI。它的强项是理解人的意图，进行流畅对话，处理模糊、开放性的问题。它像一个博学有创造力，但有点马虎的文科生。

第二种，就是以Wolfram自己的产品Wolfram Alpha为代表的，基于符号计算和结构化知识的AI。它的核心是一个巨大的、经过严格策划和验证的知识库，以及强大的计算算法。它寻求的是绝对的精确，像一个不善言辞，但是极其严谨可靠的理科状元。

当这两种大脑结合，会发生什么呢？当用户通过ChatGPT提出一个需要精确计算或者事实查询的问题时，ChatGPT不再自己“瞎猜”了，而是转过头去问Wolfram Alpha。它用自己擅长的自然语言，把这个问题抛给了Wolfram Alpha。然后，后者用强大的自然语言理解能力，把这个问题转化为精确的、可计算的**符号代码**（Symbolic Code: 使用符号和代数表达式来表示和操作数学或逻辑概念的代码），然后调用自己的知识库和算法，得出100%准确的结果，再返回给ChatGPT。最后，ChatGPT再发挥它的语言天赋，把这个冷冰冰的精确结果包装成一段流畅自然、易于理解的文字，呈现给用户。

这就是一个完美的互补：ChatGPT负责和人沟通和润色，Wolfram Alpha负责硬核计算和事实核查。这才是一个真正强大可靠的AI助手应有的形象。当然，Wolfram写这本书肯定要给自己家的产品做一些代言，但他的核心理念其实跟Gary Marcus是一样的：就是要把符号AI和基于统计的深度学习AI结合在一起。

### 深入思考：语言的规律与计算不可约性

聊到这里，我们已经把ChatGPT的技术细节和应用前景都摸得差不多了。但主持人想带大家再深挖一层：ChatGPT的巨大成功，除了技术上的突破，它还向我们揭示了一个可能更深远的人类科学事实：那就是人类的语言以及其背后的思维模式，可能比我们想象的要简单，要更有规律。

大家想一想，为什么本质上只是在预测下一个词、结构相对简单的神经网络，就能够模拟出如此丰富和复杂的语言现象？这背后一定是因为我们语言本身就存在强大的可预测的内在规律。这些规律可能不只是我们语法书上学的那些主谓宾定状补，而是一种更深层次的**语义语法**（Semantic Grammar: 指对语言中词语和句子意义结构的规则描述）。例如，有实体属性的物体可以做移动这个动作，但是抽象的概念就不行；有身体特征的东西可以吃东西，但是无生命的物体就不行。这些都是我们潜意识里遵循的语义规则。

其实，GPT在阅读了海量文本之后，它没有去“理解”这些规则，但是它通过统计发现了这些规则的存在。它学到的不是知识本身，而是知识和概念之间连接的形状和模式。就像我们发现了万有引力，不是我们理解了引力的本质，而是我们通过观察和计算，总结出一套能够完美预测天体运动的数学公式。ChatGPT的成功，可能就意味着我们第一次有了一面镜子，可以间接窥探人类思维和语言背后那隐藏的、支配一切的语法和定律。

那这是不是意味着，只要神经网络足够大，数据足够多，它就能够无所不能，解决所有问题呢？Stephen Wolfram给出了一个非常深刻的否定答案。他提出了一个概念叫做**计算不可约性**（Computational Irreducibility: 指某些系统或过程的演化无法通过捷径预测，只能通过一步步模拟才能得知结果）。这什么意思呢？就是说，在我们宇宙中存在这样一类过程，你想知道它最终结果，没有任何捷径可走，唯一的办法就是老老实实一步步地把它运行一遍。

例如，**元胞自动机**（Cellular Automata, 简称CA: 一种在离散时间和空间中，通过简单局部规则演化的动力学模型）。它的规则极其简单，但是它演化出来的图案可以复杂到无法预测。你没有办法通过一个简单公式算出它第一亿步会是什么样子，你只能从第一步开始，一步步地算到第一亿步。大量的自然现象和复杂的数学问题都具有这种计算不可约性。

而神经网络的学习，本质上是在寻找数据中的规律和捷径，也就是**计算可约性**（Computational Reducibility: 指可以通过更短的计算或简化模型来预测系统或过程结果的性质）。它通过压缩信息，找到可以被泛化的模式。这就注定了ChatGPT本质是“懒惰”的，它极其擅长处理那些**计算上很浅的任务**。而那些需要硬算，或者**计算上很深的任务**，它天生就无能为力了。写一篇文章、写一首诗、总结一段话，这些任务对人类看起来很复杂，但是从计算深度而言，它是很浅的，因为它依赖的是我们大脑已经存在的、高度优化的语言和思维模式的复用。而计算一个复杂的物理模拟，或者证明一道数学难题，这些任务从计算的深度上讲是深的，这恰恰就是传统计算机和Wolfram Alpha擅长的领域。

所以，我们不必担心ChatGPT会取代所有。它只是在计算浅的领域达到了超人的水平，但是在计算深的世界里，它依然需要传统的计算工具的帮助。这也解释了为什么ChatGPT的下棋能力和AlphaGo（阿尔法围棋: 谷歌DeepMind开发的人工智能围棋程序，曾击败世界顶尖棋手）差了这么大，就是因为它并不是一个适合计算深的这种应用场景。

### ChatGPT是否在思考？重新定义智能

现在，让我们回到那个终极问题：ChatGPT算不算在思考呢？它有智能吗？Wolfram的观点是，它和人脑的工作方式既有相似之处，又有本质不同。

**相似之处在于**，我们人类说话时，很多时候也并不是每时每刻都在进行严密的逻辑推理。我们大脑也是一个巨大的联想网络。当我们听到一个词，会激活无数相关的概念，然后我们大脑会以一种我们自己也无法完全说清的方式，选择最合适的词语串成句子。这个过程，ChatGPT基于概率选择下一个词，有异曲同工之妙。

但**本质不同在于**，ChatGPT的神经网络在生成每一个词的过程中，是纯粹的**前馈网络**（Feedforward Network: 一种神经网络结构，信息只从输入层单向流向输出层，没有循环或反馈）。也就是说，数据从输入层单向地、一次性地流向输出层，中间没有任何迭代和循环。它每生成一个新词，都要把前面所有的内容再重新看一遍，这是一个固定的机械流程。而我们的大脑以及绝大多数的计算机程序都充满了循环和反馈。我们思维可以在几个概念中来回打转，深化迭代，直到找到一个满意的答案。这种在内部反复“咀嚼”信息的能力，是ChatGPT目前不具备的（当然，Wolfram的这本书是在2023年写的，不知道现在的GPT是否具备了“深度思考”的能力）。

所以，与其说它在思考，不如说它在进行一种我们前所未见的、极其高效的模式匹配和联想生成。但ChatGPT的出现，也迫使我们反思我们所谓的思考和智能到底是什么。或许，我们过去对于智能的定义太狭隘了。我们一直以为智能必须伴随着自我意识、逻辑推理，但ChatGPT表明，仅仅通过对海量数据模式的模仿，也能够涌现出令人惊叹的、我们称之为“智能”的行为。这或许意味着智能的形态比我们想象的要丰富得多。

### 驾驭AI：三条黄金法则

理解了ChatGPT的本质，我们到底应该怎样更好地应用它，让它成为我们的“超能力”呢？主持人总结了三条黄金法则，或称“驯兽指南”：

1.  **把它当成领航员，而不是自动驾驶仪。** 记住它的核心是语言接龙，你给它的开头（也就是**提示词**Prompt: 给人工智能模型输入的文本或指令，用于引导其生成特定内容）决定了整个航行的方向。所以，你的问题要尽可能清晰具体，提供充足的上下文。你甚至可以给它举几个例子，这叫做**少样本学习**（Few-Shot Learning: 一种机器学习方法，模型仅通过少量示例就能学会新任务），告诉它你想要的风格和格式。它不是你的下属，它是你的副驾驶，你需要清晰地告诉它目的地在哪。
2.  **永远不要完全信任它的事实，尤其在关键领域。** 对于任何需要精确数据、计算和逻辑推理的地方，要保持警惕。对它生成的内容，特别是数字、日期、代码、科学公式，可以当作草稿或者假设。学会使用专业的工具，比如说搜索引擎、Wolfram Alpha或者是计算器去交叉验证。把它当成一个创意无限但有点迷糊的实习生，它的工作成果你必须亲自审核。
3.  **善用它的发散性，激发你的创造力。** 它最大的价值不在于提供最标准答案，而在于它能够基于概率，探索语言的无数种可能性。当你写作卡壳、策划没有思路、编程想不出新方法的时候，它可能给你生成10个不同版本。你会发现它总有一些意想不到的角度，给你带来启发。不要把它当成答案的终点，要把它当成思考的起点。

掌握这三条，你就不是简单在使用ChatGPT，而是在和这个强大的概率引擎共舞，真正地驾驭它的力量。

### 结语：AI时代的科学发现

今天我们跟随Stephen Wolfram一起拆解了ChatGPT，从它最简单的文字接龙游戏开始，探索了神经网络、梯度下降、词嵌入、注意力机制，甚至聊到了计算不可约性和智能的本质。

最后主持人想说，ChatGPT的出现，其意义可能远超一个好用的工具。它更像是一个科学的伟大发现。就像牛顿当年用几条简单的定律统一了天上行星和地上的苹果，就像达尔文用自然选择解释了生命世界的万千形态。今天，ChatGPT用一个基于概率的下一个词预测模型，向我们证明了人类最复杂的智慧结晶——语言和思维，背后可能也隐藏着简洁而深刻的规律。它像一面镜子，让我们第一次有机会从一个非人类、纯粹数学的视角去审视我们自己。它迫使我们去思考什么是意义、什么是创造、什么是智能。

未来最强大的力量，将不再是单纯的人类智慧，也不是单纯的机器智慧，而是两者结合。是ChatGPT这样富有联想和创造力的“文科大脑”，和Wolfram Alpha这样严谨精确的“理科大脑”的完美融合。这不仅会改变我们的工作方式，也会开启一个全新的探索知识边界的“大航海时代”。当然，作为自家产品的作者，Stephen Wolfram在书里头是不遗余力地给Wolfram Alpha做推广。Wolfram Alpha在处理数学或物理相关问题时确实非常强大，但它也有自己的局限性，例如其输入方式比较固定，可能需要学习Wolfram Language才能更好地交互。之前ChatGPT 4o刚出来的时候，它还有一个与Wolfram连接的定制GPT，现在两边应该也有合作。