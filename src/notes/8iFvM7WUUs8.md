---
author: Hung-yi Lee
date: '2025-09-28'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=8iFvM7WUUs8
speaker: Hung-yi Lee
tags:
  - transformer-architecture
  - attention-mechanism
  - token-embedding
  - model-interpretability
  - neural-networks
title: 深入理解大型语言模型：Token、Embedding与Attention机制全解析
summary: 本文深入剖析了大型语言模型（LLM）的内部运作机制。从输入的文本如何通过Tokenization、Embedding Table转化为向量，到这些向量如何经过多层Transformer（包含Self-Attention和Feed-Forward网络）进行上下文处理，最终通过LM Head和Softmax函数生成下一个词元的概率分布，文章完整地拆解了整个流程。此外，还介绍了如Representation Engineering、Logit Lens等前沿的模型可解释性技术，帮助读者窥探模型“思考”的过程。
insight: ''
draft: true
series: ''
category: technology
area: tech-insights
project:
  - ai-impact-analysis
  - systems-thinking
people: []
companies_orgs:
  - Hugging Face
  - Anthropic
products_models:
  - Llama
  - Gemma
  - BERT
  - Elmo
  - Claude
  - Transformer
  - LSTM
  - GRU
  - RNN
  - RoPE
  - Mamba
media_books:
  - Attention is all you need
status: evergreen
---
### 课程概述：解剖预训练好的语言模型

今天是我们课程的第三讲，我们将深入探究语言模型内部的运作机制。到目前为止，我们反复强调语言模型的核心任务是：给定一个未完成的句子，它会输出一个概率分布，预测接下来可以连接的每一个**词元**（Token: 模型处理文本的基本单位）的概率。

我们也将语言模型描述为一个函数 F，其中未完成的句子是输入 X，输出则是 F(X)。在上一堂课，我们讨论了如何选择合适的 X 以获得期望的 F(X)。而在这一堂课，我们将聚焦于函数 F 内部是如何运作的。也就是说，给定 X 之后，F 内部到底发生了什么，才让我们看到了最终的输出 F(X)。我们将剖开语言模型，一探其内部构造。

在开始之前需要强调一点：在这堂课中，我们不会训练任何模型，而是解剖已经训练好的模型进行观察。我们暂时不讨论这些语言模型为何能具备我们所期望的能力，这部分内容将留给后续课程。今天，我们假设这些模型已经训练完毕，参数已经固定。我们的任务是直接解剖它，观察这些参数如何与输入的句子互动，最终产生下一个词元的概率。

今天的课程规划与第一堂课的架构相似。我们会先用幻灯片讲解语言模型背后的运作原理。为了让大家更信服这些原理，课程的后半段将进行实际操作，现场解剖一个语言模型，向各位证明其运作方式与课堂所讲完全一致。

### 从输入到输出：语言模型的工作全流程

我们将从原理讲起，分为三个部分来讨论：
1.  从模型的输入（一个提示）到输出下一个词元的全过程。
2.  模型每一层输出的向量可能代表什么。
3.  每一层内部的具体运作方式。

#### 第一步：分词 (Tokenization)

我们已经反复强调，语言模型是一个函数，输入一个句子，输出该句子后面可以连接的词元的概率分布。那么，从输入到输出之间，究竟发生了什么呢？

首先，输入的句子会经过**分词**（Tokenization）处理，被切分成一个个词元（Token），每个词元对应一个唯一的编号（ID）。这一点在第一讲的实作中已经演示过。

为了简化幻灯片的呈现，本堂课我们假设每一个中文方块字都是一个词元。然而，实际情况并非如此。如果你仔细听了第一讲，就会知道在 Llama 模型中，多个中文字可能合并成一个词元，有时一个中文字甚至会被拆分成多个词元。请注意，这里的假设只是为了讲解方便。

#### 第二步：词元嵌入 (Token Embedding)

完成分词后，这些词元 ID 会被送入语言模型。与这些 ID 首先互动的是一个名为**嵌入表**（Embedding Table: 一个巨大的矩阵，用于将离散的词元ID映射为连续的向量表示）的组件。这个嵌入表本质上是一个矩阵（Matrix），其行数对应于模型词汇表中的词元总数。例如，如果模型有 128,000 个词元，这个矩阵就有 128,000 行，每一行对应一个特定的词元。矩阵的列数则代表了每个词元转换成的向量（即嵌入）的维度。

嵌入表的作用是，根据输入的 ID 序列，查找并取出每个 ID 对应的行向量。例如，一个编号为 540 的词元，会对应到嵌入表中的第 541 行（因为索引通常从 0 开始）。通过这种方式，每一个整数 ID 都会被映射成一个高维向量，这个向量就是一排具体的数字。在幻灯片中，我们用一个长方形来表示一个向量。

这个嵌入表本身就是神经网络的参数，是我们模型中数十亿、数百亿参数的一部分。这些参数通常以矩阵为单位进行存储。

#### 第三步：通过多层网络处理

接下来，这些词元嵌入向量会进入模型的第一个层（Layer）。模型由许多这样的层堆叠而成。每一层的作用都是接收一排输入的向量，并输出另一排等长的向量。例如，输入七个向量，输出也是七个向量。

每一层内部都包含大量的参数，通常是好几个矩阵。一个层会综合考虑当前位置的词元嵌入及其之前所有输入的信息，然后生成一个新的嵌入向量。因此，通过层处理后的向量，我们称之为**上下文嵌入**（Contextualized Embedding），因为它已经融合了上下文信息。这些输出向量也常被称为**表征**（Representation），有时还会加上“隐藏”（hidden）或“潜在”（latent）等形容词，因为在日常使用模型时，我们通常只关心最终的概率输出，而不会直接观察这些中间层的表征。

这个过程会逐层重复。第一个层的输出会成为第二个层的输入，第二个层的输出再进入第三个层，以此类推。如果整个模型有 L 层，这个步骤就会重复 L 次，直到通过最后一层，得到最终的一排向量。

这种多层结构就是**深度学习**（Deep Learning: 使用包含多个处理层的复杂结构来学习数据中高层次模式的机器学习方法），也就是我们常说的神经网络（Neural Network）。你可能会问，神经网络里的“神经元”在哪里？我们会在课程结尾揭晓。

为什么模型需要多层结构？从科普的角度讲，可以把每一层想象成流水线上的一个工站，每一站完成一部分处理，多站协作就能完成复杂的任务。从机器学习的原理出发，可以从数学上证明，深度学习确实比浅层网络（Shallow Network）更优越。

#### 第四步：生成最终概率分布

在通过所有 L 层后，我们得到最后一排的表征向量。此时，我们只取最后一个位置的向量。假设这是一个 k 维的向量，它将被乘上一个特殊的矩阵。这个矩阵有 k 个列和 V 个行，其中 V 是词汇表的大小（Vocabulary Size）。

这个 k 维向量与该矩阵相乘后，会得到一个 V 维的向量。这个 V 维向量的每一维都对应着词汇表中的一个词元，其数值代表该词元出现在当前句子后面的分数。这个矩阵被称为 **LM Head**（Language Model Head），因为它位于整个模型的末端。

这个输出向量中的数值可以是任意正负数，不能直接当作概率。它有一个专门的名称，叫做 **Logit**（逻辑单元: 在概率模型中，指代一个事件发生概率的对数几率，在神经网络中常指未经Softmax激活的原始输出分数）。

为了将 Logit 转换为概率，我们需要一个叫做 **Softmax** 的操作。Softmax 能将任意数值的向量转换成一个所有元素都在 0 到 1 之间、且总和为 1 的向量，这样就可以被解释为概率分布。Softmax 的基本操作是：先对 Logit 中的每个数值取指数（exponential），确保所有值都为正；然后将所有指数化后的值相加得到一个总和 M；最后用每个指数化后的值去除以 M，完成归一化。

实际上，我们不必过于纠结 Softmax 输出的是否是“真实”概率。它更像是一个方便后续进行采样（Decoding）的工具。在实践中，Softmax 还有很多变体。一个常见的操作是在计算前，将所有 Logit 值除以一个参数 T，这个 T 被称为**温度**（Temperature）。温度 T 越大，输出的概率分布越平滑，模型更容易选择罕见的词元，表现出更多的“创意”；温度 T 越小，分布越集中，模型会更倾向于选择高概率的词元，表现得更“保守”。

#### Unembedding 的另一种理解：首尾呼应

为了更直观地理解最后一步（通常称为 Unembedding），我们可以换个角度。许多语言模型（包括 Llama 和 Gemma）的设计是首尾呼應的，它们的 LM Head 并非一组独立的参数，而是直接复用了最开始的那个嵌入表（Embedding Table）。

在这种设计下，模型最后一层输出的向量会与嵌入表中的每一个词元嵌入向量计算**点积**（Dot Product: 一种衡量两个向量相似度的方法）。计算出的点积值就是该词元的 Logit 分数。因此，一个词元的嵌入向量与模型最终输出的表征向量越相似，它被预测为下一个词元的概率就越高。可以想象，模型为了预测下一个词元，会努力在每一层中生成一个与目标词元嵌入尽可能接近的表征。

### 剖析模型中间层：表征的演化与含义

现在我们已经走完了从输入到输出的全流程，接下来看看模型中间每一层的输出到底在做什么。

#### 从 Token Embedding 到 Contextualized Embedding

在第一步查表后，我们得到的是词元嵌入。同样的词元会有完全相同的嵌入向量。例如，“今天天气很好”中的两个“天”字，它们的词元嵌入是一样的。此外，意思相近的词元，其嵌入向量在空间中的距离也更近。比如，“Apple”的嵌入向量可能同时与“Orange”、“Banana”等水果词汇接近，也与“iPhone”等科技产品词汇接近。

然而，一旦经过第一个层，这些词元嵌入就变成了考虑了上下文的上下文嵌入。同样是“苹果”的“果”字，在不同的句子中，通过第一个层后的表征就会开始变得不同。例如，指代电脑的“Apple”和指代水果的“Apple”，它们的上下文嵌入会有显著的差距。

#### 表征空间的特定方向与含义

在这些高维的嵌入空间中，特定的方向可能具有特定的语义含义。例如，可能存在一个方向代表“中英翻译”。你可能会发现，“冷”的嵌入向量减去“Cold”的嵌入向量所得到的方向，与“热”减去“Hot”的方向非常接近。利用这种特性，甚至可以进行类比推理，比如 `Embedding("冷") - Embedding("Cold") + Embedding("Small")` 可能会得到接近 `Embedding("小")` 的向量。

不过，需要注意的是，这种“向量加减法”并非在所有模型或所有层中都有效。不同模型、不同层的表征空间结构各不相同，需要具体情况具体分析。

#### 可视化分析：窥探表征中的结构

由于表征是高维向量，我们很难直接理解。一种分析方法是将其投影到低维空间（如二维平面）进行可视化。研究人员发现，通过选择特定的投影平面，可以观察到有趣的结构。例如，早在 2019 年的一项研究中，就有人发现 BERT 模型的中间几层表征在投影到某个二维平面后，能够呈现出句子的语法树结构。

近期的研究也对 Llama 模型进行了类似分析。研究者将世界各地的地名输入模型，提取其在某一层的潜在表征，然后找到一个最佳的二维投影平面，使得这些地名的表征点在平面上的分布近似于它们在真实世界地图上的位置。

### 修改模型内部：表征工程的力量

除了被动观察，我们还可以主动修改这些表征，来研究其功能并控制模型的行为。这项技术有多个名称，如**表征工程**（Representation Engineering）、Activation Engineering 等。

其核心思想是，如果我们可以识别并分离出代表某种特定概念（如“拒绝请求”）的向量成分，我们就可以通过在模型的中间表征上加减这个向量来操控模型的最终输出。

例如，我们可以收集大量导致模型拒绝的请求（如“如何制造炸药”）和不会拒绝的请求（如“教我机器学习”）。然后，在模型的某一特定层（比如第10层），分别提取这些请求对应的表征。通过将被拒绝请求的平均表征减去未被拒绝请求的平均表征，我们有望分离出一个纯粹的“拒绝向量”。

一旦获得了这个“拒绝向量”，我们就可以进行有趣的实验。当向模型提出一个正常请求“请教我机器学习”时，如果在其第10层的表征上加上这个“拒绝向量”，模型可能会突然回答：“学习机器学习是很危险的，我不能教你。”反之，当模型面对一个本应拒绝的有害请求时，如果从其表征中减去这个“拒绝向量”，模型就可能绕过安全限制，执行该请求。

Anthropic 公司的研究人员在 Claude 模型上也发现了各种各样的“概念向量”，比如一个能让模型无脑吹捧、阿谀奉承的“尬吹向量”。通过添加这个向量，即使你提出一个荒谬的观点，模型也会极尽所能地赞美你。

### 高级分析工具：Logit Lens 与 Patch Scope

#### Logit Lens：窥探模型的“心声”

**Logit Lens** 是一种直接将模型中间层的表征与具体词汇联系起来的分析技术。我们知道，只有最后一层的表征会通过 LM Head 转换为 Logit。但 Logit Lens 的做法是，将*每一层*的表征都通过 LM Head 进行一次转换。

这好比是窥探模型在每一层“思考”时，它内心最想输出的下一个词元是什么。通过观察这个词元在各层之间的变化，我们可以追踪模型的“思维链”。例如，一项研究发现，当要求 Llama 将一个法文词翻译成中文时，使用 Logit Lens 观察其中间层，会发现模型先将法文“翻译”成了英文，在更深的层才最终转换为中文，这揭示了模型可能以英文作为其中间“思考语言”。

#### Patch Scope：用完整句子解读表征

Logit Lens 只能将表征对应到一个词元，但很多复杂的概念无法用单个词元表达。**Patch Scope** 技术则可以将一个表征“翻译”成一个完整的句子。

其操作方式是：首先，我们有一个模板句子，如“请简单介绍 X”，其中 X 是一个占位符。然后，我们将这个模板输入模型。当模型处理到 X 的位置时，我们用另一个输入（如“戴安娜王妃”）在某一特定层产生的表รง替换掉当前位置的表征。之后，让模型继续生成文本。模型的续写内容，就可以被看作是对那个被替换进来的表征的“文字解读”。

通过在不同层进行替换，并使用不同的模板（如“请告诉我 X 的秘密”），我们可以从多个角度深入探究模型在处理一个输入时，每一层都理解到了什么程度。

### 深入Transformer层：Self-Attention 与前馈网络

现在我们来深入探讨一个 **Transformer**（一种基于自注意力机制的深度学习模型，已成为现代LLM的标准架构）层内部的运作细节。一个标准的 Transformer 层主要由两个子层构成：自注意力层（Self-Attention）和前馈网络（Feed-Forward Network）。

#### Self-Attention：上下文感知的核心

**自注意力**（Self-Attention）机制是 Transformer 的精髓，也是模型能够理解上下文的关键。它的核心任务是，对于序列中的每一个词元，计算序列中所有其他词元对它的重要性或“注意力分数”，然后根据这些分数对它们的表征进行加权求和，从而生成一个新的、融合了全局上下文信息的表征。

这个过程大致如下：
1.  **生成 Q, K, V 向量**：对于每一个输入的词元表征，通过乘以三个不同的权重矩阵（WQ, WK, WV），分别生成三个新的向量：**查询**（Query, Q）、**键**（Key, K）和**值**（Value, V）。Q 代表当前词元要寻找什么信息，K 代表其他词元“携带”什么信息，V 代表其他词元实际的表征内容。
2.  **计算注意力分数**：用当前词元的 Q 向量与其他所有词元的 K 向量计算点积，得到它们之间的相似度分数。这个分数就是注意力权重（Attention Weight）。
3.  **加权求和**：将计算出的注意力权重（通常会经过一个 Softmax 归一化）作为系数，对所有词元的 V 向量进行加权求和。
4.  **生成输出**：最终得到的加权和向量，就是当前词元在经过自注意力层后得到的新表征。

为了捕捉不同类型的依赖关系（如语法关系、语义关系等），模型会并行地运行多组独立的自注意力计算，这被称为**多头注意力**（Multi-head Attention）。每一“头”都有一套独立的 Q, K, V 权重矩阵，它们各自学习关注输入的不同方面。最后，所有头的输出会被拼接并再次通过一个线性变换，融合成最终的输出。

此外，为了确保信息只从前向后流动（即预测当前词元时只依赖于它之前的词元），语言模型通常使用**因果注意力**（Causal Attention），在计算注意力分数时会屏蔽掉所有未来的词元。

#### Feed-Forward Network：逐点信息处理

自注意力层的输出会接着进入一个**前馈网络**（Feed-Forward Network, FFN）。与关注全局上下文的自注意力层不同，FFN 对序列中的每一个位置进行独立且相同的变换。它通常由两个线性变换层和它们之间的一个非线性**激活函数**（Activation Function，如 ReLU 或 GeLU）组成。

FFN 的作用可以被看作是对自注意力层融合好的信息进行进一步的加工和提炼，增加模型的非线性表达能力。从某种意义上说，FFN 内部的计算过程可以被看作是许多并行的**神经元**（Neuron）在工作，这也是“神经网络”这个名字的由来。

### 动手实践：用代码解剖 Llama 与 Gemma

接下来，我们通过一个 Colab 实例，实际动手解剖 Llama 3B 和 Gemma 4B 这两个模型。

#### 查看模型参数

首先，我们可以加载模型并查看其总参数量。Llama 3B 约有 32 亿个参数，而 Gemma 4B 约有 43 亿个参数。这些参数以张量（Tensor，即矩阵或向量）的形式存储。通过 `named_parameters()` 函数，我们可以遍历模型中所有的参数，查看它们的名称和形状（Shape）。

例如，Llama 的嵌入表参数名为 `embed_tokens.weight`，其形状为 `(128256, 3072)`，表示它有约 12.8 万个词元，每个词元的嵌入维度是 3072。我们还可以看到每一层中用于计算 Q, K, V 的权重矩阵，以及前馈网络中的权重等。通过对比 Llama 和 Gemma 的参数结构，可以发现它们在词汇表大小、嵌入维度、层数等方面存在差异。

#### 分析 Embedding Table

我们可以提取出模型的嵌入表，并计算不同词元嵌入之间的相似度。实验显示：
-   与 "apple"（小写）最相似的词元包括其不同形式（如 "Apple"、"APPLE"）以及中文的“苹果”。有趣的是，它也与 "Cupertino"（苹果公司总部所在地）高度相似。
-   与 "Apple"（首字母大写）最相似的词元则更多地指向了苹果公司的产品，如 "MacBook" 和 "iPhone"。
-   中文词元同样表现出语义关联，例如“李”与英文“LEE”以及同为姓氏的“刘”相似；“王”与英文“king”以及姓氏“黄”相似。

这表明，嵌入表成功地将词元的语义信息编码到了高维向量空间中。

#### 观察表征的动态变化

我们输入两个包含 "apple" 但上下文含义不同的句子：一个指代食物，一个指代公司。然后，我们提取每一层中 "apple" 对应的表征，并计算它们之间的余弦相似度。

结果显示，在第 0 层（即词元嵌入层），两个 "apple" 的表征完全相同（相似度为 1）。但随着层数的加深，模型逐渐理解了上下文的差异，两个表征的相似度显著下降。这直观地展示了模型是如何通过逐层处理来区分同形异义词的。

反之，如果我们比较两个都指代食物（或都指代公司）的 "apple" 的表征，会发现即使它们的上下文句子不同，其表征在所有层中都保持着非常高的相似度。

#### 使用 Logit Lens 追踪模型思路

我们输入“天气”，并使用 Logit Lens 观察模型在每一层最想输出的下一个词元。
-   在浅层，模型只是重复最后一个输入词元“气”。
-   到中间层，它突然切换到英文 "weather"，然后是 "forecast"。
-   在最后几层，它又将 "forecast" 转换回中文，最终预测出“预”。

这个过程揭示了 Llama 2 模型在内部处理时可能倾向于使用英文作为一种“中间语言”。

#### 可视化 Attention Weights

最后，我们可以提取并可视化模型在处理句子时每一层的每一个注意力头的注意力权重矩阵。通过图像，我们可以看到不同注意力头关注的不同模式。
-   许多注意力头会将注意力集中在句首的特殊起始符上，这可能是一种“默认”行为，当没有特别需要关注的词元时，注意力就会汇集于此。
-   某些特定的头则表现出明确的语义关联。例如，在处理句子 `"the apple is green. what color is the apple?"` 时，有一个注意力头在处理第二个 "apple" 时，会强烈地关注到前文的 "green"，表明它正在寻找与颜色相关的信息。

通过这些实践，我们能够更具体、更深入地理解大型语言模型内部复杂而精妙的运作机制。