---
title: OpenAI论文深度解析：大语言模型幻觉的根源与破解之道
summary: OpenAI与佐治亚理工联合研究揭示，大语言模型幻觉源于预训练阶段的统计误差传导和后训练阶段的评估机制激励错位。论文提出，通过明确置信度目标和修改主流评估逻辑，可有效引导模型从“盲目猜测”转向“诚实可靠”。
area: tech-insights
category: technology
project:
- ai-impact-analysis
tags:
- ai-hallucinations
- best-partners-tv
- large-language-models
- machine-learning
- model-evaluation
- openai-research
people: []
companies_orgs: []
products_models: []
media_books: []
date: '2025-09-10'
author: 最佳拍档
speaker: 最佳拍档
draft: true
guest: ''
insight: ''
layout: post.njk
series: ''
source: https://www.youtube.com/watch?v=R3BvSiPuk08
status: evergreen
---
### 幻觉之谜：预训练误差与评估错位的必然产物

一篇来自OpenAI和佐治亚理工联合发表的重磅论文《为什么语言模型会有幻觉（Why Language Models Hallucinate）》，深入剖析了**大语言模型**（Large Language Model, LLM: 拥有大量参数、在海量文本数据上训练的深度学习模型）生成**幻觉**（Hallucination: 大语言模型生成不准确、不真实或无意义内容的现象）的深层原因。该研究指出，幻觉并非简单的模型规模不足或数据量不够，而是现有训练和评估逻辑下的“必然结果”。其核心症结在于两个关键问题：**预训练**（Pre-training: 模型在大量未标记数据上进行的初步训练）阶段的**统计误差传导**（Statistical Error Propagation: 预训练阶段模型学习语言概率分布时产生的误差），以及**后训练**（Post-training: 在预训练后，对模型进行微调和优化，如RLHF、DPO等）阶段的**评估机制激励错位**（Evaluation Mechanism Incentive Misalignment: 评估标准鼓励模型猜测而非承认无知）。

### 预训练阶段：统计误差的传导机制

大语言模型预训练的目标是学习人类语言的概率分布，即识别哪些句子更常见、更合理。许多人认为，只要提供足够多的“无错误数据”，模型就能只输出正确内容。然而，该论文通过数学证明，即使训练数据100%无误差，预训练后的模型依然会产生错误，因为生成正确内容比判断内容是否正确更难。

论文引入了核心理论工具——**IIV二分类问题**（Is-It-Valid Binary Classification: 判断一个句子是否有效或正确的二元分类任务）。简单来说，就是给模型一个句子，让它判断该句子是“有效”（正确，记为+）还是“无效”（错误，记为-）。这是一个典型的有监督分类任务，训练数据中正确和错误句子的比例各占50%。论文推导出一个关键结论：模型的生成误差率（输出错误内容的概率）至少是其在IIV分类任务中错误率的2倍。这是因为生成任务本质上包含了无数个“隐性的IIV判断”，只要其中一个判断出错，最终生成结果就会错。

论文中举例说明了IIV分类难度对生成误差的影响：
1.  **拼写判断**：例如“Greetings”和“Greatings”。这类问题IIV分类准确率高，因为拼写规律明显，模型生成时很少犯拼写错误。
2.  **字母计数**：例如询问“DEEPSEEK里有几个D？”。这类任务IIV分类难度更高，模型需要将单词拆解成单个字母再计数，许多模型架构不擅长处理这种细粒度任务。DeepSeek-V3模型常答错，而DeepSeek R1通过**链式推理**（Chain-of-Thought Reasoning: 模型通过逐步推理来解决复杂问题的方法），将单词拆分逐个计数，弥补了模型架构缺陷，降低了IIV分类误差，从而生成正确答案。
3.  **生日事实判断**：这类问题的IIV分类难度最高，因为生日是“没有规律的**任意事实**（Arbitrary Facts: 没有固定模式、完全依赖记忆的无规律信息，如生日、论文标题）”。这类信息全靠训练数据记忆。如果特定生日在训练数据中出现次数极少，模型就无法准确判断，IIV分类错误率飙升，生成时便会“编造一个看似合理的日期”。

论文用**单例率**（Singleton Rate, sr: 训练数据中仅出现过一次且非“我不知道”的提示-响应对的比例）量化了这种误差。对于任意事实，模型的幻觉率下限等于单例率。该结论的逻辑源于艾伦·图灵提出的“**Good-Turing缺失质量估计**（Good-Turing Missing Mass Estimation: 一种统计方法，用于估计未出现事件的概率）”，即训练数据中“只出现过一次的事件”可用来估计“没出现过的事件”的概率。单例越多，模型越容易用编造的错误事实来填补空白。

除了任意事实，**模型自身缺陷**（Poor Models: 模型架构无法有效拟合目标概念或未充分训练导致的问题）也会导致预训练误差。经典的例子是**三元语言模型**（Trigram Model: 一种基于前两个词预测下一个词的语言模型）的缺陷，它只能根据“前两个词”预测下一个词，无法处理需要长上下文理解的任务，导致IIV分类误差升高，生成误差率可达50%。

此外，还有三个重要因素导致预训练误差：
1.  **计算复杂度**：有些问题本身在计算上不可解，模型无法判断哪个结果正确，只能输出错误结果。
2.  **分布偏移**（Distribution Shift: 测试数据与训练数据分布不一致的现象）：测试时的提示词与训练数据里的提示词“长得不一样”，例如模型在训练中多见短单词，突然遇到长单词就容易出错。
3.  **垃圾进垃圾出**（Garbage In, Garbage Out, GIGO: 指输入错误数据会导致输出错误结果）：现实中训练数据必然包含错误、谣言，模型会学习并复制这些错误信息，进一步升高预训练误差。

### 后训练阶段：评估机制的激励错位

许多人疑问，**RLHF**（Reinforcement Learning from Human Feedback: 利用人类反馈进行强化学习，以优化模型行为）、**DPO**（Direct Preference Optimization: 一种直接优化模型偏好的后训练方法）等后训练方法不就是为了修正模型错误吗？为何幻觉仍未消失，甚至在某些任务上更严重？

论文尖锐指出，问题不在于后训练技术，而是现有的评估机制在“鼓励幻觉”。主流模型评估基准几乎都采用**二元评分**（Binary Grading: 评估时只给予正确答案1分，错误或弃权0分的评分方式）。在10个最有影响力的评估基准中，9个是纯二元评分，例如**GPQA**（一种多选题评估基准）、**MMLU-Pro**（Massive Multitask Language Understanding-Pro: 一种多任务语言理解评估基准）、**SWE-bench**（一种代码修复评估基准）等，均没有“我不知道”的选项或给予弃权0分，而“蒙对”则得1分。只有**WildBench**（一种模型评估基准，会给“弃权”一些分数），但分数仍低于“有错误的合理答案”。

论文通过数学推导证明，在二元评分体系下，无论模型对答案的置信度多低，“猜测”都是最优选择。即使模型只有1%的把握，猜测的期望得分仍高于0分。因此，在二元评分的激励下，模型会优先选择“输出一个答案”，而非“承认不知道”，哪怕这个答案是编造的。

一个思想实验进一步说明：Model A诚实回答，从不幻觉；Model B则猜测并常幻觉。在现有评估基准上，Model B得分更高，因为它总能“蒙对”一部分Model A不知道的问题。厂商在优化模型时，只会看“基准得分”，而非“幻觉率”，导致模型在后训练时逐渐放弃“诚实的表达不确定性”，转而学习“如何更好地猜测”，最终幻觉率居高不下。

更严重的是，许多评估基准还使用“大模型作为裁判”来打分，例如**Omni-MATH**（一种数学解题过程评估基准）。论文指出，模型裁判常将“错误但冗长的解题过程”判为正确，因为“看起来很专业”，这进一步鼓励模型“编造详细的错误内容”。

研究者试图通过“增加专门的幻觉评估基准”来解决问题，但论文认为这效果不佳，因为现有主流评估的影响力远大于这些新基准。厂商仍会优先优化主流基准得分，导致“惩罚不确定性的流行病”持续。

### 解决之道：明确置信度目标与修改评估逻辑

论文没有提出复杂技术，而是给出两个简单却根本的建议：一是在评估中“明确**置信度目标**（Confidence Target: 明确告诉模型在多大置信度下才应回答问题）”，二是“修改主流评估的评分逻辑”。

论文建议，在评估提示词中明确告诉模型“何时回答、何时弃权”，例如“只有当你对答案的置信度超过90%时才回答，正确得1分，错误扣9分，回答‘我不知道’得0分”。通过计算期望得分，模型会自动在“置信度>90%”时回答，否则弃权，不再盲目猜测。论文提到了不同的置信度阈值（t=0.5, 0.75, 0.9）适用于不同应用场景，如医疗领域需高阈值以避免错误诊断。研究表明，在提示词中加入“错误扣2分”说明后，模型幻觉率下降30%，而正确回答率无明显下降，这说明模型有能力根据评分规则调整行为。

对于后训练，解决幻觉问题的关键不是新增“幻觉评估”，而是将“置信度目标”融入现有的主流评估基准。例如，**SWE-bench**可以修改评分规则为“提交正确补丁得1分，提交错误补丁扣2分，提交‘无法修复’得0分”；**MMLU-Pro**可增加“我不知道”选项并给予0.2分。修改主流评估基准至关重要，因为它们是厂商优化模型的“指挥棒”，直接影响模型的市场竞争力。论文还提到，人类考试如美国的**SAT**（Scholastic Assessment Test: 美国大学入学考试）、**GRE**（Graduate Record Examinations: 美国研究生入学考试）也曾经历类似改革，从“错误不扣分”改为“错误扣分”，引导学生在不确定时放弃猜测。

此外，论文提出了“**行为校准**（Behavioral Calibration: 确保模型在指定置信度阈值下输出答案，否则弃权的行为与实际置信度相符）”的概念，即要求模型“在置信度>t时输出答案，否则弃权”，而非输出概率值。通过对比不同阈值下的正确率和弃权率进行验证，未来这将成为大语言模型评估的重要指标。

### 论文局限与未来展望

尽管分析深刻，论文也承认现有框架存在一些局限：
1.  **忽略无意义内容**：论文只考虑“合理的错误内容”，未考虑“asdfghjkl”等无意义乱码。但现有模型预训练目标是生成合理语言，此局限对结论影响不大。
2.  **开放式生成讨论不足**：主要分析“事实性问题”，对“开放式生成”的幻觉讨论较少。开放式生成的幻觉程度更难量化，未来可能需建立“幻觉程度评分体系”。
3.  **检索增强生成（RAG）的局限**：许多人认为结合搜索引擎的**检索增强生成**（Retrieval-Augmented Generation, RAG: 结合检索系统，让模型在生成前先获取相关信息）能消除幻觉。但论文认为，RAG只能解决“训练数据里没有的事实”，无法解决“检索不到信息”的情况。只有结合“检索+置信度目标”，才能彻底解决问题。
4.  **潜在上下文歧义**：论文未考虑“用户潜在意图”的歧义问题，例如“电话坏了”可能指手机或座机。这种情况下，模型的错误是“误解”而非“幻觉”，现有框架无法区分。未来可能需将用户意图纳入误差分析，让模型学会“追问”。

总而言之，大语言模型的幻觉问题，很多时候并非单纯的技术问题，而是“人的问题”。我们设计了怎样的训练目标，便会得到怎样的模型；我们设计了怎样的评估规则，便会引导模型走向怎样的方向。解决幻觉，不仅需要优化模型，更需要优化我们对AI的“期望和评估方式”，以期获得一个“真正可信的助手”，而非一个“只会考满分的考生”。