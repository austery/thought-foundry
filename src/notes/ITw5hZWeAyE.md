---
author: 原子能
date: '2025-11-01'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=ITw5hZWeAyE
speaker: 原子能
tags:
  - cloud-outage
  - system-architecture
  - race-condition
  - microservices
  - fault-tolerance
title: AWS史上最长瘫痪：一场由低级代码错误引发的连锁反应与商业博弈
summary: 本文深入剖析了2025年AWS长达14小时32分钟的大规模瘫痪事件。从DynamoDB的竞态条件错误开始，逐步揭示了重试机制失效、微服务架构的复杂依赖以及单一区域部署带来的连锁反应。文章还探讨了云服务高容错承诺与实际商业成本之间的博弈，以及企业在追求技术完美与现实决策间的权衡。
insight: ''
draft: true
series: ''
category: technology
area: tech-insights
project:
  - systems-thinking
  - knowledge-pipeline
people: []
companies_orgs:
  - AWS
  - Google
  - Google Cloud
  - Microsoft
  - Azure
  - Netflix
products_models:
  - DynamoDB
  - EC2
  - Droplet WorkFlow Manager
  - Network Manager
  - NLB
  - DNS
  - ECS
  - EKS
  - IAM
  - S3
  - Lambda
  - Route53
  - MySQL
  - CDN
media_books:
  - 《让编程再次伟大#40》
  - 《让编程再次伟大#7》
status: evergreen
---
### AWS史上最长瘫痪：低级错误引发的连锁反应

2025年10月19日晚上11:48，正准备睡觉的**AWS**（Amazon Web Services: 亚马逊网络服务，全球领先的云计算平台）员工迎来噩耗：核心数据库产品**DynamoDB**（Amazon DynamoDB: 亚马逊提供的一种全托管的NoSQL数据库服务）出现连接故障。依赖它的上百个云服务产品也因此下线。故障源自一个很低级的代码逻辑错误，导致两个修改数据的命令前后冲突，这是教科书式的**竞态条件**（Race Condition: 多个进程或线程并发访问和修改共享数据时，最终结果取决于它们执行的精确时序，可能导致不可预测的错误）。虽然工程师们只用了3个小时就修复了这个bug，但灾难才刚刚开始。

作为AWS最引以为豪的产品之一，DynamoDB几乎是所有内部系统默认使用的数据库。因此，在这3个小时的宕机时间里，那些系统也累积了无数需要重试的任务。这其中包括了负责调度所有**EC2**（Amazon Elastic Compute Cloud: 亚马逊弹性计算云，提供可扩展的虚拟服务器实例）服务器资源的内部管理程序**Droplet WorkFlow Manager**（DWFM: Droplet 工作流管理器，AWS内部负责调度EC2服务器资源的管理程序）。20号凌晨2:25，在DynamoDB恢复的那一瞬间，DWFM就被堆积如山的待办事项所冲垮。调度中心挂掉，导致所有EC2服务器无法启动，这又是一次经典的**重试机制**（Retry Mechanism: 在分布式系统中，当操作失败时，自动重新尝试执行该操作的策略）设计出问题。Google表示“这个我很熟”。如此重要的程序，没有自动恢复机制，这也罢了；赶来修复的工程师，甚至没有一个可以参考的手动恢复流程。也就是说，在AWS的整个历史上，从来没有人考虑过“如果DWFM挂了要怎么处理”这个问题。这样看来，现场一边修复一边摸索的工程师们只用了3个小时就完成修复，已经是相当厉害了。

### 连锁故障的持续与蔓延

时间来到早上5:28，DWFM恢复，EC2也开始正常启动。事故终于告一段落了吗？好巧不巧，DWFM旁边还有一个负责掌控所有EC2网络功能的程序**Network Manager**（网络管理器: AWS内部负责掌控所有EC2网络功能的程序）。和DWFM相比，它累积了6个小时的待办事项。再加上刚刚大量成功重启的EC2发出的海量新请求，它的处理速度还不够待办的新增速度。所以工程师们只能再次撸起袖子，**SSH**（Secure Shell: 一种加密的网络协议，用于在不安全的网络上安全地执行命令）进去手动疏导。直到早上10:36，积压的待办事项才处理完，EC2的网络功能恢复正常。

而在云的另一个角落，在EC2重启的同一时间，负责分配网络流量的负载均衡**NLB**（Network Load Balancer: 网络负载均衡器，在OSI模型的第四层（传输层）运行，用于在多个目标之间分配网络流量）也开始为这些EC2服务器引流。但因为Network Manager卡住了，这些EC2服务器有的能联系上，有的联系不上。这就导致NLB的健康检查在红灯和绿灯中反复横跳。同时连锁反应，也导致NLB被**DNS**（Domain Name System: 域名系统，将域名转换为IP地址的服务）反复下架和上架。直到9:36，忍无可忍的工程师们只好关闭了NLB的健康检查，让它忽略那些没有回应的服务器，保持绿灯状态，让NLB可以暂时地正常工作。直到下午2:09，所有积压的EC2服务器恢复正常，NLB的健康检查重启，所有网络功能才恢复。2:20，最后一批受影响的产品：依赖最多最杂的容器服务**ECS**（Amazon Elastic Container Service: 亚马逊弹性容器服务，一种高度可扩展、高性能的容器管理服务）和**EKS**（Amazon Elastic Kubernetes Service: 亚马逊弹性Kubernetes服务，一种托管的Kubernetes服务，用于在AWS上运行容器化应用程序）恢复正常。此次故障事件正式结束。

从10月19日晚上11:48到第二天下午的2:20，总共14小时32分钟，这是有记录以来，云服务行业历时最长的一次大规模瘫痪。在我的粉丝群里，一半的人因为AWS宕机参与高强度的修复工作，煎熬了一整天；另一半的人则因为AWS宕机无法工作，爽爽地摸鱼了一整天，真是人间百态。

### DynamoDB竞态条件的深层剖析

和《让编程再次伟大#40》视频里分享的**Google Cloud**（谷歌云: 谷歌提供的云计算服务平台）瘫痪事故相比，AWS这次的草台程度其实没有那么严重。除了最开始的导火索，剩下的问题都是因为错综复杂的架构导致的连锁反应，以及体量过大导致的极端高负荷运行压力。巧合的是，这两次瘫痪事故的导火索都是入门级的代码错误。

本次AWS事故的源头，是DynamoDB系统里负责处理DNS的两个程序，它们分别是负责生成DNS记录的**规划器**（Planner: 在DynamoDB DNS处理中负责生成DNS记录的程序）和负责将新记录写入DNS系统的**执行器**（Enactor: 在DynamoDB DNS处理中负责将新记录写入DNS系统的程序）。为了保证容错，DynamoDB在三个**可用区**（Availability Zone: 云服务商在一个区域内物理隔离的多个数据中心，提供高可用性）部署了三个独立的执行器。每当规划器生成一个新记录，其中一个执行器就会拿走，写进DNS里。在写入之前，执行器会做一个前期检查，确保它手上的记录比现在DNS里的更新。而写入成功之后，执行器还会做一个清理动作，就是把DNS里比自己刚写入的记录更早的那些都抹掉。

于是在2025年10月19日晚上，出现了这么一个情况：执行器A首先从规划器那里拿到了新记录A，在它写入DNS的时候，不巧地遇到了延迟，停在中间了。此时规划器又生成了一个新记录B，执行器B拿到记录B，非常顺利地写进了DNS。就在执行器B准备执行收尾的清理动作时，执行器A的延迟结束了，于是执行器A继续执行自己的任务，把手上的记录A写进DNS，覆盖了记录B。紧接着执行器B开始清理，因为它在DNS里发现了比自己刚才写入的记录B时间更早的记录A，所以按照代码逻辑，执行器B把记录A抹掉。等执行器A和执行器B都完成任务之后，现在DNS里指向DynamoDB域名的就只剩一个空值了。而此时规划器生成了新记录C，执行器C拿到记录C，准备扮演救世主。然而很可惜，它倒在了前期检查上，因为现在DNS里没有记录，所以无法和自己手上的记录C做对比。做不了检查，就无法执行下一步的写入，整个逻辑进入死胡同。无论规划器之后再生成多少个新记录，也没有一个执行器能把它写进DNS里。DNS里DynamoDB的记录一直停留在空值上，也就是说，DynamoDB的域名从互联网上彻底消失了。找不到自家数据库的100多个AWS产品，自然也跟着瘫痪了。这个错误，也就比Google Cloud那次忘记写`try catch`导致**空指针异常**（NullPointerException: 在编程中，当尝试访问或修改一个空引用时发生的运行时错误）好一点点，虽然也不多。

在我看来，DynamoDB的这个问题，要背锅的是设计这个架构的人。这种“多个进程修改同一个数据”的场景，从基本的软件设计角度来说，就不应该把“保证数据完整性”这种任务放在进程身上。这种任务应该由数据的接收和储存方承担，在这里就是DNS系统**Route53**（Amazon Route 53: 亚马逊提供的一种可扩展的云域名系统Web服务）。因为数据在这里具有唯一性，也就是我们常说的**单一事实来源**（Single Point of Truth: 在信息系统中，确保每个数据元素只有一个权威来源，以避免数据不一致性）。让每个执行器都做事前和事后的检查和清理，不管你的代码有多严谨，都是不合理的决定。毕竟**ACID**（Atomicity, Consistency, Isolation, Durability: 数据库事务的四个基本特性，确保数据完整性）本来就是很难达成的。要不然**MySQL**（一种流行的开源关系型数据库管理系统）这种大热门的数据库，也就不会有那么多和**事务**（Transaction: 数据库操作的逻辑单元，要么全部成功，要么全部失败）相关的问题了。

### 云服务架构的现实与商业博弈

在制作本期视频时，微软的云服务**Azure**（Microsoft Azure: 微软提供的云计算服务平台）刚好也遇到了全球范围的宕机，这已经是Azure今年的第五次世界级故障了。那为啥它上新闻的次数就那么少呢？当然你可以说是因为它的大客户大多都是传统行业巨头，普通民众对于故障的感知不深。但如果你认真翻一下它的故障报告，就会发现它的问题要么是**CDN**（Content Delivery Network: 内容分发网络，通过在全球部署节点来加速内容传输）运维出错了，要么是被**DDoS**（Distributed Denial of Service: 分布式拒绝服务攻击，通过大量请求使目标服务器过载）了，要么是CDN被DDoS了，来来去去就只有那几个地方出错。很少会出现严重的连锁反应拖垮整个Azure体系。

而AWS的画风就完全不一样，就像我在很久以前的《让编程再次伟大#7》视频里面提到的，AWS这种崇尚**微服务架构**（Microservices Architecture: 一种软件开发方法，将应用程序构建为一组小型、独立的服务，每个服务运行在自己的进程中）的企业，只要能用现成的服务，就不会考虑自己造轮子。久而久之，每个服务都依赖每个服务，牵一发而动全身。本次事故出现的DynamoDB拖垮DWFM，DWFM拖垮所有EC2，EC2拖垮所有NLB的连锁反应，就完美体现了AWS内部系统同生共死的状态。而回顾AWS历史上的几次大规模瘫痪事故，我们可以看到，导火索基本都是DynamoDB、**S3**（Amazon Simple Storage Service: 亚马逊简易存储服务，提供对象存储服务）、**Lambda**（AWS Lambda: 亚马逊无服务器计算服务，允许在不预置或管理服务器的情况下运行代码）这些通用的储存和计算服务。因为大部分的AWS产品都需要储存、都需要计算，所以就着微服务架构的原则，它们都被串在一根绳上。

敏锐的观众可能会发现，包括这次事故在内，AWS每次出大事，好像总是发生在“**us-east-1**”（AWS US East (N. Virginia) Region: 亚马逊网络服务在美国东部（弗吉尼亚北部）的区域，是AWS最早且功能最丰富的区域之一）这个分区。这就让人产生了一个合理的疑问：云服务不是主打的**分布式部署**（Distributed Deployment: 将系统组件部署在多个物理位置或服务器上，以提高可用性和可伸缩性）、**高容错**（High Fault Tolerance: 系统在部分组件失效时仍能正常运行的能力）、兜底能力强吗？为什么一个分区出故障，就能够拖垮半个互联网呢？

虽然在市场营销时，云服务一直都是和分布式、高容错、稳定性等关键词挂钩，但这不代表你上了云，就能自动拥有这些好处。它只是给你提供了这么一个环境，让你可以更轻松地搭建高容错的系统架构，但你做不做，完全取决于你。就连AWS自己都没有彻底遵循这个原则，比如管理权限系统**IAM**（AWS Identity and Access Management: 亚马逊身份和访问管理，用于安全地管理对AWS服务和资源的访问），就只在us-east-1分区有写入节点，其余分区部署的都只是只读节点。当然AWS这些架构决定涉及到不少历史遗留问题，可以理解。

而对于大多数的企业来说，需要考虑的其实只有一个问题：你要不要投入额外成本进行多点部署？这不是一个技术问题，而是一个商业问题。因为从技术上来说，答案很明显，各种级别的容错方案都已经很成熟了。比如**多节点部署**（Multi-node: 在一个区域内使用多个服务器节点来运行服务，提高可用性）和**多区域部署**（Multi-region: 将服务部署在多个地理区域，提供更高的灾难恢复能力）。大多数云服务商的服务都达到了开箱即用的程度。就算是相对更复杂一点的**多云部署**（Multi-cloud: 同时使用多个云服务提供商的服务，避免供应商锁定和提高弹性），只要在架构设计和技术选型时多加注意，避免和云服务商进行绑定，做起来也不难。对有经验的架构师和**DevOps**（Development and Operations: 一种软件开发方法，强调开发和运维团队之间的协作与自动化）团队来说，只要企业愿意掏钱，搭建高容错的系统就是手到擒来的事情。

但问题就出在这个“钱”字：两个节点就是两倍的成本，两个区域就是四倍，两个云就是八倍。假设你现在是CEO，你的面前有一个按钮，摁下去能让你避免几年一次的持续几个小时的产品全线瘫痪，但坏处是你的成本会翻八倍，你会摁下去吗？这本质上其实是一个**博弈论**（Game Theory: 研究决策者之间互动行为的数学理论）问题。对一个企业来说，最怕的不是自家产品出问题，而是只有自家产品出问题。如果大家一起出问题，新闻里只会怪罪云服务商，你只是受害者。毕竟这种几年才会出现一次的事故，从大众的角度来说，已经属于**黑天鹅事件**（Black Swan Event: 指发生概率极低但一旦发生就影响巨大的事件）了。而即使你投入了八倍成本，做好了容错准备，逃过一劫，也没有人会表扬你。比如这次事件中毫发无损的**Netflix**（奈飞: 全球领先的流媒体娱乐服务公司），作为AWS最大的客户，是绝对的模范生，但`nobody cares`。所以当你从CEO的角度来考虑，你更关心的是用户、是股民的反应，那么你的最优解，就是和其他企业的做法绑定。作为AWS成立最早、功能上新最快、资源最丰富的分区，早期的企业基本上只在us-east-1上进行部署。后来者自然也会将它作为首选，甚至是唯一选择。结果就是所有人都在us-east-1上抱团，同生共死，完美的**纳什均衡**（Nash Equilibrium: 博弈论中的一个概念，指在给定其他参与者策略的情况下，每个参与者都无法通过单方面改变自己的策略来获得更好结果的状态）。

看到这里的观众可能会觉得，我是在狠狠地批判这些云服务商和用户，毕竟作为一个程序员，看到这些糟蹋技术的操作，怎么能不生气。但我其实并没有觉得他们的决定有什么问题。当然导火索的代码逻辑，和后面各种粗糙的重试机制，这一类低级技术错误还是要批判一下的。但从宏观上来看，不管是AWS内部各种产品的互相捆绑，还是用户扎堆在单一分区做部署，这些可能不是最科学的决定，但绝对是最现实的决定。我们技术人，可能想要追求技术上的完美，但世界不是围绕着技术转的。反过来，技术只是服务于世界的一个工具。归根到底，我们都只是工具人罢了。