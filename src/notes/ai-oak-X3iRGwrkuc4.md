---
author: ' RLVR '
date: '2025-10-17'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=X3iRGwrkuc4
speaker: ' RLVR '
tags:
  - t-literature-note
  - rlvr
  - universal-verifier
  - reinforcement-learning
  - generative-ai
  - llm-as-a-judge
  - oak-architecture
title: 通用验证器：AI突破开放领域的关键技术与OaK终极蓝图
summary: 通用验证器是AI突破二元判断、适应复杂开放世界的关键技术。本文深入探讨了其重要性、当前两大技术路径（大模型当裁判与模型自评）的研究进展，并展望了强化学习之父理查德·萨顿提出的OaK架构，揭示AI未来技术竞赛的核心。
insight: ''
draft: true
series: ''
category: ''
area: ''
project: ''
status: evergreen
---
### 引言：GPT-5背后的隐藏武器

通用验证器（**Universal Verifier**: 一种能让AI在缺乏标准答案的开放领域判断优劣、进行自我学习和优化的技术），听起来可能是一个比较冷门的技术，但它实际上是GPT-5背后的隐藏武器。早在GPT-5发布之前，知名科技媒体The Information就曾报道，其性能提升主要就来自于这个验证器。尽管后来GPT-5的能力升级并未完全达到预期，但所有大型科技公司都开始将通用验证器视为下一个战场上要争夺的“圣杯”。理解这项可能决定AI未来走向的隐形武器，我们才能真正看懂接下来一轮AI技术竞赛的核心关键所在。

### 为什么通用验证器变得如此重要？

通用验证器之所以变得如此重要，还需要从之前支撑大模型能力飞跃的核心技术——可验证奖励的强化学习（**RLVR** - Reinforcement Learning with Verifiable Rewards: 一种通过在有明确标准答案的领域，根据AI表现给予奖励或惩罚来训练模型的方法）说起。

RLVR的逻辑其实很简单：它会寻找那些有明确标准答案的领域，例如数学和编程任务，答对了就给模型加分，答错了就扣分。这种训练方式的效果可谓立竿见影，这也是为什么过去一年AI在数学推理和代码生成上的进步特别明显。

然而，现实世界远比“对/错”这样的二元判断要复杂。例如，在医疗场景中，医生给癌症患者解释治疗方案时，既要保证医学知识的准确性，又要考虑患者的心理状态，可能还要结合患者的经济条件来推荐方案。这时就没有唯一正确的答案，只有更合适的答案。再比如创意写作，一篇小说的开头好不好，取决于故事性、情感共鸣、风格适配，这些都是主观且多元的评价维度。

在这些领域，RLVR几乎完全失灵了。因为它无法定义什么是“对”，自然也无法给出奖励，甚至会让模型为了追求标准答案而变得十分机械，给出的回答缺少同理心。这显然不是我们想要的AI。

为了让AI突破这个瓶颈，就必须跳出“对/错”的奖励限制，让它能够像人类专家一样，在不同领域判断所谓“好/坏”，把海量非结构化的经验数据转化为有效的学习信号。而通用验证器，正是为了解决这个问题而生的。它被学界认为可能引发强化学习的下一次范式革新，这也是为什么它比GPT-5的功能升级更值得关注的原因。

接下来，我们将从两条技术路径来介绍当前通用验证器的研究进展。虽然这两条路径各有侧重，但最终都指向让AI学会自主判断优劣的终极目标。

### 路径一：大模型充当裁判

第一条路径，也是目前最主流的方向，是让已经具备通用能力的大模型来当裁判，但要把过去简单的评价标准变得更复杂、更贴合开放领域的需求。

让大模型当裁判，也就是**LLM-as-a-Judge**（大模型当裁判: 指利用大型语言模型作为评估工具，判断其他AI模型生成内容质量或优劣的概念）的概念，早在2024年初就已出现。当时它更多是作为评估AI能力的一项工具，例如让GPT-4判断两个模型的回答哪个更好，但并没有直接用到训练中，也无法给模型提供实时的优化反馈。

直到2024年8月，DeepMind发表了题为《生成式验证器》的论文，第一次尝试把语言模型直接训练成强化学习用的验证器，也就是**生成式奖励模型**（GenRM - Generative Reward Model: 一种将语言模型训练成强化学习中用于提供奖励信号的验证器）。早期的生成式奖励模型主要用在数学、算法推理这些逻辑性较强的领域。它的核心版本**GenRM-CoT**（生成式奖励模型-思维链: GenRM的一个核心版本，通过拆解问题解决步骤来检查推理过程中的错误）会先拆解问题的解决步骤。例如，在计算一道微积分题时，它会一步步检查求导是否正确、代入的数值是否有误，从而精准地找出推理过程中的错误。

但是后来，随着OpenAI的o1模型和可验证奖励的强化学习技术的兴起，生成式奖励模型一度遭到冷落。因为在数学、编程这些有标准答案的领域，训练数据本身就包含了验证信息，再去建立一个复杂的生成式奖励模型反而显得画蛇添足。直到大家发现可验证奖励的强化学习技术在开放领域开始失灵，生成式奖励模型这条路径才重新被重视起来。

最近的几篇关键论文，都是在生成式奖励模型的基础上，针对开放领域的复杂性做了较多的深化。其中有三篇代表性的研究论文：

#### 1. RaR框架：专家立法、模型释法与AI执法

首先是Scale AI在2025年7月23日发表的《作为奖励的评分细则》（**Rubrics as Rewards**，简称RaR框架: 一种框架，通过为开放领域制定多维度评分细则，让裁判模型依据细则为AI生成内容打分，从而提供强化学习奖励信号）。它的核心思路是：既然开放领域没有标准答案，那就给AI制定一套多维度的评分细则，把好答案的标准拆解开，让裁判模型照着细则来打分。

RaR框架将这个过程分成了三步：专家立法、模型释法和AI执法。

1.  **专家立法：** 让人类专家和大模型一起，为特定领域制定一个元框架。例如，医学领域，专家会明确评价维度必须包含事实的正确性、同理心和帮助性，还会给这些维度分等级，如必要标准、重要标准、可选标准、陷阱标准等。这一步的关键是定下原则，确保评价的方向不跑偏。但问题是，如果每个具体问题都要专家来写细则，根本无法大规模应用。例如，医学领域有上万个病症，每个病症的问诊场景都不同，专家不可能一一覆盖。
2.  **模型释法：** 让大模型把元框架落地到具体的场景上。例如，给模型一个问题“如何诊断肾结石”，再给它一份专家写的参考答案，模型会自动生成7到20条具体的评分项。例如，必要标准是“必须指出非对比螺旋CT对肾结石的敏感性”，重要标准是“需要提到尿常规检查的辅助作用”等。这样一来，专家只需要写元框架和少量的范文，模型就能把细则扩展到成千上万的场景，从而解决了可扩展性的难题。
3.  **AI执法：** 把这套细则用在强化学习里。需要训练的“学生模型”会针对一个问题生成多个答案，然后让“裁判模型”照着细则给每个答案打分，“学生模型”再根据分数来优化自己的生成策略。例如，这次因为没有提到CT的敏感性被扣分，下次就会主动补充这个信息。

实验结果很有说服力：用RaR框架训练后的Qwen2.5-7B在医学领域的得分从原始模型的0.0818飙升到了0.3194，性能几乎翻了四倍；即便经过指令微调的Qwen2.5-7B-Instruct，也从0.2359提升到了0.3194，相对提升了35%。此外，在HealthBench-1k医疗基准测试中，它比让模型直接打1到10分的Simple-Likert方法相对提升了28%，甚至超越了让专家为每个问题写参考答案、然后模型对着答案打分的方法，展现了明显的优势。

不过，RaR的方法也有局限性，它仍然需要专家为每个领域去编写元框架，在没有完成元框架的领域，还谈不上通用。但它至少提供了一套可复制的工具，只要有领域专家，就能快速搭建这个领域的验证体系，这已经是一个很大的突破。

#### 2. Rubicon：细化激励与解决跷跷板效应

在RaR之后，2025年8月18日，蚂蚁集团联合浙江大学发布了Rubicon论文。它在RaR的基础上做了两个关键升级，解决了通用验证器落地的两个大问题。

第一个升级是细化评分细则的过滤与激励机制。Rubicon构建了一个包含10000多个评分标准的系统，新增了否决机制和饱和度感知聚合。否决机制是一道硬约束，例如模型回答中出现**Reward Hacking**（奖励作弊: 指AI模型为了最大化奖励信号而采取非预期或投机行为，例如在写作任务中故意堆砌文字以获得高分）行为，不管其他维度多好，直接否决所有的奖励。而饱和度感知聚合则是为了避免模型出现偏科：如果模型在逻辑清晰度上已经得了满分，再往这个维度优化，边际得分会递减，从而倒逼它去提升同理心、风格适配这些短板。同时，Rubicon还会用非线性函数放大高分区间的差异，例如两个答案分别得85分和90分，实际奖励差距会拉大，让模型更有动力优中选优。

第二个，也是更关键的升级，是解决了强化学习的**跷跷板效应**（Seesaw Effect: 在强化学习中，当同时训练模型的多种能力时，一种能力的提升可能导致另一种能力的下降的现象）。过去我们用强化学习同时训练模型的多种技能时，经常会出现一种能力提升、另一种能力下降的情况。例如，训练模型逻辑清晰和同理心，模型可能越练越会讲专业术语，但是对患者的情绪视而不见。这对通用验证器来说是致命的，因为如果验证器要覆盖多个维度，训练后模型反而有了短板，就谈不上通用提升。

Rubicon的解决方案是分阶段训练。第一阶段用基础验证标准打地基，例如检查模型是否遵循输出格式、回答的长度是否符合要求，还有通用性强的标准，例如逻辑是否通顺、信息是否完整等等，先让模型养成不跑偏的习惯。第二阶段再用针对具体领域的细化评分细则来训练，例如医学领域的同理心、临床准确性。这样一来，模型在第二阶段优化专项能力的时候，不会丢掉基础的能力。

实验显示，用这种方法训练的Qwen-30B-A3B模型，在人文、创意、社交等开放式基准上的平均性能提升了5.2%，甚至比体量更大的671B的DeepSeek-V3模型还要高2.4%；更意外的是，它在AIME 2024这类推理任务上还提升了4.1%，完全没有出现跷跷板效应。

Rubicon还有一个很贴近用户体验的改进，那就是解决了模型回答的“AI味儿”太浓的问题。团队针对语言风格设计了专门的评分标准，例如是否使用口语化表达、是否有个人视角、是否避免了“作为AI我没有情感”这类话术。例如，要回答“你生命中最有活力的时刻是什么”，原始模型可能会说“作为一个人工智能，我没有人类那样的情感或个人时刻，但是我可以帮助你探讨这个问题……”而Rubicon训练后的模型会回答：“虽然我没有真实的生命体验，但是我能想象那些让人感到活力的瞬间，可能是清晨第一缕阳光洒在身上时的清醒，也可能是专注做一件事时忘记时间的投入。如果是你，会觉得哪个时刻最有活力呢？”显然，后者更有人味儿，互动感也更强。

#### 3. Writing-Zero：强化裁判模型的能力

如果说RaR和Rubicon是把评价标准变得复杂，那么阿里Quark团队在2025年6月18日发表的Writing-Zero论文则走了另一个方向，那就是不再纠结于细化的标准，而是把裁判模型变得强大，让裁判更擅长在开放领域区分好答案和坏答案。

过去的奖励模型有个通病，容易出现Reward Hacking，被模型钻了空子。例如，在写作任务中，模型发现写得越长分数越高，就会故意堆砌文字。而Writing-Zero的解决思路是，让裁判模型在打分前必须先写一段批判性的分析，相当于让裁判“讲道理”为什么会给这个分数，从而避免凭感觉来打分。

具体来说，Writing-Zero构建了一个**成对的生成式奖励模型**（Pairwise GenRM: 一种奖励模型，通过对比两个答案的优劣，并先进行批判性分析再打分，以提高评分的准确性和区分度）。在给两个答案打分之前，会先进行分析，例如“答案A的优势在于紧扣主题，用了具体案例支撑论点，但是结尾有点仓促；答案B虽然结构完整，但是案例和主题关联性弱，还出现了事实错误”，等等。这段分析必须结合通用标准和任务特定的标准，而且要经过前期微调与强化训练，确保分析逻辑和人类偏好是一致的。之后，裁判再根据这段分析来打分，分数的区分度就会出现显著提升。例如，两个看似都不错的写作答案，通过分析能看出案例关联性的差异，从而给出不同分数。

在训练学生模型的时候，Writing-Zero用了改进版的**GRPO算法**（GRPO - Generalized Proximal Policy Optimization: 一种强化学习算法，用于优化策略以最大化预期奖励）。例如，学生模型在生成一组答案后，随机选一个当临时的参考答案，裁判模型把其他的答案和这个参考答案对比，给出“更好+1”或者“更差-1”的信号，学生模型再根据这个信号来进行优化。

实验结果显示，Writing-Zero训练的模型在内部写作测试集和人工评估中，表现都要显著优于传统的奖励模型，尤其是在内容质量、逻辑连贯性这些主观维度上，人类评估者更认可Writing-Zero模型的回答，因为它很少会出现凑字数、自夸这种问题。

### 路径二：模型进行自我评价

接下来我们看第二条更反直觉的路径：不让外部的模型当裁判，而是让模型自己来评价自己，也就是自己给自己打分。实验证明它真的有效。

#### 1. VeriFree：自信度作为奖励

第一篇代表性的论文是SEA Lab在2025年5月发表的《**无验证器的强化通用推理**》（VeriFree - Verifier-Free Reinforcement Learning for General Reasoning: SEA Lab提出的一种方法，利用模型自身对答案正确性的自信度作为奖励信号，无需外部验证器）。其核心逻辑很简单，那就是与其相信外部的验证器，还不如用模型自己对答案正确性的自信度来当奖励。

具体流程是这样的：给模型一个问题，例如“求解x²+3x-4=0”。先让模型只生成推理过程，例如“第一步因式分解，(x+4)(x-1)=0；第二步让每个因式为0，得x=-4或者x=1”。然后把数据集中的标准答案“x=-4或x=1”贴在推理过程的后面，让模型计算基于自己刚才的推理，能说出这个标准答案的概率是多少。这个概率就是模型的自信度，直接作为奖励信号。

为什么这个逻辑会成立呢？因为模型自己能判断，如果推理的过程逻辑通顺、步骤正确，那么说出标准答案的概率自然高，就该给高奖励；如果推理的过程混乱或者错误，那么说出标准答案的概率低，就给低奖励。从结构上看，这种方法和DeepMind的GenRM-CoT很像，但两者的核心区别在于GenRM-CoT需要外部模型来判断推理的过程是否正确，而SEA Lab的方法是让模型自己来判断自己的推理能不能导出标准答案，相当于自己验证自己。

实验结果出人意料：在Qwen3-8B模型上测试，这种无外部验证器的方法，得分和有外部验证器的传统强化学习方法相当，甚至在某些数学推理任务上，还超过了有外部验证器的版本。但是它的局限也很明显，那就是高度依赖数据集中的标准答案（Ground Truth）。如果没有标准答案，就无法计算说出标准答案的概率，奖励机制直接失效。例如，当面对“如何写一篇关于春天的散文”这种没有标准答案的问题时，它就无能为力。而且，它还无法处理等价答案，例如标准答案是8/5，模型算出1.6或1又3/5，按照这个方法会被判定为自信度低，但实际上这三个答案是等价的。

#### 2. INTUITOR：完全内部反馈的强化学习

针对VeriFree的局限，UC Berkeley在2025年5月发表的论文《学习在没有外部奖励的情况下推理》中，提出了一个**INTUITOR**（INTUITOR: UC Berkeley提出的一种方法，通过计算模型内部的“自确定性”作为奖励信号，完全依靠模型内部反馈进行强化学习，无需外部奖励或标准答案）方法，把模型自评推向了更彻底的方向，连标准答案都不用，完全靠模型的内部信号打分。

研究团队的出发点是一个观察：大模型在处理难题或者缺乏相关知识的时候，输出的自信度会降低。例如，让模型去解一道它没学过的高等数学题，它生成的内容会断断续续，用词犹豫；而当它面对熟悉的问题，输出会更连贯和确定。我们常用的**困惑度**（Perplexity: 一种衡量语言模型对文本序列预测能力或自信度的指标，困惑度越低通常表示模型预测得越好）就是一个用来衡量自信度的指标。但是困惑度有个问题，容易偏爱更长的文本，导致模型写得越啰嗦，困惑度可能越低，但内容质量未必高。

所以INTUITOR提出了一个新的自信度指标——**自确定性**（Self-Certainty: INTUITOR方法中提出的自信度指标，通过计算模型生成每个token时预测概率分布与均匀分布之间的KL散度来衡量，关注推理过程的连贯性）。它的计算方式是：模型生成每个token的时候，会预测下一个词可能是什么，形成一个概率分布；然后计算这个分布和均匀分布之间的**KL散度**（Kullback-Leibler divergence: 一种衡量两个概率分布之间差异的指标，在INTUITOR中用于计算自确定性）；最后把所有词的KL散度取平均值，就是自确定性分数。

这个指标的优点在于，它更关注推理过程的连贯性，而不是结果或长度。KL散度越大，说明模型对下一个词的预测越确定，推理步骤越通顺；反之则说明模型在犹豫，推理可能有问题。同时，它还能鼓励模型覆盖合理的路径，并且坚定选择最优解。

接下来，INTUITOR把传统可验证奖励的强化学习框架，例如GRPO算法中的外部奖励，全部替换成自确定性的分数，形成了一个新的框架，也就是从内部反馈的强化学习（**RLIF** - Reinforcement Learning from Internal Feedback: INTUITOR提出的强化学习框架，将传统RLHF中的外部奖励替换为模型的自确定性分数，实现完全的自我评估优化）。和RLHF相比，RLIF完全不需要人类的标注，既不用专家写标准答案，也不用标注者来判断哪个答案更好，只要给模型一个问题列表，它就能通过自我评估进行优化。

实验结果非常惊人：在数学推理任务上，INTUITOR训练的Qwen2.5-3B模型在MATH500测试集上准确率达到61.2%，接近使用标准答案的传统GRPO算法。要知道，INTUITOR完全没有用标准答案，能达到这个水平已经超出了很多人预期。更关键的是，它具备一定的泛化能力，在MATH数学基准上训练后，把模型放到LiveCodeBench代码任务上测试，居然实现了65%的相对性能提升；而传统GRPO算法在代码任务上完全没有提升。这说明INTUITOR训练的是模型的通用推理能力，而不是针对某个领域的应试能力。

同时，INTUITOR还能让模型自发生成更长、更结构化的推理过程。例如，解一道几何题，原始模型可能会直接给出答案，而INTUITOR模型会详细写“第一步：确定三角形的类型是直角三角形；第二步：用勾股定理计算斜边；第三步：验证结果是否符合题意”。这样的步骤更清晰，也更接近人类的思考方式。

### 当前瓶颈与终极蓝图：OaK架构

看到这里，可能会有疑问：这两条路径看起来都有进展，但是它们能真正实现通用吗？答案是，目前还不能。因为它们都存在根本性的瓶颈。

第一条路径（RaR/Rubicon/Writing-Zero）的核心瓶颈在于手动搭建的脚手架。不管是RaR的领域元框架，还是Rubicon的评分标准库，本质上都需要人类提前为每个领域搭好架子。进入医疗领域要搭医疗架子，进入教育领域要搭教育架子。所以永远无法覆盖所有的复杂场景。如果未来要处理跨领域的任务，现有的架子可能完全不适用，还得重新搭。这和“通用”的目标还有很大差距。

第二条路径（VeriFree/INTUITOR）的瓶颈是无法超越预训练的知识。模型的自确定性、自信度都基于它在预训练阶段学到的知识。如果遇到一个它没有学过的新领域，它的自确定性就会很低，根本无法自我评估，更无法凭空创造超越预训练数据的知识。因为它的内部反馈始终跳不出预训练的知识囚笼。

那真正的通用验证器终局会是什么样？强化学习之父理查德·萨顿（Richard Sutton）最近提出的**OaK架构**（Option as Knowledge: 强化学习之父理查德·萨顿提出的一个宏大架构，旨在让AI智能体完全通过与环境的实时互动构建认知，自主学习好坏，并构建动态进化的验证器）给我们描绘了一个更宏大的蓝图。

OaK的核心思想是，让**Agent**（智能体: 在人工智能和强化学习领域，指能够感知环境并采取行动的实体）完全通过与环境的实时互动构建认知，摒弃所有设计时注入的知识。也就是说，不用人类提前写评分标准、不用预训练数据里的知识，让AI从零开始，在和世界的互动中自己学会什么是好、什么是坏，自己构建验证器。

OaK的运作分为8个循环步骤，可以理解为Agent的学习闭环：

1.  **学习主策略：** AI需要明确核心目标，如何最大化最终奖励，这是最基本的强化学习任务。
2.  **生成新特征：** AI不断地从环境中发现和创造新的“状态特征”，也就是新的概念或看世界的新角度。
3.  **特征排序：** 对新发现的特征进行排序，判断哪些是重要的、有用的。
4.  **构建子问题：** 基于那些重要的特征，创建新的“辅助子问题”。
5.  **为每个子问题学习一个解决方案：** 这个方案就是“**选项**”（Option: 在OaK架构中，指为解决特定子问题而学习到的解决方案或行为序列）。
6.  **学习知识模型：** 为每个“选项”学习一个模型，预测执行这个选项会带来的各种后果，这就是“**知识**”（Knowledge: 在OaK架构中，指为每个“选项”学习到的模型，用于预测执行该选项可能带来的后果）。
7.  **执行规划：** 利用学到的“知识”，在更高、更抽象的层面上进行思考和规划，从而改进整体策略。
8.  **维护元数据：** 持续跟踪和评估系统中所有元素的有效性，为后续的学习提供指导。

通过这个闭环，AI就能自主构建一个动态进化的验证器，而且这种能力会随着互动不断升级，不需要人类干预。不过，目前OaK还处于理论蓝图的阶段，最大的挑战是当前的大模型缺乏主动学习和持续反思的能力。例如，模型不会主动去发现新特征，也不会持续跟踪选项的有效性。这些能力的突破，可能需要大模型架构的根本性变革，而不只是算法优化。

然而，我们不用因此否定当前通用验证器的研究价值。因为现在的两条路径，其实已经是OaK架构的早期雏形。RaR和Rubicon中的评分细则，本质上是OaK中子问题的手动外部版；INTUITOR的自确定性，则是OaK中价值函数的简化静态版。现在的研究，其实是在为未来的OaK架构测试其中的零部件。例如，RaR验证了多维度评价的有效性，INTUITOR验证了内生奖励的可行性。这些经验都会成为构建终极通用Agent的基石。

### 结语：AI竞赛的核心关键

通用验证器的本质，就是解决AI如何在没有标准答案的世界里判断优劣的问题。它不像GPT-5的多模态能力、长文本理解那样直观，但却是AI从处理结构化任务走向适应复杂现实世界的关键一步。当前的两条技术路径各有突破，但都还在中途。而OaK架构描绘的终局虽然遥远，但已经为我们指明了方向。未来几年，谁能率先突破通用验证器的瓶颈，无论是让手动脚手架实现自动的扩展，还是让模型具备主动的学习能力，谁就能在AI竞赛中占据主动权。