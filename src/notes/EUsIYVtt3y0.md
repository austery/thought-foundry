---
author: 硅谷101
date: 2025-10-31
guest: ''
layout: post.njk
source: 'https://www.youtube.com/watch?v=EUsIYVtt3y0'
speaker: 硅谷101
tags:
  - ai-model-evaluation
  - lm-arena
  - benchmark-testing
  - ai-competition
  - dynamic-evaluation
title: LMArena：AI大模型评估的竞技场与未来挑战
summary: 当前AI大模型竞争激烈，传统基准测试因题库泄露和无法反映真实交互而面临局限。LMArena作为一种创新的匿名对战与动态评分平台，通过真实用户投票重新定义了模型评估方式。然而，LMArena也面临公平性、数据偏差、过拟合及商业化等质疑。文章探讨了LMArena的运作机制、挑战，并展望了未来AI评估体系将融合静态基准与动态实战，并强调高质量专家数据和强化学习环境的重要性，以应对模型能力螺旋式上升的挑战。
insight: ''
draft: true
series: ''
category: technology
area: society-systems
project:
  - ai-impact-analysis
  - systems-thinking
people:
  - 朱邦华
  - Wei-Lin Chiang
  - Lianmin Zheng
  - Andrej Karpathy
  - 姚顺雨
companies_orgs:
  - OpenAI
  - Anthropic
  - Google
  - DeepSeek
  - Meta
  - 腾讯
  - 阿里巴巴
  - LMSYS团队
  - 加州大学伯克利
  - 斯坦福大学
  - 英伟达
  - Cohere
  - Arena Intelligence Inc.
  - a16z
  - UC Investments
  - Lightspeed
  - Nof1
products_models:
  - GPT
  - Claude
  - Gemini
  - Grok
  - Nano Banana
  - Gemini 3.0 Pro
  - Gemini 3.0 Flash
  - Vicuna
  - Alpaca
  - GPT-3.5
  - GPT-4
  - GPT-4o
  - Mistral
  - Llama 4 Maverick
media_books:
  - X
  - r/LocalLlama
  - 《The Second Half》
status: evergreen
---
### AI大模型评估：从主观争议到LMArena的崛起

在大模型激战的当下，究竟谁更强？是OpenAI的GPT，还是Anthropic的Claude？是谷歌的Gemini，还是中国的DeepSeek？当AI模型排行榜开始被各种刷分作弊之后，谁家大模型最牛的问题就变得非常主观。直到一家线上排行榜的诞生，它叫做**LMArena**（Large Model Arena: 一个用于匿名对战和动态评估大型AI模型的在线平台）。或许你还记得不久之前爆火的谷歌最新文生图模型**Nano Banana**（谷歌早期推出的一个神秘代号的文生图模型），它其实最早以神秘代号出现，并且引发破圈式关注的地方就是LMArena。

最近网友们又发现，谷歌故伎重施，传闻已久的**Gemini 3.0**（Google开发的一系列多模态大型语言模型）被发现已经出现在了LMArena上。根据网友们的测试反馈，Gemini 3.0 Pro的代号应该是**lithiumflow**，而Gemini 3.0 Flash是**orionmist**。据说它们能够读表、能作曲和演奏，能力再一次全方位飞升。你可能会在对战中随机地遇到它们，如果遇到的话，记得一定要抓住机会好好地拷问拷问。不难看出，在正式发布新模型之前，让它们在LMArena上面跑一跑，似乎就已经成为了谷歌的惯例操作。

实际上，各家模型其实早就已经把LMArena当做了常规赛场，用来测试普通用户最真实的反馈。除了Google，OpenAI、Anthropic、Meta的Llama、DeepSeek、腾讯的混元、阿里巴巴的千问，几乎所有的头部模型都在LMArena上打擂台。在文字、视觉、搜索、文生图、文生视频等不同的AI大模型细分领域，LMArena上面每天都有上千场的实时对战，由普通用户来匿名投票选出哪一方的回答更好。

最近以来，很多AI研究者都纷纷发声，认为大模型竞赛的下半场，最重要的事情之一就是重新地思考模型评估。因为当技术创新趋于饱和，真正拉开差距的可能将不再是谁的参数更多、推理更快，而是谁能更准确地衡量、理解模型的智能边界。而如LMArena这样的平台，正是当前这一领域中最前沿、也是最具实践意义的探索之一。它就像一座永不打烊的AI竞技场，真实的用户、真实的投票，并且排名动态更新，用一场场真实的人机交锋，试图重新定义我们衡量模型的方式。

那么在大模型评测上，传统的**Benchmark**（基准测试: 一组预设的、标准化的测试题库，用于评估AI模型的性能）究竟存在什么样的问题？是已经过时了吗？LMArena的竞技场模式为什么会被视为一种新的标准？它的技术机制、公平性和商业化隐藏着什么样的挑战？而下一代的大模型评测又可能会走向哪里呢？

### 传统基准测试的局限性

在LMArena之前，AI大模型是怎么被评估的呢？方法其实非常传统。研究者们通常会准备一组固定的题库，比如说**MMLU**（Massive Multitask Language Understanding: 大规模多任务语言理解，一个涵盖多学科知识的AI模型评估基准）、**BIG-Bench**（一个侧重于AI模型推理和创造力的评估基准）、**HellaSwag**（一个专门测试AI模型对日常情境理解能力的评估基准）等等。这些名字普通人看起来很陌生，但是在AI学术界几乎是家喻户晓的。这些题库涵盖学科、语言、常识推理等多个维度，通过让不同模型作答，再根据答对率或者得分来对模型进行比较。

比如说MMLU，它涵盖了从高中到博士级别的57个知识领域，包括历史、医学、法律、数学、哲学等等。模型既需要回答像“神经网络中的梯度消失问题如何解决”这样的技术问题，也需要回答“美国宪法第十四修正案的核心内容是什么”这样的社会科学问题，学科跨度很大。而BIG-Bench更偏向推理和创造力，比如说让模型解释冷笑话、继续写诗或者完成逻辑填空。HellaSwag则专门用来测试模型对日常情境的理解能力，比如说“一个人正在打开冰箱，接下来最可能会发生什么”等等。

这些Benchmark在过去二十年几乎主导了整个AI的研究领域。它们的优点显而易见，就是标准统一、结果可复现。学术论文只要能够在相关公开数据集上刷新分数，就意味着性能更强。而AI的上半场也正是在这种比成绩的节奏下高速发展起来的。但是这些早期的Benchmark是**静态基准**（Static Benchmark: 指使用固定题库进行评估的传统基准测试），多以单轮问答、选择题形式为主，题目结构简单，评测维度明确，便于统一打分和横向的比较。

然而，当模型的能力越来越强，训练数据越来越庞大的时候，这些Benchmark的局限就开始显现了。首先是所谓的题库泄露，很多测试题早就出现在模型的训练语料当中。于是，一个模型在这些测试上得分再高，也不代表它真的理解了问题，只能说明它记住了答案。其次，Benchmark永远测不出模型在真实交互中的表现，它更像是一场封闭的考试，而不是一次开放的对话。

华盛顿大学助理教授、英伟达首席研究科学家，同时也是LMArena早期框架搭建的参与者朱邦华在接受采访时就表示，正是因为传统的静态Benchmark所存在的**过拟合**（Overfit: 模型在训练数据上表现良好，但在未见过的新数据上表现不佳的现象）、数据污染等问题，才催生出了Arena这种新的模型测评方式。他指出，静态基准存在几个问题：一是大家非常容易过拟合，因为问题数量少且有**标准答案**（Ground Truth: 在机器学习和AI评估中，指被认为是正确或真实的数据标签或结果），如果模型在标准答案上训练，很难被**污染检测方式**（Contamination Detection Method: 用于检测模型训练数据是否包含测试集数据的技术）完全检测出来。二是**覆盖面**（Coverage: 评估基准所能涵盖的问题类型或知识领域的广度）不足，只涵盖了最简单的数学、知识和代码生成。在这种情况下，LMArena作为一个非常独特的Benchmark出现了，因为它每一个问题都是**独特**（unique）的，可能来自世界各地的人，随时随地想出的问题，这使得模型很难在当时进行过拟合。

### LMArena的运作机制与创新

LMArena的雏形诞生于2023年5月，由加州大学伯克利的LMSYS团队核心成员Wei-Lin Chiang、Lianmin Zheng等人搭建。当时他们刚刚发布了开源模型**Vicuna**（一个基于ChatGPT数据蒸馏而来的开源大型语言模型），而斯坦福大学在此之前也推出了另外一个类似的，叫做**Alpaca**（斯坦福大学推出的一个类似Vicuna的开源大型语言模型）。因为这两个模型都是从ChatGPT的数据中蒸馏得到的，于是LMSYS的团队就想知道，从性能和表现上来看，究竟谁更胜一筹呢？当时并没有合适的测评方法能够回答这个问题。

LMSYS团队尝试了两种方法。第一种是尝试让**GPT-3.5**（OpenAI开发的一款大型语言模型）作为评委，对不同模型生成的答案打0-10分。这种方法后来演化成为了**MT-Bench**（Model-Test Benchmark: 一种由AI模型作为评委对其他模型答案进行打分的评估方法）。另外一种方式是采用**人类比较**（Pairwise Comparison: 随机选择两个模型，让人类评估并选择哪个答案更好的评估方法），即随机挑选两个模型，针对同一个问题分别生成答案，再让人类去评审，选择哪个更好。最终第二种方式被证明更可靠，并由此诞生了Arena的核心机制。

基于此，他们首先搭建了一个实验性网站Chatbot Arena，也就是今天的LMArena的前身。在传统的基准测试里，模型是在预设题库中答题，而在Chatbot Arena上，它们则要上场打擂台。当用户输入一个问题之后，系统会随机分配两个模型，比如说GPT-4和Claude，但是用户并不知道自己面对的是谁。两边的模型几乎同时生成回答，用户只需要投票左边好还是右边好。等投票完成之后，系统才会揭示它们的真实身份。这个过程被称作“匿名对战”。投票结束之后，系统会基于**Bradley–Terry模型**（一种用于分析配对比较数据的统计模型，常用于评估竞争者之间的相对实力）实现**Elo式评分机制**（Elo Rating System: 一种根据对战胜负动态调整选手或模型排名的评分系统，最初用于国际象棋）。分数会根据胜负实时变化，从而形成一个动态的排行榜。这种机制的妙处在于，它让测评变成了一场“真实世界的动态实验”，而不再是一次性的闭卷考试。

除此之外，LMArena不仅仅是“让模型打架”，它背后还有一个独特的“人机协同评估框架”。这个框架的逻辑是：用人类的投票去捕捉“**真实偏好**”（Human Preference: 用户在AI模型输出中表现出的喜好，常用于评估模型质量），再通过算法去保证“统计公平”。平台会自动平衡模型的出场频率、任务类型和样本分类，防止某个模型因为曝光量大而被高估。换句话说，它让评测既开放又可控。更重要的是，Chatbot Arena的所有数据和算法都是开源的，任何人都可以复现或者分析结果。

作为LMArena早期搭建的核心参与者，朱邦华告诉我们，LMArena的技术本身并不是新算法，更多的是经典统计方法的工程化实现。它的创新点不在于模型本身，而在于系统架构与调度机制。他解释说，虽然Bradley–Terry模型本身没有太多技术上的新东西，但如何选择模型进行比较是其创新点之一。在有大量模型需要评估时，需要**主动学习**（Active Learning: 一种机器学习策略，模型能够选择性地请求人类标注它“最不确定”的数据，以提高学习效率）来动态选择模型进行比较，以达到**最佳**（optimal）的**排名**（ranking）效果。这涉及到一系列的**系列研究**（series studies）和**实验性研究**（experimental research）来调整参数。朱邦华认为，LMArena的成功也包含时机和运气的成分，因为当时市场急需一个好的**评估基准**（evaluation benchmark），而**人类偏好**（human preference）尚未被**饱和**（saturated），能够真实反映模型能力。因此，LMArena作为行业的**黄金基准**（Gold Benchmark: 在特定领域被广泛接受和信任的最高标准评估基准）是顺理成章的。

### LMArena的普及与挑战

LMArena这种“匿名对战 + 动态评分”的方式被认为是从静态Benchmark向动态评测的一次跃迁。它不再追求一个最终的分数，而是让评测变成一场持续发生的“真实世界实验”。它就像是一个实时运行的AI智能观测站，在这里，模型的优劣不再由研究者定义，而是由成千上万的用户选择来共同决定。

2023年12月底，前特斯拉AI总监、OpenAI的早期成员Andrej Karpathy在**X**（Twitter: 一个社交媒体平台）上发了一条关于LMArena的推文，称目前他只信任两个**LLM**（Large Language Model: 大型语言模型，指参数量巨大、在海量文本数据上训练的AI模型）的测评方式：Chatbot Arena和**r/LocalLlama**（Reddit社区: 一个专注于本地运行大型语言模型的Reddit社区）。这给Chatbot Arena社区带来了第一批“流量”。2023年年底到2024年年初，随着GPT-4、Claude、Gemini、Mistral、DeepSeek等模型的陆续接入Chatbot Arena，平台的访问量迅速增长。研究者、开发者、甚至普通用户都在这里观察模型的“真实表现”。

到了2024年年底，平台的功能和评测任务开始扩展。除了语言模型的对话任务，团队还逐渐涉及到了大模型的“细分赛道”，陆续上线了专注代码生成的Code Arena、专注搜索评估的Search Arena、专注多模态图像理解的Image Arena等子平台。为了体现测评范围的扩展，平台也在2025年1月从Chatbot Arena更名为LMArena，即Large Model Arena。几个月前，谷歌的Nano Banana的爆火也让更多普通用户关注到了LMArena。至此，LMArena从一个研究者间的小众项目，彻底成为了AI圈乃至公众视野中的“大模型竞技舞台”。

LMArena的火爆让它几乎成为了大模型评测的非官方标准。但是和所有新的实验一样，随着光环越来越大，它也受到了越来越多的质疑。首先是公平性问题。在LMArena的匿名对战机制中，用户的投票结果直接决定模型的Elo排名。然而，这种人类评判的方式并不总是中立的。不同的语言背景、文化偏好，甚至是个人使用习惯，都会影响投票结果。一些研究发现，用户更倾向于选择“语气自然”、“回答冗长”的模型，而不一定是逻辑最严谨、信息最准确的那一个。这意味着模型可能因为“讨人喜欢”而获胜，而非真的更聪明。

2025年年初，来自Cohere、斯坦福大学以及多家研究机构的团队联合发布了一篇研究论文，系统地分析了LMArena的投票机制与数据分析。研究指出，Arena的结果与传统Benchmark分数之间并非强相关，而且存在“话题偏差”与“地区偏差”。也就是说，不同类型的问题或不同用户群体的投票，可能会显著改变模型的排名。

此外，还有“游戏化”和“过拟合”的问题。当LMArena的排名被广泛引用，甚至被媒体视为模型能力的“权威榜单”时，一些公司开始为“上榜”专门优化模型的回答风格。比如说更积极地去使用模糊语气、提升数字密度，或者在**提示工程**（Prompt Engineering: 设计和优化给AI模型的指令，以引导其生成期望的输出）上精细调校，以希望“赢得投票”。Cohere的那篇研究论文就明确地指出，大型供应商在获取用户数据方面拥有明显的优势。通过**API**（Application Programming Interface: 应用程序编程接口，允许不同软件系统之间进行通信和数据交换）接口，他们能够收集到大量的用户与模型交互的数据，包括提示和偏好设置。然而这些数据并没有被公平地共享。62.8%的所有数据流向了特定的模型提供商，比如说Google和OpenAI的模型就分别获得了Arena上大约19.1%和20.2%的全部用户对战数据，而其他83个开源模型的总数据占比仅为29.7%。这使得专用模型供应商能够利用更多的数据进行优化，甚至可能针对LMArena平台进行专门优化，导致过度拟合特定指标，从而提升排名。

一个典型的例子就是Meta的刷榜事件。2025年4月，Meta在LMArena上提交的Llama 4 Maverick模型版本，其表现超越了GPT-4o和Claude，跃居了整个排行榜的第二。但随着Llama 4大模型开源版的上线，开发者们发现它的真实效果表现其实并不好。因此，质疑Meta疑似给LMArena提供了经过专门针对投票机制优化的“专供版”模型，导致了Llama 4的口碑急转直下。舆论爆发之后，LMArena官方更新了排行榜政策，要求厂商披露模型版本与配置，以确保未来评估的公平性和可重复性，并把公开的Hugging Face版本的Llama 4 Maverick加入了排行榜进行重新评估。但事件依然在当时引发了业内关于评测公平性的激烈讨论。

除了系统和技术上的挑战，LMArena的商业化也让它的中立性受到质疑。2025年5月，LMArena背后的团队正式注册公司Arena Intelligence Inc.，并且宣布完成了1亿美元的种子轮融资，投资方就包括了a16z、UC Investments和Lightspeed等。这也意味着LMArena正式从一个开源研究项目转变为了具备商业化运营能力的企业。公司化之后，平台就可能会开始探索数据分析、定制化评测和企业级报告等商业服务。那么这一转变也开始让业界担忧，当资本介入、用户需求与市场压力叠加的时候，LMArena是否还能保持最初的“开放”与“中立”？它的角色是否会从“裁判”变成“利益相关方”呢？

### 大模型评估的未来：融合与螺旋式共演

在LMArena之后，大模型评测似乎就进入到了一个新的拐点。它解决了过去Benchmark静态、封闭的问题，但是却也暴露出新的矛盾，那就是当评测数据、用户偏好、甚至投票机制都可能会成为商业竞争的一部分时，我们该如何去界定“公平”这两个字呢？那么究竟什么样的模型评估方式才是当前所需要的呢？

实际上LMArena的出现并不意味着传统的Benchmark已经过时。在它之外，静态的Benchmark依然在持续地演化。最近几年来，基于传统的Benchmark，研究者们陆续推出了难度更高的版本，比如说MMLU Pro、BIG-Bench-Hard等等。此外，一些全新的聚焦于细分领域的Benchmark也在不断地被创造出来，比如说数学与逻辑领域的AIME 2025、编程领域的SWE-Bench、多智能体领域的AgentBench等等。这些新的Benchmark不再只是“考知识”，而是在模拟模型在真实世界中的工作方式，从过去单一的考试题集演化为了一个庞大而多层次的体系。有的评推理，有的测代码，有的考记忆与交互。

与此同时，评测也在进一步走向真实世界。比如说最近一家名为Alpha Arena的新平台就引发了大量的关注。它由创业公司Nof1推出，在首轮活动中，平台选取了Deepseek、Gemini、GPT、Claude、Grok和千问等六大模型，在真实的加密货币交易市场中进行对战。它给了每个模型相同的资金和**指令**（Prompt: 给AI模型的输入或问题），让它们独立决策和交易，最终以实际收益和策略稳定性作为评测依据。结果是：DeepSeek竟然赢了，不愧是量化基金母公司下面做出来的AI模型。虽然这个对战更多是“噱头”为主，大语言模型去预测股市现在还是非常不靠谱的，但是Alpha Arena的这种“实战式测评”是再一次跳出了传统的题库和问答框架，让模型在动态对抗的环境中被检验，被视为是继LMArena之后，再一次尝试让AI在开放世界中接受考验的实验。不过Alpha Arena更偏向特定任务领域的真实验证，其结果也更难复现和量化。

实际上，这些Arena出现的意义也并非是要取代静态的Benchmark，而是为这个体系提供一面镜子，试图把静态测试中难以衡量的人类偏好和语义细节重新引入到评测系统当中。也就是说，未来的模型评估不再是静态Benchmark和Arena之间的二选一，而更可能是一种融合式的评测框架。那么静态Benchmark负责提供可复现、可量化的标准，而Arena负责提供动态开放、面向真实交互的验证。两者结合，进而构成衡量智能的完整坐标系。

那么在这样的一个评估体系中，目前最重要也最具挑战的部分是什么呢？朱邦华认为，随着大模型能力提升，原有测试集“太简单”的问题愈发突出。Arena的**难度过滤版**（Hard Filter Version: LMArena中根据指令难度进行筛选的版本）提出了阶段性解决方案。但是真正的方向，是由**人类专家**（Human Expert: 具备专业知识和经验的人员）与**强化学习环境**（RL Environment: 强化学习中智能体进行交互和学习的环境）共同推动的**高难度数据建设**。他指出，随着**思维链模型**（Thinking Model: 指具备显式思维过程或推理步骤的AI模型，能展示其解决问题的思路）的引入，以及大家利用**强化学习**（Reinforcement Learning, RL: 机器学习的一个分支，通过让智能体在环境中试错学习，以最大化累积奖励）训练各种模型，原来难的**指令**（prompt）现在也不是特别难了。所以这个时候，可能就更需要人类专家去标注各种各样更难的数据作为Benchmark。这也是模型开发者现在正在做的事情。

朱邦华提到，大模型评估的未来不会是线性的一个改进，会是螺旋式的共演。一边是不断变强的模型，另外一边是不断变难的评测。模型的突破迫使评测体系升级，而新的评测又反过来定义了模型能力的边界。而高质量的数据，成为了连接这两者的中枢。**强化学习**（RL）和**评估**（Evaluation），或者是**训练**（Training）和评估，就像是双螺旋的感觉。训练不断让模型变强，评估就会有更难的Benchmark出来，指出模型仍有不足，从而推动**训练**（training）环境的难度提升，或者寻找更好的**模型架构**（model architecture）、更好的算法，提升**模型能力**（model capability）。现在似乎已经到了需要人类专家来标注的程度。现在大部分**强化学习环境标注**（RL Environment Labeling）的工作，都会去找**博士**（PhD）、顶尖的**数学博士**（Math PhD）和**计算机科学博士**（CS PhD）去标注**数学代码数据**（math coding data），这些**专家数据**（Expert Data: 由领域专家标注或生成的高质量数据）价格非常昂贵。大家慢慢都偏向于找这种专家数据，能够构造出连GPT-5或**顶尖模型**（Top model）都无法回答或回答错误的**训练数据**（Training Data）和**评估数据**（Evaluation Data）。

除了数据至关重要之外，朱邦华还认为研究者不仅是要“造benchmark”，更要学会“选benchmark”。如何在成百上千个数据集中进行筛选、组合和聚合，建立一个兼顾统计有效性与人类偏好的聚合框架，是接下来几年重要的工作方向。正如OpenAI的研究员姚顺雨在他的博客**《The Second Half》**当中写道，AI的上半场是关于“如何训练模型”，而下半场则是“如何定义与衡量智能”。如今评测不再只是AI模型性能的终点，而正在成为AI向前发展的核心科学。

究竟什么样的评估方法才是最优的呢？或许我们目前还无法下定论，但能够预见的是，这将是一场持续进行的实验。我们需要在成百上千个benchmark当中找到那些真正有价值的任务，然后在类似于LMArena这样的竞技场当中去捕捉人类偏好的信号，最后再将它们结合，成为一个动态、开放、可信的智能测量体系。也许在那一天，我们就不再需要问“哪个模型最强”，而是真正去探索“智能究竟是什么”。