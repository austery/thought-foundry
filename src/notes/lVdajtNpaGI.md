---
author: Hung-yi Lee
date: '2025-09-21'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=lVdajtNpaGI
speaker: Hung-yi Lee
tags:
  - context-engineering
  - ai-agent
  - llm-interaction
  - retrieval-augmented-generation
  - multi-agent-systems
title: 上下文工程 (Context Engineering)：超越提示工程，驱动 AI Agent 的核心技术
summary: 本文深入探讨了“上下文工程”这一新兴关键技术。内容首先阐释了它与传统“提示工程”的异同，并指出随着模型能力的增强，“神奇咒语”式提示已逐渐失效。文章详细拆解了构成语言模型完整上下文的七大要素：用户提示、系统提示、对话历史、长期记忆、外部信息（RAG）、工具使用和模型自身的思考过程。最后，重点分析了为何在 AI Agent 时代上下文工程至关重要，并介绍了应对“上下文腐烂”等挑战的三大核心策略：选择、压缩与多智能体系统。
insight: ''
draft: true
series: ''
category: technology
area: tech-insights
project:
  - ai-impact-analysis
  - systems-thinking
people:
  - Taylor Swift
companies_orgs:
  - Google
  - Anthropic
  - OpenAI
  - Meta
  - Hugging Face
  - Databricks
  - Appier
  - LangChain
  - Stanford University
  - NeurIPS
  - Reddit
products_models:
  - ChatGPT
  - GPT-3
  - GPT-3.5
  - GPT-4
  - GPT-4o
  - Claude
  - Claude 3 Opus
  - Gemini
  - Gemini 1.5
  - Gemini 1.5 Pro
  - Gemini CLI
  - Gemma
  - Gemma 2 9B
  - Llama
  - Llama 4
  - AlphaGo
  - Qwen
media_books:
  - arXiv
  - Harry Potter
  - The Lord of the Rings
status: evergreen
---
### 什么是上下文工程 (Context Engineering)？

今天我们要探讨一个近来非常热门的术语：**上下文工程**（Context Engineering: 一种系统化管理和构建语言模型输入（即上下文）的技术，旨在优化模型在复杂任务，尤其是 AI Agent 中的表现）。我们将介绍它的概念，并阐明它与大家熟悉的**提示工程**（Prompt Engineering: 专注于设计和优化输入提示（Prompt）以引导语言模型生成特定输出的技术）有何不同。在当今 **AI Agent**（AI 智能体: 一种能自主设定目标、制定计划、并与环境互动以完成复杂任务的 AI 系统）的时代，上下文工程是使其成功运作的关键技术。

如果你对 AI Agent 尚无概念，可以参考我们年初在机器学习课程中关于 AI Agent 原理的导读。你可以选择先看那个影片再来听这堂课，也可以听完这堂课再去看，两堂课从不同视角探讨 AI Agent，全部听完会有更全面的收获。

那么，什么是上下文工程呢？我们反复强调，语言模型本质上是在做文字接龙。它接收一个名为 Prompt 的输入，然后预测下一个应该接的**词元**（Token: 语言模型处理文本的基本单位，可以是一个词、一个字或一个子词）。如果预测的词元不正确，我们该怎么办？

我们可以将语言模型看作一个函数 `f`，输入为 `x`，输出为 `f(x)`。如果输出不符合预期，问题要么出在 `f`，要么出在 `x`。

选择改变 `f`，即改变函数 `f` 内部的参数，这被称为训练（Training）或学习（Learning），属于模型训练的范畴，我们将在第五讲后讨论。而今天，我们假设 `f` 本身没有问题，它已经能尽最大努力根据输入 Prompt 给出最佳的接龙结果。当然，现实中语言模型的能力仍有极限，但多数情况下，我们无法修改这些线上的闭源模型。

既然 `f` 无法改变，我们能做的就是改变输入 `x`。通过为 `f` 准备合适的输入 `x`，我们可以引导它产生我们期望的 `f(x)`。这堂课的重点，就是如何为语言模型提供合适的输入以获得正确的输出。这件事人人都可以做，即便没有预算和资源，你依然可以通过优化 Prompt 来获得想要的输出。需要强调的是，在这堂课中，没有任何模型被训练，我们唯一训练的只有人类自己。

### 从“神奇咒语”到系统化管理

说到修改语言模型的 Prompt，这不就是提示工程吗？上下文工程与它有何不同？

我认为，上下文工程和提示工程本质上没有太大区别，前者更像是后者的一个时髦新名字，如同早年的神经网络（Neural Network）后来改名为深度学习（Deep Learning），新瓶装旧酒，但听起来更前沿。

尽管它们指涉的概念相同——都是为了优化语言模型的输入以获得理想输出——但它们关注的重点有所不同。过去，提到提示工程，人们可能会想到特定的输入格式，比如使用 JSON 格式，或用井字号分隔段落。然而，随着语言模型越来越强大，这些格式的重要性已大不如前，通常人类能看懂的，模型也能看懂。

另一方面，提示工程也曾与一系列“神奇咒语”联系在一起。在语言模型尚不成熟的早期，其输入输出关系有时非常无厘头，因此人们发现了一些神奇的咒语来激发其潜力。例如，在 GPT-3（比 ChatGPT 的 GPT-3.5 更早期的版本，因其难以使用而未普及）上进行的实验显示，如果你想让它回答得更长，直接说“回答要越长越好”只能将平均长度从 18.6 个字提升到 23.76 个字。但一个神奇的咒语——在 prompt 中加入一连串的“位置”——能让它“暴走”，输出长度达到 34.3 个字，效果远超直接指令。

这类神奇咒语在模型行为尚不稳定的上古时代（2022 年，ChatGPT 诞生前）更为有效。当时的研究成果通常会发布在一个名为 **arXiv**（一个开放获取的电子预印本平台，研究人员可以在论文正式发表前在此分享研究成果）的平台上。

后来出现了更多知名的咒语，例如：
*   “一步一步地思考”（Let's think step by step）能显著提升模型能力。
*   加上“请确保你的答案是正确的”能得到更好的结果。
*   让模型“深呼吸”再回答问题。
*   通过情感绑架，说“这题的答案真的对我很重要”来提高正确率。
*   承诺给小费（tip）也能提升其表现。

曾有实验测试语言模型最喜欢什么激励。结果发现，承诺“达成世界和平”的效果最好，而“你的母亲会为你骄傲”则最差，这也很合理，因为它没有母亲。

然而，这些神奇咒语正变得越来越不神奇。一年半前（2023 年 6 月），在 GPT-3.5 上，“一步一步思考”能将数学应用题的正确率从 72% 提升到 88%。但到了 2024 年 2 月，同样的操作只能从 85% 提升到 89%。咒语的效力正在减弱，这是一个合理的趋势，因为模型本就应该全力以赴，而不应依赖特定的激励。

随着神奇咒语的失效，人们的关注点转移到了一个新的方向，并赋予其新名称：上下文工程。它真正要做的是实现语言模型输入的自动化管理，甚至让语言模型自己来管理自己的输入。为了方便讲解，在接下来的课程中，我们会暂时将 Context 和 Prompt 划上等号，都指代语言模型的输入。

### 一个完整上下文 (Context) 的构成要素

一个完整的上下文应该包含哪些内容？

#### 1. 用户提示 (User Prompt)
这是最基本的部分，即用户对语言模型下达的指令。除了任务本身（例如写一封请假信），还应包含详细的指引，如信件结构（先道歉、说明理由、承诺更新进度）、字数限制（50 字以内）、语气要求（严肃）等。语言模型不会读心术，前提条件越清晰，输出结果就越理想。

例如，直接问“有人告诉我要用载具吗”是什么意思，模型可能会给出交通工具和电子设备两种解释。但如果提供情境：“在超商结账时，店员问‘我要用载具吗’”，模型就能准确回答“载具”是指电子发票的储存工具。

同样，在图像处理中，提供情境也能改变结果。一张在曼谷运河拍摄的动物照片，ChatGPT 最初识别为鳄鱼。但当我补充“我们在泰国的曼谷”这个前提后，它给出了更准确的答案：水巨蜥，这在当地是吉祥的象征。

在用户提示中加入范例同样至关重要。当我要求 ChatGPT 用“火星文”改写文章时，它最初的理解是把每个字替换成另一个固定的字。但在我给出一个范例（如将“要去冒险的人来找我”改成“要ㄑ冒险ㄉ人来找我”）后，它立刻理解了火星文的精髓，即用注音符号替换部分中文字。

早在 GPT-3 时代，这种通过提供范例来提升模型能力的方法就被称为 **In-Context Learning**（情境学习: 指在不改变模型参数的情况下，通过在输入提示中提供范例，让语言模型“学会”执行新任务的能力）。这里的“学习”需要加双引号，因为它并未改变模型内部的参数，只是改变了输入，从而改变了输出。

Gemini 1.5 的技术报告中展示了一个惊人的 In-Context Learning 案例。它被要求翻译一种极少人使用的语言——卡拉蒙语（Kalamang）。在没有任何资料的情况下，模型完全无法翻译。但当研究人员将卡拉蒙语的教科书、文法和字典放入其上下文后，Gemini 1.5 突然“学会”了这种语言，翻译水平接近于专门学习过该语言的人类。后续研究发现，模型主要是从教科书中的例句获得翻译能力的，而非文法说明。需要再次强调，这并非真正的学习，一旦教科书从上下文中移除，模型就会恢复到无法翻译的状态。

#### 2. 系统提示 (System Prompt)
**System Prompt**（系统提示: 由模型开发者预设的一段指令，用于定义模型的角色、行为准则和基本知识，在每次对话开始时加载）是模型开发者认为每次互动都必需的背景信息。以 Claude 3 Opus 为例，其公开的 System Prompt 超过 2500 个字，内容极其详尽，包括：
*   **身份认同**：明确告知模型“你是 Claude，由 Anthropic 公司打造”，并提供当前日期。
*   **使用说明与限制**：如何引导用户使用 API。
*   **互动指南**：如果用户不高兴，引导他们按“倒赞”。
*   **禁止事项**：禁止提供关于合成化学物质或制造核武器的信息。
*   **回应风格**：避免使用“好问题”这类惯用开头。
*   **知识截止日期**：告知模型其知识更新至 2025 年 1 月。
*   **自我定位**：不要声称自己是人类或拥有意识。
*   **错误修正**：被用户纠正时，要先仔细思考再回应，不要轻易承认错误。

这些都解释了为何语言模型会有特定的行为模式。

#### 3. 对话历史与记忆
对话历史构成了语言模型的短期记忆。例如，我在一个对话中告诉 ChatGPT“隔壁老王姓‘法’，叫法老王”，在同一个对话中再问它，它就能记住。这是因为它在回答时，会将之前的对话历史作为上下文进行文字接龙。然而，一旦开启新的对话，它就会忘记，因为这种方式不会改变模型参数。

不过，自 2024 年 9 月起，ChatGPT 引入了长期记忆功能。你可以在“自订 ChatGPT”中开启。虽然具体实现方式未知，但其效果是，在你提问之前，模型会预先植入你看不到的长期记忆。OpenAI 通过一个营销活动——让用户问“我是什么样的人”——来展示这一功能。模型会根据过去与你的互动历史形成的长期记忆来回答，例如它记得我曾与它讨论过“生成式人工智慧与机器学习导论”这门课。

#### 4. 外部信息 (RAG)
由于模型的知识有限且可能过时，我们常常需要为其提供外部信息。最常用的方法是 **Retrieval-Augmented Generation (RAG，检索增强生成)**（一种技术框架，通过从外部知识库检索相关信息，并将其作为附加上下文提供给语言模型，以生成更准确、更具时效性的回答）。其流程是：先将用户的问题在网络或数据库中进行搜索，然后将搜索结果与原始提示一同输入给模型，让它基于这些额外信息进行文字接龙。

然而，RAG 并非万无一失。Google 的 AI Overview 曾闹出笑话，当被问及如何让披萨上的芝士不脱落时，它根据一篇 Reddit 上的玩笑帖，建议“加入 1/8 的无毒胶水”。即使是开启了搜索功能的 ChatGPT-4o，在介绍台湾大学专业学程联盟的课程时，虽然提供了正确的网址，但错误地声称所有课程都是“进阶课程”。这提醒我们，即使有外部信息，模型本质上仍在做文字接龍，仍有可能犯错。

#### 5. 工具使用
现代语言模型（如 Claude、Gemini、GPT）可以调用多种工具，从搜索 Gmail、读写 Google Calendar，到更强大的**计算机使用**（Computer Use: 语言模型通过控制鼠标和键盘来直接操作系统桌面环境，以完成在图形用户界面上的各种任务）。

模型使用工具的原理是，在上下文中预先定义好工具的使用说明。例如，告诉模型：“如果你无法回答问题，就使用工具。将指令放在 `<tool>` 和 `</tool>` 标签之间。” 然后提供具体工具的用法，如 `temperature(城市, 时间)`。

当用户提问（如“高雄某日的气温如何？”），模型会生成一段文本，如 `<tool>temperature('高雄', '时间')</tool>`。这串文本本身不会执行任何操作，需要一个外部小程序来捕获这段指令，真正去调用 `temperature` 函数，得到结果（如“摄氏 32 度”），再将结果以 `<output>` 标签包裹的形式返回给模型。模型再根据这个新的上下文，生成最终的回答：“高雄的气温是摄氏 32 度。” 对用户而言，这个过程是无缝的，仿佛模型自己知道答案。

#### 6. 动手实践：让语言模型使用工具

在一个 Colab 演示中，我们为 Gemma 2 9B 模型创建了两个简单的工具：乘法 `multiply(a, b)` 和除法 `divide(a, b)`。我们通过 System Prompt 告诉模型如何调用这些工具。

当我们输入“111 乘以 222 除以 777”时，模型输出了调用工具的完整流程，包括它“想象”出的工具输出。然而，这整个过程只是文字接龙，模型并没有真正调用任何工具，它只是在“扮演”使用工具的角色。Gemma 甚至心算出了乘法的结果，除法结果也与正确答案相差无几，但这都是幻觉（hallucination）。

为了让模型真正使用工具，我们需要编写一个循环程序：
1.  将用户的请求和工具说明发送给模型。
2.  模型生成回应。如果回应中包含 `<tool>` 指令，程序就提取该指令。
3.  使用 Python 的 `eval()` 函数实际执行该指令，得到真实的工具输出。
4.  将模型的工具调用指令和真实的工具输出，作为新的对话历史添加回上下文中。
5.  重复此过程，直到模型生成不含工具调用的最终答案。

通过这个流程，模型成功地调用了乘法和除法工具，并给出了精确的计算结果。同样，我们演示了如何让模型调用一个查询温度的工具，并发现模型具备一定的判断力，当工具返回一个荒谬的高温时，它会质疑结果的合理性。

#### 7. 模型自身的思考过程
现在许多模型（如 GPT-O 系列、Gemini）都具备“深度思考”能力。当你提出一个复杂问题时，它不会立即给出答案，而是先在内部进行一个“脑内小剧场”，演练多种解题方案，进行规划、尝试和验证。这个思考过程对用户可以是不可见的，但它本身也构成了上下文的一部分。模型是根据自己产生的这个思考过程，再进行最终的文字接龙，从而给出更可靠的答案。

### AI Agent 时代为何亟需上下文工程

过去，我们与 AI 的互动多为一问一答。现在，我们越来越多地采用 **Agentic Workflow**，即为模型设定一个标准操作流程（SOP）来完成复杂任务，例如分步骤批改作业。

而 AI Agent 则更进一步，它能自己决定解决问题的步骤，并根据过程中的变化灵活调整计划。例如，解决一个研究问题，它可能会经历“收集资料 -> 形成假设 -> 编写代码实验 -> 验证假设 -> 再收集资料”的循环，最终产出技术报告。

从语言模型的角度看，AI Agent 的运作始终是文字接龙。它根据观察（Observation）产生行动（Action），行动改变环境，产生新的观察，如此循环往复。其优势在于，语言模型输出的是文字，可能性近乎无限，且能理解人类语言的指令和反馈。

然而，运行 AI Agent 的最大挑战之一是，复杂任务会产生极长的互动历史。当输入过长，超出模型的**上下文窗口大小**（Context Window Size: 语言模型一次能够处理的输入文本的最大长度），模型就可能“发疯”，无法正常工作。

### 上下文工程的核心挑战：“上下文腐烂”

尽管现在模型的上下文窗口越来越大，从 GPT-4 的 3 万多 Token，到 Claude 的 10 万，再到 Gemini 的 100 万，甚至 Llama 4 的 1000 万，但这并不意味着模型能完全理解所有内容。就像我们读完哈利波特全集也记不住所有细节一样。

大量实验表明，长上下文会严重影响模型性能：
*   **RAG 效果下降**：Databricks 的研究发现，在 RAG 应用中，提供过多的检索文档反而会降低模型的正确率，因为模型会“发疯”，无法处理海量信息。
*   **中间迷失 (Lost in the middle)**：一篇论文指出，模型对长上下文的开头和结尾部分记忆最清晰，而中间部分的信息很容易被忽略。有时，将正确答案放在中间，其效果甚至不如不提供任何外部信息。
*   **冗长互动导致能力下降**：近期研究发现，将一个问题拆分成多步、以“挤牙膏”的方式提问，相比一次性给出完整需求，模型的表现会更差、更不稳定。
*   **上下文腐烂 (Context Rot)**：一篇知名文章指出，随着上下文变长，模型的性能会急剧下降。在一个简单的“复制文本”实验中，当输入长度达到一万个 Token 时（远低于 Gemini 200 万的上限），许多顶尖模型的复制准确率已大幅降低。

因此，上下文工程的核心目标就是一句话：“避免塞爆 context”，只将必要的信息放入其中。

### 上下文工程的三大基本策略

上下文工程的基本操作可以归纳为三个常用招数，这些操作本身也常借助语言模型来完成。

#### 1. 选择 (Selection)
选择意味着只挑选有用的东西放入上下文。RAG 本身就是最好的例子，它通过搜索引擎筛选出最相关的信息。在这个框架下，还可以进一步优化：
*   **查询转换**：让语言模型将用户的自然语言任务转换成更有效的搜索关键词。
*   **重排 (Reranking)**：用一个小型语言模型对搜索结果进行二次筛选，只保留最相关的文档甚至句子。
*   **工具选择**：当有成百上千个工具时，可以像 RAG 一样，根据用户需求只检索并加载最相关的工具说明到上下文中，避免信息过载。
*   **记忆选择**：对于长期记忆，不能将所有历史都塞入上下文。可以像“史丹佛小镇”实验那样，将记忆存储在外部，根据相关性、重要性和时近性三个维度进行检索，只将最相关的记忆调入当前上下文。
*   **经验选择**：在需要从过去经验学习的任务中，通过 RAG 挑选与当前问题最相关的历史案例。有趣的是，有研究发现，给模型看过去答错的负面例子，有时反而会损害其表现，而只提供成功的正面例子效果最好。

#### 2. 压缩 (Compression)
当 AI Agent 的互动历史变得过长时，需要对过去的内容进行压缩。与其直接丢弃导致失忆，不如调用一个专门的语言模型，将冗长的历史记录摘要成关键信息，作为长期记忆。

这种压缩可以是递归的，例如每互动 100 个回合就进行一次压缩，将新的互动与上次的摘要合并再压缩。这样，远古的记忆会逐渐变得模糊，只保留核心概要，从而保持上下文的简洁。我怀疑 ChatGPT 的长期记忆就采用了类似技术。

压缩之所以有效，是因为 Agent 与环境的互动会产生大量琐碎信息，尤其是在 Computer Use 场景下（如订餐厅时遇到的广告弹窗）。这些细节对于未来的决策毫无意义，完全可以压缩成一句“A 餐厅 9 月 19 日下午 6 点 10 人订位成功”这样的摘要。如果担心丢失关键信息，可以在摘要中保留一个指向原始完整记录的链接，供模型在必要时查阅。

#### 3. 多智能体 (Multi-Agent)
**多智能体**（Multi-Agent: 一种系统设计范式，通过让多个独立的 AI Agent 协同工作来解决复杂问题）是管理上下文的有效方法。

想象一个负责组织出游的单一 Agent。它需要规划行程、订餐厅、订旅馆，每一次操作（尤其是 Computer Use）都会产生大量上下文，很快就会达到极限。

但如果采用多智能体系统：一个“总召”Agent 负责总体规划，当需要订餐厅时，它不亲自操作，而是向一个专门的“订餐”Agent 下达指令。订餐 Agent 完成与网页的复杂互动后，只需向总召回报“已订好”。总召的上下文中只记录了“餐厅已订好”这个结果，而没有冗长的操作细节。同样，它再指令另一个“订旅馆”Agent 完成任务。

通过这种方式，每个 Agent 的上下文都保持简短和专注。总召只关心宏观进度，而执行任务的 Agent 只需处理自己的单一任务。这就像人类社会的分工，即使每个 Agent 并没有特殊的专长，这种结构化的分工也能有效避免单个 Agent 的上下文过载。LangChain 的研究也表明，在处理复杂任务时，多智能体系统的表现远优于单一智能体。

以上就是今天分享的关于上下文工程的内容，我们探讨了上下文的构成、其在 AI Agent 时代的重要性以及三大基本操作策略。