---
author: 老科谈科技股
date: '2025-09-27'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=-oalqqZ0WfE
speaker: 老科谈科技股
tags:
  - ai-data-centers
  - accelerated-computing
  - gpu-technology
  - cloud-computing
  - fourth-industrial-revolution
title: AI数据中心的演进、结构与第四次工业革命的核心地位
summary: 本文深入探讨了AI数据中心的演进历程、核心结构及其在第四次工业革命中的关键作用。从传统数据中心、服务器与超级计算的定义与局限性出发，详细阐述了以GPU为核心的加速计算架构如何满足生成式AI对大规模并行计算的需求。文章还介绍了AI数据中心在网络、存储、散热和能源方面的技术挑战与创新，并展望了AI Cloud、大模型操作系统等未来计算趋势，强调了AI时代云、数据中心与服务器的深度融合。
insight: ''
draft: true
series: ''
category: technology
area: tech-insights
project:
  - ai-impact-analysis
people:
  - 黄仁勋
companies_orgs:
  - 英伟达
  - Equinix
  - Digital Realty
  - 亚马逊
  - AWS
  - 微软
  - Azure
  - Google
  - Google Cloud
  - Intel
  - AMD
  - OpenAI
  - 华为
  - Arista Networks
  - Broadcom
  - Marvell
  - 海力士
  - 美光
  - 三星
  - SMCI
  - Dell
  - 阿里巴巴
products_models:
  - GPT-5
  - H100 GPU
  - DGX Station A100
  - TPU
  - NvLink
  - PCIe
  - InfiniBand
  - Ethernet
  - HBM
  - NVSwitch
  - H20芯片
  - Ascend 昇腾系列
media_books: []
status: evergreen
---
### AI数据中心：第四次工业革命的核心

在人工智能（AI）炙手可热的今天，以AI为中心的**数据中心**（Data Center: 集中放置服务器、网络设备并提供制冷和供电的设施）正在成为第四次工业革命的核心。**AI数据中心**（AI Data Center: 专门为支持人工智能应用而设计和优化的数据中心）与AI服务器、超级计算及云计算相结合，共同构成了新一代计算平台。本文将详细阐述AI数据中心的定义、结构、与AI服务器、超级计算及云计算的关系，探讨英伟达的AI数据中心解决方案，并解析其成为第四次工业革命核心的原因。

### 传统数据中心的构成与运作

首先，我们来理解什么是数据中心。在台湾，数据中心被称为“资料中心”。在公司内部，你可能去过公司的机房，机房里有可以远程访问的**服务器**（Server: 提供计算、存储和网络服务的高性能计算机）、网络设备，还需要制冷和供电。你可以把数据中心理解成一个更大规模的机房。

在数据中心里，放置着大量的服务器，这些服务器被放置在机架上。机架之间通过网线或光纤紧密连接，最终它们通过电信运营商的光纤连接到外部世界。有些公司，例如银行或政府，出于安全考量会拥有自己的数据中心。而大部分公司则选择租用数据中心，仅仅是将自己的服务器放置在由他人管理的数据中心内。因此，也出现了专门的数据中心运营商，如Equinix和Digital Realty。在中国，数据中心主要由电信运营商提供。

有些公司甚至连服务器都不是自己的，而是租用的，这就是**云计算**（Cloud Computing: 通过互联网提供计算服务，包括服务器、存储、数据库、网络、软件、分析等）的方式。这种数据中心也可以称为云数据中心，提供服务器出租的就是云计算运营商，例如亚马逊的AWS、微软的Azure和Google的Google Cloud。

### 服务器的演进与超级计算的兴起

你可以把服务器理解成高性能的计算机。传统上，服务器主要用于托管网站，世界各地的客户可以访问公司的网站。当突然有大量用户访问网站时，可以使用**负荷均衡**（Load Balancing: 将网络流量分散到多个服务器以提高系统响应速度和可用性的技术）的方法来处理。当访问量很大时，可以同时复制和启动多个服务器，将部分访问请求分配给这些新服务器处理，甚至在世界各地复制服务器内容，从而减少了原有服务器的负担和访问延迟。如果让云计算运营商来做这种负荷均衡，他们已经自动化了这些过程，你只需根据使用量向云计算运营商付费即可。

同样，YouTube每天有大量用户同时访问，Google也是通过这种方式动态管理来自世界各地的服务器，最终处理大量的需求。这种服务器结构往往是以**CPU**（Central Processing Unit: 中央处理器，擅长串行计算，是传统计算机的核心）为主的结构，是Intel和AMD的天下。这种服务器结构与我们平时使用的计算机类似，对于网站这类内容，即使访问量很大，处理起来也轻车熟路。这是因为，尽管可能需要很多服务器，但服务器之间的通信要求并不高，这是网站和YouTube视频访问这类任务的特点。

十多年前，就有人将许多服务器通过高速通信连接起来，密密麻麻地放置在一个机房里，形成**服务器集群**（Server Cluster: 将多台服务器通过高速通信连接起来，协同完成一个大型任务的系统），英文叫Cluster，来完成一个大型任务，这就是**超级计算机**（Supercomputer: 具有极高计算能力，用于执行复杂科学和工程计算的计算机系统）的概念。之所以要密密麻麻地放置在一起，原因是短距离通信延迟低、速率高，所以服务器们必须尽量靠近。

什么样的任务需要超级计算机呢？在以前，这些例子往往包括天气预报、导弹轨迹仿真等。因此，以前只有国家和国防才需要超级计算机。超级计算机所进行的计算也叫做**高性能计算**（High-Performance Computing, HPC: 利用超级计算机解决复杂计算问题的技术），超级计算机也可称为高性能计算机。虽然很多国家在竞争超级计算机的规模，但其作用并没有那么大，因为大部分公司的业务是网站，不需要超级计算机。早期的超级计算机基本上都使用Intel的CPU。当时中国、美国、日本和欧洲都竞争超级计算机的规模，但超级计算机的实际用途不大，尤其商业化程度不高。这种以计算为主的、具有超强计算能力的数据中心在中国往往被称为**算力中心**（Computing Power Center: 侧重于提供强大计算能力的现代化数据中心，尤其指用于AI等高性能计算的设施）。

### 生成式AI驱动的计算革命

**生成式AI**（Generative AI: 能够生成文本、图像、音频等新内容的AI模型）的出现，对计算和数据中心提出了更高的要求。AI需要大量的**并行计算**（Parallel Computing: 同时使用多个计算资源解决一个计算问题的方法），这是**GPU**（Graphics Processing Unit: 图形处理器，擅长并行计算，是AI训练和推理的核心）的特长。GPU拥有大量的小内核，能够同时并行处理大量的任务，而CPU内核数量很少，更适合单一串行的任务。

于是，AI的训练和推理需求使得GPU取代了CPU，成为了下一代计算的主角。例如，GPT-5的训练可能需要30,000至100,000块H100级GPU（或等效算力），运行数周到数月。就单次推理而言，小规模可用1至8块GPU，大规模服务则需要成千上万GPU集群来支撑并发。生成式AI就是依靠大量的数据（即大数据）加上大量的GPU（即大算力）来实现的，这是生成式AI的基础。

### 加速计算架构与AI服务器

因此，专门支持生成式AI的新计算机结构应运而生。CPU处理普通的任务，而AI这种需要并行处理的任务，则由CPU交给GPU来完成。同时，大量的通信任务也不应该完全由CPU来做。英伟达专门设计了处理通信任务的**DPU**（Data Processing Unit: 数据处理单元，专门处理数据传输和网络通信任务的处理器），来专门处理通信任务。这种新的计算设计结构就是**加速计算结构**（Accelerated Computing Architecture: 结合CPU、GPU和DPU等多种处理器，通过高速互联实现高效并行计算的架构）。

你可以这样理解加速计算的计算机结构：一个或多个CPU通过高速接口连接多个GPU和DPU，AI的大量任务最终由GPU完成。它们之间相连的接口，英伟达使用的是**NvLink**（NVIDIA Link: 英伟达设计的高速互联技术，用于GPU之间或GPU与CPU之间的数据传输）。你可以把NvLink等同于电脑上使用的**PCIe总线**（Peripheral Component Interconnect Express: 一种高速串行计算机扩展总线标准）。以上这种结构就是AI计算机或者叫做AI服务器。例如，NVIDIA DGX Station A100机架式AI服务器大概有8块H100 GPU。

GPU是通用芯片，主要由英伟达和AMD制造。此外，还有针对AI特定算法或专门针对推理优化的特定芯片，例如Google的TPU等。Broadcom和Marvell就与云计算运营商合作，共同开发这类可定制的AI芯片。制造AI服务器的厂商包括美国上市的SMCI和Dell等大量台湾代工厂。

### 构建AI超算集群：挑战与创新

现在的大模型训练和推理需要大量的GPU同时工作。因此，我们需要将刚才提到的AI计算机通过高速通信网络连接起来，形成集群，连接成百上千的这种AI计算机。使用什么连接呢？可以使用英伟达自己的**InfiniBand**（无限带宽: 一种高速、低延迟的计算机网络通信技术，常用于高性能计算集群），或者使用传统的标准**以太网**（Ethernet: 一种广泛使用的局域网技术）。由于通信要求极高，例如速率、延迟和稳定性，传统的以太网需要被改造才能胜任。有些通信任务需要由刚才提到的DPU处理，从而加速网络通信的速度。目前的通信速率已经达到800Gbps。

AI服务器对内存的读写要求很高，需要专门设计的低延迟、高速率**HBM**（High Bandwidth Memory: 高带宽内存，一种高性能RAM接口，适用于GPU等需要高内存带宽的处理器）。韩国的海力士、美国的美光和韩国的三星在HBM内存方面积累深厚。

英伟达在2025年年初构建的最大AI服务器集群规模已突破10万GPU，凭借NVLink、NVSwitch交换机及高速以太网技术，支撑超大规模AI模型训练，是全球最顶尖的AI超算基础设施。2025年9月，英伟达宣布将向OpenAI投资至多1000亿美元用于算力建设，要部署至少10吉瓦（gigawatts）的AI数据中心。黄仁勋表示，10吉瓦大约等于400万到500万个GPU，这个数量与英伟达2025年的总出货量相当，是2024年出货量的两倍。

所以，构建AI超算中心，一方面是提高单个GPU的性能，另一方面就是堆积GPU的数量。例如，华为近期宣布其AI算力已经超过了英伟达为中国定制的H20芯片的3倍。华为是如何做到的呢？其实很简单，华为在单个GPU方面仍然逊于英伟达的H20，但华为的看家本领是通信。华为进一步优化了通信的效率，提出了**超节点**（Supernode: 华为提出的概念，将多个AI芯片（Ascend 昇腾系列）通过高速互联、统一调度，组成一个逻辑上的“大芯片”）的概念，就是把很多AI芯片通过高速互联、统一调度，组成一台逻辑上的“大芯片”。

GPU是否很容易堆积呢？答案是否定的。主要是随着GPU数量越来越多，通信的需求和延迟就成了瓶颈。因此，随着AI数据中心拥有越来越多的GPU，突破通信的瓶颈就成了关键技术。美国市场的Arista Networks就是一家专注于数据中心低延迟和高速网络的公司，其芯片很大程度上来自于Broadcom。华为和阿里巴巴也都有自己的数据中心网络解决方案。

### AI数据中心的运营与未来趋势

以上就是AI数据中心的概念。AI数据中心对制冷要求很高，现在**液冷**（Liquid Cooling: 使用液体作为冷却介质来散发热量的技术，常用于高性能数据中心）正在成为标配。SMCI就号称其核心技术是液冷。此外，AI数据中心建设成本主要是采购GPU，而运营成本主要就是电力了，甚至到了不得不使用核能的地步。因此，这些为AI数据中心提供能源的公司也都在这轮AI大潮中受益。

关于AI数据中心，接下来澄清几个概念，这些概念代表了AI数据中心和未来计算的大趋势。这些概念来自最新的阿里巴巴开发者年会。阿里巴巴提到了未来的趋势是**超级人工智能**（Artificial Super Intelligence, ASI: 指AI能力远超人类智能的阶段）。平时我们谈论的是**人工通用智能**（Artificial General Intelligence, AGI: 指AI能力达到人类智能水平的阶段），而ASI是另外的新阶段，也就是AI远超过人的阶段。这是我头一次听说ASI这个术语，以前都是用AGI表示未来的。ASI这个术语显示了阿里巴巴对AI更加有信心，当然这仅仅是一个对AI未来能力描述的术语，不要太当真了。

阿里巴巴认为**大模型**（Large Model: 指参数量巨大、能力强大的深度学习模型，如GPT系列）是下一代操作系统。我记得黄仁勋也这样讲过，可见未来IT的核心是大模型。未来的人机接口会是自然语言，大模型是对现有计算机体系结构的颠覆。

下一个重点是**AI Cloud Computer**（AI云计算机: 将AI计算资源以云服务形式提供，融合了AI服务器、数据中心和云计算的概念）的概念。黄仁勋说过，以后一个数据中心就是一台服务器。从云计算的角度看，AI云计算就是服务器。以后，你很难区分单独的AI服务器和云服务器了。大量AI服务器形成了一个集群，整个数据中心就是一个AI集群，它们共同完成任务。所以，云、数据中心和AI服务器正在融合，这之上就是大模型操作系统，大模型操作系统之上是各种**Agent**（代理: 在人工智能领域，指能够自主感知环境、进行决策并执行行动的智能实体）。这就是全新的未来计算结构。

阿里巴巴还说“AI Cloud是下一代计算机”。现在我们经常使用AI Cloud这个词，以后这个词将与计算机等同。现在你看到的服务器将被AI Cloud取代，这就是服务器演进的大趋势。换句话说，单独的物理AI服务器不再重要，而是把成千上万颗GPU和AI芯片通过高速互联组合在一起，形成一个虚拟的“超大计算机”。

### 总结与展望

今天我们探讨了从传统数据中心到AI数据中心的演变，以及从传统服务器到AI服务器、再到AI超算中心的演进。详细阐述了AI数据中心的体系结构，从网络、制冷到供电。最后，我们讨论了AI Cloud是下一代计算机、大模型是下一代操作系统，以及整个数据中心就是一个AI集群等抽象概念。

总之，在AI时代，云、数据中心、服务器和超级计算正在深度融合，这正是全新的未来计算结构，也是黄仁勋所说的**摩尔定律**（Moore's Law: 指集成电路上可容纳的晶体管数量大约每两年翻一番的经验法则）已死、加速计算时代到来的体现。如果你对此仍感到似是而非，那是非常正常的，因为每个公司都在尝试用自己创造的晦涩词汇定义未来的技术和趋势，这非常不容易理解。

AI数据中心是一个颠覆性创新，提供了大量的投资机会。记住一点，投资大公司，投资头部公司，这是一个大者恒大、强者恒强的时代。本期侧重于AI新技术，就不展开讲AI投资了。