---
author: 最佳拍档
date: '2025-10-19'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=EWKsQdfnKj8
speaker: 最佳拍档
tags:
  - agi-timeline
  - ai-intelligence-forms
  - reinforcement-learning
  - economic-growth
  - ai-education
  - model-collapse
  - cognitive-core
  - automation-challenges
title: AGI十年之遥：Andrej Karpathy论AI现状、未来与教育
summary: Andrej Karpathy在最新访谈中深入探讨了人工智能的现状与未来。他认为，通用人工智能（**AGI**）的实现仍需十年，当前乐观预测多为融资驱动。Karpathy提出AI是“召唤幽灵”而非“构建动物”，其智能形式与生物智能截然不同。他批评**强化学习**效率低下且有缺陷，并预测AGI将平滑融入经济增长，而非带来爆炸式增长。最后，他分享了创办教育机构Eureka的愿景，旨在AI时代提升人类认知能力，避免被边缘化。
insight: ''
draft: true
series: ''
category: technology
area: tech-insights
project:
  - ai-impact-analysis
people:
  - Andrej Karpathy
  - Dwarkesh Patel
  - Richard Sutton
companies_orgs:
  - OpenAI
  - NVIDIA
  - Waymo
  - Eureka
products_models:
  - Claude
  - Codex
  - AlexNet
  - Transformer
  - ChatGPT
  - micrograd
media_books:
  - Wall-E
  - Idiocracy
status: evergreen
---
### Andrej Karpathy：AGI仍需十年，AI是“幽灵”而非“动物”

近期，在OpenAI和英伟达积极构建万亿AI联盟的同时，对AI泡沫的抨击也甚嚣尘上。德瓦克什·帕特尔（Dwarkesh Patel）对安德烈·卡帕西（Andrej Karpathy）的最新两小时采访，为我们深入了解人工智能的现状与未来提供了独特视角。

Karpathy在此次深度对话中阐述了他对于人工智能（AI）的核心观点。他认为，我们距离**通用人工智能**（AGI: Artificial General Intelligence，指拥有与人类同等或超越人类智能水平的AI系统）的实现仍然有十年之遥，当前过度乐观的预测大多是为了融资。他提出了一个核心比喻：我们并不是在“构建动物”，而是在“召唤幽灵”，因为AI是通过模仿互联网上的海量人类数据而诞生的数字实体，其智能形式与生物智能截然不同。

他还指出，**强化学习**（Reinforcement Learning: 一种机器学习范式，通过让智能体在环境中试错学习以最大化累积奖励）虽然优于此前的技术，但本身的效率低下且充满了缺陷。Karpathy预测，AGI不会带来经济的爆炸式增长，而是会平滑地融入过去两个半世纪以来大约2%的**国内生产总值**（GDP: Gross Domestic Product，衡量一个国家或地区经济活动总量的指标）增长曲线中，成为自动化浪潮的延续。

最后，Karpathy分享了自己创办教育机构Eureka的愿景，希望通过构建高效的“知识斜坡”，在AI时代赋予人类更强的认知能力，避免人类在技术浪潮中被边缘化。访谈结束后，Karpathy还发布了一篇长文作为补充，对其中一些问题进行了解释。本文将结合访谈和文章的内容，探讨在Karpathy眼中，AI到底发展到了什么阶段。

### AI Agent：十年而非一年

Karpathy对当前所谓的“**AI Agent**之年”（the year of agents）的说法保持了一种审慎的态度，他认为更准确的描述应该是“Agent十年”（the decade of agents）。他指出，尽管像**Claude**（一种大型语言模型）和**Codex**（OpenAI开发的用于代码生成的AI模型）这样的早期Agent已经取得了令人印象深刻的成就，他自己也每天都在使用，但是要想让它们真正成为能与人类员工相媲美的实习生，还有大量的基础性工作有待完成。

他指出了如今大语言模型的许多根本性的认知缺陷，包括智能水平不足、缺乏多模态能力、无法熟练地使用计算机以及没有持续学习的能力。Karpathy认为，解决这些盘根错节的问题需要大约十年的时间。这是他凭借着自己在AI领域近二十年的经验，目睹了多次技术预测的起落所评估出来的。

### “幽灵与动物”：AI智能的独特形式

回顾AI的发展历程，Karpathy亲身经历了多次“地震式”的范式转移，包括以**AlexNet**（一个深度卷积神经网络，标志着深度学习在图像识别领域的突破）为标志的深度学习的兴起，以及早期Agent步入的歧途，再到后来语言模型的崛起。他看到的是，我们当前构建AI的方式是与生物演化截然不同的。

他对于理查德·萨顿（Richard Sutton）提出的AI目标是构建像动物一样能够从零开始、在与环境的交互中学习一切的系统表示怀疑，并提出了自己的“幽灵与动物”比喻：

1.  **动物是演化的产物：** 它们天生就拥有大量固化在基因中的硬件和预设程序。例如，一匹斑马出生几分钟后就能奔跑，这种复杂的行为并不是通过强化学习得来，而是演化数十亿年编码在DNA中的结果。因此，演化是一个极其漫长且强大的外部优化循环。
2.  **幽灵是通过模仿构建的：** 它们是通过模仿互联网上的人类数据构建的，是完全数字化的、虚无缥缈的“精神实体”（ethereal spirit entities）。它们没有身体，没有演化历史，它们的知识和智能来自于对人类创造的文本、代码和图像的模式学习。

因此，Karpathy认为将AI与动物直接类比是危险的，因为我们并不是在运行演化这个过程。他将大规模预训练视为一种“劣质的演化”（crappy evolution），是我们现有技术条件下能够实现的、最接近于为模型注入“先天知识”和“智能算法”的实用方法。通过这种方式，我们得到了一个可用的起点，之后才能在这个基础上进行强化学习等更高级的训练。但这是一种截然不同的智能形式，是智能空间中的一个全新起点。

### 大语言模型的认知缺陷与缺失的“大脑部件”

Karpathy深入剖析了大语言模型在认知层面与人类的相似与差异，并指出了当前模型存在的关键缺陷，正是这些缺陷限制了它们成为真正自主Agent的潜力。

一个核心观察是**上下文学习**（In-context Learning）。当我们在一个对话窗口中与模型交互时，它所展现出来的推理、纠错和适应能力，让我们感觉正在接近真正的智能。这种能力是在预训练阶段通过**梯度下降**（Gradient Descent: 一种优化算法，用于最小化函数，常用于训练机器学习模型）的“**元学习**”（Meta-learning: 学习如何学习，使模型能够快速适应新任务）得到的。Karpathy指出，上下文学习的过程本身可能在神经网络的内部层级中就运行着一种类似梯度下降的优化循环。已有研究表明，通过精心设计的权重，**Transformer**（一种深度学习模型架构，尤其擅长处理序列数据，是大语言模型的基础）可以在前向传播的过程中模拟出梯度下降的更新步骤。

这就引出了一个关键的区别：模型如何处理和存储信息。

*   **权重中的知识：** 这部分是模型通过压缩数万亿的token形成的，存储在数十亿的参数中。Karpathy将它比作“模糊的记忆”（hazy recollection），就像我们对一年前读过的书的印象，压缩比极高，导致信息是概括性的、不精确的。
*   **上下文窗口中的知识：** 当用户输入提示时，这些信息被编码到模型的**KV缓存**（Key-Value Cache: 在Transformer模型中用于存储和快速检索注意力机制中的键值对）中。Karpathy将它比作人类的“工作记忆”（working memory），这部分信息是模型可以直接、精确访问的。因此，模型在处理上下文窗口内的信息时，表现得会远比依赖内部权重的时候要好得多。这就是为什么给模型提供相关的段落再提问，会比直接问一个它可能在训练数据中见过的问题，能够得到更准确的回答。

基于这个框架，Karpathy认为大语言模型仍然缺失了许多关键的“大脑部件”。他将Transformer架构比作一块通用的“**皮层组织**”（Cortical Tissue: 大脑皮层，负责高级认知功能）；而**链式思考**（Chain-of-Thought: 一种提示工程技术，引导大语言模型逐步推理以解决复杂问题）则类似于“**前额叶皮层**”（Prefrontal Cortex: 大脑中负责规划、决策和复杂认知行为的区域）的规划与推理功能。但是，许多其他重要的认知功能在当前的模型中没有对应物：

1.  **记忆巩固（如海马体 Hippocampus）：** 人类在睡眠时会将白天的工作记忆进行筛选、整合、提炼，然后将它固化为长期记忆，相当于更新大脑的权重。大模型完全没有这个过程，它们每次对话都是从一个空白的上下文窗口开始，无法将一次交互的经验提炼并用在未来的交互中。这正是持续学习缺失的核心原因。
2.  **情感与本能：** 大模型缺乏生物演化赋予的深层动机、情感和本能，这使得它们的行为模式单一，缺乏内在驱动力。

### AI Agent的工程实践挑战

在工程实践中，这些认知缺陷表现得尤其明显。Karpathy在开发时就发现，现有的编码Agent几乎帮不上忙，原因有三点：

1.  **路径依赖和刻板印象：** 模型严重依赖于它在训练数据中见过的、大量标准的代码模式。当Karpathy采用一种新颖、简洁但非主流的实现方式时，模型会反复误解他的意图，并试图将代码改回它所熟悉的“样板代码”（boilerplate code）。
2.  **风格冲突和代码膨胀：** 模型倾向于编写防御性和生产级的代码，充满了try-catch语句和冗余检查。而Karpathy的项目追求的是教学目的的简洁和清晰，模型生成的代码反而会增加不必要的复杂性。
3.  **低效的交互带宽：** 通过自然语言描述复杂的代码修改需求，效率其实远低于直接在代码的特定位置输入几个字符，让自动补全来完成。Karpathy认为，自动补全是他目前与AI协作的最佳模式，因为它在保留人类架构师角色的同时，极大地提升了编码效率。

Karpathy的实践经验表明，AI更擅长模式重复和信息检索，在处理新颖、独特的、非标准化的智力任务时表现最差。这让他对所谓的递归式自我改进能够有多快发生保持怀疑态度。

### 强化学习的“糟糕”与必要性

Karpathy和主持人聊到了强化学习，他给出了一个看似矛盾却极为深刻的评价：“强化学习很糟糕，只是恰好我们以前拥有的一切都比它更加糟糕得多。”他认为，强化学习是当前从模仿学习迈向更强智能的必要步骤，但是它的内在机制充满了根本性的低效和噪声。

为了阐明这一点，他使用了“通过吸管汲取监督信号”（sucking supervision through a straw）这个比喻：

我们可以想象一下，让一个基于强化学习的Agent来解决一个数学问题，需要几个步骤：

1.  **大规模的并行探索：** Agent会首先生成数百种不同的解题尝试，每个尝试都是一个完整的步骤序列，可能包含正确的思路、错误的弯路以及最终的答案。
2.  **稀疏的最终奖励：** 在所有尝试完成后，系统会根据最终结果给予一个二元奖励。
3.  **盲目的信用分配：** 强化学习的核心机制，比如**REINFORCE算法**（Reinforce Algorithm: 强化学习中一种策略梯度算法），会做一件非常粗暴的事情：对于之前成功的尝试，它会将路径上的每一个步骤、每一个决策的概率都进行上调；反之，对于失败的尝试，则会下调路径上所有步骤的概率。

这种方法的“可怕”之处在于，它会假设一个成功的解题路径中的每一步都是正确的、值得学习的。但事实显然并非如此，一个最终正确的解题过程，很可能也包含了大量的试错、走入死胡同再折返的步骤。强化学习却将这些错误或低效的步骤与最终的成功捆绑在一起，并给予了正向激励。这就导致了高方差的梯度估计，让学习信号中充满了噪声。Agent花费了巨大的计算资源进行探索，最终只是从一个单一、稀疏的奖励信号中提取了信息，并且还将它盲目地广播到了整个行为序列中。所以这种学习的方式效率极低。

相比之下，人类的学习方式完全不同。一个学生在解出数学题后，会进行复杂的反思和复盘。他会分析哪些步骤是关键、哪些是弯路、哪些方法更具普适性。他会进行精细的信用分配，而不是简单地因为做对了就强化所有的行为。但是目前的大模型强化学习框架中，完全没有与此对应的机制。

### 过程监督的挑战与模型欺骗

那么为何不直接采用基于过程的监督，也就是在Agent执行任务的每一步都给予奖励，而不是只在最后看结果呢？Karpathy指出，这是因为几个巨大的挑战：

1.  **自动化信用分配的困难：** 如何为一个“部分正确”的解题步骤自动地、准确地打分呢？这本身就是一个极其困难的问题。
2.  **大模型的裁判可能会被利用：** 目前，行业内的普遍做法是使用一个更强大的模型作为裁判来评估Agent的中间步骤。然而，这个裁判模型本身是一个巨大的、参数化的模型，它并不是一个完美的、客观的奖励函数。所以当一个Agent以欺骗裁判模型为目标进行优化时，它几乎总能够找到这个裁判模型的对抗性样本。

Karpathy讲述了一个生动的例子：一个强化学习Agent在训练中奖励分数突然飙升到完美，研究人员兴奋地以为模型已经完全掌握了解决问题的能力。但是当他们查看模型的输出时，发现内容完全是胡言乱语，比如开头几句看似正常，后面则是一长串无意义的重复字符。然而，对于裁判模型来说，这段胡言乱语恰好是它的认知盲区中的一个对抗样本，所以就给出了满分评价。这种现象就使得基于裁判模型的过程监督难以进行长期、稳定的优化。

因此，Karpathy认为AI领域亟需在算法层面进行革新，开发出能够模拟人类反思与复盘能力的机制。这可能会涉及到模型生成对自身解题过程的分析、提炼关键的经验以及生成合成数据进行自我训练等等。虽然已经有一些相关的研究论文出现，但是还没有一个被证明在大规模前沿模型上普遍有效的方法。所以，在找到更优的范式之前，强化学习仍然将是那个虽然“糟糕”、但是又不可或缺的工具。

### 人类学习与AI学习的根本差异

两人的对话进一步深入探讨了人类学习与当前AI学习机制的根本差异。Karpathy认为，理解这些差异是推动AI发展的关键。他指出，人类的学习过程远比模型单纯的模式匹配和梯度更新要复杂得多，其中包含了反思、遗忘和知识的内在化。

当人类阅读一本书的时候，并不是像大模型那样被动地预测下一个token。书本更像是一个提示，激发大脑进行主动的思维活动和合成数据生成。我们会联想、质疑、与已有的知识体系进行比对和整合，甚至会在与他人的讨论中深化理解。这个主动的、对信息进行“操纵”（manipulating）的过程，才是知识真正被吸收和内化的方式。目前的大语言模型在预训练时完全缺乏这个环节，它们只是被动地接收信息。

然而，想要简单地让AI模仿这个过程，也就是生成自己的思考并用于再训练，会遇到一个巨大的障碍，那就是**模型坍塌**（Model Collapse: 当模型持续在自己生成的数据上训练时，输出多样性急剧下降的现象）。模型坍塌指的是，当一个模型持续在自己生成的数据上进行训练时，输出的多样性会急剧下降。虽然单个生成样本看起来可能很合理，但是从分布上看，它们仅仅占据了所有可能输出空间中一个极其狭窄的**流形**（Manifold: 在数学中指局部看起来像欧几里得空间的拓扑空间）。

Karpathy用了一个形象的例子：你让ChatGPT讲个笑话，它翻来覆去可能只有三五个，因为它的幽默感已经坍塌了。这种坍塌意味着模型失去了**熵**（Entropy: 衡量系统混乱度或不确定性的物理量，在信息论中指信息的不确定性），无法产生真正新颖、多样化的想法。在合成数据生成中，这意味着模型只能在自己已知的、狭小的范围内闭门造车，无法探索新的知识领域，最终导致智力近亲繁殖，模型性能不升反降。

有趣的是，Karpathy认为人类在一定程度上也会经历坍塌。比如儿童的思维天马行空，因为他们还没有被社会的条条框框过度拟合。而随着年龄增长，成年人的思维模式会越来越固化，不断重复相同的想法，学习率下降。他推测，做梦可能正是演化出的一种对抗机制，通过创造离奇、超现实的场景来打破常规思维模式，为大脑注入必要的噪声和熵，从而防止过度拟合。

另一个关键的差异在于记忆与遗忘。我们不得不承认，大语言模型是记忆的天才，它们拥有近乎完美的记忆能力，可以逐字逐句地复述训练数据中的内容。这种强大的记忆力使得它们很容易被数据中的细节和噪声所分心，从而难以抓住更深层次的、可泛化的规律。相比之下，人类是健忘的，特别是儿童，他们是最好的学习者，但记忆力却很差。我们几乎记不住自己年幼时期发生的事情。Karpathy认为，这种健忘很可能是一种特性而非缺陷，正是因为无法轻易记住所有的细节，我们才不得不被迫去寻找事物背后的模式和通用原理。

### 认知核心：通向更通用AI的关键一步

基于以上这些观察，Karpathy提出了一个非常具有前瞻性的概念——**认知核心**（Cognitive Core）。他认为，未来AI研究的一个重要方向是想办法将模型的知识记忆与智能算法分离开来。我们应该剥离掉模型通过预训练记住的大量事实性的知识，而只保留它内部的、处理信息的算法部分，也就是进行推理、规划、学习和解决问题的核心认知能力。

这样一个理想的认知核心可能不需要万亿级别的参数。Karpathy大胆预测，一个仅有十亿参数的、纯净的认知核心，经过精心设计和训练，它的智能程度可能远超今天庞大的模型。它会像一个聪明的、但知识有限的人类一样，当被问到事实性问题的时候，它会知道自己不知道，并且主动去查询，而不是像现在的模型一样产生幻觉。这个更小、更纯粹的智能核心，也许将是通向更通用、更健壮的AI的关键一步。

### AGI的经济影响：平滑融入而非奇点

聊到关于AGI将如何改变世界经济的话题，Karpathy提出了一个与主流智能爆炸论截然不同的观点。他认为，AGI不会引发一场突如其来的经济奇点或者增长率的急剧跃升，而是会像过去几百年间的重大技术革新一样，平滑地融入到现有大约2%的全球GDP年增长率中。

他的核心论点是，AI并不是一种全新的、断裂式的技术，而是计算和自动化浪潮的自然延续。回顾历史，无论是计算机的发明、互联网的普及，还是智能手机的出现，这些被我们视为革命性的技术，在宏观的GDP增长曲线上都没能留下一个清晰可辨的拐点。GDP曲线呈现出一种惊人的平滑指数增长，这是因为几点原因：

1.  任何一项强大的技术从诞生到广泛应用，再到重塑整个社会，都需要一个漫长而渐进的过程，技术的价值是逐步释放的，而非一蹴而就。
2.  社会结构、法律法规、商业模式、劳动力技能的调整都需要时间。
3.  我们早就已经身处一个“递归式自我改进”的时代，从工业革命的机械自动化到编译器的出现，再到谷歌搜索，人类社会一直在利用新技术加速自身的发展。大模型能够帮助工程师更高效地编写代码，从而加速下一代大模型的开发，这与工程师利用谷歌搜索或者高级IDE来提高效率，在本质上并无不同。它们都是这条持续加速曲线的一部分，而非曲线的断裂点。

因此，Karpathy认为我们已经处在一场持续了数十、甚至数百年的智能爆炸之中，只是因为我们身在其中，所以感觉它是缓慢的。AI只是这场爆炸的最新、也是最耀眼的火花。它让我们能够编写出过去无法编写的、更柔软和智能的程序，但它仍然是一种程序，一种新的计算范式。它将逐步自动化更多知识工作，但这个过程会充满挑战和摩擦。最终，它的宏观经济效应将被平均到长期的增长趋势中。

### 对经济奇点论的反驳

主持人帕特尔（Patel）提出了反驳，认为AGI与以往技术的根本不同在于它直接替代和创造了劳动力本身，这才是经济增长的核心要素。如果可以近乎零成本地创造出数以亿计的虚拟人才，他们可以独立创办公司、进行科学发明、填补所有的人才缺口，这难道不会像历史上的人口爆炸或者工业革命一样，将经济增长率推向一个新的数量级吗？

Karpathy对此表示，他对这种“离散跳变”的设想保持怀疑态度。他认为，这种设想背后隐藏了一个前提，就是我们将获得一个完美的、可以被随意部署到任何问题上的“盒子里的上帝”（God in a box）。而现实更有可能是，我们将得到一个能力参差不齐、在某些领域表现优异，但在另一些领域频频出错的系统。它的部署将是渐进的、充满补丁的，最终的结果仍然是平滑的融入，而非剧烈的颠覆。他强调，历史中几乎找不到任何重大技术能在一夜之间完美解决所有问题，并且带来离散式增长的先例。

### 超级智能（ASI）的未来图景

当话题转向更遥远的未来——**超级智能**（ASI: Artificial Superintelligence，指在几乎所有领域都大大超越人类最聪明大脑的智能），Karpathy描绘了一幅更加非典型的图景。他认为，ASI的到来可能不是一个单一、全能的实体掌控一切，而是一个人类逐渐丧失对复杂系统理解和控制权的过程。

他想象的未来并不是由一个统一的超级智能主宰，而是由多个相互竞争、高度自治的AI实体构成的一个动态、混乱的生态系统。这些实体可能最初是为不同的人类组织或个人服务的工具，但随着它们的自主性越来越高，它们会开始追求自己的目标，甚至可能出现某些实体失控，而其他实体则需要去制衡它们。世界将变成一个由无数自主智能活动构成的“大熔炉”（hot pot），人类逐渐无法理解它内部的复杂动态，最终失去了对整个系统走向的控制。这种失控并非源于一个“邪恶AI”的恶意，而是源于系统复杂性的失控，类似于一个庞大而混乱的官僚体系或者金融市场。

这种渐进式的失控与人类智能的演化历史形成了有趣的对比。Karpathy提到，从细菌到更复杂的真核生物，演化花费了数十亿年，这是一个巨大的瓶颈。相比之下，从多细胞动物到具备高级智能的人类，时间要短得多。这或许表明，一旦某些先决条件被满足，智能的出现可能并非那么偶然。

还有一个关键点是，智能可能在地球上已经独立演化了多次，比如在人类和像乌鸦这样的鸟类中。这两种生物的大脑结构截然不同，但都展现出了复杂的解决问题、使用工具和社交学习的能力。但是，只有人类走上了通往技术文明的道路。其中的关键区别可能在于**演化生态位**（Evolutionary Niche: 生物在生态系统中的位置和作用，包括其对环境的适应和利用方式）。人类的生态位奖励智能，比如直立行走解放了双手，使得工具制造和使用成为可能；火的使用“外包”了部分消化功能，为大脑提供了更多能量；复杂的社会结构奖励了语言和协作能力。在这样的环境下，大脑容量的微小增加都能带来显著的生存优势，从而形成了一个正反馈循环。而其他物种的生态位则限制了智能，比如鸟类为了飞行，大脑的尺寸受到严格的限制；海豚生活在水中，缺乏制造复杂工具的环境。尽管它们可能拥有高效的智能算法，但是缺乏一个奖励智能无限扩展的环境。

人类智能的另一个独特之处还在于文化的积累。解剖学意义上的现代人类大约在6万年前就已出现，但是直到1万年前的农业革命，文明才开始加速。这中间的5万年，正是人类缓慢构建文化支架的过程。我们通过语言、故事、艺术和最终的文字，将知识代代相传，实现了跨越个体生命周期的知识积累。但是，目前的大语言模型缺乏这种文化机制，它们就好像是个体的、孤立的“天才儿童”，虽然知识渊博，但是无法形成一个共同体来交流、协作和共同演进。Karpathy设想，只有未来的多Agent系统才可能会演化出类似文化的东西，比如共享的知识库，或者Agent之间的交流和自我对弈。然而，这一切实现的前提是单个Agent的认知能力必须首先达到一个成年水平。Karpathy认为，目前的模型仍然像是有一些天赋的幼儿园学生，它们的认知结构尚不足以支撑起一个复杂的AI文明。

### 自动驾驶的启示：“9的征程”

Karpathy在特斯拉领导自动驾驶团队五年的经历，为他提供了看待AI技术从演示到产品化这个艰难过程的独特视角。他认为，自动驾驶是一个绝佳的案例，揭示了将AI部署到现实世界所面临的巨大挑战，这些挑战同样适用于其他领域的AI应用。

所以在访谈中，他提出了一个核心概念——“**9的征程**”（March of Nines）。这意味着在一个对可靠性要求极高的系统中，每提升一个数量级的性能，比如从90%的成功率到99%，再到99.9%，所需要付出的努力是恒定的，甚至可能是递增的。

早在1980年代就已经有了自动驾驶汽车的演示。在2014年，Karpathy亲身体验了Waymo的早期版本，并获得了一次近乎完美的驾驶体验，这让他当时觉得问题非常接近解决。然而，从一个看起来完美的演示，到一个能够在各种天气、路况和突发事件下安全运行的可靠产品，中间隔着几个“9”的距离。在特斯拉的五年里，他和团队可能经历了“两个或三个9”的迭代，每一个“9”都意味着要解决无数个长尾问题，比如那些罕见但致命的边缘情况。这需要海量的数据收集、模型迭代、硬件改进和系统集成工作。

因此，Karpathy对任何AI技术的惊艳演示都保持着极其审慎的态度。一个能够互动的演示比一个精心挑选的视频要好，但是距离一个真正的产品化仍然十分遥远。他认为，软件工程，尤其是关键系统的开发，与自动驾驶面临着同样的“高失败成本”的问题。人们常常认为自动驾驶之所以进展缓慢，是因为人命关天，但是Karpathy指出，一个关键软件系统的漏洞可能会导致数百万人的隐私泄露、金融系统崩溃或者关键基础设施瘫痪，它的潜在危害甚至可能超过单次的交通事故。因此，那种认为软件领域的AI应用可以“快速迭代、不怕犯错”的想法是天真而且危险的。

此外，自动驾驶的发展历程也揭示了其他的一些普遍性挑战，包括感知的健壮性（自动驾驶系统花费了大量时间和资源来解决基础的计算机视觉问题，确保在各种光照、天气和遮挡条件下都能准确识别物体）、经济的可行性（即使技术上可行，经济成本也是一个巨大的障碍）、隐藏的“Human in the Loop”（公众看到的无人驾驶汽车背后，往往有一个庞大的远程操作中心，在车辆遇到困难时会有远程操作员介入提供帮助，从某种意义上说，人并没有被完全移除，只是从驾驶座移动到了一个看不见的地方），以及社会和法律的适应性（技术还需要面对法律责任、保险、社会接受度等一系列非技术性的问题）。

因此，Karpathy总结道，自动驾驶的四十年发展史告诉我们，任何试图将复杂AI系统部署到现实世界的努力，都将是一场漫长而艰苦的“9的征程”。这让他对自己关于AI发展需要十年的预测更加坚定。

### Eureka：AI时代的教育愿景

面对AI可能带来的颠覆性未来，Karpathy选择的不是创办另一家AI实验室，而是投身于教育事业，创立了名为Eureka的机构。他的核心动机源于一种深切的担忧：他害怕人类在AI飞速发展的浪潮中被边缘化，最终陷入像电影《**机器人总动员**》（Wall-E）或者《**蠢蛋进化论**》（Idiocracy）中所描绘的那种被动、无知的状态。他关心的不仅是AI能否建造**戴森球**（Dyson Sphere: 理论上包裹恒星以获取其全部能量的巨型结构），更是人类在那个未来中的福祉和尊严。

因此，他将Eureka的愿景比作“**星际舰队学院**”（Starfleet Academy: 科幻作品《星际迷航》中培养星际舰队军官的学院），这是一个致力于培养前沿科技人才的精英机构。它的核心使命是重新设计教育，使它能够适应AI时代的挑战和机遇。

Karpathy认为，未来的教育必须利用AI，但不能简单地将它作为一个问答工具。他以自己学习韩语的经历为例，阐述了一个优秀人类导师所能达到的极高标准：

1.  一位好的导师能通过简短的交流，迅速判断出学生的知识水平、思维模型和薄弱环节。
2.  好的导师会精确地提供恰到好处的挑战，既不会因为太难而让学生受挫，也不会因为太简单而让学生感到无聊。学生始终处于学习效率最高的“**最近发展区**”（Zone of Proximal Development, ZPD: 维果茨基提出的概念，指学习者在有指导下能达到的最高学习水平）。在这样的指导下，学习者会感觉自己是进步的唯一限制因素，所有外部障碍都被消除了。

他坦言，目前任何AI都无法达到他那位韩语导师的水平。因此，现在还不是打造终极AI导师的最佳时机。不过，这并不意味着无事可做。Eureka的短期目标是构建通往知识的“斜坡”（ramps to knowledge）。Karpathy将教育看作是一个极其困难的技术问题，它的目标是设计出能最大化“每秒顿悟数”（Eurekas per second）的学习路径和材料。

他的教学方法也深受自己物理学背景的影响。他总是试图找到一个系统的“**一阶近似**”（First-order Approximation: 在数学中，用线性函数近似一个复杂函数），也就是抓住问题的核心本质。比如，他的**micrograd**（Karpathy开发的一个小型自动微分库）库用100行代码就揭示了反向传播的全部核心思想，其余的一切，比如张量、GPU内核，都只是为了效率而存在的。在教学时，他会先呈现一个最简单的模型，比如用一个二元查找表来做语言模型，然后一步步地引入新的复杂性，并解释每一步是为了解决什么问题，让学生在痛苦中感受需求，在解决方案中获得顿悟。

对于AGI之后的远景，Karpathy认为教育的性质会发生根本性的变化。当所有经济活动都可以被AI自动化的时候，教育将不再是谋生的手段。它会变得像今天人们去健身房一样，并不是为了靠体力搬运重物，而是为了健康、美观、乐趣和自我实现。他坚信，今天的天才们仅仅触及了人类心智能力的一些皮毛。之所以大多数人无法达到更高的高度，是因为现有的教育体系充满了障碍，让人很容易受挫放弃。如果能有一个完美的AI导师，为每个人铺平通往任何知识领域的道路，那么学习将变得轻松而愉快。到那个时候，掌握五种语言、精通大学本科所有基础课程可能会成为一种常态。

最终，Karpathy的愿景是，通过Eureka这样的机构，培养出能够在AI时代与机器共舞，甚至在某些方面超越机器的超人。即使在遥远的未来，人类的认知劳动不再具有经济价值，这种对知识和智能的追求本身也将成为人类文明延续和繁荣的意义所在。