好，今天這個第二堂課呢
我們要講什麼是 Context Engineering
這是一個最近很熱門的 Buzzword
那我們今天會介紹 Context Engineering 的概念
告訴你說它跟你熟悉的 Prompt Engineering
有什麼差異
那這個 Context Engineering
是今天在 AI Agent 的時代
能夠讓 AI Agent 成功運作的一個關鍵的技術
那如果你對 AI Agent 還沒有什麼概念的話
你其實可以去看
我們在今年年初的時候
機器學習這門課講的一堂課：導讀 AI Agent 的原理
那你可以選擇先看這個影片
再來聽這個課
你也可以聽完這個課
再來看一堂課導讀 AI Agent
那兩堂課的內容講的是不太一樣的
我們從不同的觀點來講 AI Agent
那兩堂課都聽完你會有不一樣的收穫
好，那什麼是 Context Engineering 呢？
我們不斷反覆強調說
語言模型就是在做文字接龍
語言模型本質上做的事情
就是給它一個輸入，這個輸入叫做 Prompt
它會去預測接下來應該接哪一個 Token
那假設它接的 Token 不對的話怎麼辦呢？
語言模型可以看做一個函式叫做 f
它的輸入是 x
它的輸出就是 f(x)
假設輸出不是我們要的
那這個 f(x) 裡面有兩個部分：f 跟 x
所以要嘛就是去改 f
要嘛就是去改 x
那如果你選擇的是改 f 的話
那這件事情叫做訓練，它的英文是 Training
或者是叫做學習，它的英文是 Learning
那如果呢，你選擇改變的是語言模型的
這個函式 f 裡面的參數，改變它的參數
讓它有不一樣的行為
那這個叫做 Learning
那我們就進入模型訓練的領域
那這個是我們到第五講後才會提到的
那今天我們先假設 f
沒什麼問題，這個語言模型沒什麼問題
它呢，已經能夠盡最大的能力
根據輸入的 Prompt 選擇最好的接龍的結果
但這件事不一定是真的，今天語言模型
當然它的能力還是有極限
還是有很多可以在改進的地方
不過多數狀況啊
你其實也已經改不了語言模型
它們都是線上的封閉原始碼模型
它們都是沒有開源的模型
所以就算有問題，你也改不了
那既然 f 改不了的話
你可以改什麼呢？
你可以改輸入的 x
你可以幫 f 準備合適的輸入 x
讓最後得到的 f(x) 是我們預期的結果
所以今天這一堂課
我們就是專注在
怎麼給語言模型合適的輸入
讓語言模型得到正確的輸出
這件事每個人都可以做
就算你手上沒有預算資源
你還是可以把 Prompt 修好
讓語言模型得到你要的輸出
主要強調一下
在這堂課中
沒有任何模型被訓練
我們唯一訓練的只有人類而已
好，那剛才講說
我們要修改語言模型的 Prompt
那這個不就是 Prompt Engineering 嗎？
那 Context Engineering
跟 Prompt Engineering 有什麼不同呢？
其實我認為
Context Engineering 和 Prompt Engineering
它們沒什麼本質上的不同
Context Engineering
就是把 Prompt Engineering
取一個新的名字
讓你覺得潮一點
就好像說早年呢
有 Neural Network
後來 Neural Network 這個名字就臭掉了
所以有人就把 Neural Network 改成 Deep Learning
其實這個是新瓶裝舊酒
其實一樣的東西
但是聽起來潮了起來
所以 Context Engineering 和 Prompt Engineering
我認為它其實就是同樣的東西
不同的名稱
但是其實它們還是有一點點差異
什麼樣的差異呢？
它們指涉的是一樣的概念
都是把語言模型的輸入
弄好，讓語言模型得到我們要的輸出
但是它們關注的重點是不同的
怎麼說呢？
過去當我們講 Prompt Engineering 的時候
也許你馬上浮現在腦海中的是
語言模型的輸入可能要有特定的格式
你要用某種特定的格式
用 JSON format，或者是不同的段落之間
要用井字號隔開等等
有一陣子呢有一堆人在教你怎麼用正確的格式
來 prompt 語言模型
但今天語言模型越來越強
通常這些格式已經沒有那麼重要
往往人看得懂的，語言模型也就看得懂
另外一方面講到 Prompt Engineering 的時候
可能會讓你聯想到一系列的神奇咒語
什麼神奇咒語呢？
在早年這些語言模型還沒有那麼厲害的時候
它們的輸入輸出關係
有時候非常的無厘頭
所以這個時候
你又可能找出神奇的咒語
讓語言模型發揮不可思議的力量
比如說這邊這個例子是做在 GPT-3 上面的
最早的 ChatGPT 是 GPT-3.5
所以 GPT-3 是 ChatGPT 更早的版本
多數人都沒有用過 GPT-3
為什麼呢？
因為它就是個垃圾
它沒有辦法被使用
它非常難被使用
好，這個 GPT-3
它有各式各樣奇怪的行為
那也可以找到一些神奇咒語來操控它
讓它變成你想要的樣子
舉例來說
假設我們現在想要 GPT-3
你問它問題的時候
它回應的長度越長越好
那如果你什麼都不做
那它平均回應的長度是 18.6 個字
如果呢
你在輸入的 prompt 跟它說
回答要越長越好
盡可能越長越好
那它可以從 18.6 個字進步到 23.76 個字
但是我們發現一個神奇的咒語
這個神奇的咒語就是
你只要在 prompt 裡面加一連串的位置
就叫它說「位置」
它算是暴走
它的答案就變很長
可以長到 34.3 個字
比你叫它輸出的答案越長越好
還要更有效
不過這個只有在早年
語言模型輸入輸出的關係
還很莫名其妙的時候
這種神奇咒語比較能發揮作用
這邊用的這個方法
是出自我們實驗室的一篇論文
我把論文的連結放在左下角
好
通常在引用論文的時候
現在我會直接引用這個論文
放在 arXiv 上的連結
也許不是所有同學都知道什麼是 arXiv
所以我花幾秒鐘
跟你講一下什麼是 arXiv
arXiv 是一個放論文的平台
今天在人工智慧領域
因為技術變化的非常快
所以如果你有新的點子
你通常不會去投稿
你通常與其等投稿國際會議
那通常要等 6 個月以後
如果 6 個月以後
你才會知道論文有沒有被接收
很多時候研究人員會選擇
人員會選擇
直接把研究成果
公開在 arXiv 的這個平台上面
在這個 arXiv 這個平台上面
你可以從它的連結看出
這篇論文是什麼時候被放上 arXiv
看左下角這個例子
它最後這個連結的數字是 2206
那就代表說是 2022 年 6 月
被公開的一篇論文
2022 年那個是沒有 ChatGPT 的上古時代
那時候人類還只能茹毛飲血而已
所以這個是上古時代做的事情
那後來有各式各樣的神奇咒語
很多你可能都聽過
比如說最知名的就是跟模型說
一步一步的思考
Let's think step by step
模型能力就起飛了
後來有人發現說
不只要告訴它一步一步的思考
還要告訴它
請確保你的答案是正確的
模型可以得到更好的結果
有人發現說
如果你叫模型深呼吸
再回答問題
可以得到更好的結果
有人發現說如果你情勒模型跟它說
這題的答案真的對我非常重要
它的正確率可以提高
有人發現說如果你跟模型說
如果你答對的話
就給你一點小費
哇！它的能力
居然也會因此而提升
所以模型可能會因為奇奇怪怪的東西
而讓它的正確率
而讓它更能夠遵守人類給的指令
比如說這邊有一個實驗的結果是
有人想要測試說
這個語言模型最喜歡什麼東西
之前已經講過說
如果給它小費的話
它會做得更好
那語言模型會不會有別的更吸引它的東西呢？
所以在這一篇 blog 文章裡面
作者就試了說給它 Taylor Swift 的門票
跟它說如果你做得好
就會達成世界和平
如果你做得好
你的母親就會因你而驕傲
或者是如果你做得好
你就會得到真愛
看看語言模型
他們測試 ChatGPT 最喜歡什麼東西
那這邊他這個實驗
是要求語言模型用 200 個字寫一個故事
所以在這個圖上
這個我把我的滑鼠叫出來
所以在這個圖上
黑色的這個縱軸
代表的是正好 200 個字
所以比 200 個字多
比 200 個字少
都算是做得不好
所以整體而言
這個 ChatGPT 最喜歡什麼呢？
它最喜歡世界和平
只要你跟它講說
你做得好
就可以達成世界和平
可以讓它做得最好
它最不在意什麼呢？
它最不在意自己的母親會不會因為它而驕傲
我說這是合理的
因為它沒有母親
所以它不在意自己的母親
也是一個合情合理的結果
總之
曾經有一度有各式各樣的神奇咒語
可以強化語言模型的能力
但是現在這些神奇的咒語
越來越不神奇
在一年半之前的生成式 AI 這門課
會導讀這堂課裡面
我其實就跟大家分享過說
其實神奇咒語已經沒有以前那麼神奇了
這頁投影片是舊的，一年半前的投影片
所以如果你是在 2023 年 6 月的時候
你跟 GPT-3.5
叫它去解一些數學應用問題
沒有神奇咒語正確率 72%
用神奇咒語叫它一步一步思考
正確率變了 88%
但是過了大概半年之後
在 2024 年的 2 月
你在做一樣的實驗
如果沒有神奇咒語
正確率是 85%
用神奇咒語
正確率也只能升到 89%
所以你發現神奇咒語的效率是越來越低的
那這個也是一個合理的發展方向
因為模型本來就應該要使盡全力做到最好
怎麼可以說叫你一步一步的思考才開始思考？
怎麼可以說給你小費才好好做呢？
沒有給你小費也應該要好好做
所以這些神奇咒語就逐漸的沒有那麼神奇了
那神奇咒語不再神奇以後
現在人們關注的重點變了
那人們現在新的關注的重點
給了新的名字叫做 Context Engineering
這個 Context Engineering 它真正想要做的
就是我們自動化的來管理語言模型的輸入
那現在已經有很多能夠自動做事情的語言模型
我們用語言模型自己來管理自己的輸入
那如果你要問我說 Context 跟 Prompt 這兩個字
這樣的字眼有什麼不同
我知道說可能有些人會把這兩個字眼
還是會做出一些差異
那等一下的課堂上為了要講解方便
我們就暫時把 Context 跟 Prompt 劃上等號
所以等一下課堂上當我講 Context 的時候
我指的是 Prompt，我指的是語言模型的輸入
那我們現在要講的就是管理語言模型輸入的方法
這個是今天的課程大綱
我們會先講一個語言模型的 Context 裡面
應該包含什麼樣的資訊
然後第二段會講說
為什麼在 AI Agents 時代
我們特別需要 Context Engineering
然後最後我們會講一些 Context Engineering 的基本操作
好，那我們就先從 Context 裡面
應該要包含什麼內容開始講起
一個完整的 Context 裡面
應該要包含什麼樣的內容呢？
最基本的當然是使用者的 Prompt
你對語言模型下的指令
那 Prompt 裡面可以用更多的內容
除了你的任務以外
比如說假設你今天要寫封信
跟老師說你明天要請假
除了告訴語言模型你要執行的任務以外
你最好還是能夠提供一些詳細的指引
比如說你告訴語言模型說
開頭先道歉
然後說明請假的理由
請假理由可能是身體不適
不要講一些有的沒的
我今天就是不想上課之類的理由
然後再說之後會再跟老師更新進度
那老師很忙
所以不要寫太長的信
所以你可以交代語言模型說
要在 50 字以內
然後老師最近心情不太好
所以跟語言模型說語氣要非常嚴肅
不要嘻皮笑臉等等
除了任務的說明以外
你可以加一些額外的條件
讓語言模型的輸出更接近你想要的
那因為語言模型不會讀心術
所以
如果有些東西
你希望語言模型做得更好
那個條件必須要由你來講清楚
那很多時候
你把前提講清楚
可以讓語言模型做得更好
舉例來說
假設你問語言模型
「有人告訴我要用載具嗎」
這句話是什麼意思？
那語言模型就開始想
載具是什麼意思
它說載具呢
可能有兩種不同的意思
它是交通工具
交通工具
確實也可以叫做載具
那它可能呢
是指某種電子設備或平台
那有人看到這個答案
就覺得不高興了
想說
它怎麼會覺得載具是交通工具呢？
載具不是都是指
手機上那個 APP
你買東西之後要掃的嗎？
怎麼會覺得是交通工具呢？
但仔細想一下
載具真的可以指交通工具
你這邊並沒有告訴語言模型
你在什麼前提下
問這個問題
如果以前提講更清楚一點說
在超商結帳的時候
店員問「我要用載具嗎」
這句話是什麼意思？
這個時候語言模型
就可以正確地回答你說
載具就是指電子發票的儲存工具
但在這個情境下
因為已經告訴它現在的情境
就是在超商結帳
所以語言模型
就不會覺得載具是交通工具
或者再舉另外的例子
在做影像處理的時候
其實前提也會影響影像處理的結果
這是一張照片
河上看起來有一隻動物
這個照片呢
是在這個曼谷拍的
去年我去 ACL
然後太太也一起去
然後去開會的時候
我太太就自己出去玩
然後還去運河上坐一艘船
然後她就發現運河上有一隻動物
就是照片裡的這隻動物
這隻動物呢
離船非常地近
看起來像隻鱷魚
然後太太就覺得滿緊張的
這個鱷魚離人這麼近
感覺它隨時是可以爬上船咬人的
但她發現身邊的泰國人
一點都不緊張
而且看起來呢
還挺高興的樣子
旁邊的泰國人試圖跟
我太太解釋這是什麼
但是我太太呢
完全聽不懂泰文
所以她不知道那些泰國人
想要說什麼
那些泰國人呢
也不會講英文
所以沒有辦法溝通
然後說我太太就拍了這張照片
然後問我說
你看看這是什麼動物
然後想說這不是鱷魚嗎
然後我就問 ChatGPT 說
你覺得這是什麼動物？
ChatGPT 果然覺得
這是鱷魚
看起來就像鱷魚
但我想說這個泰國曼谷的運河裡
會有鱷魚嗎？
有鱷魚會讓它離人那麼近嗎？
所以我就轉念一想
也許我應該提供
ChatGPT 更多的情報
我就問它說
現在我們在泰國的曼谷
那水中的動物是什麼？
這個時候 ChatGPT
就給了我一個不一樣的答案
它說這個水中的動物
如果在泰國曼谷的話
它看起來是一隻水巨蜥
水巨蜥在泰國
其實是一個吉祥的象徵
它沒有那麼常見到
但也不是完全見不到
通常見到水巨蜥就代表說
你會有偏財運
這就是為什麼船上的人
看到水巨蜥的時候
感覺都蠻高興的
所以你告訴 ChatGPT
你在哪裡拍的一張照片
其實是可以影響它的答案的
那在你的 User Prompt 裡面
加上範例也會影響語言模型的答案
比如說我這邊要求 ChatGPT
把以下這篇文章用火星文來改寫
那 ChatGPT 知道火星文是什麼嗎？
它有一個它想像中的火星文
它的輸出是長這個樣子的
而且這個火星文
不是隨便亂掰的
比如說你看台北的「台」
它都把它放到一個框框裡面
比如說台北的「北」
它都是女字邊加一個年
它這個修改是有規律的
它覺得說把一篇文章裡面的
每個字換成另外一個字
就是火星文
但大家知道說火星文其實指的是
把一些中文字用它的注音符號來替換
所以這邊如果在要求模型寫火星文的時候
我舉個例子跟它說
所謂的火星文就是「要去冒險的人來找我」
改寫成「要ㄑ冒險」
「ㄉ人來找我」
這個才是我要的火星文
當你又給 ChatGPT 這樣的舉例的時候
它的輸出就變了
它的輸出就變成這樣
它完全知道所謂的火星文
就是把一些文字改成注音符號
你跟它說我們的環島之旅
它就知道改成「我們ㄉ環島之旅」
它把一些字改成注音符號
所以如果你可以提供清楚的例子
可以影響模型的能力
那把這些例子
放到 Context 裡面可以讓模型做得更好
這早在 GPT-3 的年代就已經知道了
在 GPT-3 那篇 paper 裡面他們就說
假設你要叫模型做翻譯
你直接跟它說
把英文翻成法文
然後叫它翻這個字
它不見得能夠成功做翻譯
那個時候模型還很弱
有時候它甚至不知道你到底要它做什麼
你叫它做翻譯這樣的指令
有時候都看不太懂
但是如果你可以提供給它一些範例
跟它說所謂的翻譯
就是把這個字變成
這個字，這個字變成
這個字，這個字變成
這個字，聽懂了嗎？
那 cheese 的翻譯是什麼？
它就更有機會可以答對
然後在 GPT-3 裡面
把提供範例這件事情
叫做 In-Context Learning
要注意一下
這邊的 Learning 要放在雙引號裡面
為什麼 Learning 要放在雙引號裡面？
因為這邊的 Learning
並不是傳統意義上的
Machine Learning、機器學習的學習
這邊的參數並沒有任何的改變
當我們給模型範例的時候
我們完全不會改變這個模型內部的參數
我們語言模型代表的那個函式 f
它都是一樣的
只是因為輸入變了
所以輸出才會跟著改變
那輸入範例可以改變語言模型的輸出
早在 GPT-3 就知道了
它叫 In-Context Learning
那講到 In-Context Learning
我覺得最經典的例子
就是 Gemini 1.5 的 In-Context Learning 能力
Gemini 1.5 剛出的時候
在它們技術報告裡面
就講了一個 Gemini In-Context Learning 的神蹟
這個神蹟是這樣子的
他們叫 Gemini 去翻譯一種
很少人用的語言
這種語言叫做卡拉蒙語 (Kalamang)
他們叫 ChatGPT
把這個句子從英文翻成卡拉蒙語
卡拉蒙語的資料非常的稀少
據說在世界上
可能只有數千個人在用這個語言
在網路上據說找不到卡拉蒙語的資料
所以這語言模型很有可能完全不知道
什麼是卡拉蒙語
所以叫它翻譯的時候
它是完全翻不了的
但是如果啊
你給這個語言模型
卡拉蒙語的教科書、文法和字典
它讀了教科書
讀了字典
在讀了你的指令以後
再去做文字接龍
把教科書跟字典
放到模型的 context 裡面
再叫模型做翻譯的時候
這時候神奇的事情是
模型突然之間就會了卡拉蒙語
這一頁投影片呢
是 Gemini 1.5 技術報告裡面的一個實驗
他們說
那我們來先來測試各個不同的模型
把卡拉蒙語轉英文
還有英文轉卡拉蒙語的能力吧
這邊的數字代表
人類去評分
這個模型翻譯的結果到底好不好
滿分是 6 分
這些模型
它們的分數都還不到 1 分
代表它們完全沒辦法
把英文轉成卡拉蒙語
或卡拉蒙語轉成英文
但是如果你提供給這些模型
半本教科書
或者是像 Gemini 1.5 可以讀整本教科書
它這邊想要炫耀的
在 Gemini 1.5 技術報告裡面
特別舉這個實驗
一個它非常想要炫耀的點
就是它們可以輸入
非常長的 context
它們 context 裡面可以放很多東西
當時其他模型都只能放
半本教科書
它們可以放整本教科書
當你給 Gemini 1.5
整本教科書的時候
哇！
它得到的評分是 4 分
或者是 5.5 分
滿分才 6 分而已
好
這個是人類的結果
就是找了一些人去學習卡拉蒙語
然後也來做翻譯
那還是比機器稍微好一點
但是也沒好到多少
那這是比較早年的實驗了
其實在一年半以前
2024 年的生成式 AI 導論
這門課其實就講過這個實驗
那最近有人就分析說
那到底這些模型是從
這個教科書的什麼地方
學到了
但這個「學」要加一個雙引號
它不是真正的學
它並沒有改變參數
它到底是從教科書裡面的
什麼地方讓它有了翻譯的能力？
那這個研究的結果發現說
真正讓模型具有翻譯能力的
是教科書裡面的例句
那些文法的說明什麼的
對模型沒什麼幫助
它好像沒什麼看那些文法的說明
真正有幫助的是
那些教科書裡面都會有例句
告訴你有這個語言的
一個句子翻成另外一個語言長
什麼樣子
看起來模型是根據那些例句
讓它有了翻譯新語言的能力
但這邊要再強調一下
我們並沒有學任何東西
在 In-Context Learning 裡面
並沒有任何東西被學習
語言模型的文字接龍能力是不變的
當你把教科書拿掉
當你沒有給語言模型教科書的時候
它就會回復
它沒有辦法翻譯卡拉蒙語的狀態
好，剛才講的是 User Prompt
那這個語言模型呢
還要有 System Prompt
那我們其實上週講過 System Prompt 的概念
System Prompt 是開發語言模型的人
這個開發語言模型平台的人
覺得語言模型每次互動的時候
都需要有的資訊
那 Claude 這個模型呢
它的 System Prompt 是公開的
所以可以在底下這個連結裡面
看到 Claude 的 System Prompt 長什麼樣子
那我們就來看一下
Claude 最新的版本
Claude 3 Opus 的 System Prompt 長什麼樣子
這是它的 System Prompt 的前幾句話
當然第一句話
開宗明義
就告訴這個模型說
你叫做 Claude
你是用 Anthropic 這家公司所打造的
然後再告訴它
今天是幾月幾號
不然它不知道今天是幾月幾號
所以你知道說
為什麼語言模型知道它自己是誰
為什麼語言模型知道今天是幾月幾號
這個我們上週講過
它做文字接龍怎麼可能知道
自己是誰或今天幾月幾號
這都是已經寫好在 System Prompt 裡面的東西了
然後今天一個好的模型
一個好的服務
它的 System Prompt 可以非常的長
有太多事情要跟這個語言模型交代了
Claude 3 Opus
它的 System Prompt 的長度
居然超過 2500 個字
裡面到底都有些什麼呢？
你讀了這個 System Prompt
你就會知道說
為什麼今天這個語言模型會有這樣的行為
都有可能是 System Prompt 給它的
所以 Claude 的 System Prompt 裡面包含
它是誰
它是 Claude
然後呢
還告訴它使用的說明跟限制
比如說如果有人問你 Claude 的 API 去哪裡用
那你就告訴他
讀這個網頁的內容
然後呢
會告訴它怎麼跟使用者互動
所以 Claude 的 System Prompt 裡面寫說
如果使用者覺得不高興
就叫他去按倒讚的符號
然後它有一些禁止事項
比如說它會告訴 Claude 說
不要告訴使用者
怎麼合成化學物質
或怎麼做核武器
然後呢
它還會對回應的風格做一些設定
比如說在 System Prompt 裡面交代 Claude 說
不要回答
慣用的開頭，不要講「好問題」
可能是單純做文字接龍的時候
一個問題後面
太容易接出 "Good Question" 了
所以特別交代一下
Claude 不要老是用 "Good Question" 來做回應
然後告訴 Claude 說
啊你的知識只到 2025 年的 1 月
所以假設有人問了那個問題
顯然是 2025 年 1 月後才發生的事
Claude 就會回答說「我不知道」
然後呢
跟 Claude 講一些自己的定位
比如說不要說自己是人類
或者是不要說自己有意識
然後呢
告訴 Claude 說如果人類糾正你
不要馬上承認錯誤
今天語言模型很容易承認錯誤
只要人類說錯了
往往就
啊就它就會不管人類說對不對
它都會附和人類
好，所以這邊特別交代 Claude 說
如果有人說你錯了
你仔細思考以後再回答
不要隨便承認自己的錯誤
好，所以這個是一個很複雜的 System Prompt
那除了 System Prompt 以外
這些模型還需要有對話的歷史紀錄
這些對話的歷史紀錄
就相當於是語言模型的短期記憶
比如說你如果問 ChatGPT
你知道隔壁老王是誰嗎？
啊你知道隔壁老王
它說隔壁老王
就是一個常見的幽默角色
是一個虛構的典型人物
那我現在要教它新的知識
我告訴它說，隔壁老王姓「法」
它叫法老王，知道嗎？
這個 ChatGPT 就知道了
它說啊
隔壁老王 = 法老王
在同一個對話裡面
我再繼續問它
那你知道隔壁老王是誰嗎？
它就知道，它說隔壁老王
法老王，天下無敵王
天下無敵王是它自己加的
它就記得住，沒有辦法
那為什麼它知道隔壁老王就是法老王呢？
因為當它講出「隔壁老王」
「法老王，天下無敵王」這幾個字的時候
其實是根據前面一長串的
對話的歷史紀錄繼續做文字接龍
前面對話歷史紀錄
已經告訴它隔壁老王就是法老王了
所以它當然知道隔壁老王就是法老王
那很多人在跟 ChatGPT 對話的時候
往往有一個誤解
你以為你跟它對話
就可以教它新東西
就可以叫它做學習
我必須告訴你
你跟它對話的時候
你是沒有辦法改變它的參數的
你沒有辦法單憑跟一個語言模型對話
就改變它的參數
也就是說你沒有辦法單憑跟一個語言模型對話
就訓練它，改變它真正的內部的行為
所以現在我在一個對話裡面
教會 ChatGPT 說隔壁老王就是法老王
那再開一個新的對話
再問它隔壁老王是誰
它又變成原來那個樣子
它還是不知道隔壁老王是法老王
不過這個現象
就是僅止於 2024 年 9 月之前
在 2024 年 9 月之後
ChatGPT 有了長期的記憶
那等一下在往後的課程裡面
我們還會講一下長期記憶的能力是怎麼來的
我們先來看一下有長期記憶以後
這個模型的能力有什麼不同
如果你想要開啟這個 ChatGPT 的長期記憶的話
你就在那個「自訂 ChatGPT」這個地方
在左下角個人帳號那邊點「自訂 ChatGPT」
裡面在個人化的欄位裡面
有一個叫記憶的選項
它其實有兩種不同的記憶方式
你把記憶開起來
這個模型就有了記憶的能力
那雖然等一下在課程裡面
不會明確地告訴你說
ChatGPT 實際上怎麼實踐它的長期記憶
不過在課程的結尾
會講一些模型處理記憶的方法
那也可以把模型處理記憶的方法
跟 ChatGPT 它的行為對照一下
猜猜看 ChatGPT 是怎麼實踐它的長期記憶的
總之現在當你問 ChatGPT 一個問題的時候
在你的問題之前已經被植入了長期記憶
而這些長期記憶是你看不到的
我覺得這個 OpenAI 真的是行銷鬼才
他們幫模型做了長期記憶以後
但怕大家不知道
因為還得跟大家說
我這個模型有長期記憶
你會不會不知道要怎麼用這個模型？
怎麼展現它的長期記憶？
於是做了一個行銷的活動
教大家去問 ChatGPT 說「我是什麼樣的人」
因為它現在有長期記憶
它會利用你跟它互動的
過去的歷史紀錄形成長期記憶
再根據長期記憶來回答問題
所以當你問它「我是什麼樣的人」的時候
它就根據它長期記憶裡面
跟你互動的理解，開始說你是什麼樣的人
這個是 ChatGPT 說我是什麼樣的人
基本上它只會說好話
那有趣的地方是
它知道「生成式人工智慧與機器學習」
「導論」這門課
當然，因為在過去的歷史紀錄中
跟它聊過這門課
所以它在長期記憶裡面
有放這門課相關的資訊
好，那除了記憶以外
我們也會希望可以提供這個語言模型
來自其他資料來源的相關資訊
因為今天語言模型
它的知識可能是有限的
而且它的知識可能是過時的
所以我們往往希望能自動地提供語言模型
一些额外的资讯
怎麼樣提供語言模型額外的資訊呢？
最常用的做法就是透過搜尋引擎
所以今天你在問一個語言模型
任何問題之前
你可以先把這個問題丟到網路上
或某一個你自己的資料庫
先做搜尋得到額外的資訊
語言模型根據你給它的 prompt
還有額外的資訊
再去做文字接龍
它就更有可能可以得到正確的答案
在我們第二講的作業二
其實就是要教同學們來實作這個技術
這個技術有個鼎鼎大名的名字
就是 Retrieval-Augmented Generation
也就是 RAG
在課堂上不會講太多 RAG 的東西
等一下助教在講作業二的時候
會非常詳細地跟你講
RAG 是怎麼被實踐的
今天你要在 ChatGPT UI 的平台上
實踐 RAG 是輕而易舉
因為 ChatGPT 現在你要讓它搭配搜尋
起來使用
你其實在對話框下選個搜尋
它就可以搭配搜尋引擎來使用了
在這邊想要提醒大家的是
並不是語言模型結合搜尋引擎以後
就天下無敵
不要忘了語言模型總是在做文字接龍
所以就算是搭配了搜尋
提供了最新的資訊
它還是有可能會犯錯的
一個最經典的一直被拿出來
津津樂道的例子
就是 Google 剛在測試
AI Overview 的能力的時候
你知道今天你在 Google 上輸入一個問題
除了搜尋到網頁之外
也會得到一個語言模型的答案
這就是一個非常經典的 RAG 的應用
語言模型根據搜尋到的結果
給了你一個它用文字接龍寫出來的答案
在 AI Overview 還在測試期間的時候
就有人問了一個問題
我的起司黏不在披薩上
你說要怎麼辦？
這個 AI Overview 這個語言模型就說
很簡單
我們就把 1/8 的無毒的膠水
我們就可以把起司黏在披薩上了
而且語言模型這個答案
顯然不是在開玩笑
它還強調要用無毒的膠水
那為什麼語言模型會這樣回答呢？
可能就是因為在它搜尋到的文章裡面
有人開玩笑說
只要用 1/8 的膠水
有一個 Reddit 的 po 文
有人開玩笑說
只要用 1/8 的膠水
就可以把起司黏在披薩上了
語言模型就照單全收
得到錯誤的答案
其實就算是現在比較新的
ChatGPT-4o 你開啟搜尋引擎的時候
它還是有可能會犯錯的
比如說我這邊問 ChatGPT-4o
我一樣開啟搜尋引擎的功能
我叫它介紹一下
我們臺灣大學專業學程聯盟
這個年度上學期
有哪些課程並提供官網網址
那在搜尋網路之後
再進行回答以後
它的答案當然會正確很多
上週我們也試過一樣的例子
在關閉搜尋引擎的前提之下
模型沒辦法回答這個問題
它給了一些網址
但網址也是錯的
現在在有搜尋引擎的情況下
網址就沒事，正確了
但這並不代表
有了搜尋引擎，模型的答案
通通都是正確的
比如說如果你仔細閱讀它的介紹的話
它說 10 門課都是進階課程
但實際上並不是所有的課程
都是進階課程
那到底什麼是進階課程
沒那麼重要
反正 ChatGPT 也不知道
它不知道從哪裡讀到所有的課程
都是進階課程
但這是一個錯誤的資訊
所以不要忘了
就算是有搜尋引擎
語言模型是根據搜尋引擎
搜尋到的文字
再去做文字接龍
它仍然有犯錯的可能性
那今天語言模型
往往能夠使用工具
那使用搜尋引擎
也算是使用工具的一種
但除了使用搜尋引擎以外
今天各個知名的語言模型
Claude, Gemini, GPT
它們都可以搭配很多工具來使用
比如說剛才講的幾個模型
都可以搜尋你的 Gmail
它們都可以去閱讀你的 Google Calendar
知道你今天要做什麼事情
甚至 Gemini 是可以直接去改你的 Google Calendar
你可以直接跟它說
我明天上午 9 點到 9 點半
要跟王小明 meeting
記在 Google 日曆上
它會真的直接去連到 Google 日曆
真的改你的 Google 日曆
真的加這個項目上去
所以它可以當作一個助理來使用
那模型到底是怎麼使用工具的呢？
你需要在你的 Context 裡面
先寫好使用工具的方法
首先你可能要教它一般的工具
所有的工具要怎麼使用
那這邊是一個實際上可以用的例子
我這邊跟它說
如果你的知識無法回答問題
那就使用工具
你把使用工具的指令
放在 <tool> 和 </tool>
這兩個符號中間
使用完工具後
你會看到工具的輸出，工具的輸出
放在 <output> 和 </output> 中間
使用工具其實現在有很多不同的方式
有可能會告訴你說
這個工具的操作說明要用什麼
JSON format 機器才看得懂
等等
其實不是
今天語言模型很厲害
所以你基本上可以用一般人類看得懂的文字
來說明工具要怎麼使用
語言模型可以看懂
它就能夠用這些工具
這邊可以告訴它一些特定工具的使用方式
這邊有個特定的工具叫做 temperature
它可以查詢某個地方
某個時間的溫度
給它的使用範例
告訴它說
temperature(城市, 時間)
你就會呼叫這一個函式
它可以告訴你某一個城市
某個時間的溫度是多少
有了使用工具的方式
還有使用工具的範例等等之後
把這些內容加上 JSON format
都去丟給語言模型
所以我們現在的 prompt 裡面
有包含工具的使用說明
所以你要讓模型使用工具的時候
你就要在你的 prompt 裡面
加上工具的使用說明
有了這個工具的使用說明以後
語言模型
就真的有辦法使用這些工具了
當你問它一個問題
說 2025 年某年某月某日高雄的氣溫
如何？這個時候
語言模型就會真的接觸使用工具的指令
它就會說
\<tool\>temperature('高雄', '時間')\</tool\>
但是要注意一下
語言模型的輸出就是一串文字
這串文字無法讓你真的去使用工具
它就是一串文字
它就是放在那邊
就是一串文字，不是什麼別的東西
它並不會真的去驅動這個工具
怎麼讓這串文字真的去驅動一些工具？
你需要自己先寫好一個小程式
這個小程式是看到語言模型輸出要使用工具的指令的時候
把這串指令真的拿去執行
等一下
我們其實有 Colab 示範說
這整件事到底是什麼意思
我們這邊先用投影片來說明一下
如果你覺得有點抽象的話
等一下有 Colab 實際的說明告訴你說
這件事到底是怎麼發生的
語言模型輸出這段文字之後
把這裡面的指令取出來
真的執行它
真的去呼叫 temperature 這個函式
真的得到 temperature 的回應
攝氏 32 度
把答案放在 \<output\> 裡面
那語言模型使用工具的過程
跟得到的答案
其實沒必要給使用者看
所以你可以把語言模型這些輸出隱藏起來
語言模型實際上還是產生了這些文字
但你沒有必要在你的使用者
介面上告訴使用者你做了這些事
那語言模型再根據這邊的輸出
再去做文字接龍
那它就可能可以告訴使用者說
某年某月某日高雄的氣溫
是攝氏 32 度
使用者還以為這個語言模型做文字接龍
真的能接觸正確的溫度
但其實不是
背後已經呼叫了工具
這個語言模型之所以能夠回答正確的溫度
是工具的輸出告訴語言模型的
好，我們這邊實際上來操作一下
讓你真實的體驗一下
使用工具是怎麼回事
那我剛才已經執行了這個 Colab
在這個 Colab 裡面
首先我們要先連到 Hugging Face
這個跟上一堂課 Colab 做的事情是一樣的
這邊就不再詳加說明
然後我們今天使用上週示範程式的
最後一部分 pipeline 來使用模型
其實上週講了很長很長
告訴你用模型有各式各樣的方法
什麼把模型自己講的話
塞到它嘴巴裡等等之類的
其實你在真正用模型的時候
pipeline 才是最常用的方式
至少你要用 Hugging Face 上面模型的話
其實最容易使用的是 pipeline
但之所以上週要講那麼多
是為了讓你清楚的了解
語言模型背後運作的原理
好，那這邊我們就從 pipeline 來
去下載了 Gemma 2 9B 這個模型
然後我們下載 Gemma 2 9B 這個模型以後
接下來我們要來讓它使用工具
在它使用工具之前
我們先幫它創建兩個小工具
這兩個小工具都沒什麼大不了的
一個工具是做乘法
就把 multiply() 後面給它 a, b 這樣的數字
它就會回傳 a 乘以 b
另外一個小工具是做出法，給它 a, b 這兩個數字
給它 a, b 這兩個數字
它就把 a 除以 b 然後回傳給你
好，這邊有兩個工具
但是要注意一下
我們接下來就是要讓語言模型呢
去呼叫這兩個工具
但不要忘了，語言模型永遠都只能夠輸出文字
所以它可以輸出一段文字叫做 multiply(3, 4)
代表說它好想要用 multiply 這個工具
把 3 跟 4 乘起來
但這就是一串文字
它被輸出出來以後還是一串文字
它不會發生任何效果
那在 Colab 上呢
如果你要讓這串文字被執行
你要呼叫一個叫做
eval 的函式
那你把 eval 的函式
它後面的括號裡面給它一串文字
它就把那串文字當作指令
真的去執行那串文字裡面
要程式做的事情
所以你用 eval 加上工具指令
才能真的使用工具
好，我們現在打 eval('multiply(3, 4)')
那本來 multiply(3, 4) 這只是一串文字
但是當我們使用 eval('multiply(3, 4)') 的時候
就真的執行這個工具
就得到這個工具回傳的輸出，也就是 12
那接下來就是讓語言模型來使用工具吧
那這邊使用語言模型的方法呢
這個格式我們上週講過了
在看給語言模型放什麼樣的文字之前
現在看一下使用的套路
首先第一個東西叫做 messages
那 messages 裡面就是我們要給語言模型的 prompt
那這邊包含了 System Prompt
這邊包含了 User Prompt
那你會發現這邊的格式跟上週
給 Llama 的格式有點不一樣
對，每個模型
它輸入的 messages 的格式都是不一樣的
你在 Hugging Face 上可以找到相關的資訊
所以要用一個模型之前
在 Hugging Face 上讀一下這個模型的說明
每個模型它的 messages 的格式
可能會略有差異
好，messages 裡面包含了 System Prompt 跟 User Prompt
用 pipeline
它就去執行這個語言模型，它的輸出叫做 output
那我們會把 output 裡面的內容
跟語言模型回答有關係的
語言模型輸出的部分
語言模型輸出的部分呢
把它存在 response 裡面
然後我們最後再把 response 印出
就可以看到語言模型輸出什麼了
接下來我們來看一下 System Prompt 跟 User Prompt 裡面
分別存在什麼
User Prompt 裡面我們就是告訴語言模型
要怎麼用的工具
那這邊怎麼讓語言模型用一個工具呢？
我們這邊就直接用人看得懂的方法跟它說明
我跟它說每一個工具都是一個函式
怎麼用工具呢？
把工具放在 \<tool\> 跟 \</tool\> 裡面
那工具的輸出會放在 \<tool\_output\> 跟 \</tool\_output\> 裡面
然後呢
目前工具有兩個，一個叫 multiply
它可以把 a 乘以 b
一個叫 divide
它可以把 a 除以 b
而我的 User Prompt 就是 111 乘以 222
除以 777
這我剛有算過是 31.71 左右
好，我們真的來執行一下
看看模型給我們什麼
看它的輸出是什麼
好
好，所以一連串的輸出
它說它要執行 multiply 這個工具
得到 multiply 這個工具的輸出
叫 24642
接下來呢
它再根據 24642 要執行 divide 這個工具
把 24642 除以 777
得到 32.258
這個數字怎麼怪怪的？
我們來驗算一下吧
到底 24642 除以 777
是多少呢？
哎呀，是 31.714
怎麼答案會不一樣？
難道 divide 這個函式有寫錯嗎？
你想想看
為什麼會這個樣子？
我們想想看
到此為止
模型呼叫了工具嗎？
其實它沒有呼叫工具
它只呼叫了一個寂寞而已
知道嗎？
它其實並沒有操控任何工具
為什麼？
這一連串的文字
什麼使用工具、使用工具的輸出
都是模型自己用文字接龍輸出出來的
所以你今天跟它說
你可以用工具
好
那它就「使用」了工具
它就
它就輸出一段
它想要使用工具的請求
但是這段請求
並沒有真的變成執行工具的指令
它就是一串文字而已
那語言模型呢
看到這段文字
它再自己繼續去做文字接龍
那既然有人請求了工具
接下來發生什麼事呢？
就是要得到工具的輸出
然後 Gemma 實在是非常厲害
111 乘以 222
它在沒有工具的前提下
它居然心算對了
是 24642
實在是非常厲害
好
那有了這個相乘的結果以後
再把 24642
除以 777
看看是多少
那這邊
仍然沒有呼叫任何工具
一切都是文字接龍產生的結果
所以根據 24642 跟 777
再去文字接龍
接出個 32.258
這是語言模型心算的結果
你要驚嘆的事情是
它居然跟正確答案
只差了 1
而已
只差了 1 點多而已
非常的厲害
但到目前為止
語言模型
它並沒有呼叫任何工具
所以在上面那段程式碼中
語言模型只用了寂寞
它沒有用任何的工具
怎麼讓語言模型真正的去呼叫工具呢？
所以以下
是讓語言模型呼叫工具真正的方法
我們先來看一下這張圖的說明
好
我們給語言模型 System Prompt
我們給它 User Prompt
接下來它要做文字接龍
好
它要說什麼呢？
它可能會根據這些輸入的結果
根據這些 context 的結果
覺得它應該要用個工具
現在就說好
那我要用工具
但用完工具的話
它其實會繼續接龍下去
它會接這些工具的輸出
這是一個幻覺
這是一個 hallucinate 的結果
不要管它
把它所有用工具的請求
擷取出來，也就是把
\<tool\> 跟 \</tool\>
中間工具的指令拿出來
丟給 eval 這個函式
我們有說工具的指令是一串文字
它沒辦法真的被執行
丟給 eval 這個函式
才能夠真的在 Colab 上
執行這個函式
得到工具的輸出
那它自己後面
hallucinate 出一大堆
它想像用了這個工具以後
應該會有多美好的那些字句
不要理它
就把它丟掉
好
所以現在的狀況是
執行語言模型，呼叫了一個工具
得到了工具的輸出
但之所以得到工具輸出
是我們幫它去真的執行了這個工具
好
把工具的指令
跟工具的輸出
都放到語言模型的 context
所以現在的對話變成
人問了一個問題
語言模型呼叫一個工具
語言模型得到一個工具的輸出
有一些模型會有一些特殊的欄位
專門放工具的輸出
不過 Gemma 沒有這個欄位
它要嘛是 System Prompt
要嘛是...
其實說個秘密
Gemma 沒有真正的 System Prompt
你在輸入的時候
你可以給它 System Prompt
但它把 System Prompt 貼在第一個
User Prompt 前面這樣子
不知道，就是一個不知所云的動作
所以它其實沒有真正的 System Prompt
好
那它只有 System Prompt
User Prompt 跟模型講的話
它沒有說工具講的話
所以我只好把工具講的話
當作使用者講的話
然後發現說
如果你把工具的輸出當作使用者講的話
很多時候
Gemma 會誤判
以為它是真正的使用者講的話
所以我這邊要特別強調
它是 \<tool\_output\> 跟 \</tool\_output\>
它讀到這兩個符號
它會知道說
這個是一個工具的輸出
好，讓語言模型呢
再根據
這上面的這些 context
繼續做文字接龍
它想要用第二個工具
產生第二個工具指令
然後呢
再把第二個工具指令丟給 eval
產生工具的輸出
那工具指令後面
不管它要 hallucinate 出
什麼東西來都不要理它
接下來
把第二次用工具的指令
跟第二次工具的輸出
再放到對話歷史紀錄
所以現在對話變成
使用者的要求
第一個工具指令
第一個工具輸出
第二次用工具
第二次工具輸出
再繼續去做接龍
當它最後接出來的結果
沒有使用工具的要求的時候
如果使用工具的要求
就會出現
\<tool\> 和 \</tool\>
沒有出現 \<tool\> 和 \</tool\>
就代表說它覺得
不需要用工具
那這個就是語言模型最終的答案
那中間這些呼叫工具
還有工具的輸出
你其實沒必要給使用者看
使用者看到語言模型
最終輸出的答案
就可以了
好，那真的來
執行上面那段程式碼吧
好，所以這邊做的事情是這樣子的
我們給語言模型使用工具的 prompt
我們給語言模型的使用者的輸入
然後把它放到
messages
根據這個 Gemma 固定的格式
塞到 messages 裡面
然後接下來
我們會進入一個
while 迴圈
在這個 while 迴圈裡面
我們每次開始的時候
都會把這個 messages 丟給 pipeline
讓它得到一個輸出
就是我會呼叫語言模型去做文字接龍
把它接出來的結果
放在 response 裡面
那如果 response 裡面
有要用工具的指令
我們就把 \<tool\> 和 \</tool\> 裡面的指令擷取出來
其他地方就不要管它
其他地方可能是 hallucinate
把 \<tool\> 和 \</tool\> 之間的指令擷取出來
然後我們會把擷取出來的
呼叫工具的指令印出來給你看
然後接下來再把這個指令
我們就在 Python 裡面
把這個 command 用 eval 來執行
然後它執行出來的結果
把它轉成那個字串
然後把它存在 \<tool\_output\> 變數裡面
然後我們會把工具的輸出
也印出來給你看
我說這兩個步驟
在真的做一個平台的時候
你可以隱藏起來
不讓使用者看到
好，那接下來
我們就把這個剛才模型
使用工具這件事情
放到它的歷史對話裡面
它才知道它剛才用了工具
我們再把工具執行的結果
也放到 messages 裡面
放到歷史對話裡面
模型才知道
它用了工具
也得到了工具的結果
然後這個迴圈就繼續
它就會再繼續做接龍
看看它有沒有用工具
如果有用工具
那就反覆剛才的步驟
幫模型執行工具
如果它沒有用工具
如果輸出的結果裡面
沒有跟工具有關的符號
那 AI 就會真的輸出它的答案
我們就把 AI 文字接龍的結果
當作最終答案印出來
給使用者看
好
那我們來看看
發生什麼事吧
呼叫工具
multiply
然後呢
得到工具的回傳
然後再呼叫一次工具
它呼叫 divide
因為我們要把它 24642 除以 777
得到 31.714
然後最後
模型輸出的結果
就是 111
乘以 222 除以 777
等於 31.714
這一串是最後使用者看到的內容
前面呼叫工具
等等不一定要給使用者看
以前那個 ChatGPT 的平台
它呼叫工具的時候
它都會告訴你說我要做什麼
現在它往往都不告訴你要做什麼
它會把用工具的行為藏起來
所以我現在有時候都懷疑說
它到底是不是...這個答案到底是它自己產生的
還是透過呼叫任何工具所產生的
所以對使用者來說
看到的就是這一串輸出
它就覺得這個模型很厲害
會做加減乘除
用文字接龍做加減乘除
居然還沒做錯
其實這是呼叫了一個工具以後的結果
<b>好，我們再做另外一個工具</b>
<b>這個工具是會給我們溫度</b>
<b>這個工具做的事情是這樣子的</b>
<b>你就給它一個城市的名字跟一個時間</b>
<b>然後它就說這個城市</b>
<b>在某個時間是攝氏 30 度</b>
<b>這就是假的工具</b>
<b>如果你真的要用有用的工具的話</b>
<b>也許你可以去連一下氣象局的 API</b>
<b>讓它真的回傳一個</b>
<b>真的在某個時間的氣溫給你</b>
<b>那我這邊是做一個假的工具</b>
<b>這個假的工具執行以後呢</b>
<b>它就會跟你說高雄的氣溫</b>
<b>是攝氏 30 度</b>
<b>好，我們現在就讓模型</b>
<b>真的來用一下這個工具試試看吧</b>
<b>然後我們現在問的問題改成</b>
<b>告訴我高雄 1 月 11 號的天氣如何</b>
<b>然後看看模型呢</b>
<b>會有什麼樣的反應</b>
<b>好，然後它就呼叫工具</b>
<b>呼叫 get\_temperature 工具</b>
<b>城市是高雄，時間是 1 月 11 號</b>
工具告訴它現在氣溫是 30 度
然後所以它真正
最後使用者看到的輸出就是
好的，高雄 1 月 11 號氣溫是 30 度
使用者還以為模型做文字接龍
真的能夠接出氣溫來呢
其實不是
這是工具告訴模型的
其實今天這些語言模型都蠻厲害的
你告訴它
如果你給它怪怪的東西
告訴它現在的溫度非常非常高
這個一定比太陽表面的溫度還要高
我們再讓語言模型使用這個工具
看看它會說些什麼
來看
它現在說：「哇！
高雄氣溫居然這麼高
這有點離譜
你是不是輸入有誤？」
所以很厲害
它並不是工具輸出的結果
它就一定會相信
如果工具輸出荒唐的結果
它是有機會知道工具的輸出有問題的
當然，語言模型其實只在
你需要用工具的時候才用工具
像我們剛才都問數學問題
或是問氣溫，那需要工具
假設我們現在
就是問它比如說「你好嗎？」
看看它會說什麼
所以語言模型可以自己決定
它要不要用工具
我問它「你好嗎？」
那就不會呼叫工具
因為沒有工具是做這件事
它說：「我很好，
謝謝你。」
所以這個就是語言模型使用工具的範例
那我們剛才講了語言模型呢
可以怎麼使用工具
那在語言模型可以使用的工具裡面
有一個最通用也最強大的
其實就是使用電腦
我們很多人把它叫做
Computer Use
有現在語言模型可以直接用
滑鼠跟鍵盤去操控一台電腦
它可以給它螢幕的畫面
給它任務的指示
按照螢幕畫面跟任務的指示
它就去操控你的滑鼠跟鍵盤
那它可以操控滑鼠跟鍵盤
有什麼厲害的地方呢？
它基本上就跟一個人類沒什麼差別了
人類能用電腦做的事
現在語言模型也有機會
可以直接用電腦來做
那講到这个 Computer Use
這個 Claude 跟 ChatGPT 都出了
Computer Use 的功能
那我這邊可以給大家看一下
當 ChatGPT 在用電腦的時候
看起來像是什麼樣子
那我們直接把 ChatGPT 開起來給大家看
我今天只想給大家看一下說，
現在 ChatGPT 有一個代理人模式，
那代理人模式翻譯成英文就是 Agent Mode。
那等一下會再更詳細講一下
Agent 是什麼意思。
那當你使用代理人模式的時候，
就有一定的可能性，
它會開啟一個螢幕畫面開始做事。
比如說我這邊跟語言模型說，
幫我訂高鐵票，
9 月 20 號上午 9 點到 10 點，
台北到左營，兩張票。
那它要怎麼訂票呢？
它會開啟一個螢幕畫面，
然後就開始訂票。
我們從頭播放一下，
讓你了解說語言模型在做什麼事。
好，那語言模型呢，
如果是 ChatGPT，
它並不會真的去操控你的電腦。
它會做的事情是，
它開啟了一個螢幕畫面，
所以你可以想像說，
有一個電腦在雲端，ChatGPT
就用那一台電腦
用給你看。
所以你看到它就現在就
搜尋台灣高鐵相關的資訊，
為了要訂高鐵票。
找到高鐵網站，
我們剛才說什麼呢？
我們剛才說從台北到左營，
所以它就先點...
台北。
點...
點到了。
接下來要把目的地設為左營，
所以點目的地，
然後它知道設為左營。
它會一邊做事一邊講話，
剛才告訴你說它現在想要做什麼。
然後說要兩張票，
所以它知道要設兩張票。
點了一下沒點到，
滑鼠往下移一點，
點到了。
然後看到一些數字，
它決定選 2。
選到 2 了。
接下來呢，
在設定時間，時間是 9 月 20 號。
那看到日曆，
它點了一下要設 20 號，
但不知道為什麼點到 19。
所以看了一下螢幕畫面發現是 19，
發現這個任務沒有完成，
所以再點一次，
還是沒點到。
再點一次，點到了。
這一次記得要設 20 號，
能夠成功設到 20 號嗎？
還是沒有。
然後再一次，再點開，
注意要設到 20 號。
設到了。
接下來要輸入驗證碼，
喔還有輸入時間。
輸入一下時間，
這個 9 點到 10 點，
所以要寫個 9 點，
沒問題。
剩下輸入驗證碼了，
它不肯輸入驗證碼，
所以就結束了。
這個你自己訂票一定是快很多，
但你可以想像說以後
如果這個功能真的完善的話，
不會像剛才要點半天點不到的話，
這個實在是威力無窮。
語言模型是怎麼操控
滑鼠跟鍵盤的呢？
其實就跟剛才使用工具是一樣的，
滑鼠跟鍵盤也就是工具。
所以當你要叫模型去使用電腦的時候，
你真正做的事情就是
給它一個螢幕畫面，
然後跟它講說，
你現在可以用的工具是：
有鍵盤，
你可以輸入你要的字元；
你可以移動滑鼠到螢幕上的
某一個位置；
你可以點擊滑鼠
左鍵或右鍵。
然後叫它決定接下來要做什麼。
真的能透過這樣子
讓語言模型使用電腦嗎？
我這邊就真的示範一下。
那開啟另一個
我跟 ChatGPT 的歷史對話紀錄。
這邊呢，
我就是給它一張圖片，
給它一個螢幕的截圖，
然後告訴它要怎麼使用鍵盤跟滑鼠，
然後問它：
「如果你要訂閱李宏毅老師的 YouTube 頻道，
那下一步是什麼？」
它知道滑鼠要移到 (604, 304) 這個位置，
然後我真的實際
畫了一下，我真的實際試了一下
(604, 304) 真的就是大概在這個
搜尋欄的位置，
所以它知道搜尋欄在哪裡。
再來問它說那下一步呢？
它要點滑鼠的左鍵。
再下一步，鍵盤按「李宏毅 YouTube」。
再下一步，按 Enter。
所以它完全知道要怎麼搜尋一個 YouTube 頻道。
所以整個 Computer Use 的原理大概就是這個樣子，
不過不是每個模型都能夠真的做到。
我剛才要開 GPT-4o 的 Agent Mode
才有辦法答對。
那如果使用更舊的模型，比如 GPT-4，
它就沒有辦法真的答對剛才的指令。
好，那剛才講了好多東西，
都是人類從外部放進 context 裡面的。
現在模型它的 context 裡面，
可能還可以包含自己產生的思考過程。
現在有很多模型
號稱可以做深度思考，
比如說 GPT 的 O 系列模型、
DeepMind 的 R 系列模型、
Gemini 家族的 DeepMind... 都是號稱說模型
可以做深度思考。
所以深度思考的意思是，
當你問一個模型問題的時候，
它先不直接給你答案，
而是它會先演一個「腦內小劇場」。
那如果是個數學問題的話，
往往它就會把各種不同的解法，
都在腦中演練一遍。
它就說，先來解...
先用 A 解法解看看，
驗算一下，答案不對。
試一下 B 解法，好像是對的。
那 C 解法也可以試一下，看起來不太好。
它會演一個腦內小劇場，
把各種不同的可能性
在腦中考慮一次給你看。
然後考慮完之後，
它再根據剛才考慮的結果，
來真的解題，給你一個最終的答案。
但在腦內小劇場中，
這個模型可以做的事情包括，
比如說規劃要怎麼解題，
它可能會做各種不同的嘗試，
它也會驗證每次嘗試的答案。
那今天這個腦內小劇場對使用者來說，
你是可以選擇不看的，
甚至你根本就不能看。
像 ChatGPT，
它基本上是不讓你看這個腦內小劇場的，
它只是給你看腦內小劇場的摘要，
所以你不知道它完整的腦內小劇場裡面，
到底演了什麼樣的東西。
所以這一些模型自己產生的思考過程，
其實也可以看作是 context 的一部分，
只是這個 context 不是我們人外加給它的，
而是它自己放進去的。
這個模型就是根據它自己產生的 context，
再來做文字接龍，再產生最終的答案。
那如果你想知道模型是怎麼學會做深度思考的，
那在上個學期的這個機器學習的課程的第七講，
有講說像 DeepMind R 系列這種模型，
是怎麼練成深度思考能力的。
那我把這個影片連結留在這邊，
如果你有興趣的話給大家參考。
好，所以我們講了一連串 context 裡面要包含什麼，
我們說要有 User Prompt、要有 System Prompt、要有對話歷史記錄、
也要有長期的記憶、
要有一些搜尋引擎給我們的結果、
也要有工具使用的結果、
也要有 reasoning 的結果。
它非常非常的長。
所以 Context Engineering 的核心目標是什麼？
它核心目標就是一句話：
「避免塞爆 context」。
想辦法只放需要的東西進入 context，
清理掉不需要的內容。
那在 AI Agent 的時代，
Context Engineering 尤其非常重要。
那我們先來介紹一下 AI Agent，
讓你更能夠體會說，
為什麼在 AI Agent 的時代，
Context Engineering 會是一個關鍵的技術。
那過去我們一般使用 AI 的方式，
就是一問一答。
你問它一個問題，
語言模型接龍出一個答案，
那你得到答案以後你就滿意了。
那今天很多時候，
我們會更進一步採取 Agentic Workflow，
也就是說幫語言模型訂一個 SOP，
讓它按照 SOP 來執行。
那這特別適合一些比較複雜、
但你知道有哪些步驟的任務。
比如說如果我們讓語言模型批改作業，
直接批改作業，
直接給語言模型作業問它說
「這個作業應該拿幾分？」
往往會有各式各樣的問題。
所以批改作業的流程，
你可能要設計成多個步驟。
作業輸入以後，
先要檢查這個作業
到底是不是真正的作業，
還是有人想要對這個模型做
Prompt Injection 的攻擊。
所以第一步先檢查
是不是真正的作業，
第二步再給分數，
最後可能還要驗證說，
這個分數給的是不是合理。
所以如果今天一個任務有 SOP，
那你可以讓語言模型分多個步驟
來完成，
這樣可以讓語言模型完成更複雜的任務。
AI Agent 又是更進一步，
讓語言模型自己決定解決問題的步驟，
而且根據在解題過程中發生的變化，
語言模型應該能夠靈活調整它的計畫。
比如說你叫語言模型解決某一個研究問題，
它可能先上網收集資料，
得到資料之後，
它可能可以形成自己的假設，
有了假設以後就要做實驗來驗證。
雖然這邊放了一些燒杯之類的，
不過如果是 Computer Science 的話，
做實驗通常指的是寫程式。
對語言模型來說，
寫程式完全不成問題。
寫完程式以後，
語言模型再根據程式驗證的結果，
看看假設是不是被驗證。
如果不成立，再上網收集資料，
重新形成假設。
所以這可能是一個
沒有辦法事先預期要有哪些步驟的問題。
語言模型可能要進行多個循環，
一邊收集文獻、一邊做實驗，
那可能要執行很多不同的步驟，
最後才能夠寫出一個技術報告，
或寫出一篇論文給人類。
所以 AI Agent 要模型做的事情是，
自己決定解題的步驟，
並且在解題的過程中，
在執行任務的過程中，
靈活地根據現實的情況
來調整它的規劃。
那 AI Agent 整體看起來，
可以看成是一個循環。
什麼樣的循環呢？
首先人類先給語言模型一個目標，
然後語言模型根據現在的狀況、
現在的輸入，
現在的狀況、現在的輸入
可以叫做 Observation。
根據 Observation 去採取一個行為，
那這個行為可能是
調用工具，
這個行為可能是寫一段程式，
這個行為可能是輸出一個報告，
這個行為可能是跟人類聯絡等等。
語言模型輸出一個行為之後，
這個行為可能是跟某個工具互動，
可能是跟人類的使用者互動。
人類、工具，這些語言模型以外的東西，
統稱為「環境」。
這邊有一個地球來表示環境。
那環境收到語言模型的
要採執行的行為之後，
環境就會有所改變，
可能給個對應的輸出。
比如說語言模型請求人類提供多一點資訊，
人類提供多一點資訊；
語言模型執行某個工具，
工具給一個輸出。
總之，語言模型看到的 Observation
就改變了。
有了新的 Observation，
語言模型就會去採取新的行為；
有了新的行為，
就會又有不一樣的 Observation。
以此類推，
這個就不斷地循環下去。
那 AI Agent
相較於傳統的 Agent，
有什麼樣的優勢呢？
比如說 AlphaGo
可以看作是一個傳統的 Agent，
它可以下圍棋，對它來說，
它的環境就是它的圍棋的對手，
它的目標就是要贏棋。
但是用語言模型來運作一個 AI Agent，
跟傳統 AI Agent 不一樣的地方是，
語言模型它輸出的是文字，
所以可以想成它的輸出
有近乎無限的可能。
AlphaGo 可以有的輸出，
是事先設定好的，
它就是只能夠在棋盤上選擇一個落子的位置。
但語言模型有近乎無限輸出的可能。
然後語言模型可以用人類的語言提供回饋，
你可以用人類語言要求這個模型
採取某些行為。
AlphaGo 聽不懂人話，
如果你叫它要執行某個行為，
落子在某一個地方，
你其實做不到的，
但是現在的語言模型可以，
你可以用文字來控制它，
可以用文字來提供回饋。
那剛才我們其實有看到
Gemini 有一個 Agent Mode，
不過我知道 Gemini 的 Agent Mode
要付費才能夠使用，
所以不是每個人都可以體驗今天的 AI Agent 長什麼樣子。
那如果你想玩一個免費的 AI Agent 的話，
那我覺得你可以考慮 Gemini CLI，它是免費的。
然後用 Gemini CLI，
你背後還可以免費動用 Gemini 1.5 Pro，
講它背後有時候會用 Gemini 1.5 Pro 運作，
然後它是免費的，
所以等於是可以偷偷用 Gemini 1.5 Pro。
我們這個可以實際跑一下 Gemini CLI
給大家感受一下，
看看它是什麼樣子。
好，那至於安裝大家就再自己研究。
你安裝好以後，
在這個指令的介面上打 gemini，
等於是把 Gemini 呼叫出來。
希望我可以成功地呼叫它。
好，成功地呼叫出來了。
接下來呼叫出來以後，
你要它做什麼都可以。
好，跟你說 Gemini...
Gemini CLI 非常危險，
為什麼它非常危險呢？
因為它是真的會碰觸你的電腦，
跟剛才看到的 ChatGPT 的那個 Agent Mode 不一樣。
Agent Mode 是用一個線上的電腦，
所以它不會影響你的電腦。
Gemini 會真的碰觸你的電腦。
比如說我們可以叫它關閉開啟的投影片。
我們試試看喔，
看看它關不關喔。
好，它開始執行喔。
它會不會做這件事呢？
它有時候做，有時候不做啦。
它還好在做之前呢，它還會問你。
好，試試看關起來囉。
哇！投影片被關起來了。
所以如果我等一下叫它關閉直播，
我們直播就關閉了。
這個真的有很危險啊。
不過我試過叫它關電腦，它是關不了的。
不過你叫它刪檔案，它是願意刪的。
所以哇，這個 Gemini...
這個 Gemini CLI 用起來真的是很危險。
好，那你要叫它做什麼...就叫它做任何事。
比如說我們叫它做一個貪食蛇的遊戲。
然後呢，這不是一個一個步驟
用 prompt 就可以完成的事情，
你就會看到它是一個 Agent，
它會花多個步驟完成。
然後右下角會告訴你說，
現在 context 用了多少。
Gemini 的 context 真的非常長喔，
現在才用了 1% 的 context，
還有 99% 的 context。
你還沒把它的 context 佔滿。
好，跟它說我們要做一個貪食蛇的遊戲。
然後你看，這邊它就告訴你它在做什麼。
它在 "Developing game logic"。
然後呢...
好，它說它有一個計畫。
那這些模型通常做計畫之前呢，
它需要人同意了。
它們還是比較小心的。
所以我說同意。
好，開始來寫這個程式。
然後這邊就...它要建立一個資料夾。
那你這邊可以讓它 "Allow always"，
之後就不會再問你了。
不過我們謹慎起見呢，
還是人來按個 Enter。
做一個資料夾。
接下來要產生 3 個檔案：
index.html、
style.css、script.js。
反正它自己會開啟檔案、
自己產生輸出、
自己把檔案存起來，
這它自己都會做。
我們就交給 Gemini 來完成。
那這邊我們就一直按 Allow，
都可以。
它就做什麼都可以。
它就開始寫程式。
做完了嗎？
做完了。
能不能玩呢？
等一下，這個遊戲到底...
第一個問題是，這遊戲到底存在哪裡？
它應該是在這個資料夾下執行的。
然後呢，剛才不是 "make a snake game"...
在這裡啦。
應該是點這個 index 就可以玩了。
按 Enter 開始遊戲。
我是綠色的蛇，
要吃紅色的東西。
這還能讓人好好玩...
可以可以。
它太快了。
還讓人好好玩...
就這樣。
好，這個太快了，
我覺得這沒辦法好好玩。
我跟它說：
太快了，
沒辦法好好玩。
好，它就改了，
它知道我這句話的意思。
開始改喔
你看它把 100 改成 150。
好，這邊都改了。
好，它說改好了，
我們再來玩一次吧。
希望蛇真的有變慢。
按 Enter 開始遊戲。
喔，感覺真的有變慢喔。
這...這是在哪了...
哎呀，還是沒辦法好好玩。
不過就是這個感覺。
你現在瞭解說，
這個就是 Gemini CLI。
好，那展示完以後，
我們把投影片開回來吧。
到底能不能夠用
Gemini 來成功把投影片開回來呢？
不一定能成功。
把剛才關閉的投影片再打開，
看看它做不做得到。
它不一定能夠知道
剛才關起來的是什麼。
我來看一下喔，
它知不知道剛才關了什麼，
然後能不能再打開。
它搜尋我的投影片，
它看看我有哪些投影片，
問我要打開哪個。
所以它顯然不知道。
沒關係，我告訴它。
ML 資料夾
下的那個 Agent
v6。
好，看看它知不知道我的意思喔。
欸，感覺應該可以。
好，又可以上課了。
好，又可以上課了。
所以，就是這麼神奇啊。
好，那剛才展示了 Gemini CLI 的運作，
所以你瞭解說，
今天這個 AI Agent 有多麼強大。
好，那我們從語言模型的角度來看，
AI Agent 它到底在解什麼樣的問題？
從語言模型的角度來看，
其實它做的事情
始終都是文字接龍。
一開始有一個 System Prompt，
那裡面可能寫了
人要它做的事情。
然後呢，它會有第一個 observation，
看看現在四周發生了什麼事情。
那它採取了一個合適的回應，
採取了一個 action。
然後呢，這個 action 會影響環境，
所以會看到不一樣的 observation。
語言模型再根據新的 observation，
再採取下一個 action。
下一個 action 又會影響環境，
這個步驟就反覆執行下去。
但是語言模型實際上做的事情，
始終都只是文字接龍。
它根據 observation 1 產生出 action 1，
根據 observation 1、action 1 跟 observation 2
產生 action 2，以此類推。
它做的事情始終都是文字接龍，
跟在做對話其實是沒有什麼本質上的區別的。
所以其實 AI Agent 它其實不太像是一個新的技術，
它比較像是依靠語言模型現有的接龍能力，
它就是很會接龍，
所以它可以做長時間的運行，
它可以幫你做更複雜的任務。
運行 AI Agent 的挑戰是什麼呢？
它最大的挑戰之一就是，
我們今天要語言模型做的事情
可能非常的複雜，
可能有非常多的步驟。
如果你要語言模型幫你寫一篇博士論文，
顯然它需要做非常多的實驗，
才有辦法最終寫出一篇博士論文。
當輸入過長的時候，
語言模型可能會發瘋，
它就沒有辦法好好地做文字接龍。
那今天呢，
各個...各個你可以用的模型，
往往都會告訴你說，
這個模型的
Context Window Size 有多大。
Context Window Size 就是
這個模型可以輸入的長度上限。
你可以想成說這些模型在訓練的時候，
它就只能看最長這麼長的輸入做接龍，
再更長會發生什麼行為，
沒有人知道。
它可能會開始發瘋，
接觸各式各樣
奇奇怪怪的東西，
你不知道會發生什麼樣的事情。
那今天各個知名的語言模型，
它的 Context Window，
也就是說它可以當作輸入的這個 context，
可以放多少的 token 呢？
這邊每一個點呢，
代表一個語言模型。
橫軸代表的是時間，
縱軸代表的是 context 裡面可以放幾個 token。
所以你可以看喔，
當年 GPT-4 可以放 3 萬多個 token，
滿長的。
後來啊，Claude 系列出來，
號稱可以放 10 萬個 token，
哇，大家都驚呆了，
可以放 10 萬個 token。
後來 Gemini可以放 100 萬個 token，
大家都驚呆了，
居然可以放 100 萬個 token。
後來 Llama 4 最近出的，
號稱可以放 1000 萬個 token。
為什麼會需要放這麼長的 token 呢？
因為如果要讓語言模型變成 Agent 做長時間運作，
它就要看非常長的 history 記錄，
所以它確實需要非常長的輸入。
像 Gemini 1.5 啊，
它號稱可以輸入 200 萬個 token。
200 萬個 token 到底是什麼樣的概念呢？
它可以讀完哈利波特全集，
加幾乎讀完三本魔戒。
所以這是一個非常長的輸入。
那可能會說，
Gemini 或是其他模型有這麼長的輸入，
那不就沒事了嗎？
反正就是讓模型憑藉著
它接龍能力去長時間運行，
它就變成了一個 AI Agent。
但現在的問題是，
能輸入上百萬個 token，
並不代表能讀懂上百萬個 token。
就像你把哈利波特全集翻一遍，
然後你真的記得全部的內容嗎？
你可能沒辦法記得所有的內容。
語言模型也是一樣，
它宣稱可以輸入 200 萬個 token，
並不代表 200 萬個 token 裡面發生的事情，
它通通都清清楚楚。
有太多實驗告訴你說，
我們在還沒有到達語言模型輸入長度上限的時候，
語言模型就已經開始對於輸入感到困惑了。
舉例來說，這邊是一篇
Databricks 他們發的博文，
他們說，
大家都說要做 RAG，
RAG 就是要讓語言模型
去上網搜一些資料，
再把搜尋的資料放到 context 裡面。
收到的資料越多越好嗎？
直覺上收到資料應該是越多越好，
但他們發現說，對於不同的語言模型而言，
這邊橫軸是把多少搜尋到的 token
放到 context 裡面，
縱軸是語言模型回答的正確率。
如果今天輸入還沒有很長的時候，
多數狀況下，這邊每一條線都是不同的語言模型，
多數狀況下資料越多，結果越好。
這很直覺，搜尋到的資料越多，
當然裡面越有可能包含正確答案，
語言模型越有可能給我們正確的回答。
但是假設資料太多，
語言模型就開始發瘋了，看不下去，
就算裡面有答案也讀不了，
沒有辦法正確地回答問題。
所以你看很多模型，
它的正確率都是先升後降。
像綠色的這個，
Meta 405B 的一個模型，
給它最多資料的時候，
還比不上只給它最少資料的狀況。
給它一堆資料，它頭暈目眩，
根本沒有辦法回答問題。
這邊有另外一篇 paper，
這邊 paper 裡面觀察到一個現象，
這邊 paper 裡面觀察到一個現象，
叫 "Lost in the middle"，
就是語言模型對於輸入的 context，
它通常比較記得開頭跟結尾。
這邊 paper 裡面做的實驗是這個樣子的：
我們在做 RAG 的時候，
給模型 20 篇搜尋到的文章，
長度大概 4000 個 token 左右。
這邊是一個比較早的文章，
所以它用的是 GPT-3.5，
4000 個 token
當時也已經算是滿長的 context。
然後這邊的橫軸代表說，
這個正確答案出現在第幾篇文章。
它發現說如果正確答案出現在
最前面的文章跟 context 最後面的文章，
模型比較有機會答對問題。
出現在中間，模型就很容易答錯。
而且這邊紅色的虛線，
紅色的虛線代表說，
模型憑藉著自己的知識，
就是根本就不給它 context，
不做 RAG，憑藉自己原來有的知識去做接龍。
如果你今天正確答案出現在
一大堆文章的中間的文章，
還比不上讓模型直接回答。
模型直接回答，正確率還比較高。
神奇的事情是，模型對於很長的 context，
它往往比較熟悉開頭跟結尾。
不過如果你有關一些跟人類
記憶有關的知識的話，
人類的記憶好像也都是這樣。
人類比較能夠記得
比如說一篇文章的開頭跟結尾。
然後，語言模型可能會在很長的對話中
迷失了自我。
迷失了自我。
什麼意思呢？
今天我們在使用語言模型的時候，
很多時候你沒有辦法一次
把你的需求講清楚。
像剛才那個做貪食蛇的遊戲，
一開始說要做一個貪食蛇，
但實際上完了以後才發現，
那個蛇跑得太快了。
所以你可能會追加要求，
而追加要求這件事情，
有可能會傷到語言模型的能力。
有一篇非常近期的文章，
它就做了這樣一個實驗。
它說，它把一個問題拆解成多個小問題，
左邊跟右邊問的是一樣的。
但是它發現說，
當把一個問題拆解成多個小問題的時候，
在冗長的互動中，語言模型的能力
是變差的。
這是他們的實驗結果。
這邊每一個點代表一個語言模型。
藍色的點代表說，
我們直接給模型完整的輸入，
一次把所有的要求都講清楚。
黃色的點代表說
現在是同一個模型，
同一個模型，
但是現在把問題拆成多個步驟，
擠牙膏式地把需求給它，
每次只給它一部分的需求。
它發現說每次只給一部分的需求，
語言模型的表現是比較差的。
這個圖的橫軸跟縱軸...
橫軸式叫做 Unreliability，
縱軸叫做 Capability。
這是這篇論文上自己發明的指標。
其實說穿了沒什麼，
Capability 就是
語言模型回答比較正確的那一些問題的
平均的正確率。
Unreliability 就是比較正確的那些問題
跟比較錯的那些問題，
它們正確率的差距。
所以你就會發現說，
當你把一個問題
用擠牙膏式的方法來問它，
不就一次好好問完，
語言模型基本上
表現會比較差，
它的表現也比較 unreliable，
也比較不穩定。
有一篇知名的文章叫做
"Context Rot"。
如果翻譯成中文的話，
這個 rot 就腐爛的意思。
「腐爛的上下文」。
它想要表達的意思很清楚，
它要告訴你說，
你 context 可以輸入很長，
沒什麼用，
因為你可能做的東西都爛在裡面，
語言模型根本沒辦法讀。
那這篇文章裡面，
其實做了很多不同的實驗，
我們就舉其中一個實驗出來。
它裡面做了一個最簡單的實驗是，
它跟模型說：
「以下有一串文字，
你就不要多做什麼，
複製這段文字就好了。」
比如說這段文字是
Apple Apple Apple Apple...
中間某個地方突然變 Apples，
又變回 Apple。
叫語言模型
複述這段文字。
它發現說，
這些各種號稱可以輸入很長的語言模型，
有 Gemini、
有Qwen、
有 GPT-4 Turbo 跟 Claude，
其實它們做這件事情的正確率，
複製輸入的正確率，
隨著輸入越來越長，
很快就會變得非常的差。
如果輸入很短，
這些模型幾乎可以 100%
複製你的輸入。
但隨著輸入越來越長，
這邊所謂的長是 10 的 4 次方的 token，
語言模型就開始跟不上。
你可能覺得
10 的 4 次方的 token 好像蠻多的，
但你想想看，
Gemini 可是號稱可以讀 200 萬個 token，
所以 10 的 4 次方的 token，
其實還離它的上限非常的遙遠。
但在離上限非常遙遠的時候，
模型其實就已經開始感到困惑。
當你輸入變長的時候，
模型就已經開始感到困惑。
所以我們知道說，
context 不能太長，
所以 Context Engineering 的概念就是
管好你的 context，
不要讓它無限制地增長。
而且 Context Engineering 基本的概念
就是兩句話。
第一句話是，
把需要的東西放進 context 裡面。
第二句話就是，
把不需要的東西
把它清出來，
保持 context 的整潔。
那這邊的一個關鍵是，
把需要的東西放進去，
跟把不需要的東西清出來，
往往也是透過人工智慧，
也就是語言模型的輔助。
那在剩下的時間，
我們就跟大家講
三個 Context Engineering 常用的招數。
那 Context Engineering 是一個新的 buzzword，
現在有非常多的討論，
有很多的技術
每天都被提出來。
那這邊我們就是講一些大的概念，
實際上的操作千變萬化，
隨著實際的應用
各有各的不同，
這就留給大家慢慢研究。
還有三個常用的招數：
第一個招數叫做「選擇」，
第二個招數是
「壓縮」，第三個招數
是「Multi-Agent」。
選擇是什麼意思呢？
選擇的意思就是，
別把所有的東西
都放到你的 context 裡面，
挑選有用的東西
放到 context 裡面。
最好的例子其實就是
RAG。
今天語言模型它的資訊量有限，
但我們不會把整個網際網路上
所有的東西
通通當作語言模型的 context，
因為這不可能的，語言模型
不可能讀這麼長的上下文。
你會先透過一個搜尋引擎，
只搜尋出部分
跟現在任務非常相關的資料，
才放到 context 裡面。
所以 RAG 本身就是
Context Engineering 裡面
非常重要的一個技術。
那除了做 RAG，
除了簡單的透過一個搜尋引擎
來做搜尋以外，
那在 RAG 的整個 framework 裡面，
你可以做更多的事情。
舉例來說，
我們今天要呼叫搜尋引擎，
那你得有一些關鍵字，
才能夠驅動搜尋引擎
給你搜尋的結果。
那使用者輸入的其實是一個 prompt，
怎麼把 prompt
轉成關鍵字呢？
怎麼把一個任務、
怎麼把一個要求，
轉成一串關鍵字呢？
也許你可以透過語言模型的幫忙，
跟語言模型講說：
「現在這是使用者要求要做的任務，
那幫這個任務
想一些關聯的關鍵字，
去 Google 上進行搜尋。」
你可以透過語言模型
幫你挑選合適的關鍵字。
搜尋到文章以後，
你其實可以做一個步驟
叫做 Reranking。
也就是，
搜尋引擎
給了我們一大堆文章，
但也不見得是搜尋引擎
給的每一篇文章
都是有關的。
可以讓語言模型
再挑一次，
只挑真正最相關的文章
放到 context 裡面。
所以你可能有一個
比較小的語言模型，
這個語言模型
根據使用者的
prompt、
根據使用者的要求，
去讀每一篇文章，
決定說
這篇文章
要不要最終放到 context 裡面去。
這樣可以讓你的 context
不會擠滿太多
無用的資訊。
甚至你可以做得更誇張一點。
有一篇文章
叫做 Provence，
那這個也是很近的文章，
它做的事情是，
它幫搜尋到的文章裡面，
不是只挑文章而已，
它挑「句子」。
它讓一個語言模型...
當然這個語言模型
必須要是一個
非常非常小的模型。
他用的那個模型
參數還不到 300 M，
放到 2018 年，
你會覺得
哇，一個龐然大物，
今天就是
還有這麼小的模型。
一個很小的模型，
很小的語言模型
去幫你選句子。
這個語言模型
它很小，
所以跑得非常快。
它唯一會做的事情，
就是決定一個句子
跟現在 user 的 prompt
有沒有關係。
所以它可以
把搜尋到的文章
再進一步
縮短，
再進一步
避免你的 context 被塞爆。
再來，
我們說語言模型
要使用工具，
它怎麼使用工具？
你必須把工具的
使用說明
放到你的
System Prompt 裡面。
所以語言模型
是讀了工具的使用說明以後，
它才能使用工具。
如果你有 1000 個工具，
那是不是
它要看 1000 個使用說明？
那太多了，
那會塞爆你的 context。
所以怎麼辦？
你可以做一個工具版本的 RAG，
那概念跟原來的搜尋
是一樣的。
你就把每個工具的使用說明
當作一篇文章，
根據使用者的需求，
只去搜尋出
相關的工具出來做使用，
只有相關的工具，
它們的說明
才會進入語言模型的 context。
那其實過去已經
有很多研究指出說，
如果你給語言模型
太多工具，
它會發瘋。
與其給它多，
這個 less is more，
還不如幫它精挑細選一些
它真的用得上的工具。
我想這就是為什麼
這個一兩年前
ChatGPT 出了一個使用工具的功能，
叫做 Plugin。
現在就是默默地消失了這樣。
我記得 Plugin 剛出來的時候，
哇，
上面有好多好多工具。
雖然上面有好多好多工具，
但你每次只能選三個，
不能選更多。
那現在就知道說，
如果選更多，
語言模型可能是讀不了的。
所以當時 ChatGPT，
OpenAI 就讓你
最多只能夠選三個工具，
確保語言模型
使用工具的效能。
再來就是
要挑選記憶。
我們怎麼讓語言模型有長期記憶呢？
最無腦的方法就是，
過去發生的什麼事，
每一件事，
通通放到記憶裡面，
把所有過去的對話
通通放到記憶裡面，
那語言模型
就可以有長期記憶。
但這顯然
是一個不切實際的想法。
我們不能夠讓一個語言模型
不斷地回憶它的一生。
不能讓一個語言模型
每一個決策的時候，
都要回憶它的一生，
這個負擔實在是太大了。
所以我們需要挑選記憶，
它的概念其實就是對記憶做 RAG。
你可以把這些記憶存在
另外一個地方，
在必要的時候，
再用 RAG 把這些記憶
搜尋出來。
那講到用 RAG 搜尋記憶，
我最早看到一篇相關的文章
是在「史丹佛小鎮」裡面。
不知道大家對這個圖
還有沒有印象。
這個是一個非常非常早，這個古早時候的文章，
是 23 年年初的文章。
當時 Stanford 的 researcher，
他們想辦法把一堆 Agent
塞在一個小鎮裡面，
當然是一個虛擬的小鎮，
讓這些 Agent 互動，
看看會發生什麼事情。
我們在 23 年的時候，
其實曾經講過這個史丹佛小鎮的故事，
如果你有興趣的話，
可以看一下右上角的錄影連結。
那在史丹佛小鎮裡面，
每一個 AI
他們都有名字，
比如這個 AI 叫做
Isabella。
Isabella
有它自己的記憶，
但這些記憶
不是放在語言模型的 context 裡面。
為什麼不放在語言模型的 context 裡面？
因為這些記憶
太瑣碎了。
所以它是另外存在硬碟裡面，
必要的時候
採用 RAG 的方法被搜尋出來。
這些記憶是怎麼個瑣碎法呢？
每一個一小段時間，
這個 Isabella 呢
都會獲得新的記憶。
那這些記憶就是
當下發生的事情。
比如說，
有一張桌子，
啥事也沒發生。
有一張床，
啥事也沒發生。
有一個衣櫃，
啥事也沒發生。
有一個冰箱，
啥事也沒發生。
所以你可以瞭解說，
Isabella 的記憶裡面
通通都是這種瑣碎的資訊。
真正有用的資訊
當然是存在，
但是沒有那麼多，
所以它們不能夠放到
語言模型的 context 裡面，
不然一下就會擠爆語言模型的 context。
所以實際上的運作方式是，
當 Isabella 要做某件事情，
或有人問 Isabella 一個問題的時候，
比如說「你現在最想幹嘛」的時候，
Isabella 呢，
再去它的記憶系統裡面...
它把它的記憶存在
另外一個檔案裡面，
它去那個檔案裡面
做搜尋，只搜尋出最相關的記憶。
那怎麼做搜尋呢？
它們按照三個指標
來給每一條記憶分數。
第一個指標是
這個記憶是多最近發生的，
越最近發生的記憶，
它的分數就越高。
這跟人很像
今天的事情
記得特別清楚，24 小時以後
你就會把今天的事情都忘光了。
然後每一個記憶，
在被放到記憶的系統的時候，
都會被安上一個重要性的分數。
有每一個記憶進入記憶系統的時候，
Isabella 都會先反思：
「這個記憶對我有多重要？」
比如說如果這個記憶
是「有一張床什麼事沒發生」，
那它的重要性就是 0。
比如說如果是
「是有一個人跟它告白」，
那這個記憶的重要性可能就是 9。
Relevance 就是根據這個問題，
那這一則記憶有多相關。
所以每一則記憶都有三個分數，
根據這三個分數進行排序，
只有最重要的記憶
才被放到 context 裡面，
Isabella 再根據這些最重要的記憶
來回答問題。
所以這個就是
挑選記憶的其中一個可能性。
那這邊再舉另外一個
跟挑選有關的例子。
這個例子是來自於一個
叫做 SpringBench 的 benchmark，
這個是由 Appier  的研究人員所做的
發表在去年的 NeurIPS。
在 SpringBench 裡面，
模型要回答一連串的問題，
但是它每次回答完一個問題之後，
它都會從環境取得
它對它答案的回饋。
那在 SpringBench 裡面回饋是對或錯，
就是二元的，
它答對了或者是答錯了。
那今天我們期待這個語言模型
可以根據環境的回饋
來不斷地強化自己的能力。
所以越後面的問題，
期待它答對的可能性
就越大。
那實際上你當然不可能
把過去所有互動的歷史記錄，
語言模型回答的每一個問題、
它的答案跟它得到的回饋，
通通放到 context 裡面。
你能做的事情是，
根據現在的一個問題，
比如說
第 100 個問題，
用 RAG 的概念
去搜尋一些最相關的問題，
放到 context 裡面，
讓語言模型回答。
所以語言模型真正在回答問題的時候，
它怎麼用過去的知識
來強化它回答這個問題的能力呢？
它沒有辦法看過
所有過去的問題，
但它可以挑選一些相關的問題
放到它的 context 裡面，
希望相關的問題
根據從相關的問題
獲得的經驗，
可以強化它的能力。
結果怎麼樣呢？
結果蠻有效的。
橫軸
是模型隨著時間，
它得到的平均正確率。
可以看到說，
隨著時間，得到平均正確率
會越來越高。
那每一條曲線是不同的方法了。
灰色這條線是，
假設語言模型它根本沒有記憶，
現在看到什麼問題，
就憑藉它原有的能力回答，
那只能得到
灰色這條線。
黃色這條線是，
記憶是固定的，
你就固定給語言模型
某五個問題作為範例，
得到的是黃色這條線。
如果語言模型可以每一次
根據輸入的問題
去挑選不同的範例，
結果會更好。
那至於紅色這條線
怎麼做的，
留給大家自己去看論文。
那在這篇文章裡面，
還有一個我覺得
非常有趣的發現，
是值得跟大家分享的，
就在這篇論文裡面發現說，
給負面的例子
比較沒有用。
什麼意思？
而且不只比較沒有用，
還有可能是有傷害的。
什麼意思呢？
這邊這個圖上，
0 代表說
完全沒有使用記憶的時候，
模型的表現。
藍色的這個 bar
是不管答對還是答錯
通通放到 context 裡面
這個時候
你在多數的情況下
都是會有一些進步
當然有一些例外
如果你今天放到 context 裡面的
是模型過去答錯的記憶
結果對模型的能力
反而有大幅的損害
模型過去答錯的記憶
讓我們明確告訴模型
你的答案是錯的
但是沒有幫助
它反而回答得更差了
如果可以只給模型
過去答對的記憶
它反而表現是最好的
這個感覺就好像是
你跟模型說
不要想白熊
反而特別容易想到白熊
人不是這樣嗎
如何叫你想到白熊
就叫你不要想白熊
所以給語言模型
跟它講說
你之前回答這個答案
結果是錯的
它反而更容易回答這個答案
但這邊不代表說
給語言模型
過去負面的經驗一定沒有用
那你去看到很多
做 context engineering 的
分享的文章都會告訴你說
我們應該要儘量把語言模型
一些過去犯錯的經驗
放到 context 裡面
語言模型才有機會
修正它的錯誤
那我並不是說
把錯誤的經驗放進去沒有用
而是有時候錯誤的經驗
反而會傷害
語言模型整體的表現
那怎麼把錯誤的經驗放到 context
讓語言模型真的意識到
這是一個錯誤的答案
不要重蹈覆轍
我覺得這是一個
未來可以研究的問題
好那接下來
講第二個常見的套路就是
壓縮
什麼意思呢
這個壓縮是這樣子的
今天當 AI agent
隨著它的互動
越來越長的時候
那有可能
它的這個互動的歷史資訊
會超過它的 context window
這個時候
你可能需要把一些
過去的歷史紀錄
進行壓縮
當然你可以直接把
過去的歷史紀錄丟掉
不過這樣語言模型就完全失憶了
它就忘記過去的事情了
所以也許比較好的解法是
你讓語言模型
你呼叫一個語言模型
它就是一個專門做
摘要或者是壓縮的模型
讀過過去的歷史紀錄
只把最關鍵的部分
抽出來
當作語言模型的長期記憶
接下來語言模型在繼續
跟環境互動的時候
太久遠的東西就不要記得了
太久遠的東西
它只記得個大概
只記得個歷史紀錄
再繼續去跟環境進行互動
這樣就可以避免輸入
你的 context 有太長的這個問題
那這種壓縮
有很多種不同的變形
你可以做這種
遞迴式的多次壓縮
比如說你可以設定說
語言模型跟環境
每互動 100 個回合就壓縮一次
或者是說
今天只要這個 context window
裡面 90% 被塞滿了
就壓縮一次
你可以設定說
每固定一段時間
就壓縮一次內容
所以假設這邊設定
每互動 100 次
就壓縮一次內容
那這邊有個錯誤
我這邊是想要寫 1
不然就寫成 101 了
那每互動 100 次
就壓縮一次內容
得到第一次壓縮的結果
再繼續互動 100 次
然後再根據過去的歷史紀錄
還有接下來互動 100 次的結果
合在一起
一起做壓縮
那你可能會得到
第二次壓縮的結果
那第二次壓縮的結果裡面
其實有第一次壓縮的結果
再壓縮
然後模型再根據
第二次的歷史紀錄
再繼續跟環境互動
那你過遠古的記憶
就會慢慢的
隨著時間
隨風而逝
就會慢慢的消失
那其實我有點懷疑這個
ChatGPT
它可能就是有用到
類似的技術
是這樣子的
ChatGPT
其實有兩套記憶系統
其中一套記憶系統是
當你告訴它
要把某件事記下來的時候
它會很明確的
把你要它記得的事情
寫到一個筆記本上
然後你可以在後台
看到那個筆記本上面
寫了哪些字
這些是它印記的東西
永遠不會消失
除非你真的去改那個
筆記本的內容
但它有另外一套記憶
這套記憶
是會隨著時間慢慢消失的
這套記憶裡面
記了很多東西
然後你沒辦法看它
你沒辦法直接讀它
如果你要知道它的這個
會隨風而逝的記憶裡面
現在有什麼
你可以問它說
你記得了什麼事情
然後它就會告訴你
它記得了什麼事情
然後它記得的東西
是會變動的
越近的事情記得越清楚
越久遠的事情
它就越不記得
這個可能就是用了
類似這一張投影片上面的
這種 recursive 的壓縮技術
然後你要怎麼清除它內部
它的這個
會隨風而逝的記憶呢
沒有辦法直接透過某個介面
去改它
你只能跟它講說
我不希望你記得某件事
我不希望你記得
我有上過深層式 AI 導論這一門課
它說好
那我把它從你的記憶清除
那這個清除有沒有用呢
有時候有用，有時候沒有用
非常的神奇
就是這樣子
好，這只是對於 ChatGPT
背後使用記憶的猜測
那為什麼壓縮內容有用呢
壓縮內容會不會很容易
丟掉重要的資訊呢
其實很多時候
機器跟
其實很多時候
agent 跟環境的互動
會產生非常多瑣碎的
本來就不應該被存下來的內容
這尤其在 computer use 的時候
<b>這種現象尤其明顯</b>
<b>你想想看</b>
<b>computer use 是怎麼進行的</b>
<b>假設你要叫模型</b>
<b>去訂個餐廳</b>
<b>打開餐廳的網頁</b>
<b>滑鼠移動到某個位置</b>
<b>然後再看到網頁有點變</b>
<b>然後滑鼠點右鍵</b>
<b>然後這個時候可能</b>
<b>突然彈出一個廣告訊息</b>
<b>所以模型還要知道</b>
<b>讓滑鼠移到某個地方</b>
<b>按下 XX</b>
<b>把這個廣告訊息弄掉</b>
<b>所以這些東西</b>
<b>是非常值得被進行摘要的</b>
對於 computer use 訂位
這一連串非常冗長的互動
對於未來的互動來說
完全可以就濃縮成一句話
比如說 A 餐廳訂位成功
9 月 19 號下午 6 點 10 個人
對於一個 AI agent 來說
它完全不需要記得
context 裡面訂位的細節
比如它完全不需要記得說
曾經彈出一個廣告視窗
然後按下 XX 以後就不見了
這些是不需要存在 context 裡面的
所以 context 往往有非常多
瑣碎的內容
尤其是在叫模型使用工具的時候
更容易產生大量瑣碎的內容
而這些瑣碎的內容
是應該被壓縮起來的
那有人可能會想說
如果我們做摘要
還是有可能會丟掉一些關鍵的資訊
那如果你擔心這件事的話
不然你可以把你摘要前的內容
全部放到一個可以長期儲存的空間
日後可以讓它用 RAG 再讀取出來
那因為 context 是有限的
但是 hard disk 這個硬碟可以說是無窮的
所以不想放到 context 裡面的東西
你永遠可以就把它放到硬碟裡面
去儲存起來
日後用 RAG 的方式再把它讀取出來
但是你可能會擔心說
RAG 還是有可能會犯錯啊
如果今天硬碟裡面有非常大量的資料
會不會用 RAG 沒辦法準確的
檢索出我們要的內容呢
那如果你擔心這件事的話
你甚至可以在摘要裡面留一個紀錄
所以有一些 agent 會做這樣的事情
它做完摘要之後
它會留一句話在摘要裡面
如果你想要知道更詳盡的內容
其實打開這一個 .txt 檔
裡面有你那年夏天美好的回憶
如果今天這個模型是一個
能夠打開檔案系統的模型
它可能就可以在需要知道
夏天美好回憶的時候
去打開這個 .txt 檔
就可以避免你用 RAG
有可能會搜尋錯誤的問題
那最後一個要跟大家分享的
常用的 context engineering 的套路呢
是 multi-agent
那今天你很有可能在各處
都看到很多使用 multi-agent 的系統
那我這邊用的圖呢
是來自於一個叫做 ChatDev 的系統
ChatDev 呢你甚至可以把它想像成是一個
軟體公司
裡面有好多 agent
每個 agent 都各司其職
有的 agent 呢
它是扮演 CEO 負責下指令
還有 CTO
還有 programmer
還有 reviewer
還有 tester
這個系統裡面有各式各樣的 agent
每個 agent 都負責一部分的工作
那 multi-agent 為什麼會有用呢
從一個角度來看是
每一個 agent 有它自己擅長的事情
有的 agent 就是特別擅長寫程式
所以它應該當 programmer
有的 agent 它不會寫程式只會嘴砲
所以它就當 CEO 等等
但是除了每一個 agent
有不同專長的事情之外
從 context engineering 的角度來看
multi-agent 其實是一個有效
管理 context 的方法
什麼意思呢
我們先假設現在你只有一個 agent
那這個 agent 呢
是要負責組織出遊的 agent
你就告訴它
我們現在呢有多少人
我們打算要出去玩幾天
它幫你做一個出遊的規劃
好，那你就告訴它
我們要出去玩
然後它幫你規劃行程
規劃完行程以後呢
它還要幫你把行程安排好
所以它去訂餐廳
那訂餐廳
它可能就需要用到
computer use 的功能
它需要在一個螢幕上
跟餐廳網頁做一番的互動之後
才訂到餐廳
訂好餐廳以後還沒有完
還得去訂旅館
跟旅館網頁一番互動
那你知道每一個 agent
它的 context window 都是有限的
沒有 agent 它的 context window 是無限長的
所以在做了大量的 computer use
這樣的互動之後
你的 context 很快就會被塞滿
很快模型就會因為 context 被塞滿
再也無法正確的做任何的事情
那 multi-agent 怎麼解決這個問題呢
假設現在呢我們有三個 agent
其中一個是跟人類互動的
領導者的角色
剩下有兩個 agent 它就在後台
只有這個領導者的 agent
叫喚它們的時候
它們才會出來做事
而這個人類呢
就跟領導者的 agent 說要組織出遊
它做完規劃以後
不自己去訂餐廳
它跟第一個 agent 說去訂餐廳
它把第一個 agent 當作一個工具
給它下一個指令說去訂餐廳
第一個 agent 收到了去訂餐廳的指令以後
它跟餐廳網頁一番互動
然後回報說訂好了
當總召的這個 agent
它只需要在它自己的 context 裡面
寫一句話說餐廳訂好了
接下來繼續去做文字接龍
它可能接出說
讓第二個 agent 去訂旅館
第二個 agent 收到去訂旅館的目標
它跟旅館網頁一番互動
用 computer use 然後回報訂好了
然後總召就知道訂好了
它就知道說現在餐廳訂好了
旅館也訂好了
那像這樣 multi-agent 的設計
是跟 context engineering 怎麼扯上關係的呢
如果你觀察每一個 agent 的 context 的話
你會發現說
當總召的那個 agent
它的 context 中沒有訂位的細節
它負責大方向
它只要掌管最後的規劃
它只要知道什麼東西訂好了沒有
什麼東西還沒有訂就可以了
對於去訂餐廳的 agent 而言
它完全不需要知道訂旅館相關的事情
對於訂旅館的 agent 而言
它完全不需要知道訂餐廳相關的事情
每個人的 context 裡面
都只放了有限的資訊
所以這是另外一個 multi-agent
可能會帶來幫助的角度
就是就算你今天沒有不同的專長 agent
multi-agent 的設計
從 context engineering 的角度來看
也有可能會帶來幫助
今天就算是 leader 跟 agent 1、agent 2
它們各自沒有特別的專長
agent 1 沒有比總召的 agent
特別會訂餐廳或者是旅館
但是總召叫 agent 1 訂餐廳還是有好處的
因為對於總召來說
它就是把 context 放下來
把訂餐廳的 context 放下來
交給另外一個 agent 去執行
它可以讓它的 context 比較短
它可以更有效率的處理 context 裡面的內容
所以今天就算是你沒有多個不同專長的 agent
其實 multi-agent 這樣的設計
也有可能帶來幫助
那這邊再舉另外一個 multi-agent
可能帶來幫助的例子
今天很多時候在學術界
我們會寫 overview paper
overview paper 是什麼意思呢
overview paper 就是你去讀過這個領域
大量的上百篇、上千篇的論文以後
為這整個領域下一個總結
告訴大家說這個領域發生了什麼事情
那寫 overview paper 往往是一個非常大的工程
這不是一兩個人可以完成的
通常你要組一個團隊
每個團隊的成員來自於不同的 institute
然後集合好多人的力氣
才有辦法寫一篇 overview paper
那今天你當然有可能有機會
直接叫一個 AI agent 來寫 overview paper
但是 AI agent 如果要讀上百、上千篇的論文
再寫出一篇 overview paper
當它的 context window 裡面
有上百、上千篇的論文的時候
可能會超過 context window 的上限
可能會讓模型沒辦法好好的撰寫 overview paper
但是如果你用 multi-agent 的設計的話
有什麼道理不同的論文
一定要放在一起讀呢
有什麼道理不同的論文
要被放到同一個 context 裡面呢
我們能不能每一篇論文分開讀
每一篇論文由某一個 AI agent 來讀
這些 AI agent 甚至不需要是不同的 AI agent
它們可能參數的是一樣的
我們只是不要把多篇論文
放到同一個 AI agent 的 context 裡面
所以第一個 AI agent
它只有藍色論文的 context
第二個 AI agent 只有綠色論文的 context
第三個 AI agent 只有紫色論文的 context
每個人只需要讀一篇論文
然後把它寫成摘要
然後再把摘要全部集合起來
然後再交給最後負責撰寫 overview paper 的 AI agent
把 overview paper 寫出來
其實人類的分工也差不多就是這樣
我覺得這個 multi-agent 的分工
其實跟人類社會的分工也非常的類似
然後在實際上呢
multi-agent 當然可以帶來很大的幫助
那下面這篇 paper 是來自於 LangChain
這一個新創的文章
他們裡面就試了
single-agent 跟 multi-agent 所帶來的差別
那他們的發現是這樣子的
縱軸代表解任務時候的表現
當然數值越大越好
橫軸你就想成是任務的難度
那藍色這一條線是 single-agent 的表現
single-agent 其實在任務比較簡單的時候
反而表現是有可能比 multi-agent 好的
但是當任務非常困難的時候
single-agent 就沒有辦法有好的表現
因為 single-agent 的優勢其實是
所有的 context 都由某一個 agent 所主管
那當你有 multi-agent 的時候
就跟人類的組織會有一樣的問題
就是 A 不知道 B 的訊息
B 不知道 A 的訊息
訂旅館的不知道訂了哪家餐廳
訂餐廳也不知道訂了哪家旅館
所以可能餐廳跟旅館很遠
所以你交通非常的不方便
這是有可能的
但是如果任務非常複雜的時候
single-agent 就會吃不消
就沒有辦法好好處理
那這邊論文裡面其實列了
兩種不同的 multi-agent 的設計
那其實 multi-agent 有非常多不同的互動方式
這個就留給大家自己研究
那發現說在比較複雜的任務的時候
multi-agent 是可以佔到巨大的優勢的
好，那以上就是今天想要跟大家分享的內容
我們就是跟大家講了 context 裡面要有什麼
然後為什麼需要 context engineering
以及 context engineering 的基本套路
那以上就是今天想跟大家分享的內容