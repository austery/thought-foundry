These
 models
 are
 allowing
 creators
 to
 do


um
 less
 tedious
 parts
 of
 the
 job,
 right?


They
 can
 be
 more
 creative
 and
 they
 can


spend,
 you
 know,
 90%
 of
 their
 time
 being


creative
 versus
 90%
 of
 their
 time
 like


editing
 things
 and
 doing
 these
 tedious


kind
 of
 manual
 operations.


>> I'm
 convinced
 that
 this
 ultimately


really
 empowers
 the
 artists,
 right?
 It


gives
 you
 new
 tools,
 right?
 It's
 like,


hey,
 we
 now
 have,
 I
 don't
 know,


watercolors
 for
 Michelangelo.
 Let's
 see


what
 he
 does
 with
 it,
 right?
 And
 amazing


things
 come
 out.


maybe
 start
 by
 telling
 us
 about
 the


backstory
 behind
 the
 nano
 banano
 model.


How
 did
 it
 come
 to
 be?
 How
 did
 you
 all


start
 working
 on
 it?


>> Sure.
 So
 um
 you
 know
 our
 our
 team
 has


worked
 on
 image
 models
 for
 some
 time.
 We


developed
 the
 imagine
 family
 of
 models


which
 is
 goes
 back
 a
 couple
 years.
 Um


and
 and
 actually
 there
 was
 also
 an
 um
 an


image
 generation
 model
 in
 Gemini
 before


the
 Gemini
 2.0
 image
 generation
 model.


So
 what
 happened
 was
 the
 um
 the
 teams


kind
 of
 started
 to
 focus
 more
 on
 the


Gemini
 use
 cases.
 So
 like
 interactive


conversational
 and
 and
 editing


>> um
 and
 and
 essentially
 what
 happened
 is


we
 teamed
 up
 and
 we
 we
 built
 this
 model


which
 became
 what's
 known
 as
 nano


banana.
 Um
 so
 yeah
 that's
 sort
 of
 the


origin
 story
 but


>> yeah
 and
 and
 I
 think
 maybe
 just
 some


more
 background
 on
 that.
 So
 our
 imagine


models
 were
 always
 kind
 of
 top
 of
 the


charts
 for
 visual
 quality
 and
 you
 know


we
 really
 focus
 on
 kind
 of these


specialized
 generation
 editing
 use
 cases


and
 then
 when
 2.0
 Flash
 came
 out
 that's


when
 we
 really
 started
 to
 see
 some
 of


the
 magic
 of
 like
 being
 able
 to
 generate


images
 and
 text
 at
 the
 same
 time
 so
 you


can
 maybe
 tell
 a
 story.
 Um
 just
 the


magic
 of
 being
 able
 to
 talk
 to
 images


and
 edit
 them
 conversationally.
 Uh
 but


the
 visual
 quality
 was
 maybe
 not
 where


we
 wanted
 it
 to
 be.
 And
 so
 Nano
 Banana


or
 Gemini
 2.5
 flash
 image
 um


>> Nano
 Banana
 is
 way
 cooler.


>> It's
 it's
 easier
 to
 say.
 It's
 a
 lot


easier.


>> It's
 the
 name
 that
 stuck.


>> Yes,
 it's
 the
 name
 that
 stuck.
 Uh
 but
 it


really
 became
 kind
 of
 the
 best
 of
 both


worlds
 in
 that
 sense
 like
 the
 Gemini


smartness
 and
 the
 multimodal
 kind
 of


conversational
 nature
 of
 it
 plus
 the


visual
 quality
 of
 imagine.
 And
 I
 feel


like
 that's
 maybe
 what
 resonates
 a
 lot


with
 people.


>> Wow.
 Amazing.
 Um,
 so
 I
 guess
 when
 you


were
 testing
 out
 a
 model,
 as
 you
 were


developing
 it,
 what
 were
 some
 wow


moments
 um
 that
 you
 found,
 I
 know
 this


is
 going
 to
 go
 viral.
 I
 know
 people
 will


love
 this.


>> I
 So
 I
 actually
 didn't
 feel
 like
 it
 was


going
 to
 go
 viral
 until
 we
 had
 released


on
 Ellarina.
 And
 what
 we
 saw
 was
 that
 we


budgeted
 like,
 you
 know,
 a
 comparable


amount
 of
 queries
 per
 second
 as
 we
 had


for
 our
 previous
 models
 that
 were
 on
 Elm


Marina.
 And
 we
 had
 to
 keep
 upping
 that


number
 as
 people
 were
 going
 to
 Ella


Marina
 to
 use
 the
 model.
 And
 I
 feel
 like


that
 was
 the
 first
 time
 when
 I
 was


really
 like,
 "Oh,
 wow.
 This
 is
 something


that's
 very
 very
 useful
 to
 a
 lot
 of


people."
 Like
 I
 it
 surprised
 even
 me.
 I


don't
 know
 about
 the
 whole
 team,
 but


like
 we,
 you
 know,
 we
 were
 trying
 to


make
 the
 best
 conversational
 editing


model
 possible.
 But
 um
 but
 then
 it


really
 started
 taking
 off
 when
 when
 yeah


when
 people
 were
 like
 going
 out
 of
 their


way
 and
 using
 a
 using
 a
 website
 that


would
 actually
 only
 give
 you
 the
 model


some
 percentage
 of
 the
 time.
 But
 even


that
 was
 worth
 like
 using
 going
 to
 that


website
 to
 use
 the
 model.
 So
 I
 think


that
 was
 really
 the
 moment
 at
 least
 for


me
 that
 I
 was
 like
 oh
 wow
 this
 is
 this


is
 going
 to
 be
 bigger.


>> That's
 actually
 the
 best
 way
 to


condition
 people
 like
 only
 give
 them
 a


rewards
 partially
 not
 all
 the
 time
 by


design.
 Uh
 I
 had
 a
 moment
 earlier
 um
 and


that
 was
 when
 so
 I've
 been
 trying
 some


similar
 queries
 on
 kind
 of
 multiple


generations
 of
 models
 over
 time.
 Um
 and


a
 lot
 of
 them
 have
 to
 do
 with
 like


things
 I
 wanted
 to
 be
 as
 a
 kid.
 So
 like


an
 astronaut
 explorer
 or
 you
 know
 put
 me


on
 the
 red
 carpet
 and
 I
 tried
 it
 on
 a


demo
 that
 we
 had
 internally
 before
 we


released
 the
 model.
 It
 was
 the
 first


time
 when
 the
 output
 actually
 looked


like
 me.
 Um
 and
 you
 know
 you
 guys
 play


with
 these
 models
 all
 the
 time.
 The
 only


time
 that
 I've
 seen
 that
 before
 is
 if


you
 know
 you
 fine-tune
 a
 model,
 you


know,
 using
 Laura
 or
 some
 other
 method


to
 like
 do
 that
 and
 you
 need
 multiple


images
 and
 takes
 a
 really
 long
 time
 and


then
 you
 have
 to
 like
 actually
 serve
 it


somewhere.
 So
 this
 was
 the
 first
 time


when
 it
 was
 like
 zero
 shot.
 Oh
 wow,
 just


one
 image
 of
 me
 and
 it
 looks
 like
 me
 and


I
 was
 like
 wow.
 And
 then
 there
 became


these
 like
 we
 have
 decks
 that
 are
 just


like
 covered
 in
 my
 face
 as
 I
 was
 trying


to
 convince
 other
 people
 that
 it
 was


really
 cool.
 Um,
 and
 really
 I
 think
 the


moment
 more
 people
 realized
 that
 it
 was


like
 a
 really
 fun
 feature
 to
 use
 is
 when


they
 tried
 it
 on
 themselves
 because
 it's


it's
 kind
 of
 fun
 when
 you
 see
 it
 on


another
 person,
 but
 it
 doesn't
 really


resonate
 with
 people
 emotionally.
 It


makes
 it
 so
 personal.
 It's
 like
 you
 your


kids,
 you
 know,
 your
 spouse
 and
 and
 I


think
 that's
 your
 dog,


>> your
 dog.
 And
 and
 and
 that's
 really
 what


started
 kind
 of
 resonating
 internally.


And
 then
 people
 just
 started
 making
 all


these
 like
 80s
 makeover
 versions
 of


themselves.
 And
 that's
 when
 we
 really


started
 to
 see
 like
 a
 lot
 of
 internal


activity
 and
 we
 were
 like,
 "Okay,
 we're


on
 to
 something."


>> It's
 it's
 a
 lot
 of
 fun
 to
 test
 these


models
 when
 we're
 making
 them
 because


you
 just
 you
 see
 all
 these
 amazing


creative
 things
 that
 people
 make.
 Oh,


wow.
 I
 I
 never
 thought
 that
 was


possible.


>> So,
 it's
 it's
 really
 fun.


>> No,
 it's
 I
 mean,
 we've
 dealt
 with
 the


whole
 with
 the
 whole
 family
 and
 it's


it's
 it's
 a
 it's
 a
 crazy
 amount
 of
 fun.


>> So,
 think
 a
 bit
 about
 long
 term.
 Where


does
 this
 lead?
 Right.
 I
 mean
 we
 we


built
 these
 new
 tools
 that
 I
 think
 will


change
 visual
 arts
 forever,
 right?
 I


mean
 there
 we
 suddenly
 can
 transfer


style.
 We
 suddenly
 can
 you
 generate


consistent
 images
 of
 a
 subject,
 right?
 I


have
 I
 have
 what
 used
 to
 be
 a
 very


complex
 manual
 Photoshop
 process.


Suddenly
 I
 type
 one
 command
 and


magically
 happens.
 But
 what's
 the
 end


state
 of
 this?
 I
 mean
 do do
 we
 have
 an


idea
 yet?
 You
 know
 how
 will
 how
 will


creative
 arts
 be
 taught
 in
 a
 university


in
 you
 know
 five
 years
 from
 now?


want
 to
 take
 that.


>> So
 I
 I
 think
 it's
 going
 to
 be
 a
 spectrum


of
 things,
 right?
 I
 think
 on
 the


professional
 side,
 a
 lot
 of
 what
 we're


hearing
 is
 that
 these
 models
 are


allowing
 creators
 to
 do
 um
 less
 tedious


parts
 of
 the
 job,
 right?
 They
 can
 be


more
 creative
 and
 they
 can
 spend,
 you


know,
 90%
 of
 their
 time
 being
 creative


versus
 90%
 of
 their
 time
 like
 editing


things
 and
 doing
 these
 tedious
 kind
 of


manual
 operations.
 So
 I'm
 really
 excited


about
 that.
 Like
 I
 think
 we'll
 see
 kind


of
 an
 explosion
 of
 creativity
 like
 on


that
 side
 of
 the
 spectrum.
 Um
 and
 then
 I


think
 for
 consumers
 there's
 sort
 of
 like


two
 spect
 two
 sides
 of
 the
 spectrum
 for


this
 probably.
 One
 is
 you
 know
 you
 might


just
 be
 doing
 some
 of
 these
 fun
 things


like
 Halloween
 costumes
 for
 my
 kid,


right?
 And
 and
 the
 out
 the
 goal
 there
 is


probably
 just
 to
 like
 share
 it
 with


somebody,
 right?
 Your
 family
 or
 your


friends.
 Um
 on
 the
 other
 side
 of
 the


spectrum,
 you
 might
 have
 these
 tasks


like
 putting
 together
 a
 slide
 deck,


right?
 I
 started
 out
 as
 a
 consultant.
 We


talked
 about
 that
 at
 the
 beginning.
 Um,


and
 you
 spend
 a
 lot
 of
 time
 on
 like
 very


tedious
 things
 like
 trying
 to
 make


things
 look
 good,
 trying
 to
 make
 the


story
 make
 sense.
 I
 think
 for
 those


types
 of
 tasks,
 you
 probably
 just
 have


an
 agent
 who
 you
 give
 the
 specs
 of
 what


you're
 trying
 to
 do
 and
 that
 it
 goes
 out


and
 like
 actually
 lays
 it
 out
 nicely
 for


you.
 It
 creates
 the
 right
 visual
 for
 the


information
 that
 you're
 trying
 to


convey.
 And
 it
 really
 is
 going
 to
 be


this,
 I
 think,
 spectrum
 depending
 on


what
 you're
 trying
 to
 do.
 Like
 do
 you


want
 to
 be
 in
 the
 creative
 process
 and


actually
 tinker
 with
 things
 and


collaborate
 with
 the
 model
 or
 do
 you


just
 want
 the
 model
 to
 like
 go
 do
 the


task
 and
 be
 as
 minimally
 involved
 as


possible?


>> So
 So
 in
 this
 new
 world
 then
 what
 what


is
 art?
 I
 mean
 somebody
 recently
 said


art
 is
 if
 you
 can
 create
 an
 an
 out
 of


distribution
 sample.
 Is
 is
 that
 a
 good


definition
 or
 or
 is
 it
 is
 it
 is
 it


aiming
 too
 high?
 Right.


>> Do
 you
 think
 if
 art
 is
 out
 of


distribution
 or
 in
 distribution
 for
 the


model?


>> There
 we
 go.


I
 think
 that
 out
 of
 distribution
 sample


that
 is
 a
 little
 bit
 too
 restrictive.
 I


think
 a
 lot
 of
 great
 art
 is
 actually
 in


distribution
 for
 art
 that
 occurred


before
 it.
 So
 I
 I
 mean
 what
 is
 art?
 I


think
 it's
 like
 a
 very
 philosophical


debate
 and
 there's
 a
 lot
 of
 people
 that


do
 discuss
 this.
 Like
 to
 me
 I
 think
 that


the
 most
 important
 thing
 for
 art
 is


intent.
 And
 so
 the
 the
 what
 is
 generated


from
 these
 models
 is
 is
 a
 tool
 to
 allow


people
 to
 create
 art.
 And
 I'm
 actually


not
 worried
 about
 the
 high-end
 and
 the


creatives
 and
 the
 professionals
 because


I've
 seen
 like
 if
 you
 put
 me
 in
 front
 of


one
 of
 these
 models
 like
 I
 can't
 create


anything
 that
 anyone
 wants
 to
 see


>> but
 like
 and
 but
 I've
 seen
 what
 people


can
 do
 who
 are
 creative
 people
 and
 who


have
 like
 intent
 and
 these
 ideas
 and
 I


think
 it's
 like


>> that's
 the
 most
 interesting
 thing
 to
 me


is
 is
 the
 things
 they
 create
 are
 really


amazing
 and
 and
 inspiring
 for
 me.
 So,
 I


feel
 like
 the
 the
 high-end
 and
 the
 the


the
 professionals
 and
 the
 creatives,


like
 they'll
 always
 use
 state-of-the-art


tools,
 and
 this
 is
 like
 another
 tool
 in


the
 tool
 belt
 for
 people
 to
 make
 cool


things.
 I
 think
 one
 of
 the
 the
 really


interesting
 things
 that
 I
 kept
 hearing


about
 this
 model
 in
 particular
 from
 like


creatives
 and
 artists
 was
 a
 lot
 of
 them


felt
 like
 they
 couldn't
 use
 a
 lot
 of
 AI


tools
 before
 because
 it
 didn't
 allow


them
 the
 level
 of
 control
 that
 they


expected
 for
 their
 art.
 what
 on
 one
 side


that
 was
 like
 the
 um
 characters
 or


object
 consistency
 like
 they
 really
 used


that
 to
 have
 a
 compelling
 narrative
 for


a
 story
 and
 so
 before
 when
 you
 couldn't


get
 the
 same
 character
 over
 and
 over
 it


was
 very
 difficult
 and
 then
 I
 think


second
 the
 like
 second
 thing
 I
 hear
 all


the
 time
 from
 artists
 is
 like
 um
 they


love
 being
 able
 to
 upload
 multiple


images
 and
 say
 like
 use
 the
 style
 of


this
 on
 this
 character
 or
 add
 this
 thing


to
 this
 image
 which
 is
 something
 that
 I


think
 was
 very
 hard
 to
 do
 even
 with


previous
 image
 edit
 models.
 I
 guess
 I'm


curious
 like
 was
 that
 something
 you
 guys


were
 really
 optimizing
 for
 when
 when
 you


trained
 this
 one
 or
 or
 how
 did
 you
 think


about
 that?


>> I
 mean
 yeah
 definitely
 sort
 of


customizability
 and
 character


consistency
 are
 things
 that
 we
 closely


monitored
 during
 the
 development
 and
 we


tried
 to
 do
 the
 best
 job
 we
 could
 on


them.
 Um,
 I
 think
 another
 thing
 is
 also


uh
 the
 iterative
 nature
 of
 kind
 of
 like


an
 interactive
 conversation.
 Um,
 and
 you


know,
 art
 tends
 to
 be
 iterative
 as
 well


where
 you
 you
 make
 lots
 of
 changes,
 you


see
 how
 it
 where
 it's
 going
 and
 you
 make


more.
 Um,
 and
 this
 is
 another
 thing
 I


think
 makes
 the
 model
 more
 useful
 and


and
 actually
 that's
 an
 area
 that
 I
 also


feel
 like
 we
 can
 improve
 the
 model


greatly.
 Like
 I
 know
 that
 um
 once
 you


get
 into
 really
 long
 conversations
 like


it
 it
 starts
 to
 follow
 um
 your


instructions
 a
 little
 bit
 worse.
 But


like
 this
 something
 that
 we're
 planning


to
 improve
 on
 and
 make
 the
 model
 more


kind
 of
 like
 a
 natural
 conversation


partner
 or
 like
 a
 creative
 partner
 in
 in


making
 something.


>> One
 thing
 that's
 so
 interesting
 is
 after


you
 guys
 launched
 Nano
 Banana,
 we
 start


to
 hear
 about
 editing
 models
 all
 the


time
 everywhere.
 Like
 it's
 like
 after


you
 launched
 the
 world
 woke
 up
 and
 they


were
 editing
 model,
 it's
 great,
 everyone


wants
 it.
 And
 then
 obviously
 like
 it
 it


kind
 of
 um
 you
 know
 goes
 into
 the


customizability
 the
 personalization
 of


it
 and
 then
 uh
 Oliver
 I
 know
 you
 used
 to


be
 Adobe
 and
 then
 there's
 also
 software


where
 we
 used
 to
 manually
 edit
 things.


How
 do
 you
 see
 the
 knobs
 evolve
 now
 on


the
 model
 layer
 versus
 what
 we
 used
 to


do?
 Um,
 yeah.
 I
 mean,
 I
 think
 that,
 you


know,
 one
 thing
 that
 that
 Adobe
 has


always
 done
 and
 the
 professional
 tools


generally
 require
 is
 lots
 of
 of
 control,


lots
 of
 knobs,
 lots
 of
 of
 So,
 there's


always
 a
 balance
 of
 like
 we
 want
 someone


to
 be
 able
 to
 use
 this
 on
 their
 phone.


>> Um,
 maybe
 with
 just
 like
 a
 a
 voice


interface,


>> and
 we
 also
 want
 someone
 who
 can
 really


like
 a
 really
 professional
 art
 creative


to
 be
 able
 to
 do
 fine
 skill
 adjustments.


I
 think
 we
 haven't
 exactly
 figured
 out


how
 to
 enable
 both
 of
 those
 yet.
 Um,
 but


there's
 a
 lot
 of
 people
 that
 are


building
 really
 compelling
 UIs
 like
 um


and
 and
 and
 I
 think
 it's
 a
 you
 know


we're
 Yeah,
 I
 think
 I
 think
 there's


different
 ways
 it
 can
 be
 done.
 Um,
 I


don't
 know.
 You
 have
 thoughts?


>> Well,
 I
 also
 hope
 that
 we
 get
 to
 a
 point


where
 you
 don't
 have
 to
 learn
 what
 all


these
 controls
 mean
 and
 the
 model
 can


maybe
 smartly
 suggest
 what
 you
 could
 do


next
 based
 on
 the
 context
 of
 what
 you've


already
 done,
 right?
 Um,
 and
 that
 feels


like
 it's
 kind
 of
 prime
 for
 someone
 to


tackle
 that
 on.
 So
 like
 what
 do
 the
 UIs


of
 the
 future
 look
 like
 um,
 in
 a
 way


where
 you
 probably
 don't
 need
 to
 learn
 a


hundred
 things
 that
 you
 had
 to
 before,


but
 like
 the
 tools
 should
 be
 smart


enough
 to
 suggest
 to
 you
 what
 it
 can
 do


based
 on
 what
 you're
 already
 doing.


>> That's
 such
 an
 insightful
 take.
 I


definitely
 had
 moments
 when
 when
 I
 used


Nano
 Banana,
 I
 was
 like,
 I
 didn't
 know
 I


wanted
 this,
 but


>> but
 I
 didn't
 even
 ask
 for
 this
 style.
 I


don't
 even
 know
 have
 the
 words
 for
 that


what
 that
 style
 even,
 you
 know,
 is


called.
 So
 this
 is
 like
 very
 insightful


on
 how
 image
 embedding
 and
 the
 language


embedding
 is
 not
 one
 to
 one
 like
 we


cannot
 map
 to
 like
 all
 the
 editing
 task


with
 language.
 So
 oh
 go
 ahead.


>> Yeah,
 let
 me
 let
 me
 sort
 of
 take
 a


little
 the
 counter
 point
 just
 to
 see


where
 this
 goes.


>> The
 other
 the
 question
 of
 how
 complex


the
 interface
 be
 can
 be
 limited
 by
 sort


of
 what
 we
 can
 express
 in
 software,
 how


easy
 we
 can
 make
 something
 in
 software


which
 to
 some
 degree
 is
 also
 limited
 by


how
 much
 complexity
 is
 a
 user
 willing
 to


tolerate.
 And
 you
 know
 if
 you
 have
 a


professional


>> they
 only
 care
 about
 the
 result
 they're


willing
 to
 tolerate
 a
 vast
 amount
 of


complexity
 they
 they
 have
 the
 training


they
 have
 the
 education
 they
 have
 the


experience
 to
 use
 that
 right
 then
 we
 may


end
 up
 with
 lots
 of
 knobs
 and
 dials
 it's


just
 very
 different
 of
 dials
 right
 I


mean
 today
 if
 you
 use
 a
 cursor
 or
 so
 for


coding
 it's
 not
 that
 it
 has
 a
 super
 easy


you
 know
 single
 text
 prompt
 interface
 it


has
 it
 has
 a
 a
 good
 amount
 of
 you
 know


here
 add
 context
 here
 different
 modes


and
 so
 on
 right
 so
 so


will
 we
 have
 Will
 we
 have
 like
 the
 the


ultra
 sophisticated
 interface
 for
 the


for
 the
 power
 user
 and
 how
 how
 would


that
 look
 like?
 So
 I'm
 a
 big
 fan
 of


Comfy
 UI
 and
 nodebased
 interfaces
 in


general


>> and
 that
 is
 complex


>> and
 it's
 complex
 but
 it's
 also
 it's
 very


robust
 and
 you
 can
 do
 a
 lot
 of
 things


and
 so
 you
 know
 after
 we
 released
 Nano


Banana
 we
 saw
 people
 building
 all
 these


really
 complicated
 comfy
 UI
 workflows


where
 they
 were
 combining
 a
 bunch
 of


different
 models
 together
 and
 different


tools
 and
 that
 generated
 some
 of
 the


like
 for
 example
 using
 nanobana
 as
 um
 as


a
 way
 to
 get
 storyboards
 or
 key
 frames


for
 video
 models
 like
 you
 can
 plug
 these


things
 together
 and
 and
 get
 really


amazing
 outputs.
 So,
 I
 I
 think
 that
 like


at
 the
 the
 pro
 or
 the
 developer
 level,


like
 these
 kinds
 of
 interfaces
 are
 are


great.
 Um,
 in
 terms
 of
 like
 the
 proumer


level,
 I
 think
 it's
 it's
 very
 much


unknown
 what
 it's
 going to
 look
 like
 in


in
 a
 couple
 years.


>> Yeah.
 I
 think
 it
 just
 really
 depends
 on


your
 audience,
 right?
 Because
 for
 the


regular
 consumer,
 like
 I
 use
 my
 parents


always
 as
 an
 example.
 The
 chatbot
 is


actually
 kind
 of
 great.


>> Oh,
 yeah.


>> Because
 you
 don't
 have
 to
 learn
 a
 new


UI.
 You
 you
 just
 upload
 your
 images
 and


then
 you
 talk
 to
 them,
 right?
 Like
 it's


it's
 kind
 of
 amazing
 that
 way.
 Then
 for


the
 pros,
 I
 agree
 that
 like
 you
 need
 so


much
 more
 control
 than
 you
 know
 and
 then


there's
 somewhere
 in
 between
 probably


which
 are
 people
 who
 may
 want
 to
 be


doing
 this
 but
 they
 were
 too
 intimidated


by
 the
 professional
 tools
 in
 the
 past


and
 for
 them
 I
 do
 think
 that
 there's
 a


space
 of
 like
 that
 you
 need
 more
 control


than
 the
 chatbot
 gives
 you.
 Uh
 but
 you


don't
 need
 as
 much
 control
 as
 what
 the


professional
 tools
 give
 you
 and
 like


what's
 that
 kind
 of
 in
 between
 state?


>> There's
 a
 ton
 of
 opportunity
 there.


>> There's
 a
 ton
 of
 opportunity
 there.
 It


is
 interesting
 you
 mentioned
 comfy
 UI


because
 it's
 on
 the
 other
 far
 spectrum


of
 workflow
 like
 a
 workflow
 can
 have


hundreds
 of
 steps
 and
 notes
 and
 you
 need


to
 make
 sure
 all
 of
 them
 work
 whereas
 on


the
 other
 side
 of
 the
 spectrum
 there's


nano
 banana
 you
 kind
 of
 describe


something
 with
 words
 and
 then
 you
 get


something
 out
 like
 I
 don't
 know
 what's
 a


model
 architecture
 stuff
 like
 that
 but


um
 I
 guess
 is
 your
 view
 that
 the
 world


is
 moving
 to
 an
 ensemble
 of
 a
 model


hosted
 by
 one
 provider
 doing
 it
 all
 or


do
 you
 think
 the
 world
 is
 moving
 to
 more


of
 everyone
 building
 a
 workflow.
 Nano


Banana
 is
 one
 of
 the
 nodes
 in
 comfy
 work


UI.


>> Um
 I
 I
 definitely
 don't
 think
 that
 that


the
 the
 broad
 amount
 of
 use
 cases
 will


be
 fully
 satisfied
 by
 one
 model
 at
 any


point.
 So
 I
 think
 that
 there
 will
 always


be
 a
 diversity
 of
 models.
 some
 like
 um


I'll
 give
 you
 an
 example,
 but
 some
 you


know
 we
 could
 we
 could
 optimize
 for
 um


instruction
 following
 in
 our
 models.


Make
 sure
 it
 does
 exactly
 what
 you
 want,


but
 it
 might
 be
 um
 a
 worse
 model
 for


someone
 who's
 looking
 for
 ideiation
 or


kind
 of
 inspiration
 where
 they
 want
 the


model
 to
 kind
 of
 take
 over
 and
 and
 do


other
 things,
 go
 crazy.
 So
 like
 I
 just


think
 there's
 so
 many
 different
 use


cases
 and
 so
 many
 types
 of
 people
 that


like
 there's
 a
 lot
 of
 space
 there's
 a


lot
 of
 room
 in
 this
 space
 for
 multiple


models.
 So
 that's
 that's
 where
 I
 see
 us


going.
 I
 don't
 think
 this
 is
 going
 to
 be


like
 a
 single
 to
 rule
 it
 a
 single
 model


to
 rule
 them
 all.


>> Complete
 sense.
 Let's
 go
 to
 the
 very


other
 end
 of
 the
 spectrum
 from
 the


professional.
 Do
 you
 think


kindergarteners
 in
 the
 future
 will
 learn


drawing
 by
 by
 sketching
 something,
 you


know,
 on
 a
 on
 a
 little
 tablet
 and
 then


you
 have
 the
 AI
 make
 turn
 that
 into
 a


beautiful
 image
 and
 and
 so
 that's
 how


how
 they
 they
 allow
 get
 in
 touch
 with


art.
 I
 don't
 know
 if
 you
 always
 wanted


to
 turn
 into
 a
 beautiful
 image,
 but
 I


but
 I
 think
 there's
 something
 there


about
 the
 AI
 being
 again
 a
 partner
 and
 a


teacher
 to
 you
 in
 a
 way
 that
 you
 like


didn't
 have.
 So
 I


>> didn't
 know
 how
 to
 draw,
 still
 don't
 um


don't
 have
 any
 talent
 for
 it
 really.
 Uh,


but
 I
 think
 it
 would
 be
 great
 if
 we


could
 use
 these
 tools
 in
 a
 way
 that


actually
 teaches
 you
 kind
 of the
 step
 by


steps
 and
 helps
 you
 critique
 and
 maybe


again
 shows
 you
 kind
 of
 like
 an


autocomplete
 almost
 for
 images
 like
 what


like like
 what's
 the
 next
 step
 that
 I


could
 take,
 right?
 Or
 maybe
 show
 me
 a


couple
 of
 options
 and
 like
 how
 do
 I


actually
 do
 this?
 So,
 I
 hope
 it's
 more


that
 direction.
 I
 I
 don't
 think
 we
 all


want,
 you
 know,
 every
 5-year-old's
 image


to
 suddenly
 look
 perfect.


>> We
 we
 we
 would
 probably
 lose
 something


um
 in
 the
 process.
 As
 someone
 who


struggled
 the
 most
 in
 high
 school
 out
 of


all
 my
 classes
 of
 the
 art
 and
 the


sketching
 class,
 I
 actually
 would
 have


would
 have
 preferred
 it,
 but
 I
 know
 a


lot
 of
 people
 want
 their
 kids
 to
 learn


to
 draw,
 which
 I
 understand.


>> It's
 funny
 because
 we've
 been
 trying
 to


get
 the
 model
 to
 create
 um
 like


childlike
 crayon
 drawings,
 which
 is


actually
 quite
 challenging.


>> Um,
 ironically,
 you
 know,
 sometimes
 the


the
 things
 that
 are
 hard
 to
 make
 are


because
 the
 level
 of
 abstraction
 is
 very


large,
 right?


>> So,
 it's
 actually
 quite
 difficult
 to


make
 those
 types
 of
 images.
 your


dedicated
 prek
 fin.


>> We
 we
 do
 have
 seminar
 evals
 right
 now


>> to
 try
 to
 see
 if
 we're
 getting
 better.


>> I'm
 in
 general
 I'm
 very
 optimistic
 about


AI
 for
 education.
 And
 you
 know
 part
 of


the
 reason
 is
 I
 think
 that
 most
 of
 us


are
 visual
 learners,


>> right?
 So
 that
 AI
 right
 now
 as
 a
 tutor


basically
 all
 it
 can
 do
 is
 is
 talk
 to


you
 or
 give
 you
 text
 to
 read
 and
 that's


definitely
 not
 how
 students
 learn.
 So
 I


think
 that
 these
 models
 have
 a
 lot
 of


potential
 as
 a
 way
 to
 help
 education
 by


giving
 people
 sort
 of
 visual
 cues.
 You


know,
 imagine
 if
 you
 could
 get
 an


explanation
 for
 something
 where
 you
 get


the
 text
 explanation,
 but
 you
 also
 get


images
 and
 figures
 that
 kind
 of
 like


help
 explain
 how
 they
 work.
 I
 think
 it


just
 everything
 will
 be
 much
 more


useful,
 much
 more
 accessible
 for


students.
 So
 I'm
 really
 excited
 about


that.
 That
 is
 a


>> on
 that
 point,
 one
 thing
 that's
 very


interesting
 to
 us
 is
 that
 when
 Nano


Banana
 came
 out,
 it
 almost
 felt
 like


there's
 part
 of
 a
 use
 case
 is
 the


reasoning
 model.
 Like
 you
 have
 a


diagram.
 Absolutely.
 Right.
 Like
 you
 can


explain
 some
 knowledge
 visually.
 So
 the


model
 not
 just
 doing
 approximation
 of


the
 visual
 aspect.
 There's
 the
 reasoning


aspect
 to
 it
 too.


>> Do
 you
 think
 that's
 where
 we're
 going


to?
 Do
 you
 think
 all
 the
 model
 large


models
 will
 realize
 that
 oh
 like
 to
 be
 a


good
 LM
 or
 VL
 like
 uh
 VLM
 we
 have
 to


have
 both
 image
 and
 language
 and
 audio


and
 so
 on
 so
 forth.


>> 100%.
 I
 definitely
 think
 so.
 Um
 the
 the


future
 for
 these
 AI
 models
 that
 I'm
 most


excited
 by
 is
 where
 they
 are
 tools
 for


people
 to
 accomplish
 more
 things.
 Like
 I


think
 if
 you
 imagine
 a
 future
 where
 you


have
 these
 agentic
 models
 that
 just
 talk


to
 each
 other
 and
 do
 all
 the
 work,
 then


it
 becomes
 a
 little
 bit
 less
 necessary


that
 there's
 like
 this
 visual
 mode
 of


communication.
 But
 as
 long
 as
 there's


people
 in
 the
 loop
 and
 as
 long
 as
 the


the
 kind
 of
 the
 the
 motivation
 for
 the


task
 they're
 solving
 comes
 from
 people,


I
 think
 it
 makes
 total
 sense
 that
 that


visual
 modality
 is
 going
 to
 be
 really


critical
 for
 any
 of
 these
 AI
 agents


going
 forward.


Will
 we
 get
 to
 a
 point
 where
 there's


actually
 so
 of
 you
 know
 I'm
 I'm
 asking


you
 to
 create
 an
 image
 it
 sits
 for
 two


hours
 reasons
 with
 itself
 has
 drafts


explores
 different
 directions
 and
 then


comes
 back
 with
 a
 final
 answer.


>> Yeah.
 Absolutely.
 If
 it's
 if
 necessary.


Yeah.
 Like


>> and
 maybe
 not
 just
 for
 a
 single
 image


but
 to
 the
 point
 of
 you
 know
 maybe


you're
 redesigning
 your
 house
 and
 maybe


you
 actually
 really
 don't
 want
 to
 be


involved
 in
 the
 process
 right
 but
 you're


like
 okay
 this
 is
 what
 it
 looks
 like


this
 some
 this
 is
 some
 inspiration
 that


I
 like.
 And
 then
 you
 send
 it
 to
 um
 a


model
 the
 same
 way
 that
 you
 would
 send


it
 to
 like
 a
 designer.


>> It's
 the
 visual
 deep
 research.


>> The
 vis
 it's
 like
 visual
 deep
 research


basically.
 I
 really
 like
 that
 term.
 And


then
 it
 goes
 off
 and
 does
 its
 thing
 and


searches
 for
 maybe
 the
 furniture
 that


would
 go
 with
 your
 environment
 and
 then


it
 comes
 back
 to
 you
 and
 maybe
 presents


you
 with
 options
 because
 maybe
 maybe
 you


don't
 want
 to
 sit
 for
 two
 hours
 at
 one


thing
 art
 book


you
 know
 10
 10
 slide
 deck.
 I
 also
 I


think
 if
 you
 if
 you
 think
 about
 like
 um


instruction
 manuals
 or
 like
 IKEA


directions
 or
 something
 then
 like


breaking
 down
 a
 hard
 problem
 into
 many


intermediate
 steps
 could
 be
 really


useful
 as
 as
 a
 way
 to
 communicate.


>> So
 when
 can
 we
 generate
 Lego
 sets?


>> Yeah,
 soon
 maybe


do
 we
 at
 some
 point
 need
 3D
 as
 part
 of


it?


>> Right.


>> I
 mean
 there's
 a
 whole
 debate
 around


world
 models
 and
 image
 models
 and
 how


they
 fit
 together.
 thoughts


enlighten
 us
 here.
 What
 is
 the
 what
 is


the
 short
 summary
 of
 where
 we'll
 end
 up


there?


>> Um
 I
 mean
 I
 don't
 know
 the
 answer.
 I


think
 that
 um
 obviously
 the
 real
 world


is
 in
 3D.
 So
 if
 you
 have
 3D
 a
 3D
 world


model
 or
 world
 model
 that
 has
 explicit


3D
 representations.
 There's
 a
 lot
 of


advantages.
 For
 example,
 everything


stays
 consistent
 all
 the
 time.


>> Um
 now
 the
 main
 challenge
 is
 that
 we


don't
 walk
 around
 with
 3D
 capture


devices
 in
 our
 pocket.
 So
 in
 terms
 of


like
 the
 available
 data
 for
 training


these
 models,
 it's
 largely
 the


projection
 onto
 onto
 2D.
 So
 I
 think
 that


both
 viewpoints
 are
 totally
 valid
 for


where
 we're
 going.
 I
 come
 a
 bit
 from
 the


projection
 side
 like
 I
 think
 it
 we
 can


solve
 almost
 all
 the
 problems
 if
 not
 all


the
 problems
 working
 on
 the
 projection


of
 the
 3D
 world
 directly
 and
 letting
 the


models
 learn
 the
 latent
 world


representations.
 I
 mean
 we
 see
 this


already
 that
 the
 video
 models
 have
 very


good
 3D
 understanding.
 You
 can
 run


reconstruction
 algorithms
 over
 the


videos
 you
 generate
 and
 they're
 they're


very
 accurate.
 Um,
 and
 in
 general,
 if


you
 look
 at
 like
 the
 history
 of
 human


art,
 like
 it
 it
 it
 starts
 as
 like
 the


projection,
 right?
 People
 drawing
 on
 on


cave
 walls.
 Um,
 all
 of
 our
 interfaces


are
 in
 2D.
 So,
 I
 think
 that
 like
 humans


are
 very
 very
 well
 suited
 for
 working
 on


this
 projection
 of
 the
 3D
 world
 into
 a


2D
 plane.
 And
 it's
 a
 really
 natural


environment
 for
 interfaces
 and
 for


viewing.
 So,


>> that
 is
 very
 true.
 like
 um
 so
 I'm
 a


cartoonist
 in
 my
 spare
 time
 and
 then


drawing
 in
 2D
 is
 just
 light
 and
 shadow


and
 then
 you
 present
 yourself
 with
 3D


cannot
 we
 trick
 ourselves
 to
 believing


it's
 3D
 or
 it's
 you
 know
 on
 a
 piece
 of


paper
 but
 then
 what
 human
 can
 do
 that


you
 know
 like
 a
 drawing
 or
 like
 a
 model


can
 do
 is
 you
 we
 can
 navigate
 the
 world


like
 we
 see
 a
 table
 we
 can't
 walk
 past


it
 I
 guess
 the
 question
 becomes
 if


everything
 is
 2D
 how
 do
 you
 solve
 that


problem


>> well
 I
 don't
 think
 yeah
 so
 if
 we're


trying
 to
 solve
 the
 robot
 products


problems.
 I
 think
 maybe
 the
 2D
 um


representation
 is
 useful
 for
 planning


and
 visualizing
 kind
 of
 at
 a
 high
 level.


>> Like
 I
 think
 people
 navigate
 by
 um
 by


remembering
 kind
 of
 2D
 projections
 of


the
 world.
 Like
 you
 don't
 you
 don't


build
 a
 3D
 map
 in
 your
 head.
 You're
 more


like
 oh
 I
 know
 I
 see
 this
 building
 I


turn
 left.


>> Yeah.


>> So
 I
 think
 that
 like
 for
 that
 kind
 of


planning
 it's
 reasonable
 but
 for
 the


actual
 locomotion
 around
 the
 space
 like


I
 definitely
 3D
 is
 important
 there.
 So


>> robotics
 Yeah.
 They
 probably
 need
 3D.


That's
 the
 saving
 grace.


>> Yeah.
 Yeah.
 Um,
 so
 character


consistency,
 which
 you
 previously


mentioned,
 I
 really
 love
 the
 example
 of


like
 when
 a
 model
 feels
 so
 personal,


like
 people
 are
 so
 tempted
 to
 try
 it.


>> How
 did
 you
 unlock
 that
 moment?
 The


reason
 why
 I
 asked
 is
 that
 character


consistency
 is
 so
 hard.


>> Uh,
 there's
 a
 huge
 uncanny
 valley
 to
 it.


like
 you
 know
 like
 if
 it's
 someone
 I


don't
 know
 if
 I
 see
 their
 AI
 generation


I'm
 like
 okay
 it's
 maybe
 the
 same
 person


but
 if
 it's
 someone
 I
 know
 if
 there's


just
 a
 little
 bit
 of
 a
 difference
 uh
 I


I'm
 actually
 felt
 very
 turned
 off
 by
 it


because
 I'm
 like
 this
 not
 a
 real
 person.


So
 in
 that
 case
 how
 do
 you
 know
 what


you're
 generating
 is
 good
 and
 then
 is
 it


mostly
 by
 user
 feedback
 or
 like
 I
 love


this
 or
 is
 it
 something
 else?
 You
 look


at
 faces,
 you
 know,
 and
 but


face
 detection
 camera
 user
 and


>> no.
 So,
 so,
 so
 not
 even
 before
 you
 ever


released
 this,
 right?
 So,
 when
 when


we're
 we
 were
 developing
 this
 model,
 we


actually
 started
 out
 doing
 character


consistency
 evolves
 on
 faces
 we
 didn't


know
 and
 it
 doesn't
 tell
 you
 anything,


right?


>> Um,
 and
 then
 we
 started
 testing
 it
 on


ourselves
 and
 quickly
 realized
 like,


okay,
 this
 is
 what
 you
 need
 to
 do


because
 this
 is
 a
 face
 that
 I'm
 familiar


with.
 And
 so
 there
 is
 a
 lot
 of
 sort
 of


eyeballing
 evaluations
 that
 happens
 and


just
 the
 team
 testing
 it
 on
 themselves.


Um,
 and
 just
 generally
 people
 they
 know


like
 Oliver
 probably
 knows
 my
 face
 at


this
 point
 enough
 to
 be
 able
 to
 tell


whether
 or
 not
 it's
 actually
 me
 when


it's
 generated.


>> Um,
 and
 so
 we
 do
 do
 a
 lot
 of
 that.
 Um,


and
 then
 you
 know
 you
 you
 ideally
 test


it
 on
 different
 sets
 of
 people,


different
 ages,
 right?
 Different


different
 kind
 of
 groups
 of
 folks
 to


make
 sure
 that
 it
 kind
 of
 works
 across


the
 board.


>> Yeah,
 I
 think
 they're
 right.
 I
 mean
 that


that
 touches
 that
 touches
 a
 little
 bit


on
 this
 this
 um
 bigger
 issue
 which
 is


that
 like
 eval
 are
 really
 difficult
 in


this
 space


>> um
 because
 human
 perception
 is
 very


uneven
 in
 terms
 of
 the
 things
 that
 it


cares
 about.
 So
 um
 so
 really
 it's
 hard


it's
 very
 hard
 to
 know
 like
 how
 good
 is


the
 character
 consistency
 of
 a
 model
 and


um
 is
 it
 good
 enough?
 Is
 it
 not
 good


enough?
 Like
 you
 know
 I
 think
 there's


there's
 still
 a
 lot
 of
 improvement
 we


can
 make
 on
 character
 consistency
 but
 I


think
 that
 for
 some
 use
 cases
 like
 we


got
 to
 a
 point
 and
 that's
 you
 know
 we


weren't
 the
 first
 edit
 model
 by
 any


means
 but
 I
 think
 that
 like
 once
 the


quality
 gets
 above
 a
 certain
 level
 for


character
 consistency
 it
 can
 kind
 of


just
 take
 off
 because
 it
 becomes
 useful


for
 so
 much
 more.


>> And
 I
 think
 as
 it
 gets
 better
 it'll
 be


useful
 for
 even
 more
 things
 too.
 Yeah,


>> I
 think
 one
 of
 the
 really
 interesting


things
 we're
 seeing
 across
 a
 bunch
 of


modalities
 of
 which
 image
 edit
 and


generation
 obviously
 is
 one
 is
 like
 um
 I


think
 the
 arenas
 and
 benchmarks
 and


everything
 are
 awesome
 but
 especially


when
 you
 have
 like
 multi-dimensional


things
 like
 image
 and
 video
 um
 it's
 very


hard
 as
 all
 of
 the
 models
 get
 better
 and


better
 to
 condense
 every
 quality
 of
 a


model
 into
 like
 one
 judgment.
 So
 it's


like,
 you
 know,
 you're
 judging,
 okay,


you
 swap
 a
 character
 into
 an
 image
 and


you
 change
 the
 style
 of
 the
 image.
 Maybe


one
 did
 the
 character
 swap
 and


consistency
 much
 better
 and
 the
 other


did
 the
 style
 much
 better?
 Like
 how
 do


you
 say
 which
 output
 is
 better?
 And
 it


probably
 comes
 down
 to
 like
 what
 the


person
 cares
 most
 about
 and
 what
 they're


what
 they
 want
 to
 use
 it
 for.
 Um,
 are


there
 like
 certain,
 you
 know,


characteristics
 of
 the
 model
 that
 you


guys
 value
 more
 than
 other
 things
 in


like
 making
 those
 trade-offs
 when


deciding
 which
 version
 of
 the
 model
 to


deploy
 or
 like
 what
 to
 really
 focus
 on


during
 training?


>> Um,
 yes,
 there
 are.
 One
 of
 the
 things
 I


like
 about
 this
 space
 is
 that
 uh
 there


is
 no
 right
 answer.
 So,
 actually
 there's


quite
 a
 lot
 of
 of
 I
 don't know
 if
 it's


taste,
 but
 it's
 like
 preference
 that


goes
 into
 the
 models.
 And
 I
 think
 you


can
 kind
 of
 see
 the
 difference
 in


preferences
 of
 the
 different
 research


labs
 in
 the
 models
 that
 they
 release.


>> So
 like
 when
 we're
 balancing
 two
 things,


a
 lot
 of
 it
 comes
 down
 to
 like,
 oh
 well,


I
 I
 don't know.
 I
 just
 like
 this
 this


look
 better
 or
 I
 you
 know,
 this
 this


feature
 is
 more
 important
 to
 us.


>> I'd
 imagine
 it's
 hard
 for
 for
 you
 guys,


too,
 because
 you
 have
 you
 have
 so
 many


users,
 right?
 like
 Google
 like
 being
 in


the
 Gemini
 app
 like
 everyone
 in
 the


world
 can
 use
 that
 versus
 like
 many


other
 AI
 companies
 just
 think
 about
 like


we're
 only
 going
 for
 the
 professional


creatives
 or
 we're
 only
 going
 for
 the


consumer
 meat
 makers
 and
 like
 you
 guys


have
 the
 unique
 and
 exciting
 but


challenging
 task
 of
 like
 literally


anyone
 in
 the
 world
 can
 do
 this.
 How
 do


we
 decide
 what
 everyone
 would
 want?


>> Yeah.
 And
 it
 is
 sometimes
 we
 do
 make


these
 trade-offs.
 We
 do
 have
 a
 set
 of


things
 that
 are
 sort
 of
 like
 super
 high


priority
 that
 we
 don't
 want
 to
 regret


restress
 on.
 Right?
 So
 now
 because


character
 consistency
 was
 so
 awesome
 and


so
 many
 people
 are
 using
 it,
 we
 don't


want
 our
 next
 models
 to
 get
 worse
 on


that
 dimension.
 Right?
 So
 we
 pay
 a
 lot


of
 attention
 to
 it.
 We
 care
 a
 lot
 about


images
 looking
 photorealistic
 when
 you


want
 photos
 and
 this
 is
 important.
 One,


I
 think
 we
 all
 prefer
 that
 style.


too.
 Um,
 you
 know,
 for
 advertising
 use


cases,
 for
 example,
 like
 a
 lot
 of
 it
 is


kind
 of
 photorealistic
 images
 of


products
 and
 people.
 And
 so,
 we
 want
 to


make
 sure
 that
 we
 can
 kind
 of
 do
 that.


And
 then
 sometimes
 there
 are
 just
 things


that
 like
 will
 kind
 of
 fall
 down
 the


wayside.
 So,
 for
 this
 first
 release,
 the


model
 is
 not
 as
 good
 as
 text
 rendering


at
 as
 we
 would
 like
 it
 to
 be,
 and
 that's


something
 that
 we
 want
 to
 fix
 in
 the


future.
 But
 it
 was
 kind
 of
 one
 of
 those


things
 where
 we
 looked
 at,
 okay,
 the


model's
 good
 at
 XY
 Z,
 it's
 not
 as
 good


at
 this,
 but
 we
 still
 think
 it's
 okay
 to


release
 and
 it
 will
 still
 be
 an
 exciting


thing
 for
 people
 to
 play
 with.


>> If
 you
 look
 at
 the
 past,
 right,
 we
 we
 we


had
 for
 previous
 model
 generations,
 a


lot
 of
 things
 we
 did
 with
 like
 sidecar


models
 like
 control
 net
 or
 something


like
 that
 where
 we
 basically
 figured
 out


a
 way
 to
 provide
 structured
 data
 to
 the


model
 to
 achieve
 a
 particular
 result.
 It


seems
 like
 these
 newer
 models
 that
 has


taken
 a
 step
 back
 just
 because
 they're


so
 incredibly
 good
 in
 just
 prompting
 or


or
 you
 know
 giving
 a
 reference
 image
 and


picking
 things
 up
 from
 there.
 Where
 will


this
 go
 long
 term?
 Do
 you
 think
 this


will
 come
 back
 to
 some
 degree?
 Um
 you


know
 like
 I
 mean
 for
 from
 the
 creators


perspective
 right
 having
 I
 don't
 know


open
 pose
 information
 so
 I
 can
 get
 get
 a


pose
 exactly
 right
 right
 for
 multiple


characters.
 This
 seems
 very
 very


tempting,
 right?
 Is
 it
 like
 or
 to


rephrase
 it
 a
 little
 bit,
 it's
 like
 does


the
 bitter
 lesson
 hold
 here
 that
 at
 the


end
 of
 the
 day
 everything's
 just
 one
 big


model
 and
 you
 throw
 things
 in
 or
 is


there
 is
 a
 little
 of
 structure
 we
 can
 we


can
 offer
 to
 make
 this
 better?


>> Um,
 I
 mean
 I
 think
 that
 there
 will
 be


there'll
 always
 be
 users
 that
 want


control
 that
 the
 model
 doesn't
 give
 you


out
 of
 the
 box.
 But
 I
 think
 we
 we
 tried


to
 make
 it
 so
 that
 um
 you
 know
 because


really
 what
 really
 what
 an
 artist
 wants


when
 they
 want
 to
 do
 something
 is
 they


want
 the
 intent
 to
 be
 understood.
 And
 I


think
 that
 that
 these
 um
 AI
 models
 are


getting
 better
 at
 understanding
 the


intent
 of
 users.
 So
 often
 when
 you
 ask


text
 queries
 now
 the
 model
 gets
 what


you're
 going
 for.


>> So
 you
 know
 in
 that
 sense
 I
 think
 we
 can


we
 can
 get
 pretty
 far
 with
 understanding


the
 intent
 of
 our
 users.


>> And
 um
 and
 maybe
 some
 of
 that
 is


personalization
 like
 we
 need
 to
 know


information
 about
 what
 you're
 trying
 to


do
 or
 what
 you've
 done
 in
 the
 past.
 But


I
 think
 once
 you
 can
 understand
 the


intent
 then
 you
 can
 you
 can
 generally
 do


the
 the
 type
 of
 edit
 like
 is
 this
 like
 a


very
 structure
 preserving
 edit
 or
 is


this
 like
 a
 free
 form
 kind
 of
 like
 we


can
 learn
 these
 these
 kinds
 of
 effects
 I


think.
 Um
 but
 still
 of
 course
 there's


one
 person
 who's
 going
 to
 really
 care


about
 every
 pixel
 and
 like
 this
 this


thing
 needs
 to
 be
 slightly
 to
 the
 left


and
 a
 little
 bit
 more
 blue
 and
 like


those
 people
 will
 use
 existing
 tools
 to


do
 that.


>> I
 mean
 I
 think
 it's
 like
 you
 know
 I
 want


an
 image
 with
 26
 people
 spelling
 out


every
 letter
 of
 the
 alphabet
 or


something
 like
 that.
 Right.
 That's
 off


the
 thing
 where
 I
 think
 we're
 still


quite
 a
 bit
 away
 from
 getting
 that


right,
 you
 know,
 in
 the
 first
 try.
 On


the
 other
 hand,
 with
 pose
 information,


it
 could
 potentially
 get


>> But
 then
 the
 then
 the
 question
 I
 guess


is
 like
 do
 you
 really
 want
 to
 be
 the
 one


who's
 like
 extracting
 the
 pose
 and


providing
 that
 as
 an
 information
 or
 do


you
 just
 want
 to
 provide
 some
 reference


image
 and
 say
 like
 this
 is
 actually
 what


I
 want
 like
 model
 go
 figure
 this
 out


right


>> there
 are
 26
 people
 every
 now
 and


different
 style.
 Fair
 enough.
 Yeah,
 I


think
 in
 that
 in
 that
 case
 I
 wouldn't


spend
 a
 ton
 of
 time
 building
 a
 custom
 um


interface
 for
 making
 this
 this
 picture


of
 46
 people
 seems
 like
 the
 kind
 of


thing
 that
 we
 can
 we
 can
 solve.


>> Just
 transfer.


>> Do
 you
 think
 the
 representation
 of
 what


the
 AI
 images
 are
 will
 change?
 So
 the


reason
 why
 I
 asked
 the
 question
 is
 that


as
 artists
 there's
 different
 formats
 we


play
 with.
 There's
 the
 SVGs,
 we
 have


anchor
 points
 and
 bezier
 curves.


>> And
 on
 the
 other
 side,
 there's,
 you


know,
 porcy
 or
 like
 fresco,
 what
 have


you.
 There's
 layers
 that
 we
 can
 also


play
 with.
 There's
 the
 other
 parameter


which
 is
 what's
 the
 brush
 you
 use
 like


the
 brush,
 the
 texture
 of
 it.
 So,
 every


one
 parameter
 you
 can
 write
 script
 and


actually
 uh
 do
 something
 very
 personal


about
 it.
 Mhm.


>> Do
 you
 think
 like
 pixel
 is
 the
 right


representation
 um
 the
 endgame
 for
 image


generation
 model
 or
 do
 you
 think
 there's


a
 net
 new
 representation
 that
 we
 haven't


invented
 yet?


>> That's
 an
 easy
 question
 to


>> wow.
 Um
 I
 I'll
 say
 that
 uh
 that
 um


everything
 is
 a
 subset
 of
 pixels.


>> That's
 true.


>> So
 text
 is
 a
 subset
 of
 pixels
 because
 I


could
 just
 render
 all
 the
 text
 as
 an


image.


>> So
 how
 far
 can
 we
 get
 with
 just
 pixels


is
 an
 interesting
 question.
 I
 think
 you


know
 if
 the
 model
 is
 really
 um


responsive
 and
 handles
 multi-turn


interactions
 well
 then
 I
 think
 you
 can


probably
 get
 pretty
 far
 because
 the


primary
 reason
 I
 think
 you
 would
 want
 to


leave
 the
 pixel
 domain
 is
 for


editability.


>> Um
 and
 so
 you
 know
 in
 cases
 where
 you


need
 to
 have
 your
 font
 or
 you
 want
 to


change
 the
 text
 or
 you
 want
 to
 move


things
 around
 just
 like
 with
 control


points
 um
 it
 could
 be
 useful
 to
 have
 um


kind
 of
 mix
 generation
 which
 consists
 of


pixels
 and
 SVGs
 and
 other
 other
 forms.


Um
 but
 if
 we
 can
 do
 it
 all,
 if
 we
 can
 if


the
 multi-
 interaction
 is
 enough,
 then
 I


think
 you
 can
 get
 pretty
 far
 with


pixels.
 Um
 I
 will
 say
 that
 one
 of the


things
 that's
 exciting
 about
 these
 um


these
 models
 that
 have
 native


capabilities
 is
 that
 you
 now
 have
 a


model
 that
 can
 generate
 code
 and
 it
 can


generate
 images.


>> So
 there's
 a
 lot
 of
 interesting
 things


that
 come
 in
 that
 intersection,
 right?


Like
 maybe
 I
 wanted
 to
 write
 some
 code


and
 then
 make
 make
 some
 some
 things
 be


rasterized,
 some
 things
 be
 parametric.


>> Yeah.


>> Like
 stick
 it
 all
 together,


>> train
 it
 together.
 Like
 this
 would
 be


very
 cool.
 That's
 such
 a
 good
 point


because
 I
 did
 see
 a
 tweet
 of
 someone


asking
 Cloud
 Sauna
 to
 replicate
 a
 image


on
 an
 Excel
 sheet
 where
 every
 cell
 is
 a


pixel


>> which
 is
 like
 a
 very
 fun
 exercise.
 It


was
 like
 a
 coding
 model
 like
 doesn't


really
 know
 anything
 about
 you
 know


images
 yet
 it
 worked.


>> Yeah,
 there's
 the
 classic
 pelican
 riding


a
 bicycle
 test.


>> Yeah,
 totally.
 I
 have
 one
 on
 on
 model


like
 on
 interfaces
 if
 that's
 okay.
 I


don't
 sorry
 if
 I'm
 bringing
 up
 too
 much


product
 stuff
 guys.
 I'm
 just
 very


curious
 on
 on
 the
 product
 front
 like
 um


I
 guess
 I'm
 curious
 how
 you
 think
 about


like
 owning
 the
 interface
 where
 people


are
 editing
 or
 generating
 images
 with


Nano
 Banana
 versus


really
 just
 wanting
 a
 ton
 of
 people
 to


use
 the
 model
 for
 different
 things
 in


the
 API.
 Like
 we've
 talked
 about
 so
 many


different
 use
 cases
 like
 ads,
 you
 know,


education,


um
 design,
 uh
 like
 architecture.
 Each
 of


those
 things
 could
 be
 there
 could
 be
 a


standalone
 product
 built
 on
 top
 of
 Nano


Banana
 that
 prompts
 the
 model
 in
 the


right
 way
 or
 allows
 certain
 types
 of


inputs
 or
 whatever.
 Is
 your
 guys'
 vision


like
 that
 the
 kind
 of
 the
 product
 in
 the


Gemini
 app
 is
 like
 a
 playground
 for


people
 to
 explore
 and
 then
 developers


will
 build
 the
 individual
 products
 that


are
 used
 for
 certain
 use
 cases
 or
 is


that
 something
 you're
 also
 kind
 of


interested
 in
 owning?


>> I
 think
 it's
 a
 little
 bit
 of
 everything.


Um,
 so
 I
 definitely
 think
 that
 the


Gemini
 app
 is
 an
 entry
 point
 for
 people


to
 explore.
 And
 the
 one
 the
 nice
 thing


about
 Nano
 Banana
 is
 I
 think
 it
 shows


that
 fun
 is
 kind
 of
 a
 gateway
 to
 utility


where
 you
 know
 people
 come
 to
 make
 a


figurine
 image
 of
 themselves
 but
 then


they
 stay
 because
 it
 helps
 them
 with


their
 math
 homework
 or
 it
 helps
 them


write
 something,
 right?
 And
 and
 so
 I


think
 that's
 a
 really
 powerful
 kind
 of


transition
 point.
 Um
 there's
 definitely


interfaces
 that
 we're
 interested
 in


building
 and
 exploring
 as
 a
 company.
 And


so
 um
 you
 know
 you
 may
 have
 seen
 Flo


from
 Josh's
 team
 in
 labs
 that's
 that's


really
 trying
 to
 rethink
 like
 what's


what's
 the
 tool
 for
 AI
 filmmakers
 right


and
 for
 AI
 filmmakers
 image
 is
 actually


a
 big
 part
 of
 the
 iteration
 journey


right
 because
 video
 creation
 is


expensive
 a
 lot
 of
 people
 kind
 of
 think


in
 frames
 um
 when
 they
 when
 they


initially
 start
 creating
 and
 a
 lot
 of


them
 even
 start
 in
 the
 LLM
 space
 for


like
 brainstorming
 and
 thinking
 about


what
 they
 want
 to
 create
 in
 the
 first


place
 um
 and
 so
 there's
 definitely
 kind


of
 place
 that
 we
 have
 in
 that
 space
 of


just
 us
 trying
 to
 think
 about
 like
 what


does
 look
 like.
 Um,
 we
 have
 the


advantage
 of
 it
 kind
 of
 sitting
 close
 to


the
 models
 and
 the
 interfaces
 so
 we
 can


kind
 of
 build
 that
 in
 in
 a
 tight


coupling.
 Um,
 and
 then
 there's


definitely
 the,
 you
 know,
 we're
 probably


not
 going
 to
 go
 build
 a
 software
 for
 an


architecture
 firm.
 My
 dad
 is
 an


architect
 and
 he
 would
 probably
 love


that.
 Um,
 but
 I
 don't
 think
 that's


something
 that
 we
 will
 do,
 but
 somebody


should
 go
 and
 do
 that.
 Um,
 and
 that's


why
 it's
 exciting
 because
 we
 do
 have
 the


developer
 business
 and
 we
 have
 the


enterprise
 business
 and
 so
 people
 can
 go


use
 these
 models
 and
 then
 figure
 out


like
 what's
 the
 next
 generation
 workflow


for
 like
 this
 specific
 audience
 so
 that


I
 can
 help
 them
 solve
 a
 problem.
 So
 I
 I


think
 the
 answer
 is
 kind
 of
 like
 yes
 all


three.


>> Yeah.


>> Yeah.
 I
 I
 brought
 that
 up.
 I
 don't
 know


if
 you
 guys
 have
 been
 following
 the


reception
 of
 Nano
 Banana
 in
 Japan,
 but


um
 I'm
 sure
 you've
 had
 it's
 it's
 been


insane.
 And
 it's
 so
 funny
 like
 I
 now


half
 of
 my
 X
 feed
 is
 these
 really
 heavy


Nano
 Banana
 users
 in
 Japan
 who
 have


created
 like
 Chrome
 extensions
 called


there's
 one
 called
 like
 Easy
 Banana


that's
 specifically
 for
 using
 Nano


Banana
 for
 like
 manga
 generation
 and


specific
 types
 of
 anime
 and
 things
 like


that.
 And
 like
 they
 go
 super
 deep
 into


basically
 prompting
 the
 model
 for
 you


and
 storing
 the
 outputs
 in
 various


places.
 Um
 using
 obviously
 your
 your


underlying
 model
 to
 generate
 these
 like


amazing
 anime
 that
 you
 would
 never
 guess


were
 AI
 generated
 because
 like
 the
 level


of
 of
 precision
 and
 consistency
 and
 that


sort
 of
 thing
 is
 just
 beyond
 what
 I've


seen
 any
 single
 model
 be
 able
 to
 do


today.


I
 guess
 um
 what
 are
 some
 like
 to


Justin's
 point
 what
 are
 some
 force


multipliers
 that
 you
 guys
 have
 seen
 in


the
 model?
 So
 what
 I
 mean
 by
 this
 is
 for


example
 if
 you
 unlock
 character


consistency
 you
 can
 generate
 different


frames
 and
 then
 you
 can
 make
 a
 video
 and


then
 you
 can
 make
 a
 movie
 right.
 Um
 so


these
 are
 the
 things
 that
 if
 you
 get
 it


right
 and
 get
 it
 really
 well
 there's
 so


much
 more
 downstream
 tasks
 that
 can


derive
 from
 it.
 Um
 just
 curious
 like
 how


do
 you
 think
 about
 what
 are
 the
 force
 m


multipliers
 that
 you
 want
 to
 unlock?
 So


the
 next


>> what's
 the
 next
 big
 one


>> what's
 the
 next
 yeah
 big
 wave
 of
 people


who
 can
 just
 use
 nano
 nano
 as
 the
 base


model
 for
 all
 the
 downstream
 tasks.


So
 I
 think
 one
 one
 current
 one
 actually


is
 also
 the
 latency
 point
 right
 because


I
 think
 because
 I
 think
 it's
 also
 just


like
 it
 makes
 it
 really
 fun
 to
 iterate


with
 these
 models
 when
 it
 just
 takes
 10


seconds
 to
 generate
 the
 next
 frame
 right


if
 you
 had
 to
 sit
 there
 and
 wait
 for
 two


minutes
 like
 you
 would
 probably
 just


give
 up
 and
 leave
 a
 very
 different


experience
 so
 I
 think
 that's
 one
 just


like
 there
 has
 to
 be
 some
 quality
 bar


because
 if
 it's
 just
 fast
 and
 the


quality
 isn't
 there
 then
 it
 also
 doesn't


matter
 right
 like
 you
 have
 to
 hit
 a


quality
 bar
 and
 then
 um
 then
 speed


becomes
 a
 force
 multiplier
 I
 think
 this


general
 idea
 of
 just
 visualizing


information
 to
 your
 education
 point
 from


earlier
 is
 sort
 of
 another
 one,
 right?


And
 that
 needs


>> good
 text.
 It
 needs
 factuality,
 right?


Because
 if
 you're
 going
 to
 start
 making


kind
 of
 visual
 explainers
 about


something,
 um
 it
 it
 looks
 nice,
 but
 it


also
 needs
 to
 be
 accurate,


>> right?


>> And
 so,
 and
 so
 I
 think
 that's
 probably


kind
 of
 the
 next
 level
 where
 at
 some


point
 then
 you
 could
 also
 just
 have
 a


personalized
 textbook
 to
 you,
 right?


Where
 it's
 not
 just
 the
 text
 that's


different,
 but
 it's
 also
 the
 visuals.


Yeah,
 the
 diamond
 age
 that
 was
 basically


Yeah,
 basically.
 Um,
 and
 then
 it
 should


also
 internationalize
 really
 well,


right?
 Because
 a
 lot
 of
 the
 times
 today


you
 might
 actually
 be
 able
 to
 find
 a


diagram
 that
 explains
 the
 thing
 that


you're
 trying
 to
 learn
 about
 on
 the


internet,
 but
 it's
 maybe
 not
 in
 the


language
 that
 you
 actually
 speak.
 Um,


right?
 And
 so
 I
 think
 that
 becomes
 just


like
 another
 way
 to
 improve
 and
 open
 up


accessibility
 um
 of
 information
 to
 just


a
 lot
 more
 people
 and
 again
 visually


because
 a
 lot
 of
 people
 are
 visual


learners.


>> Interesting.
 How
 do
 you
 think
 about
 like


images
 generated?
 So
 the
 reason
 why
 I


asked
 is
 that
 there's
 another
 very
 cool


example.
 I've
 seen
 someone
 making
 it


work
 with
 nano
 banana
 which
 is
 he
 wrote


a
 script
 and
 then
 he
 kept
 prompt
 the


model
 to
 say
 generate
 the
 frame
 one


second
 after
 this
 and
 then
 it
 became
 a


video.
 So
 and
 then
 when
 I
 saw
 it
 I'm


like
 well
 is
 every
 image
 just
 one
 frame


in
 a
 continuum
 like
 you
 always
 know


about
 the
 continuum
 in
 a
 parallel


universe.
 you
 could
 have
 you
 know


generated
 any
 one
 of
 them.


>> It's
 one
 big
 directive
 graph
 that


>> right
 exactly
 and
 then
 maybe
 it's
 video


at
 the
 end
 of
 the
 day.
 So
 how
 do
 you
 see


that?
 Where
 does
 it
 you
 know
 intersect


or
 not
 intersect?


>> I
 think
 it's
 very
 yeah
 video
 and
 images


are
 very
 closely
 related.
 Um
 and


also
 I
 think
 what
 we're
 seeing
 in
 these


kind
 of
 what's
 coming
 next
 or
 sequence


predicting
 um
 use
 cases
 is
 the
 the


generalization
 in
 world
 knowledge
 of
 the


model
 as
 well.
 Um
 and
 and
 this
 is
 and
 so


where
 where
 do
 I
 think
 it's
 going?
 I


think
 that
 we
 will
 have
 yeah
 I
 think


video
 is
 um
 an
 obvious
 next
 kind
 of


domain.
 I
 think
 that
 like
 when
 when
 you


have
 editing
 um
 a
 lot
 of
 times
 what


you're
 asking
 is
 like
 you
 know
 what


happens
 if
 I
 do
 this
 and
 that's
 what


video
 has
 it
 has
 the
 the
 time
 sequence


of
 actions.
 So
 it's
 like
 we
 have
 a
 slow


frames
 per
 second
 video
 that
 you
 can


interact
 with,


>> but
 obviously
 making
 something
 that's


like
 fully
 interactive
 and
 real
 time
 and


um
 is
 the
 direction
 this
 this
 field
 is


headed.


>> So
 you
 are
 probably
 in
 the
 zero
 I
 don't


know
 how
 many
 zer
 0.001%
 of
 most


experienced
 people
 in
 the
 world
 using


image
 models.


>> What
 are
 your
 your
 personal
 favorite
 use


cases?
 How
 how
 do
 you
 use
 it
 dayto-day


if
 you're
 not
 just
 testing
 an
 existing


model?


Well,
 I
 so
 I'm
 not
 sure
 I
 am
 in
 the
 the


very
 top,
 but
 but
 I'll
 tell
 you
 what,


>> um
 I
 mean
 it's
 it's
 like
 we
 were
 saying


earlier,
 the
 personalization
 aspect
 is


the
 the
 thing
 that
 totally
 drives
 it


home
 for
 me.
 I
 have
 I
 have
 two
 young


kids
 and
 like
 the
 best
 things
 that
 I
 do


with
 the
 model
 are
 the
 things
 I
 do
 with


my
 kids
 and
 like
 we
 can
 make,
 you
 know,


make
 their
 their
 stuffed
 animals
 come
 to


life
 in
 these
 types
 of
 applications
 and


it's
 just
 so
 personal
 and
 gratifying
 to


see.
 Um,
 we
 also
 a
 lot
 of
 people
 um


taking
 old
 pictures
 of
 their
 family
 for


example
 and
 like
 um
 showing
 what


restoring
 them
 and
 and
 like
 so
 I
 think


that
 that's
 that's
 the
 the
 real
 beauty


of
 the
 edit
 models
 is
 that
 you
 can
 you


can
 make
 it
 about
 the
 one
 thing
 that


matters
 most
 to
 you.


>> So
 that's
 what
 I
 use
 it
 for
 is
 is
 my


kids
 basically.


>> Very
 nice.


>> Yeah.
 You're
 you're
 basically
 making


content
 that
 you
 probably
 would
 have


never
 made
 before
 and
 it's
 like
 for
 the


consumption
 of
 one
 person,
 right?
 Or
 or


or
 one
 family
 and
 you're
 kind
 of
 telling


these
 stories
 that
 you
 would
 have
 never


told
 before.
 So,
 kind
 of
 similar
 like
 I


do
 a
 lot
 of
 family
 holiday
 cards
 and


birthday
 cards
 and
 whatnot.
 Um,
 now


anytime
 I
 make
 a
 slide
 deck,
 I
 like


force
 myself
 to
 generate
 some
 images


that
 are
 like
 contextually
 relevant
 and


then
 try
 to
 get
 the
 text
 right
 um
 and


all
 of
 those
 things.
 And
 then
 we
 try
 to


push
 the
 boundaries
 around
 like
 can
 you


make
 a
 chart
 in
 the
 pixel
 space?
 Do
 you


want
 to
 that's
 another
 question,
 right?


Because
 you
 also
 want
 the
 um
 you
 want


the
 bars
 in
 the
 bar
 chart
 to
 be


accurately
 positioned
 relative
 to
 one


another.
 Um
 so
 I
 I
 think
 we
 do
 a
 lot
 of


these
 things.
 I'm
 actually
 really


impressed
 with
 the
 people
 we
 work
 with


on
 the
 team
 who
 are
 just
 like
 very


creative.
 Um
 we
 have
 a
 team
 um
 who
 just


works
 really
 closely
 with
 us
 on
 models


that
 we're
 developing
 and
 then
 they
 just


like
 push
 the
 boundary.
 They'll
 do
 like


crazy
 things
 with
 the
 models.


>> What's
 the
 most
 surprising
 thing
 you've


seen
 here?
 Like
 I
 didn't
 know
 our
 model


can
 do
 this.
 Yeah.


>> This
 is
 even
 just
 kind
 of
 like
 simple


things
 where
 people
 have
 been
 doing
 like


texture
 transfer.
 Like
 they
 will
 take


>> Yeah.
 like
 you
 take
 a
 portrait
 of
 a


person
 and
 then
 you're
 like
 what
 would


it
 look
 like
 but
 if
 it
 had
 the
 texture


of
 this
 piece
 of
 wood
 and
 I'm
 like
 I


would
 have
 never
 I
 would
 have
 never


thought
 of
 this
 being
 a
 use
 case
 because


my
 brain
 just
 doesn't
 work
 that
 way.
 Um


but
 people
 like
 kind
 of
 just
 push
 the


boundaries
 of
 what
 you're
 what
 you
 can


do
 with
 these
 things.


>> That
 is
 an
 interesting
 uh
 example
 of
 a


world
 knowledge
 because
 texture


technically
 is
 3D
 because
 there's
 like


the
 whole
 3D
 aspect
 of
 it.
 There's
 a


light
 and
 shadow
 of
 it
 but
 this
 is
 a
 2D


transfer.
 Yeah.
 So
 that's
 very
 cool.
 I


think
 for
 me
 the
 the
 thing
 I'm
 most


excited
 by
 and
 maybe
 most
 impressed
 by


is
 um
 are
 the
 the
 use
 cases
 that
 test


the
 reasoning
 abilities
 of
 the
 models.


So
 um
 some
 people
 in
 our
 team
 figured


out
 you
 could
 like
 give
 geometry


problems
 to
 the
 model
 and
 like
 ask
 it
 to


kind
 of
 you
 know
 solve
 for
 X
 here
 or


fill
 in
 this
 missing
 thing
 or
 like


present
 this
 this
 from
 a
 slightly


different
 like
 a
 different
 view


>> and
 like
 these
 types
 of
 um
 of
 things


that
 really
 require
 world
 knowledge
 and


the
 reasoning
 ability
 of
 like
 a


state-of-the-art
 language
 model
 are
 the


things
 that
 make
 me
 really
 go
 wow
 that's


amazing
 I
 didn't
 think
 we
 would
 be
 able


to
 do
 that.


>> Can
 it
 uh
 generate
 compile
 code
 on
 a


blackboard
 yet?
 And
 like
 if
 I
 take
 a


picture
 of
 my
 I
 don't
 know
 like
 code
 on


the
 laptop,
 would
 it
 know
 if
 it
 compiles


on
 the
 image
 model?


>> Um
 I've
 I've
 seen
 examples
 where
 people


give
 it
 like
 an
 image
 of
 HTML
 code
 and


have
 the
 model
 render
 the
 the
 web
 page


and
 it
 can
 do
 that.


>> That's
 very
 cool.
 The
 coolest
 example
 I


saw,
 so
 I
 came
 from
 academia,
 so
 I
 spent


a
 lot
 of
 time
 writing
 papers
 and
 making


figures.
 And
 um
 one
 of
 our
 colleagues
 uh


took
 a
 picture
 of
 one
 of
 the
 result


figures
 uh
 from
 one
 of
 their
 papers
 with


a
 method
 that
 could
 do
 a
 bunch
 of


different
 things.
 This
 this
 one,
 you


know,
 a
 bunch
 of
 different
 um
 type
 of


applications
 in
 the
 paper
 and
 asked
 the


model
 to
 and
 like
 sort
 of
 erased
 the
 um


the
 results.
 So
 you
 have
 like
 the
 inputs


and
 asked
 the
 model
 to
 like
 solve
 all
 of


these
 in
 picture
 form
 in
 a
 figure
 of
 a


paper
 and
 it
 was
 able
 to
 do
 that.
 So
 it


could
 actually
 like
 figure
 out
 what
 is


the
 problem
 that
 this
 one
 figure
 is


asking
 for,
 find
 the
 answer
 and
 put
 it


in
 the
 image
 and
 then
 do
 that
 for
 a


bunch
 of
 different
 applications
 at
 the


same
 time
 which
 was
 really
 amazing.
 Very


cool.


>> That's
 very
 cool.
 Have
 um
 has
 anyone


built
 application
 on
 top
 of
 that


capability
 yet?
 Like
 what's
 the


application
 that
 will
 come
 out
 of
 that?


>> I
 think
 that
 there
 are
 a
 lot
 of
 very


interesting
 I
 would
 say
 zero
 transfer


capability
 like
 problem
 solving
 type


things
 that
 we
 don't
 even
 know
 the


boundary
 of
 yet.
 And
 some
 of
 these
 are


are
 probably
 quite
 useful
 like
 you
 know


if
 you
 want
 to
 have
 a
 method
 that
 does


solves
 some
 problem
 X
 I
 don't
 know
 like


finds
 the
 the
 the
 the
 normals
 of
 the


scene
 or
 something
 like
 the
 service


orientations
 or
 something
 um
 you


probably
 can
 prompt
 the
 model
 to
 give


you
 kind
 of
 a
 reasonable
 estimate.
 Um
 so


I
 think
 there's
 lots
 of
 problems
 like


sort
 of
 understanding
 problems
 and
 other


types
 of
 things
 that
 we
 could
 maybe


solve
 with
 zero
 or
 few
 shop
 prompting


that
 we
 don't
 know
 yet.
 Yeah,
 there's


one
 thing
 you
 mentioned
 I
 found
 super


interesting,
 which
 is
 the
 world


knowledge
 transfer,
 but
 in
 a
 lot
 of


world
 models
 like
 or
 video
 models,
 there


always
 is
 something
 that
 keeps
 the
 state


like
 just
 because
 you
 look
 away
 doesn't


mean
 that
 the
 chair
 should
 disappear
 or


change
 color
 because
 it's
 that's
 not


what
 the
 state
 of
 the
 world
 is.
 How
 do


you
 see
 that?
 Do
 you
 think
 there's


relevance
 there
 in
 image
 model?
 Is
 that


something
 you
 even
 consider
 optimizing


for?
 Yeah,
 I
 mean
 if
 you
 think
 about
 an


image
 model
 that
 has
 a
 a
 long
 context


where
 you
 can
 put
 other
 things
 in
 that


context
 like
 text,
 images,
 audio,
 video,


then
 I
 think
 it's
 definitely
 like
 your


reasoning
 over
 the
 context
 of
 things
 you


have
 to
 produce
 a
 final
 output
 image


>> or
 video.
 Um
 so
 yeah,
 I
 think
 there's


definitely
 um
 some
 model
 capability
 to


do
 this
 type
 of
 stuff
 already.


>> Got
 it.
 I
 haven't
 tested
 it
 out
 yet
 for


this
 big
 use
 case,
 but
 I'll
 let
 you


know.
 That's
 one
 of
 my
 favorite
 things


about
 these
 models
 is
 just
 finding
 and


I'm
 sure
 it's
 really
 fun
 for
 you
 guys


and
 you
 guys
 probably
 have
 much
 more
 of


a
 hint
 than
 we
 do
 about
 what
 they
 can


do.
 But
 sometimes
 you'll
 just
 see
 some


crazy
 X
 or
 Reddit
 or
 wherever
 post
 about


some
 incredible
 thing
 that
 someone
 has


figured
 out
 um
 how
 to
 do
 that
 you
 would


never
 expect
 that
 the
 model
 might
 be


able
 to
 do
 necessarily
 and
 then
 other


people
 kind
 of
 build
 on
 that
 and
 say


like
 oh
 and
 then
 I
 tried
 the
 next


iteration
 of
 this
 thing
 and
 suddenly
 you


have
 this
 like
 almost
 entirely
 new
 space


that's
 been
 discovered
 in
 terms
 of
 what


the
 what
 the
 models
 are
 capable
 of.
 It


must
 be
 fun
 as
 people
 much
 more
 deeply


involved
 in
 kind
 of
 building
 these


models
 and
 building
 the
 interfaces
 to


kind
 of
 watch
 that
 happen.


>> Yeah.


>> So,
 so
 if
 you
 talk
 to
 visual
 artists


today,
 I
 I've,
 you
 know,
 I
 I
 personally


love
 this
 stuff.
 I
 post
 about
 it
 on
 the


internet.
 You
 can
 get
 some
 very


skeptical
 answers.
 People
 like,
 "Oh,


this
 is
 terrible."
 Right?
 Like
 what
 do


you
 have
 any
 idea
 what
 triggers
 this


this
 reaction,
 right?
 I'm
 convinced
 that


this
 ultimately
 really
 empowers
 the


artists,
 right?
 It
 gives
 you
 new
 tools,


right?
 is
 like
 hey
 we
 now
 have
 I
 don't


know
 watercolors
 for
 Michelangelo
 let's


see
 what
 he
 does
 with
 it
 right
 and


amazing
 things
 come
 out
 it's
 of
 the


similar
 thing
 but
 but
 what
 triggers
 this


this
 strong
 reaction
 against
 it


>> so
 I
 think
 it's
 something
 something
 to


do
 with
 the
 amount
 of
 control
 over
 the


output
 so
 you
 know
 in
 the
 beginning
 when


we
 had
 these
 kinds
 of
 text
 image
 models


they
 would
 be
 very
 much
 like
 a
 oneshot


you
 put
 in
 some
 text
 you
 get
 an
 output


and
 people
 would
 be
 like
 oh
 this
 is
 art


this
 is
 this
 thing
 I
 made
 and
 I
 think


that
 maybe
 rubs
 people
 a
 little
 bit
 the


wrong
 way
 who
 are
 come
 from
 the
 creative


community
 because
 um
 you
 know
 that
 it's


it's
 most
 of
 the
 decisions
 that
 were


made
 were
 made
 by
 the
 model
 by
 the
 data


that
 was
 used
 to
 train


>> express
 yourself
 anymore
 physically


right


>> yeah
 exactly
 it's
 not
 yeah
 so
 as
 a


creative
 person
 you
 want
 to
 be
 able
 to


express
 yourself
 so
 I
 think
 as
 we
 make


the
 models
 more
 controllable
 then
 a
 lot


of
 these
 concerns
 of
 like
 oh
 that's
 just


that
 the
 computer
 is
 doing
 everything


kind
 of
 may
 may
 go
 away
 um
 and
 the
 other


thing
 is
 I
 think
 that
 that
 there
 was
 a


period
 of
 time
 where
 we
 were
 all
 so


amazed
 by
 the
 images
 these
 models
 could


create
 that
 like
 we
 were
 we
 were
 pretty


like
 uh
 happy
 to
 see
 just
 like
 oh
 this


stuff
 comes
 out
 of
 these
 models
 but
 I


think
 humans
 get
 really
 bored
 fast
 of


this
 type
 of
 thing.
 So
 like
 there
 was
 a


big
 rush
 and
 now
 if
 you
 see
 a
 if
 you
 see


an
 image
 that
 you
 know
 was
 just
 like
 oh


that's
 just
 like
 a
 single
 prompt
 person


didn't
 think
 about
 it
 much
 you
 can
 kind


of
 tell
 like
 that's
 an
 AI
 generated


image
 not
 that
 interesting.
 So
 I
 think


like
 there's
 still
 this
 boundary
 of
 like


now
 you
 need
 to
 be
 able
 to
 make


interesting
 things
 with
 the
 AI
 tools
 um


which
 is
 hard
 but
 it
 this
 will
 yeah
 this


will
 always
 be
 you
 know
 a
 requirement.


We
 need
 someone
 to
 be
 able
 to
 do
 this.


And
 I
 think


>> we
 still
 need
 artists.


>> We
 still
 need
 artists.
 And
 I
 think


artists
 will
 be
 able
 to
 also
 recognize


when
 when
 people
 have
 actually
 like
 put


a
 lot
 of
 control
 and
 intent


>> and
 still
 not
 be
 an
 artist.


>> Maybe
 get
 but
 but
 it
 it
 is
 there's
 a
 lot


of
 craft
 and
 there's
 a
 lot
 of
 taste,


right,
 that
 you
 accumulate
 sometimes


over
 decades,
 right?
 And
 I
 don't
 think


these
 models
 really
 have
 taste,
 right?


And
 so
 I
 think
 a
 lot
 of
 like
 a
 lot
 of


the
 reactions
 that
 you
 mentioned
 maybe


also
 come
 from
 that.
 And
 so
 we
 do
 work


with
 a
 lot
 of
 artists
 across
 all
 the


modalities
 that
 we
 work
 with.
 Um
 so


image,
 video,
 um
 music
 because
 we
 really


care
 about
 like
 building
 the
 technology


step
 by
 step
 with
 them
 and
 trying
 to


figure
 out
 they
 really
 help
 us
 kind
 of


like
 push
 the
 boundary
 of
 what's


possible.
 A
 lot
 of
 people
 are
 really


excited,
 but
 they
 they
 really
 do
 bring
 a


lot
 of
 their
 knowledge
 and
 expertise
 and


kind
 of
 like
 30
 years
 of
 design


knowledge.
 We
 just
 work
 with
 um
 Ross


Loveg
 Grove
 um
 on
 fine-tuning
 a
 model
 on


his
 sketches
 so
 that
 he
 can
 then
 create


something
 new


>> out
 of
 that
 and
 then
 we
 design
 an
 actual


physical
 chair
 that
 we
 like
 have
 a


prototype
 of
 um
 and
 so
 there
 there's
 a


lot
 of
 people
 who
 want
 to
 kind
 of
 bring


the
 expertise
 that
 they've
 built
 and


kind
 of
 like
 the
 rich
 language
 that
 they


use
 to
 describe
 their
 work
 and
 and
 have


that
 dialogue
 with
 the
 model
 so
 that


they
 can
 push
 their
 work
 kind
 of
 to
 the


frontier.
 And
 it
 is,
 you
 know,
 it


doesn't
 happen
 in
 like
 one
 prompt
 and


two
 minutes.
 Um,
 it
 it
 does
 require
 a


lot
 of
 that
 kind
 of
 taste
 and
 human


creation
 and
 and
 craft
 that
 goes
 into


building
 something
 that
 actually
 then,


you
 know,
 becomes
 art.


>> At
 the
 end,
 it's
 still
 a
 tool
 that


requires
 the
 human
 behind
 it
 to
 to


express
 the
 feelings
 and
 the
 emotions


and
 the
 story
 and
 everything.


>> Yeah,
 absolutely.
 Absolutely.


>> And
 that's
 what
 resonates
 with
 you
 when


you
 probably
 look
 at
 it,
 right?
 Um,
 you


you
 will
 have
 a
 different
 reaction
 when


you
 know
 that
 there's
 a
 human
 behind
 it


who
 has
 spent
 30
 years
 thinking
 about


something
 and
 then
 pour
 that
 into
 a


piece
 of
 art.


I
 think
 there's
 also
 a
 bit
 of
 this
 um


phenomenon
 that
 like
 most
 people
 who


consume
 creative
 content
 and
 maybe
 even


ones
 that
 are
 that
 care
 a
 lot
 about
 it


like
 they
 they
 don't
 know
 what
 they're


going
 to
 like
 next.
 You
 need
 someone
 who


has
 a
 vision
 and
 can
 do
 something
 that's


interesting
 and
 different,
 right?
 And


then
 you
 show
 it
 to
 people
 like,
 "Oh,


wow.
 That's
 amazing."
 But
 like
 they


wouldn't
 necessarily
 like
 think
 of
 that


on
 their
 own,


>> right?


>> So
 when
 we're,
 you
 know,
 when
 we're


optimizing
 these
 models,
 like
 one
 thing


we
 could
 do
 is
 we
 could
 optimize
 for


like
 the
 the
 average
 preference
 of


everybody.


>> But
 I
 don't
 think
 you
 end
 up
 with


interesting
 things
 by
 doing
 that.
 You


end
 up
 with
 something
 that
 everyone
 kind


of
 likes,
 but
 you
 don't
 end
 up
 with


things
 that
 people
 are
 like,
 "Oh,
 wow.


That's
 amazing.
 like
 I'm
 going to
 change


my
 my
 my
 whole
 like
 perspective
 of
 art


because
 I
 saw
 that


>> there's
 the
 avantguard
 edition
 of
 the


model


if
 I
 use
 it
 with
 the
 term
 there's
 the
 I


don't
 know
 what's
 what's
 the
 other
 end


of
 the
 spectrum
 the
 marketing
 edition
 or


so
 where
 it's
 very
 predictable
 and


>> very
 straightforward.


>> Yeah.
 Well,
 since
 we're
 coming
 up
 on


time,
 uh,
 last
 couple
 question.
 One
 is,


what's
 one
 feature
 that
 you
 know
 the


model
 is
 capable
 of
 that
 you
 wish
 people


ask
 you
 more?


>> Interle.


>> Yeah,
 in
 I
 think
 we've
 always
 been


amazed
 that
 nobody
 ever
 posts
 anything


about
 in
 solely
 generation
 is
 what
 we


call
 the
 model's
 ability
 to
 generate


more
 than
 one
 image
 for
 a
 specific


prompt.
 So,
 you
 can
 ask
 for
 like
 I
 want


a
 story
 like
 a
 bedtime
 story
 or


something
 like
 generate
 the
 same


character
 over
 these
 series
 of
 images.


And
 I
 think
 that
 um
 yeah,
 people
 haven't


really
 found
 it
 useful
 yet
 or
 haven't


discovered
 it.
 I
 don't
 know.


>> Oh,
 interesting.
 Well,
 if
 you're


listening
 to
 the
 podcast,
 go
 try
 this


out.


>> Try


>> Yeah.
 And
 what's
 the
 most
 um
 exciting


technical
 challenge
 that
 you
 look


forward
 to
 tackling
 in
 the
 next,
 I
 don't


know,
 months,
 years.


>> So,
 I
 think
 that
 there's
 really
 a
 high


ceiling
 in
 terms
 of
 quality
 for
 where


we're
 going.
 Like,
 I
 think
 you
 people


look
 at
 these
 images
 and
 say,
 "Oh,
 it's


almost
 perfect.
 we
 must
 be
 done.
 And
 for


a
 while,
 we
 were
 in
 this
 like
 cherrypick


phase
 where
 we
 would,
 you
 know,
 everyone


would
 pick
 their
 best
 images.
 So,
 you


look
 at
 those
 and
 they're
 great.
 But


actually,
 what's
 more
 important
 now
 is


the
 worst
 image.
 We're
 in
 a
 lemon


picking
 stage
 because
 every
 model
 can


cherrypick
 images
 that
 look
 perfect.


>> So,
 like
 now
 I
 think
 the
 real
 question


is
 like
 how
 expressable
 is
 this
 model


and
 what's
 the
 worst
 image
 you
 would
 get


given
 what
 you're
 trying
 to
 do.


>> So,
 I
 think
 by
 raising
 the
 quality
 of


the
 worst
 image,
 we
 really
 open
 up
 the


amount
 of
 use
 cases
 for
 things
 we
 can


do.
 like
 there's
 all
 kinds
 of


productivity
 use
 cases
 like
 um
 you
 know


beyond
 this
 kind
 of
 like
 immediate


creative
 tasks
 that
 we
 know
 the
 model


can
 do
 and
 I
 think
 that's
 a
 direction


we're
 headed
 we're
 headed
 to
 where
 if


these
 models
 can
 do
 more
 things


reasonably
 then
 they're
 just
 the
 the
 use


cases
 will
 be
 far
 greater


>> so
 that's
 the
 that's
 the
 moral


equivalent
 of
 the
 monkeys
 on
 typewriters


basically
 any
 model
 given
 enough
 tries


will
 eventually
 make
 an
 amazing


adventure


>> but
 the
 the
 other
 way
 around
 it's
 hard


>> yeah
 the
 other
 round
 is
 hard
 one
 monkey


writing
 a
 book
 would
 be
 very
 hard


>> it
 would
 be
 a
 good
 monkey
 for
 that
 one


>> what
 are
 the
 applications
 you
 think
 that


would
 come
 out
 when
 we
 reach
 the
 lower


bound?


>> So,
 the
 one
 I'm
 most
 interested
 in,
 we


mentioned
 this
 before,
 is
 education


factuality.
 I
 have
 um
 you
 know,
 I
 I
 have


every
 I
 don't
 know
 how
 many
 times
 I
 want


to
 use
 these
 models
 for
 creative


purposes
 a
 month,
 but
 like
 I
 have
 way


more
 use
 cases
 for
 information
 seeking,


factuality,
 kind
 of
 like
 learning,


education
 type
 use
 cases.
 So,
 I
 think


like
 once
 that
 starts
 working,
 then


it'll
 be
 opening
 up
 all
 these
 new
 areas.


Amazing.


>> There's
 also
 something
 about
 I
 think


taking
 more
 advantage
 of
 the
 models


context
 window.
 Um
 so
 you
 can
 input
 a


really
 large
 amount
 of
 content
 right


into
 these
 LLMs.
 And
 um
 some
 companies


um
 you
 mentioned
 a
 few
 before
 um
 they


will
 have
 like
 150
 page
 brand
 guidelines


on
 like
 what
 you
 can
 and
 cannot
 do,


right?
 And
 they're
 like
 very
 precise,


right?
 Like
 colors,
 fonts,
 and
 right


>> um
 and
 like
 the
 the
 the
 size
 of
 like
 a


Lego
 brick
 maybe.
 Um
 and
 so
 being
 able


to
 actually
 like
 take
 that
 in
 and
 follow


that
 to
 a
 tea
 when
 you're
 doing


generation
 that's
 like
 a
 whole
 new
 level


of
 control
 um
 that
 we
 just
 can't
 we


don't
 have
 today
 right
 um
 to
 to
 make


sure
 that
 you're
 actually
 kind
 of
 like


following
 that
 to
 a
 tea.
 I
 think
 that


will
 build
 a
 lot
 of
 trust
 with
 you
 know


very
 established
 brands.
 where
 we
 have
 a


second
 creative
 compliance
 review
 model


that
 then
 double
 checks
 everything
 that


I
 could
 do
 against
 the
 the
 model
 should


do
 it
 on
 its
 own,
 right?
 Like
 like
 it


should
 kind
 of
 have
 this
 yes
 it
 should


have
 this
 loop
 as
 like
 okay
 I
 generate


this
 but
 then
 page
 52
 says
 that
 I


shouldn't
 have
 right
 and
 I'm
 going
 to
 go


back
 and
 try
 again
 and
 then
 two
 hours


later
 it
 will
 come
 back
 to
 you
 with
 that


respect.


>> Yeah.


>> And
 we
 saw
 with
 the
 text
 models
 how
 this


inference
 time
 scaling
 how
 much
 it
 can


help
 right
 being
 able
 to
 to
 critique


your
 own
 work.
 Yep.


>> So
 this
 this
 feels
 really
 important.


>> Boy,
 an
 incredibly
 amazingly
 exciting


future
 for
 for
 image
 models.


>> Yes.
 And
 congrats
 on
 all
 the
 amazing


work.


>> Thank
 you.


>> Thank
 you.


>> Thanks
 for
 having
 us.


>> Well,
 thank
 you
 so
 much
 for
 coming
 on


the
 pod.


[Music]


[Music]