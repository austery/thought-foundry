大家好欢迎来到我的频道
今天这期节目呢
我们来讲一讲ChatGPT的本质啊
那基于的内容呢
是这个Stephen Wolfram写的这本书啊
叫做ChatGPT在做什么
以及他为什么能够成功
这本书呢是2023年的时候发表的
那我其实这个频道呢
之前也做过两期
关于他这个书
里头的一些章节的内容啊
但是当时呢
这个频道的流量实在是非常的可怜
然后呢
做出这种硬核的东西就没人看啊
因为本来做这种
涉及到偏硬核一点的东西
就看的人很少
再加上你的订阅数又不行
那自然就没什么人看啊
现在我来想试一试再做一下这本书
因为我觉得确实写的挺好的
前段时间呢
做了那个Gary Marcus 的这个批评
做了那个Gary Marcus 的这个批评
ChatGPT的视频啊
底下好多朋友在那留言说啊
这个Gary Marcus一看就是不懂AI啊
但是呢尽管Gary Marcus
他已经曾经创办过一个AI公司
还被Uber给收购了
还赚了很多钱
但是呢他就是在这些观众的口中啊
就是不懂AI的
对吧
大家可以对自己不太认可的观点啊
保持相对宽容一点
有的时候呢
你可能获得更多信息啊
理解更深刻一些
你可能会发现哎
这个观点其实还是有一些道理的
我觉得大家要有一个open mind啊
你像我
都能够做这个关于Peter Thiel的视频
我是本来觉得他是一个这种MAGA
背后的mastermind啊
这个右翼MAGA
但是呢我深入了解这个人之后呢
我发现他其实还是
不一样的地方的
那我希望接下来做的这个
Peter Tiel的这个系列
能给大家带来一个不一样的呃视角
那么我们今天讲的这本书呢
其实就是给大家拆解一下
ChatGPT的本质
其实呢呃
Stephen Wolfram啊
他对于GPT 的这个看法
跟这个Gary Marcus有一点相似啊
这里算不算剧透呢
就他认为啊
我们感受到这种智能和理解
以及创造力啊
是一个彻头彻尾的幻觉啊
那我们先直接进入主题了
那么在Stephen Wolfram的看法中啊
这个能够写诗
能写代码
能跟你谈天说地啊
通过各种考试
看起来无所不知的这个ChatGPT啊
在本质上
只是一个无比强大的文字接龙工具啊
这个Stephen Wolfram呢
其实在他的这本书里头呢
对这个GPT 内核的这个神经网络啊
做了非常深入的解释
那么大家感兴趣的话
可以去看一下他写的这些技术细节啊
就是在Stephen Wolfram看来啊
我们人类感觉到那种智能
理解创造力
是一种彻头彻尾的幻觉啊
我们觉得能够做到这一切的背后
一定有一个复杂的
类似于人脑的思考过程哎
但是Stephen Wolfram告诉大家
他生成每一个词
都是在做一个极其简单的数学题
就是根据前面内容下一个词最大概率
是什么
接下来节目呢
我们就借助Stephen Wolfram这一本小册子
给大家拆解一下ChatGPT底层的逻辑啊
在介绍这本书之前呢
我必须先隆重介绍一下
这本书的作者啊
Stephen Wolfram啊
这个人绝对是一个猛人啊
如果你大家不知道他这个背景的话
看完我下面的介绍
你肯定会对他有了新的认知啊
这个Stephen Wolfram
他不是一个普通的程序员
或者是科技评论家
他是一个跨越了物理学
数学和计算机科学与哲学的通才
猛人啊我们能看到
一本写的
把这个GPT讲的那么透彻
的一个小册子啊
是因为他过去40多年人生啊
几乎全在为理解这种复杂系统做准备
我们先看一下他的这个履历啊
这位老兄呢
15岁的时候
就发表了第一篇
关于粒子物理的科研论文啊
他20岁就拿到了
加州理工学院的理论物理学位
的这个博士学位
那时候
我们还大部分人都还在读本科啊
然后呢他就拿了麦克阿瑟天才奖
成为那个时代的
最耀眼的学术明星之一啊
我们今天
不是在感叹于这个神经网络的神奇吗
这个Stephen Wolfram
他在1983年
自己就亲手写过一个神经网络程序了
这概念是什么
那会儿连Windows都还没诞生啊
所以他对这项技术的理解呢
其实有长达40年的历史纵深啊
然后呢他人生最重要的项目
就是花了40多年时间
几乎以一己之力啊
打造了一门全新
的语言叫做Wolfram language
也是Mathematica背后的语言
这个Mathematica呢
就有点类似于MATLAB
是一个数学的编程软件啊
其实是非常好用的
那我那时候
上这个量子物理的课程的时候呢
必须得用这个东西来算
解这些什么偏微分方程啊
它背后是有一个非常强大的
这个计算引擎的
在做各种运算的时候是非常的迅速
那么他的野心
其实就是想要创造一种
能够精确描述和计算世界万物的
这种符号语言
从程式，化学分子到金融数据
他都想用一种统一的
可计算的框架去表示啊
他也是这个知识引擎啊
这个Wolfram Alpha的发明者
如果你用过Siri啊
你可能间接用过他的技术
那个能够直接回答你啊
珠穆朗玛峰有多高
或者是帮你解数学题的这种搜索引擎
啊
Wolfram Alpha就是基于Wolfram language打造的
它其实代表了和ChatGPT啊
完全不同的另一条AI的路线
这还不是最夸张的
他自己一生出版了近300万字的著作啊
过去30年写了1,500万字的邮件啊
总共打了大概5,000万字
你敢信啊
他连自己写了多少邮件
都统计的清清楚楚啊
这展现他对于万物量化和记录的
这种极致追求
你看到
当这样一个人来解读ChatGPT的时候
他的视角是非常独特的
他既是那个
亲手实践过这个神经网络的圈内人
而且呢
他
又是创造了另一条技术路线的旁观者
他能够从最底层的这个代码
和数学原理出发啊
又能够上升到计算物理
以及宇宙的哲学高度啊
他不是在猜测ChatGPT在做什么
他是用自己构建了半辈子的
关于这种计算和知识的
庞大理论体系啊
去框定和解释ChatGPT这个新物种啊
这也是为什么
他这本小册子我非常推荐啊
在介绍完他的背景之后呢
我们来看一看他的这个解剖之旅啊
就大家知道吗
这个ChatGPT背后的核心技术啊
神经网络
其实一点都不新啊
Stephen Wolfram
他自己就提到啊
早在1983年的时候
他就亲手写过一个神经网络程序
那时候呢
电脑还是个大家伙
慢得像个老牛拉车
结果呢
这个程序啊
什么有趣的事情也干不了
哎这就很奇怪了
一个40年前就有的老技术
怎么就在今天核爆了呢
这其实就跟做菜一样
你光有食谱啊
也就是这个神经网络的一个理论
是没用的
你还需要有顶级的食材和给力的厨具
你看这40年间
发生了三件大事
才凑齐了AI这盘满汉全席啊
第一就是算力爆炸
我们今天电脑啊
比40年前快了何止100万倍啊
过去要跑几年的计算
现在可能几秒钟就搞定了
这为训练一个巨大无比的神经网络
提供了可能
第二呢就是数据海洋
这是互联网诞生啊
给我们带来数十亿计的网页数字
化的图书和各种文本资料
相当于给AI提供了几乎无限的食材
去学习去品尝人类语言的各种味道
第三呢就是工程上的各种骚操作
也就是一系列算法和架构上的创新
这让顶级大厨们
琢磨出来各种烹饪技巧啊
能让同样的食材呢
做出这个米其林3星级的这个味道
所以你看啊
ChatGPT的成功啊
并不是凭空冒出来的魔法
而是一场酝酿了40年的技术风暴
而是一场酝酿了40年的技术风暴
当一台啊
比这个Steven Wolfram
当年那台大了十几亿倍的神经网络
被喂进了整个互联网的知识之后啊
一个奇迹就发生了
他真的学会了像人一样说话
但是呢这正是像人一样说话
恰恰是让我们最困惑的地方
我们人类说话背后是意义在驱动
我想表达一个意思
然后组织语言说出来
而机器呢
它懂得什么是意义吗
这就是我们要解开的核心谜题
ChatGPT这一个黑箱啊
到底是怎么运转的
他生成那些看似充满意义的文字啊
内部发生了什么
他为什么能够做到
而且还做那么好
这个Wolfram这本书啊
其实就像一把手术刀
给我们精准的剖开这个黑箱
他没有用太多这个花里胡哨的术语啊
而是用近乎第一性原理的方式
带我们从最基本的单元开始
一步步搭建啊
对这个ChatGPT的理解
接下来内容可能有点硬核
但别担心啊
我会用比较接地气的比喻
带你弄懂这一切
我的目标呢
就是当这个视频结束的时候
你不仅知道ChatGPT是什么
更能理解它为什么是这样
以及你未来和AI打交道时呢
有更深刻的洞察力
好我们现在正式进入ChatGPT的大脑啊
记住我们开头那个暴论嘛
它是在玩文字接龙
我们来看一个例子啊
假设我们输入
AI最好的地方在于他能力去
接下来呢
ChatGPT要做的
不是要去思考AI到底有什么能力
而是要去他的记忆头搜索他
记忆是什么呢
是他读过数十亿人类写的网页和书籍
他会统计啊
在人类的语言中呢
当出现AI最好的地方在于他的能力去
这句话之后
最可能出现的词是什么
它有可能出现一个概率列表啊
就像这个学习啊
这个概率可能是最高的
然后呢就是预测predict
然后呢就是创造create
或者是理解understand
但是呢他并没有真正在理解
他只是在做统计和预测
那接下来怎么选呢
最简单的想法
就是每次都选频率最高那个词
对吧但有意思的来了
这个Wolfram管呢
这个叫做一点巫术的开始啊
就是如果每次你都选最正确的答案
那写出来的文章会非常无聊死板
甚至有点不断重复
所以呢为了让文章显得更有趣啊
更有创造力
那么ChatGPT呢
会引入一个随机性他偶
他偶尔会选择一个概率没那么高的词
这种随机性的程度呢
由一个叫做温度
temperature的参数来控制
温度越高呢
他越可能不走寻常路
选出一些更意外的词
这就是为什么
你用同一个问题问他好几次啊
都会得到不同答案的原因
所以现在你明白了吗
一篇洋洋洒洒的千字长文
在ChatGPT看来
就是不断重复一个动作
根据我已经写出来的所有文字
下一个最合理的词是什么
然后啪加上一个词
然后再重复
啪又是一个词
就是这么简单粗暴
听到这你肯定会问啊
这概率是从哪来的呢
难道
是硬生生统计所有的这个词语组合吗
我们先从一个更简单的模型说起啊
如果我们不用这么复杂ChatGPT
只是用最原始的统计方法
能生成像样的句子吗
比如说啊
我们用统计英语单个单词出现的频率
然后像抽奖一样随机往外蹦词
结果呢肯定是一堆胡言乱语啊
比如程序过度被研究
是不是这样的啊
这根本就没法看
然后我们再升级一下
统计两个词连在一起的概率啊
这个叫做2-gram啊
比如呢
cat后面很有可能跟的是is或者是that
这样生成的句子会稍微通顺一点
但依然是胡说八道
问题在哪呢
问题在于啊
语言的依赖关系是长程的
一句话呢开头
可能会影响到十几个
甚至几十个词的结尾
所以呢
要统计所有20个词组合的概率啊
这数字比宇
宙里的粒子总数还要多
历史上所写过的文字加起来
总个零头都凑不够
所以呢靠死记硬背的统计是不行的
那怎么办呢
那就得引出这个故事里头
最关键的主角了
模型简单来说啊
模型就是这种举一反三的工具
我们不需要看过所有的句子
我们只需要给他喂足够多的例子
让他自己去总结
出一套生成概率的规则
这样呢
哪怕遇到一个他从来没有见过的句子
根据这套规则呢
也能够估算下一个词的概率
ChatGPT用的就是这种模型
也就是我们前面所提到的神经网络
神经网络呢
这个词听起来很高大上
但我把它拆开来看
其实没那么玄乎
你可以把它看成一个超级复杂的函数
我们都知道
函数嘛给一个输入x
它给出一个输入y
神经网络也是这样
只不过它的输入和输出呢
就超级复杂
我们拿一个简单的例子啊
就是识别手写数字来理解一下
就是识别手写数字来理解一下
你看到一张呢
手写着数字2的图片啊
对于电脑来说呢
它就是一堆像素点
每个点有一个灰度值
我们可以把所有的
这些像素点的灰度值啊
拉长成一长串的数字
作为神经网络的输入
然后呢这串数字就在网络里
开始他的奇幻漂流了
这个网络有很多层的人工神经元组成
每一层神经元呢
都会对上一层传过来的数字
进行一次加权求和
然后呢
再套上一个简单的激活函数比
如说呢小于0就归0
大于0就不变
算出来结果呢
再传递给下一层
就这样一层层的传达下去
像多米诺骨牌一样
最后呢在输出层啊
大概有10个神经元
分别代表了数字0-10
经过计算之后啊
哪个神经元输出的数值最大
网络就会认为啊
那张图片就是哪个数字
比如在这里呢
2那个神经元的数值啊
会是最高的
很神奇吧
一个由成千上万
极其简单的数学运算组成的庞大系统
竟然能够识别这么智能的任务
那么这些神经元之间的连接权重
就是那个加权求和的权
是怎么来的呢
是天上掉下来的吗
当然不是了
是训练出来的training
或者说呢
是调教出来的
我们给他看过成千上万张
已经标注好答案的图片
比如说给他一张2的图片
如果他识别错了
我们就告诉他错了
正确答案是2
然后呢用一个反向传播的算法
微调网络里成千上万个权重
那下次再看到这张图时呢
犯错的可能性小一点点
这个过程呢
Wolfram打了个比方啊
就是在一个有着无数山谷
和山峰的复杂地形上
找最低点
每一次调整权重呢
相当于我们朝这个山坡最陡峭的方向
挪一小步
也就是所谓的梯度下降
经过几万次几亿次的挪动
我们最后就能找到一个足够低的山谷
这时候呢
网络的权重基本上就调教好了
他学会了识别数字
我们知道了神经网络
也知道它怎么被训练的
但有一个关键问题还没解决啊
怎么把文字这个东西
喂给只懂数字的神经网络呢
总不能简单的给猫编号为一
狗编号为2吧
这样机器没有办法完全理解
猫和狗在意义上比猫和椅子更接近
于是呢一个天才般的想法诞生了
叫做词嵌入word embedding
简单说法就是说呢
我们不要用一个数字来代表一个词
而是用一个向量
也就是一长串数字来表示一个词
这个词呢
可以看作是在一个几百甚至上千维度
我们没有办法想象的意义空间的坐标
这个坐标怎么算出来的呢
还是靠训练
比如说呢
一个神经网络里
是预测这个句子中间的词
经过海量文本的训练
网络会发现
鳄鱼alligator和短吻鳄crocodile
在出现的上下文环境总是很相似
于是呢
它就会在那个高维度的意义空间里啊
这两个词的坐标放的特别近
而萝卜和老鹰
这个两个词的上下文天差地别
它们坐标就会离得特别远
你猜怎么着
当这个意义空间构建好之后
一些奇妙的事情就发生了
人们发现啊
在这个空间里啊
词语之间关系竟然可以用向量的
加减法来表示
最经典的例子就是
国王的向量减去男人的向量
加上女人的向量
等于王后的向量
那这就很神奇了
这意味着
在神经网络在无监督的学习中啊
自己领悟了人类语言中
蕴含的复杂语义关系
并且把它们编码到了这个高维空间
现在呢
我们就可以把整个ChatGPT的工作流程
串起来了
他拿到你输入的文字
先把每个词或者词根呢
转化为他的意义空间那个坐标向量
然后呢这个巨大的神经网络
也就是Transformer
就开始对这些向量
进行一些极其复杂的计算
最后
预测出下一个最可能的词的坐标向量
然后再把这个向量呢
翻译成我们能够看懂的词
所以呢ChatGPT生成文本的过程
本质就是
在我们看不见的
一个高维的意义空间里头
顺着一条啊语义上最通顺的轨迹散步
每一步呢
都迈向下一个概率最高的点
我们刚刚提到了一个关键角色Transformer
也就是ChatGPT所用的神经网络架构
的名字
也是它效果如此惊人的核心工程之一
Transformer最厉害的一点呢
在于他发明了一种叫做注意力机制
attention mechanism的东西
这个东西是什么意思呢
打个比方啊
我们人在读一个长句子的时候
注意力也不是平均分配的
比如这个句子
我今天在公园里看到一只非常可爱的
毛茸茸的白
色小猫它正在
他正在草坪上懒洋洋的晒着太阳
并且时不时的摇着尾巴
当你读到他的时候啊
你大脑会立即把注意力
放到前面的这个小猫身上
而不是公园或者草地
早期语言模型呢
没有这个能力啊
他看每一个词都差不多
所以呢处理长句的时候
很容易忘掉前面的内容
而Transformer的注意力机制啊
就很完美的模拟了这一点
在决定下一个词是什么的时候呢
他会给前面所有出现的词啊
都分配一个注意力权重
哪些词对预测下一个词更重要
权重就更高
这让ChatGPT
拥有超长的长文本理解和生成能力
他能记住几十页前你跟他聊过的内容
并且
在你写长篇大论的开头和结尾之间
建立联系
让整个文章的逻辑啊
看起来非常连贯
这就是注意力的机制的功劳
当然这也是谷歌当时写的
那篇跨时代的论文啊
就是attention is all you need啊
这篇文章发出来之后
才让这个Openai能够呃开发出这个ChatGPT
不过呢光靠网上读书啊
还不足以打造出
我们今天看到这个彬彬有礼
乐于助人
而且三观很正ChatGPT
因为网上内容可是鱼龙混杂
什么都有
如果放任自流的学啊
AI可能会学到很多偏见错误的信息
甚至出现一些攻击性的言论
比如像这个Grok
之前做了一些纳粹的言论啊
所以呢Openai的工程师啊
给他最后加了一道也非常重要的工序
基于人类反馈的强化学习
这个过程啊
简单来说呢
就是请人类当老师
他们先让最初的模型啊
对各种问题生成好几个不同的答案
然后呢雇佣一批标注员
像老师改作业一样
给这些回答排序
哪个最好
哪个次之
哪个最差
接着呢
他们就用这些人类偏好的数据啊
又训练另一个奖励模型
这些模型的作用呢
就是去模仿人类老师的品味啊
去给AI的任何回答打分
最后呢ChatGPT
去跟这个奖励模型去玩
ChatGPT不断生成新的回答
而奖励模型呢
则不断地给它打分
ChatGPT的目标啊
就是想办法让自己生成的回答
能在这个内置老师那里
拿到尽可能高的分数
通过这个过程呢
ChatGPT
学会了如何生成更符合人类期望
更安全更有帮助的回答
就像一个孩子
不仅读了很多书
还有一位耐心的老师在旁边
时刻纠正他
引导他
最终才成长成一个优秀的学生
好我们现在已经把ChatGPT的底层原理
和训练过程
都扒了一遍了
理论上看呢
它看起来很完美
但实践中
呢它真的是无所不能吗
当然不是了
记住他本质啊
他是一个概率模型
他追求的是听起来最合理
而不是事实最正确
这导致了他会经常的
一本正经的胡说八道
比如说啊
你问他一个精确计算的问题
Wolfram在书里头就举了个例子啊
问他芝加哥到东京有多远
ChatGPT
会给你一个看起来很自信的答案
甚至还贴心的换算了公里
但是呢那个数字是错的啊
当然呢
这是Wolfram当时用的这个ChatGPT吧
为什么呢
他并不是在计算啊
他只是在他记忆头
找到了无数个描述距离的文本
然后模仿这些文本的风格啊
生成一个看起来那么回事的数字
再比如数学题
你让他算个微积分
他可以给出非常详细的解题步骤
格式工整
术语专业
但是呢十有八九是错的
更搞笑的是呢
他编造这些解题步骤啊
犯的错误
和人类学生犯的几乎是一模一样
因为他学习材料里头呢
包含了大量学生在网上问的错题
和错误的解法
这就告诉大家一个极其重要的原理啊
ChatGPT是一个语言模型
不是一个计算引擎
那么怎么解决ChatGPT
事实不靠谱这个问题呢
难道就没救了吗
当然有了
Wolfram在这个书里的第二部分
给出了一个非常绝妙的解决方案
就是把两种AI结合起来
第一种呢
就是ChatGPT为代表的
基于统计处理非结构化语言的AI
他强项是理解人的意图
进行流畅对
话处理模糊开放性的问题
他像一个博学有创造力
但有点马虎的文科生
第二种呢
就是以Wolfram自己的产品啊
Wolfram Alpha为代表的
基于符号计算和结构化知识的AI
它的核心呢
是一个巨大的
经过严格策划和验证的知识库
以及强大的计算算法
它寻求的是绝对的精确
像一个不善言辞
但是极其严谨可靠的理科状元
当这两种大脑结合
会发生什么呢
你看到啊
当用户用ChatGPT
去提出一个需要精确计算
或者事实查询的问题时呢
这ChatGPT不再自己瞎猜了
而是转过头去问Wolfram Alpha
他用自己擅长自然语言
把这个问题抛给了Wolfram Alpha
然后呢
后者用强大的自然语言理解能力啊
把这个问题转化为精确的
可计算的符号代码
然后调用自己的知识库和算法
得出100%准确的结果
然后再返回给ChatGPT
最后呢ChatGPT再发挥他语言天赋啊
把这个冷冰冰的精确的结果
包装成一段流畅自然
可以用于
易于理解的文字
呈现给用户
这就是一个完美的互补
ChatGPT呢
负责和人沟通和润色
Wolfram Alpha负责硬核计算和实时核查
这才是一个真正强大可靠的AI助手
应有的形象啊
当然呢他写这本书啊
肯定要给自己家的产品
做一些这个代言啊
但他的核心理念
其实跟那个Gary Marcus是一样的
就是说你要把这个符号的呃AI
就是说你要把这个符号的呃AI
和基于这个统计的deep learning的这个AI
和基于这个统计的deep learning的这个AI
和基于这个统计的deep learning的这个AI
结合在一起
聊到这里呢
我们已经把ChatGPT的技术细节
和应用前景
都已经摸得差不多了
但我想带大家再深挖一层啊
ChatGPT的巨大成功
除了技术上的突破
它还向我们揭示了一个可能更深远
关于我们人类的科学事实啊
那就是呢
人类的语言以及其背后的思维模式啊
可能比我们想象的要简单
要更有规律
大家想一想
为什么本质上只是在预测下一个词
结构相对简单的神经网络
就能够模拟出
如此丰富和复杂的语言现象
这背后一定是因为啊
我们语言本身
就存在强大的可预测的内在规律
这些规律呢
可能不只是我们语法书上学的那些
主谓宾定状补啊
而是一种更深层次的语义语法
比如呢有实体属性的物体
可以做移动这个动作
但是呢抽象的概念就不行
有身体特征的东西可以吃东西
但是呢无生命的物体就不行
这些
都是我们潜意识里遵循的语义规则
其实GPT在阅读了海量文本之后
他没有去理解这些规则
但是他通过统计发现了这些规则存在
他学到不是知识本身
而是知识和概念之间
连接的形状和模式
就像我们发现了万有引力
不是我们理解了引力的本质
而是我们通过观察和计算
总结出一套
能够完美预测天体运动的数学
公式ChatGPT的成功啊
可能就意味着
我们第一次有了一面镜子
可以间接的窥探人类思维和语言
背后啊
那隐藏的支配一切的语法和定律
那这是不是意味着呢
只要神经网络足够大
数据足够多
它就能够无所不能
解决所有问题呢
Stephen Wolfram
给出了一个非常深刻的否定答案
他提出一个概念叫做计算不可约性
这什么意思呢
就是说啊
在我们宇宙中存在这样一类过程
你想知道它最终结果
没有任何捷径可走
唯一的办法呢
就是老老实实一步步的把它运行一遍
比如说这个元胞自动机（Cellular Automata，简称CA）是一种时空和状态均离散的动力学模型自动机啊
它的规则极其简单
但是它演化出来的这个图案
可以复杂到无法预测
你没有办法通过一个简单公式
就算出它第一亿步会是什么样子
你只能从第一步开始
一步步的算到第一亿步
大量的自然现象和复杂的数学问题
都具有这种计算不可约性
而神经网络的学习呢
本质上是在寻找数据中的规律和捷径
也就是计算可约性
它通过压缩信息啊
找到可以被泛化的模式
这就注定了ChatGPT本质是懒惰的
它极其擅长处理
那些计算上很浅的任务
而那些需要硬算
而那些需要硬算
或者是计算上很深的任务
他天生就无能为力了
写一篇文章
写一首诗
总结一段话
这些任务对人类看起来很复杂
但是从计算深度来言
它是很浅的因
为它依赖的
是我们大脑已经存在的
高度优化的语言和思维模式的复用
而计算一个复杂的物理模拟
或者证明一道数学难题
这些任务从计算的深度上讲
是深的这
恰恰就是
传统计算机和Wolfram Alpha擅长的领域
所以呢
我们不必担心ChatGPT会取代所有
它只是在计算浅的领域
达到了超人的水平
但是在计算深的世界里
它依然需要传统的计算工具的帮助啊
啊这其实也解释了
为什么ChatGPT 的下棋能力
和Alphago差了这么大啊
就是因为
它并不是一个适合计算深的
这种应用场景
现在呢让我们回到那个终极问题啊
ChatGPT算不算在思考呢
它有智能吗
这个Wolfram的观点是啊
它和人脑的工作方式既有相似之处
又有本质不同
相似之处在于啊
我们人类的说话时候
很多时候
也并不是每时每刻
都在进行严密的逻辑推理
我们大脑也是一个巨大的联想网络
当我们听到一个词呢
会激活无数相关的概念
然后我们大脑
会以一种
我们自己也无法完全说清的方式
选择最合适的词语串成句子
这个过程呢
ChatGPT基于概率选下一个词
有异曲同工之妙
但本质不同在于啊
ChatGPT的神经网络
在生成每一个词的过程中呢
是纯粹的前馈网络
也就是说呢
数据从输入层单向的
一次性的流向输出层
中间没有任何迭代和循环
它每生成一个新词
都要把前面所有的内容再重新看一遍
这是一个固定的机械的流程
而我们的大脑
以及绝大多数的计算机程序
都充满了循环和反馈
我们思维可以在几个概念中来回打转
深化迭代
直到找到一个满意的答案
这种在内部反复咀嚼信息的能力啊
是ChatGPT目前不具备的
当然不知道这个他那时候的GPT
和现在这个可以呃
deep thinking的GPT是不是一样了
因为他毕竟是在23年写的这本书
所以呢与其说他在思考
不如说他在进行一种我们前所未见的
极其高效的模式匹配和联想生成
但ChatGPT的出现啊
也迫使我们反思
我们所谓的思考和智能到底是什么
或许
我们过去对于智能的定义太狭隘了
我们一直以为
智能必须伴随着自我意识
逻辑推理
但ChatGPT认为啊
仅仅通过对于海量数据模式的模仿
也能够涌现出令人惊叹的
我们称之为智能的行为
这或许意味着
智能的形态比我们想象的要丰富得多
好了讲了这么多硬核的原理啊
那我们最后得落到实处
理解了ChatGPT的本质
我们到底应该怎样更好的应用它呢
让它成为我们的超能力呢
我给大家总
结了3条黄金法则或者是驯兽指南啊
第一条呢
就是把它当成领航员
而不是自动驾驶仪
记住啊它的核心是语言接龙
你给它的开头
也就是提示词啊
决定了整个航行的方向
所以呢你的问题要尽可能清晰具体
提供充足的上下文
你甚至可以给他举几个例子
叫做few short learning啊
告诉他你想要的风格和格式
他不是你的下属
他是你的副驾驶
你需要清晰地告诉他目的地在哪
第二呢
永远不要完全信任他的事实
尤其在关键领域
和对于任何需要精确数据计算
和逻辑推理的地方
要保持警惕
对它生成内容
特别是数字日期代码
科学公式
可以当做草稿或者假设
学会使用专业的数据
比如说搜索引擎
Wolfram Alpha或者是计算器去交叉验证
把当成一个创意无限
但有点迷糊的实习生
他的工作成果你必须亲自审核
第三呢要善用他的发散性
激发你的创造力
他最大价值呢
不在于提供最标准答案
而是在对于他能够基于概率
探索语言的无数种可能性
当你写作卡壳
策划没有思路
编程想不出新方法的时候
他可能给你生成10个不同版本
你会发现他总有一些意想不到的角度
给你带来启发
不要把它当成答
案的终点
要把它当成思考的起点
掌握这三条呢
你就不是简单在使用ChatGPT
而是在和这个强大的概率引擎啊共舞
真正的驾驭它的力量
好了今天这期节目呢
我们跟着这个Stephen Wolfram啊
一起拆解了ChatGPT
我们从他最简单的文字接龙游戏开始
啊一起探索了神经网络
梯度下降，词嵌入，注意力机制
甚至聊到了计算不可约性
和智能的本质
最后我想说啊
ChatGPT的出现
它的意义可能远超一个好用的工具
它更像是一个科学的伟大发现
就像是牛顿当年用几条简单的定律啊
统一了天上行星和地上的苹果
就像达尔文
用自然选择
解释了生命世界的万千形态
今天呢ChatGPT
用一个基于概率的下一个词
预测的模型啊
向我们证明了人类最复杂的智慧结晶
语言和思维
背后可能也隐藏着简洁而深刻的规律
像一面镜子
让我们第一次有机会啊
从一个非人类纯粹数学的视角
去审视我们自己
它迫使我们去思考什么是意义
什么是创造
什么是智能
未来最强大的力量
将不再是单纯的人类智慧
也不是单纯的机器智慧
而是两者结合
是ChatGPT
这样富有联想和创造力的文科大脑
和Wolfram Alpha这样严谨
精确的理科大脑的完美融合啊
这不仅会改变我们的工作方式
也会开启一个全新的探索知识边界的
大航海时代啊
当然了作为这个自家的产品啊
Stephen Wolfram
在书里头
是不遗余力地给Wolfram Alpha做推广
Wolfram Alpha我其实也用过
他在做这种数学相关的
或者是这个物理相关的问题
其实是非常强的
但是呢他也有自己的局限性
就是说呃
他的输入啊
是比较rigid的
就是你可能需要想办法呃
要学他的一些呃
就是这个叫Wolfram language啊
有这样一个语言
然后呢要跟他进行更好的交互
之前那个ChatGPT 4o刚出来的时候
它还有一个就是跟Wolfram连接的这种
一个定制的这个GPT
但现在
好像不知道有没有继续调用它的
这个API两边肯定是有什么合作的
好的
那我们今天的这个节目就录到这里了
非常感谢大家观看
我们下期节目再见