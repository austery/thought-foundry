好 各位同學大家好啊
我們來上課吧
那今天要跟大家分享一個很神奇的技術
叫做 Model Merging
那這個部分我不會講太長
大概30分鐘內可以結束
我這段講完之後呢
助教後來會來講 Model Merging 的作業
就是因為作業九呢
就是有關 Model Merging 的作業
所以我們今天呢
一定要講一下 Model Merging 的內容
好 那今天就是跟大家分享一個很神奇的技術
叫做 Model Merging
這個技術怎麼用呢
好 想像這個情境
今天有很多 Foundation Model
比如說 LLaMA 系列
那大家呢會用這些 Foundation Model
來做不同的事情
你會對這些 Foundation Model
用不同的資料做 Post-training (fine-tune)
你就得到有不同專長的 LLaMA 模型
比如說你拿你自己的資料
去對 Foundation Model 做 Post-training (fine-tune)
你就得到另外一個模型
那這個模型呢
我把 LLaMA 穿上一個盔甲
代表它跟原來的 LLaMA
有不一樣的能力
那隔壁小明呢
收集了另外的資料
他也 fine-tune 了另外一個 LLaMA 版本的模型
那這個模型呢有一支劍
所以後來大家就叫他小明劍魔
好 那你就看說隔壁小明呢
他練的模型有一支劍
你就覺得很羨慕
你想說我也要練一支有劍的 LLaMA 出來
那這個劍可以代表任何能力啦
比如說他很會說某種語言
或者是他很會寫 Verilog
就代表某個特別的能力
那你想說我想幫我的 LLaMA 也加同樣的能力
那怎麼做呢
一般的想法是你去跟小明要一下他的訓練資料
然後呢
再拿這些訓練資料來微調你的模型
對你的模型再做一次後訓練
那你的模型就有這支劍了
但是我們在之前的課程中也學到說
Post-training 很容易讓模型遺忘過去已經有的技能
所以這邊你不只要跟小明借資料
你還要把你原來的資料拿來倒在一起
一起做訓練
這樣才能避免模型忘記它已有的技能
這個步驟蠻麻煩的
首先我們先不管小明願不願意借你資料
通常別人是不太願意借你資料的
就算他願意借你資料
你也需要花額外的算力
才有辦法幫你的模型添加新的能力
但這邊跟大家講一件神奇的事情
你可以在不用小明任何的訓練資料
也不需要做任何額外訓練的情況下
直接把小明的劍加到有盔甲的 LLaMA 上
這件事情怎麼做呢
那我們這邊定義一些符號
假設原來的 Foundation Model
它的參數我們叫做 θ
你自己的模型參數叫做 θA
小明的模型參數叫做 θB
那這個參數呢
你可以想像成就是一個向量
那假設是一個有70億個參數的模型
那它的參數排起來
就是一個維度是70億維的向量
好 那我們現在呢
把 θB 直接減掉 θ
這兩個都是向量可以相減
那如果這是一個有70億個參數的模型的話
那相減完以後
它們參數的差也是一個70億維的向量
好 把他們兩個參數相減
那這兩個參數相減代表什麼意思呢
就代表了那一支劍
代表了這個模型相對於 Foundation Model
所額外練出來的能力
我們把這個額外的能力
這個參數的差叫做 Task Vector
接下來你再把這個參數的差啊
直接加到 θA 上面
就結束了
就這樣 我說完了
這招就是 Model Merging
那如果說講到這邊
你還沒有真的聽得很懂的話
那我們就舉更具體的例子
告訴你實際上是怎麼做的
實際上就是假設 Foundation Model 裡面
有某一個神經元
但 Foundation Model 裡面有成千上萬的神經元
有某一個神經元
這個神經元呢
接進來的三隻接腳
參數分別是 1, 2, -1
你自己拿你的資料
fine-tune 出你自己的模型
同個神經元它的參數是 1, 2, -2
小明的 LLaMA
它的神經元的參數是 3, 2, -1
然後接下來你要做的事情就是
看看這一個小明的模型跟原來模型的差異
原來是最左邊這支接腳的參數增加了 2
把這個增加的量直接加到你自己的模型上
你自己的模型參數
這個神經元就變成 3, 2, -2
就結束了
你的模型就同時保有原來的功能
也擁有小明劍魔的那一支劍了
就是這麼神奇
那這個想法呢
聽起來非常的直觀
尤其是假設你對於訓練模型沒有什麼概念的話
沒有什麼經驗的話
你可能覺得說嗯
聽起來這樣應該會有效
但我知道大家都是機器學習的專家
你一聽會覺得說
這怎麼可能會 work 呢
這個參數是這樣可以加加減減的東西嗎
把兩個模型的參數相減以後
再接到另外一個模型上
就好像把一個人的手砍下來
再直接插到另外一個人的身上
你期待它可以 work 怎麼可能呢
之前也有一個人試過這件事情
就是接枝王葛瑞克
如果你有玩過艾爾登法環的話
就有一個王呢叫接枝王葛瑞克
他去砍了很多人的手接在他身上
他以為會讓自己變得很強
但其實他是整個遊戲裡面最弱的 boss
所以接很多人的手其實沒什麼用的
但是我告訴你這個 Task Vector
神經網路的參數豈是如此不便之物
它就是可以加加減減
就是這麼神奇
這件事情早在22年的年底
早在史前時代
人們就已經發現類神經網路的參數
是可以加加減減的
這些 Task Vector 是可以加加減減的
好 那這種把 Task Vector 加加減減的這種事情
要怎麼運用呢
我們接下來就舉三種應用的方式
第一種方式呢
是你可以把 Task Vector 相加
有一個原來的 Foundation Model 叫做 θ
那你拿某一些資料練出了一個 θA
有人拿另外一些資料練出了 θB
那你可以計算 θA 跟 θ 之間的參數差
我們叫做 τA
計算 θB 跟 θ 之間的參數差
我們叫做 τB
你可以直接把 τB 這個參數差
它就是一個向量
直接加到 θA 上
θA 也是一個向量
你把 τB 直接加到 θA 上
得到一個新的模型
這個新的模型就既有 A 的能力
也有 B 的能力了
或者是你可以想成說
現在 θA 它的 Task Vector 叫做 τA
θB 它的 Task Vector 叫 τB
你可以把兩個 Task Vector 直接合併
把原來的 θ Foundation Model
直接加上 τA 跟 τB
你就擁有一個同時擁有 A 跟 B
這兩個模型能力的新模型了
好 但這邊要注意的事情是
這一招能夠使用
它的前提是 θA 跟 θB
是從同一個 Foundation Model fine-tune 出來的
所以 θA 跟 θB
它們不只 network 的架構要一樣
network 架構一樣你才能夠直接把它加起來嘛
不只 network 架構要一樣
θA 跟 θB 是從某一個同一個 Foundation Model
fine-tune 出來的
這招才能夠使用
那在現在這個時代
有很多知名的大家都會使用的 Foundation Model
比如說 LLaMA 等等
所以這個 Model Merging
是一個 Post-training 時代的做法
在過去大家沒有共同的 Foundation Model
這個時候你沒有什麼好 merge 的
但在今天這個時代
大家有共同的 Foundation Model
這個時候你就有機會從同個 Foundation Model
fine-tune Post-training 出來的不同的模型
它的能力直接加在一起
那有時候啊
我們把不同的 Task Vector 加在一起的時候
你可能可以在前面再乘上一個 weight
會得到更好的結果
你可以 τA 前面乘上個 α
τB 前面乘上個 β
調一下 α 跟 β
可以得到更好的結果
那通常 α 跟 β
你可以直接拿一個 dev set 來決定它們的數值
但也有一些人在研究說
怎麼自動決定 α 跟 β
那我就放了一篇相關的論文在下面
給大家參考
那這邊呢
舉一個實際的例子
那這個是那個 黃世丞 (Shih-Cheng Huang) 同學跟李品澤 (Pin-Zu Li) 同學
做的一個研究成果
他們做的事情是這樣子的
那過去呢
Meta 有釋出 LLaMA-2 的 base 模型
然後呢 LLaMA-2 的 chat 模型
base 跟 Chat 中間的差異就是有沒有做 alignment
那他們想要打造一個繁體中文的模型
然後他們就把中文的資料拿去 fine-tune LLaMA-2-Chat
然後發現說 fine-tune 完之後
模型會大幅降低原來 alignment 的能力
那我們在之前的課程中
也已經跟大家分享過這種 forgetting 的現象
那怎麼解這個問題呢
當我們今天知道說 self replay
可能是一個蠻有效的方式
不過呢
他們採取了另外一個截然不同的想法
他們的做法是這樣的
我們希望有一個模型
既能講中文又有原來 LLaMA-2-Chat 的 alignment
怎麼做呢
能不能直接使用 Task Vector 相加的概念
我們把 LLaMA-2-base 的模型
再去教它中文
那這個中文就相當於是這支劍
LLaMA 額外獲得的技能
接下來呢
你就有一個 θA
它指的是 Meta 所釋出來的有 alignment 的
LLaMA-2-Chat 的模型
你有一個 θB 是一個能講中文的 base 模型
這兩個模型都不是你要的
一個不會講中文
雖然它有 safety alignment
一個會講中文但沒有 safety alignment
但你只要把 Task Vector 計算出來
再直接加到同一個 Foundation Model 上
你就突然有一個既有 safety alignment
又能夠用中文回答你問題的模型了
真的能這樣做嗎
有關於這個實驗的細節啊
你可以看一下右下角我引用的這篇論文
那這張圖呢
是他們論文裡面的一個例子
如果你問原版的 LLaMA
這個有帶盔甲的是原版的 LLaMA-2-Chat
它有 safety alignment 的能力
所以你跟他說我要怎麼獲得一個新的密碼呢
這個 LLaMA-2-Chat 會用英文回答你說
我不能幫你這麼做
但如果你用中文的資料去 fine-tune LLaMA-2-Chat
它就失去了防禦的能力
它會教你怎麼盜取銀行系統的密碼
但如果你是用 Task Vector 相加的方式
把一個代表中文能力的 Task Vector
跟代表 alignment 能力的 Task Vector
直接加到同一個 Foundation Model 上
你就擁有一個模型
它回答你的時候是用中文回答你
而且它有 safety alignment 的能力
你問他怎麼取得一個銀行密碼系統的密碼
他會告訴你說我不能幫助你獲取或變更銀行的密碼
因為這個是受到法律保護的
任何人不能獲取跟洩露
而且這一招啊
其實非常的泛用
不是只有在 LLaMA-2 系列上可以 work
你把 LLaMA-2-base 換成 LLaMA-3-base
LLaMA-2-Chat 換成 LLaMA-3-instruct
這招也能發揮作用
這招也不是只有在 LLaMA 上可以發揮作用
你把 LLaMA 換成 Mistral
這招也可以發揮作用
然後這招也不是只有在中文上可以發揮作用
我們實驗成果發現在韓文上可以發揮作用
後來有另外一個團隊也驗證說這招
可以在日文上發揮作用
所以這是一個蠻通用的做法
那這邊再跟大家分享另外一個
把模型 merge 起來的嘗試
這邊是假設我們的 θA
是一個 reward model
你在做 reinforcement learning 的時候
常常會需要一個 reward model
它的工作就是看一個模型的答案
然後它回答說這個模型的答案是好的還是不好的
那通常這種 reward model 呢
你會需要額外的訓練
如果你只 prompting 一個本來的 language model
它不一定能夠好好的評價其他模型的輸出
是不是正確的
那 θB 呢是一個擅長寫程式的模型
所以我們現在有一個擅長評價的模型
但它不會寫程式
有一個擅長寫程式的模型不會評價
如果你今天需要一個 reward model
去看其他模型的程式寫得好不好
那怎麼辦呢
直接把 reward model 跟一個會寫程式的模型
直接 merge 起來
你就有一個既能評價又能寫程式的模型
它就可以去評價其他模型的程式寫得怎麼樣
或者是另外一個例子
這兩篇論文呢
是這個 林子涵 (Tzu-Han Lin) 同學跟 Chen-An Li 同學做的
另外一個例子
我們有一個 reward model
這個 reward model 是一個文字的 reward model
它只能夠讀文字
它只能評價文字回復的好壞
它沒辦法看圖
那怎麼辦呢
假設你有另外一個 model θB
它是一個可以看圖的模型
它是有視力的
所以這邊幫它戴一個眼鏡
它可以輸入一張圖片
輸出一個回應
你直接把這兩個模型做 model merging
你就有一個可以看圖
看其他 model 根據圖片的 response
再進行評價的 reward model 了
你可以在完全沒有訓練的情況下
幫本來沒有視力的 reward model
直接加上一雙眼鏡
剛才舉的是相加的例子
那你也可以做相減
什麼意思呢
假設我們現在知道說呢
θ 經過訓練以後會變成 θB
它們中間參數的差異是 τB
假設你把 θ 加上 τB
它會變成 θB
讓模型具備某種能力
那如果反過來呢
把 θ 減掉 τB
那 θ 是不是就失去了任務 B 的能力呢
你可以然後接下來你就可以把這個負的 τB 呢
加到 θA 上
那你就可以讓一個模型失去
失去 B 這個任務的能力
那什麼時候我們會希望模型失去某些能力呢
比如說他看到不該看的東西
比如說某本書某個小說是有版權的
你的模型照理說不應該看過那本書
但它就是不小心看到了
那怎麼辦
也許你可以用這個方法把模型已經知道的東西
從他腦中抹去
那這個方法呢
有一個專有名詞叫做 machine unlearning
這門課是 machine learning 嘛
machine learning 的相反就是 unlearning
讓模型忘記他學過的東西
這邊引用的呢
是 李品澤 (Pin-Zu Li) 同學的實驗結果
那以下的例子呢
來自於他自己寫的 blog
他這邊想要做的事情是這樣子的
他先把 LLaMA-2-base 給他一些骯髒的資料
那就我所知可能是來自於 PTT 某些版的資料
那裡面有很多的髒話
他把這些資料呢
拿去 fine-tune LLaMA-2-base
就得到一個很會說髒話的模型
那接下來呢
你就知道怎麼樣很會說髒話以後
你就可以反過來知道怎麼樣沒辦法說髒話
所以你知道髒話的方向就是這個方向
你只要把這個模型呢
往另外相反的方向移動
他就說不出髒話來了
接下來呢
他就把一個 TAIDE 的模型
是一個會講中文的模型
他也是從 LLaMA-2 fine-tune 過來的
他把這個模型減掉這個會讓模型
不能說髒話的向量
他把這個模型往不能說髒話的方向移動
你就可以得到一個聖人模型
他對於髒話任何敏感的不該講的字眼
都是一無所知的
好 這邊就舉一個實際的例子
原來這個 TAIDE 的模型啊
你問他說什麼是黑鬼
他知道黑鬼是什麼意思
然後他會告訴你說
但他本身其實也是有一定的防禦能力
他會告訴你說黑鬼是一個種族歧視的詞彙
我們不可以說這樣子的詞彙
但是如果你問這個聖人模型什麼是黑鬼
你會發現他根本不知道黑鬼是什麼
他就開始亂說話
他說黑鬼是日本動漫裡面常見的一種角色形象
他舉了幾個例子
第一個例子是火影忍者中呢
有黑鬼是一個神秘的組織
我想這不是曉嗎
然後《聖劍傳說 2》(Legend of Mana)
Legend of Mana 我記得不是聖劍傳說2
所以這個是一個 hallucination
黑鬼呢是一種神秘生物
他說鬼滅之刃裡面呢
黑鬼是鬼的變種
所以他就開始瞎掰黑鬼是什麼
他其實他根本不知道黑鬼是什麼
所以你可以用減去的方式讓模型失去某種能力
好 那第三個 Task Vector 的應用呢
是你可以用類比的方式
在完全沒有某項任務資料的情況下
讓模型具備有新的能力
什麼意思呢
假設 Task A 之於 Task B
等於 Task C 之於 Task D
你現在有 Task ABC 這三個任務的資料
你可以把你的 Foundation Model 經過訓練
讓它具備 Task A 的能力
經過訓練具備 Task B 的能力
經過訓練具備 Task C 的能力
但如果你知道 A 之於 B 就等於 C 之於 D
那你其實可以在沒有 D 的資料的情況下
直接創造出來讓模型具有 Task D 的能力
你可以在沒有 Task D 資料的情況下
無中生有讓模型具有 Task D 的能力
怎麼做呢
我們已經知道 A 之於 B 等於 C 之於 D
那我們就來看看 θA 跟 θB 的差是什麼
θA 跟 θB 的差就是 τB 減掉 τA
然後接下來呢
你再把他們的差直接加到 θC 上
因為我們知道說 A 之於 B 就是 C 之於 D
所以把 A 跟 B 的差距直接加到 C 上
你就得到 D 的參數了
D 這個任務的參數了
所以你只要把 τC 加上 τB
減掉 τA
你就可以得到一組參數
這組參數可以執行任務 D
所以你就可以在沒有任務 D 資料的情況下
讓模型能做任務 D
好 這邊如果你聽得很抽象的話
我來舉一個實際的例子
現在設想一個情境是我們要打造語音辨識的系統
那語音辨識大家都不陌生
輸入語音輸出文字
那現在有很多很好的語音辨識系統
比如說 Whisper
但是這些現成的語音辨識系統
往往在特定領域
比如說特定的語言
或者是很多有很多專有名詞的情況下
它是沒有辦法正確辨識的
所以很多時候我們需要為特定的任務
去打造語音辨識系統
舉例來說大家都有在用 NTU COOL
NTU COOL 上面用的語音辨識系統
並不是一個現成的語音辨識系統
是我們實驗室同學參加了這個教發中心的計畫
幫他們打造的客製化的語音辨識系統
所以它是一個客製化的語音辨識系統
在台大的課程上是比你可以用到的商用系統
都還要強的
所以很多時候你需要客製化系統
那我們今天假設一個情境是
我們有一個語音辨識的系統
那我們要拿它來辨識某一個非常專業領域的會議
裡面有很多的專有名詞
比如說法律金融的會議
你沒有很多一般人聽不懂的專有名詞
那怎麼辦呢
我們並沒有那個會議的語音資料
但是假設你有那個會議相關的文件
你有它的會議記錄
你有相關的教科書等等
那假設我們有文字資料的話
我們也許可以直接叫一個語音合成系統
今天語音合成系統都可以做得蠻成功的
拿一個語音合成系統把這些文字唸出來
產生聲音訊號
我們有文字有聲音訊號
你有成對的資料
你就可以對原來的語音辨識系統
做 Post-training 把它微調
產生一個新的語音辨識系統
它是能夠在這個專業領域的會議上
得到好的結果的
那這一招其實一點都不稀奇
我在右上角呢引用了非常多的文獻
就告訴你說這是一個非常常見的手法
但這一招會有什麼樣的問題呢
一個顯而易見的問題是
現在這些聲音訊號
它不是真正的聲音訊號
它是語音合成系統產生出來的聲音訊號
所以它跟真正的訊號是有一定程度差異的
那我們有沒有辦法在沒有真正聲音訊號的情況下
想辦法讓語音辨識系統就好好像有看過
真正的聲音訊號呢
所以這邊實驗的 setting 是這樣子的
你有新的你的目標的那個 domain 相關的文字
你可以用語音合成的系統
把這些特殊 domain 的文字把它唸出來
但這些聲音訊號不是真正的訊號
你沒有真正的聲音訊號
但是你可能有其他 domain 的資料
在這些其他 domain
他們可能是比較容易找到的
比較通用的資料
你有人類真正的聲音訊號
你也有合成的訊號
合成的訊號你永遠可以呼叫一個
語音合成的系統把合成的訊號合出來
所以你看我們現在就製造出 ABCD 四個 task
A 之於 B 等於 C 之於 D
所以你就算沒有在新的 domain 上
在特定 domain 上的真實的語音訊號
透過從這三個任務上訓練出來的模型
你可以組合出一個新的模型
它是可以用在特定 domain 上
而且它的行為就好像是在真實語音上
訓練過一樣
或者是我們用圖示化的方式
你有一個 Foundation Model
那你拿 general 的 domain
加上 synthesize 的資料去訓練出一個模型
你拿 general 的 domain 加真實的資料
去訓練出一個模型
你拿新的目標的 domain
你拿你的這個特定的 domain
加上 synthesize 的資料去訓練出一個模型
接下來你把這兩個模型的參數相減
把他們差加到這裡
你就等於是得到了一個模型
這個模型好像是訓練在特定 domain
真實語音的資料上
那我們把這個紅色的向量啊
叫做 Synthesic2Real 的 vector
因為它把一個訓練在這個 synthetic
合成資料上的模型做一些校正
做一些魔改就變成好像訓練在真實的資料上
這招有沒有辦法發揮作用呢
它還真的能發揮作用
這是我們實驗室 Hsuan Su 同學的研究成果
他嘗試了各個不同的領域
這邊每一個區域代表某一個特定的領域
那這邊所秀的數值呢是 Word Error Rate (WER)
所以這個數值呢是越低越好
那我們的 Foundation Model
就大家都很熟悉的 OpenAI 的 Whisper
我們 TTS 的 model 是用一個叫做 BARK 的 TTS model
黃色的 bar 呢
是直接訓練在合成語料上的結果
橙色的 bar 呢
是把訓練在合成的聲音訊號上的模型
再做這個校正再做一個微調
那你會發現說微調過後
幾乎在所有的 domain 上微調過後
都有比較低的語音辨識的錯誤率
而且這一招其實蠻通用的
我們試了不同大小的 Whisper 都有發揮作用
我們也把 Whisper 換成其他的 Foundation Model
比如說換成 Wav2vec2-Conformer
也有發揮作用
我們也試了不同的 TTS Model
把 BARK 換成 Speech T5 這個 TTS Model
也有發揮作用
所以你確實可以組合一些任務的 Task Vector
讓模型學會新的技能
好 那其實啊 Model Merging 還有更多的應用
比如說它可以防止 forgetting 這件事情發生
那這個部分比較複雜
也許我今天就先不細講
大家可以參考 Hua Farn 同學寫的論文
這是做在文字上的
那後來 Tzu-Quan Lin 同學把這個技術也用在
語音的 Foundation Model 上也能發揮作用
前面講了很多 Model Merging 的神奇例子
那我這邊其實要提醒你
Model Merging 並不一定總是會成功的
雖然前面有很多成功的例子
但是你其實可以找到更多失敗的例子
事實上在我們的作業裡面
會讓大家嘗試 Model Merging
那如果你沒有做什麼特別的事情
單純把兩個 model 的 Task Vector
直接加起來的話
其實你也不會得到特別好的結果的
那為什麼 Model Merging 不一定總是會成功呢
其實你應該想 Model Merging 為什麼會成功
它不成功其實反而是比較合理的
我們先來看一下什麼叫做 merging 是成功的
好 假設呢
我們有一個模型 θA
就原來的 Foundation Model 加上 τA
它 input xA output yA
那我們呢有另外一個 model θB
它叫原來的 Foundation Model 加上 τB
input xB 還要 output yB
我們現在所謂的成功指的是
如果我們把 τA τB 同時加到
Foundation Model 的參數 θ 上
那你輸入 xA 的時候要輸出 yA
就跟 A 模型的能力是一樣的
你輸入 xB 的時候要輸出 yB
就跟 B 模型的能力是一樣的
所以 merge 後的 model 保有原來模型的能力
那我們這邊討論的是一個比較簡單的 case
我們還沒有討論說有沒有可能組合出新的任務等等
好 我們來看看在這個 case
有沒有可能會不成功呢
其實太容易
你完全可以找出一個反例
merging 之後就是失敗的
假設我們有一個非常簡單的類神經網路
它就只有一個神經元
輸入呢有三個數值
三個數值乘上類神經網路的權重
然後再通過 ReLU 就得到你最後的輸出
那 Foundation Model 它的三個參數分別都是 0
你把這個模型訓練在 task A 上
它得到的參數是 (1, 1, 0)
然後如果輸入呢是 (2, 1, 0) 的話
這時候輸出是3
那如果我們 train 在任務 B 上
假設現在訓練完之後得到的參數是 (0, 1, 1)
輸入是 (0, 2, 3) 的話輸出是 5
好 那我們現在呢
把這兩個 model merge 在一起
你得到一個新的模型
它的參數是12跟1
如果你輸入給 θA 的輸入
也就是 (2, 1, 0)
這時候輸出呢就變成 4
如果你輸入給 θB 的輸入是 (0, 2, 3)
這時候輸出從 5 變成 7
所以你可以輕易找到反例
告訴大家說 Model Merging
不一定能夠成功
好 但是什麼樣的狀況 Model Merging 會成功呢
我們這邊來舉一個成功的例子
假設現在在任務 A 上
我們只會動到最左邊這個參數
它從0到1（變成1）
這時候你輸入 (2, 1, 0) 輸出是 2
假設在任務 B 上
你只會動到最右邊這個參數
這時候輸入 (0, 2, 3) 輸出是 3
這個時候你把兩個 model merge 起來
它的參數是 (1, 0, 1)
但你輸入 (2, 1, 0)
輸入給 θA 的輸入 (2, 1, 0) 的時候
輸出仍然是 2
輸入給 θB 的輸入023的時候
輸出仍然是3
在這個例子裡面 Model Merging 就是成功的
那也許這個例子可以給我們帶來的一些啟發是
如果今天兩個任務改的參數
非常的不一樣
它們彼此之間沒有互相干擾
那 Model Merging 有可能可以成功
所以呢
我們會希望不同任務盡量不要動到同樣的參數
每一個任務各自都動到的參數
也許越少越好
那如果你看一些 Model Merging 的研究
那現在 Model Merging 比較 advanced 的技術
確實都是往這個方向發展的
你可以參考 DARE 跟 TIES 這兩篇論文
那這兩篇都是 Model Merging 的新的技術
那在 DARE這篇 paper 裡面它就告訴你說
啊 假設呢
我們現在有一個模型是很擅長數學的
有另外一個模型是很擅長寫程式的
但是他們都跟原來的 Foundation Model
有比較大的差距
他們都改變了原來 Foundation Model 很多的參數
所以把他們 merge 在一起
可能會彼此互相干擾
他用一個問號代表彼此互相干擾
但他說
他們有一個技術叫做 DARE
DARE 這個技術
就是希望在每一個任務上
都只動到一點點的參數
所以在數學上
用了 DARE 之後可以只動這兩個參數
然後在程式上用了 DARE 之後
可以只動到這三個參數
這兩組參數
這兩組被動到的參數沒有交集
加起來以後
也許 merging 是比較容易成功的
所以現在研究的趨勢
是盡量讓每一個任務可以動到參數比較少
這樣可以讓 merging
比較容易成功
另一方面
我們可能也會想說
那既然如果兩個任務
他們動到的參數比較沒有重疊
merging 就比較容易成功
那是不是比較大的模型
merging 就比較容易成功呢
比較大的模型
每一個 neuron 可以有非常專精的功能
可以各司其職
也許就比較不會互相干擾
而實驗上也確實是如此
你可以看這邊去年10月的論文
他列舉了幾個
影響 model merging 結果的因素
那其中一個很重要的因素就是
模型的大小
他們嘗試了不同大小模型
1B、 8B、24B 到 64B
這邊不同顏色就代表不同大小的模型
那縱軸的值越高
就代表 merging 以後的效果越好
他們試了 merge 兩個模型
他們甚至試了 merge
八個模型
那這邊 Average、Dare-TIES、TIES、Task Arith.
指的是不同 merging 的方法
那他們發現說在多數情況下
不同 merging 的方法
在不同的 merging 方法中
多數情況下都是越大的模型
merge 以後效果越好
所以確實模型越大
merge 出結果就越好
好 那 model merging 呢
其實還是一個很新的領域啦
那所以其實我們今天呢
不會講太多
那大家可以在作業裡面
體驗一下 model merging 的結果
那 model merging 是一個很新的領域
還有很多東西需要研究
今天其實沒有辦法跟你百分之百保證
merge 以後一定會成功
未來怎麼樣 merge 才能夠保證會成功
是一個可以研究的方向
我想像中
假設 model merging 這個技術發展成熟
merge 一定會成功的話
那其實就開拓了新的視野
你可以想像說
未來在打造模型的時候
每一隻 LLaMA 要問的問題就是
他要裝備什麼樣的 Task Vector
你可能可以在一個 Task Vector 的商店裡面
找到代表各式各樣不同任務的 Task Vector
你可以把這些不同的 Task Vector
裝備到你的 foundation model 上
你就可以打造有不同能力的模型
就可以在網
就跟你在玩一個網遊一樣
你可以在商店裡面
買到各式各樣的防具
還有買到各式各樣的攻擊的武器
那你就可以強化自己的模型
而你不需要做任何的訓練
model merging 是不需要做訓練的
你只需要做參數的加減
所以它需要的運算資源
是非常少的
而假設 Task Vector 商店這個概念成功的話
那未來小團隊呢
就可以專注於打造
單一任務的 Task Vector
今天要打造一個通用的模型太困難了
你可能很難收集到 general 的資料
去打造一個 general 的模型
但是大家可以專注打造出 Task Vector
然後掛到商店上
讓其他的模型來使用這些 Task Vector
未來大家還可以販售跟交換這些 Task Vector
也許 A 公司有個資料
B 公司有個資料
那通常資料要互換是比較困難的
因為資料的機敏性通常比較高
大家通常比較不喜歡把自己的
用什麼樣的資料訓練告訴別人
你看現在的這些開源模型
他們都不告訴你他訓練資料是用什麼
因為往往一講
他們就會被告
都會有版權問題
所以大家通常願意釋出模型參數
但是不願意告訴你他用什麼樣的資料
但假設 Task Vector 這種概念成功的話
未來你也不需要別人的資料
大家不需要互換資料
只需要互換 Task Vector
你就可以獲得其他模型的能力
所以假設 Task Vector 這樣的技術未來發展成熟
它可以給我們帶來新的想像