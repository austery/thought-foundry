在大模型激战的当下
究竟谁更强
是OpenAI的GPT还是Anthropic的Claude
是谷歌的Gemini
还是中国的DeepSeek
Claude is the best
GPT is the best
Google is the best
Grok is the best
停 大家都别吵了
当AI模型排行榜开始被各种刷分作弊之后呢
谁家大模型最牛这个问题啊
就变得非常的主观
直到一家线上排行榜的诞生
它叫LMArena
或许你还记得不久之前
爆火的谷歌最新文生图模型Nano Banana
对 就是把我家娃
变成小猩猩的那个AI应用哈
它其实最早以神秘代号出现
并且引发破圈式关注的地方
就是LMArena
哦对了 最近网友们又发现
谷歌又故伎重施
传闻已久的Gemini 3.0
被发现已经出现在了LMArena上
根据网友们的测试反馈
Gemini 3.0 Pro的代号应该就是lithiumflow
而Gemini 3.0 Flash是orionmist
据说能够读表
能作曲和演奏
能力再一次全方位的飞升
你可能会在对战中随机地遇到它们
如果遇到的话
记得一定要抓住机会好好地拷问拷问
不难看出
在正式发布新模型之前
让它们在LMArena上面跑一跑
似乎就已经成为了谷歌的惯例操作
而实际上
各家模型
其实早就已经把LMArena
当做了常规赛场
用来测试普通用户最真实的反馈
除了Google
OpenAI、Anthropic、Llama
DeepSeek、混元、千问
几乎所有的头部模型都在LMArena上打擂台
在文字、视觉、搜索、文生图、文生视频等
不同的AI大模型细分领域
LMArena上面每天都有上千场的实时对战
由普通用户来匿名投票选出哪一方的回答更好
最近以来
很多AI研究者都纷纷发声
认为大模型竞赛的下半场
最重要的事情之一
就是重新地思考模型评估
因为当技术创新趋于饱和
真正拉开差距的
可能将不再是谁的参数更多
推理更快
而是谁能更准确地衡量
理解模型的智能边界
而如LMArena这样的平台
正是当前这一领域中最前沿
也是最具实践意义的探索之一
它就像一座永不打烊的AI竞技场
真实的用户、真实的投票
并且排名动态更新
用一场场真实的人机交锋
试图重新定义我们衡量模型的方式
那么在大模型评测上
传统的Benchmark基准测试
究竟存在什么样的问题
是已经过时了吗
LMArena的竞技场模式
为什么会被视为一种新的标准
它的技术机制、公平性和商业化
隐藏着什么样的挑战
而下一代的大模型评测
又可能会走向哪里呢
Hello大家好
我是陈茜
欢迎收看硅谷101
那这期视频
我们来聊一聊LMArena
那么在LMArena之前
AI大模型是怎么被评估的呢
方法其实非常的传统
研究者们通常会准备一组固定的题库
比如说MMLU、BIG-Bench、HellaSwag等等
那么这些名字普通人看起来很陌生
但是在AI学术界几乎是家喻户晓的
这些题库涵盖学科、语言
常识推理等多个维度
通过让不同模型作答
再根据答对率或者得分
来对模型进行比较
那比如说MMLU
全称是“Massive Multitask Language Understanding”
它涵盖了从高中到博士级别的
57个知识领域
包括历史、医学、法律、数学、哲学等等
模型既需要回答
像“神经网络中的梯度消失问题如何解决”
这样的技术问题
也需要回答
“美国宪法第十四修正案的核心内容是什么”
这样的社会科学问题
学科跨度很大
而BIG-Bench更偏向推理和创造力
比如说让模型解释冷笑话
继续写诗或者完成逻辑填空
HellaSwag则专门
用来测试模型对日常情境的理解能力
比如说“一个人正在打开冰箱”
“接下来最可能会发生什么”等等
这些Benchmark 在过去二十年
几乎主导了整个AI的研究领域
它们的优点
显而易见
就是标准统一、结果可复现
那学术论文
只要能够在相关公开数据集上刷新分数
就意味着性能更强
而AI的上半场
也正是在这种比成绩的节奏下
高速发展起来的
但是这些早期的Benchmark是静态的
多以单轮问答、选择题形式为主
题目结构简单
评测维度明确
便于统一打分和横向的比较
然而当模型的能力越来越强
训练数据越来越庞大的时候
这些Benchmark的局限
就开始显现了
首先是所谓的题库泄露
很多测试题
早就出现在模型的训练语料当中
于是一个模型在这些测试上得分再高
也不代表它真的理解了问题
能说明它记住了答案
其次Benchmark永远测不出
模型在真实交互中的表现
它更像是一场封闭的考试
而不是一次开放的对话
华盛顿大学助理教授
英伟达首席研究科学家
同时也是LMArena早期框架搭建的参与者
朱邦华在跟我们硅谷101的采访中就表示
正是因为传统的静态Benchmark
所存在的过拟合、数据污染等问题
才催生出了Arena这种新的模型测评方式
Static Benchmark（静态基准）
比如说Math500或者MMLU
有几个问题
一个问题就是说大家非常容易overfit（过拟合）
因为比如说一共就几百个问题
而且我都有ground truth（标准答案）
如果训练在ground truth（标准答案）上
没有人detect（测试）出来
虽然有一些所谓的
contamination detection method（污染检测方式）
但其实这个是比较难
真的100%就是做到detection（检测）
所以这种Static Benchmark（静态基准）
当时大家就说
首先一是数量很少
二是大家可能coverage（覆盖面）不太够
它可能就有最简单的math（数学）
然后最简单的一些knowledge（知识）
然后最简单的一些coding（代码生成）
像这个HumanEval这种
然后当时的Benchmark数量少
同时这个就coverage（覆盖面）也不太好的情况下
这Arena
就作为一个非常独特的Benchmark出现了
因为它每一个问题都是unique（独特的）
它可能是世界各地的人问
 然后它可能是俄罗斯或者越南的人
在问你这样一个问题
然后同时
他问的问题都是随时随地
当时当地去想的一个问题
所以这个事就很难去
在当时overfit（过拟合）
尤其是在当时
大家都没有什么Arena data（数据）的时候
那么LMArena究竟是如何运作的呢
2023年5月
LMArena的雏形
诞生于加州大学伯克利的LMSYS团队
核心成员包括Wei-Lin Chiang、Lianmin Zheng等人
当时啊他们刚刚发布了开源模型Vicuna
而斯坦福大学在此之前
也推出了另外一个类似的
叫Alpaca
因为这两个模型
都是从ChatGPT的数据中蒸馏得到的
于是LMSYS的团队就想知道
从性能和表现上来看
究竟谁更胜一筹呢
当时并没有合适的测评方法
能够回答这个问题
那LMSYS团队呢
是尝试了两种方法
第一种是尝试让GPT-3.5作为评委
对不同模型生成的答案打0-10分
这种方法后来演化成为了MT-Bench
也就是Model-Test Benchmark
另外一种方式呢
是采用人类比较（Pairwise Comparison）
即随机挑选两个模型
针对同一个问题分别生成答案
再让人类去评审
选择哪个更好
最终第二种方式被证明更可靠
并由此诞生了Arena的核心机制
基于此
他们首先搭建了一个实验性网站
Chatbot Arena
也就是今天的LMArena的前身
在传统的基准测试里
模型是在预设题库中答题
而在Chatbot Arena上
它们则要上场打擂台
当用户输入一个问题之后
系统会随机分配两个模型
比如说GPT-4和Claude
但是用户并不知道自己面对的是谁
两边的模型几乎同时生成回答
用户只需要投票左边好还是右边好
等投票完成之后
系统才会揭示它们的真实身份
这个过程啊被称作“匿名对战”
投票结束之后
系统会基于Bradley–Terry模型
实现Elo式评分机制
分数会根据胜负实时变化
从而形成一个动态的排行榜
Elo排名机制最早来自于国际象棋
每个模型都会有一个初始分数
每次赢一场就会涨分
输一场就会扣分
随着对战次数的增加
分数呢会逐渐收敛
最终形成一个动态的模型排行榜
这种机智的妙处在于
它让测评变成了一场“真实世界的动态实验”
而不再是一次性的闭卷考试
除此之外
LMArena不仅仅是“让模型打架”
它背后还有一个独特的“人机协同评估框架”
这个框架的逻辑是
用人类的投票去捕捉“真实偏好”
再通过算法去保证“统计公平”
平台会自动平衡模型的出场频率
任务类型和样本分类
防止某个模型因为曝光量大而被高估
换句话说
它让评测既开放又可控
更重要的是
Chatbot Arena的所有数据和算法
都是开源的
任何人都可以复现或者分析结果
那作为LMArena早期搭建的核心参与者
朱邦华就告诉我们
LMArena的技术本身并不是新算法
更多的呢
是经典统计方法的工程化实现
它的创新点不在于模型本身
而在于系统架构与调度机制
一方面就是说
它虽然这个Bradley–Terry Model
本身没有什么太多技术上的新的东西
但是你怎么选model这个事
可能是something new
就是也不是特别new
但就说是大家摸索出来的
就说因为你现在假设有100个model
然后我想了解它们互相之间
到底哪一个更好
那这个事你其实需要一些active learning（主动学习）
就说你需要假设我选了一些model出来
我已经知道它们大概怎么样了
那我下面选model的时候
我应该选一些我更不确定的模型ranking（排名）
然后去做这个comparison（比较）
然后怎么去dynamically（动态）选
去做比较optimal（最佳的）这种
这个active model selection（主动模型选择）
这个是我们当时探索的比较多的一个事儿
然后当时就是做了一些这种
相关的这个series studies（系列研究）
然后同时就大家又一起去做了一些
这种experimental research（实验性研究）
就大家去比较一下不同的这种
这个怎么去调这些参数
能让这个model能更好的去被选出来
这是一个（LMArena成功的）因素
当然就是说我个人觉得
就说这种项目
它可能本身
还有一些时机和运气的成分在里面
因为当时确实是所有人都需要
很好的一个evaluation benchmark（评估基准）
然后这个时候呢
human preference（人类偏好）又完全没有
被saturated（饱和）
就是大家那个时候的human preference
确实比较真实的反映这个model本身的能力
所以在那个时候
我觉得Arena作为
这个行业的gold benchmark（黄金基准）
是非常make sense（合理）
也是就是比较顺理成章的
LMArena这种“匿名对战 + 动态评分”的方式
被认为是从静态Benchmark
向动态评测的一次跃迁
它不再追求一个最终的分数
而是让评测
变成一场持续发生的“真实世界实验”
它就像是一个实时运行的AI智能观测站
那么在这里啊
模型的优劣不再由研究者定义
而是由成千上万的用户选择
来共同决定
2023年12月底
前特斯拉AI总监
OpenAI的早期成员Andrej Karpathy
在X（推特）上是发了一条关于LMArena的推文
称目前他只信任两个LLM的测评方式
Chatbot Arena 和 r/LocalLlama
给Chatbot Arena社区中
是收获到了第一批“流量”
2023年年底到2024年年初
 随着GPT-4、Claude、Gemini、Mistral
DeepSeek等模型的陆续接入Chatbot Arena
平台的访问量迅速增长
研究者、开发者、甚至普通用户
都在这里观察模型的“真实表现”
到了2024年年底
平台的功能和评测任务开始扩展
除了语言模型的对话任务
团队还逐渐涉及到了大模型的“细分赛道”
陆续上线了专注代码生成的Code Arena
专注搜索评估的Search Arena
专注多模态图像理解的Image Arena等子平台
为了体现测评范围的扩展
平台也在2025年1月
从Chatbot Arena更名为LMArena
即Large Model Arena
几个月前
谷歌的Nano Banana的爆火
也是让更多普通用户关注到了LMArena
至此
LMArena从一个研究者间的小众项目
彻底成为了AI圈乃至公众视野中的
“大模型竞技舞台”
LMArena的火爆呢
让它几乎成为了
大模型评测的非官方标准
但是和所有新的实验一样
随着光环越来越大
它也受到了越来越多的质疑
首先是公平性问题
在LMArena的匿名对战机制中
用户的投票结果
直接决定模型的Elo排名
然而这种人类评判的方式并不总是中立的
不同的语言背景、文化偏好
甚至是个人使用习惯
都会影响投票结果
一些研究发现
用户更倾向于选择“语气自然”
“回答冗长”的模型
而不一定是逻辑最严谨
信息最准确的那一个
这意味着
模型可能因为“讨人喜欢”而获胜
而非真的更聪明
2025年年初
来自Cohere、斯坦福大学
以及多家研究机构的团队
联合发布了一篇研究论文
系统地分析了LMArena的投票机制与数据分析
研究就指出
Arena的结果
与传统benchmark分数之间并非强相关
而且存在“话题偏差”与“地区偏差”
也就是说
不同类型的问题
或不同用户群体的投票
可能会显著改变模型的排名
此外 还有“游戏化”和“过拟合”的问题
当LMArena的排名被广泛引用
甚至被媒体视为模型能力的“权威榜单”时
一些公司开始为“上榜”
专门优化模型的回答风格
比如说更积极地去使用模糊语气
提升数字密度
或者在提示工程上精细调校
以希望“赢得投票”
Cohere的那篇研究论文就明确地指出
大型供应商在获取用户数据方面拥有明显的优势
通过API接口
他们能够收集到大量的
用户与模型交互的数据
包括提示和偏好设置
然而这些数据并没有被公平地共享
62.8%的所有数据
是流向了特定的模型提供商
比如说Google和OpenAI的模型
就分别获得了Arena上
大约19.1%和20.2%的全部用户对战数据
而其他83个开源模型的总数据
占比仅为29.7%
这使得专用模型供应商
能够利用更多的数据进行优化
甚至可能针对LMArena平台进行专门优化
导致过度拟合特定指标
从而提升排名
那一个典型的例子
就是Meta的刷榜事件
那今年2025年4月
Meta在LMArena上
提交的Llama 4 Maverick模型版本
它的表现是超越了GPT-4o和Claude
是跃居了整个排行榜的第二
但随着Llama 4大模型开源版的上线
开发者们发现
它的真实效果的表现其实并不好
因此质疑Meta疑似
给LMArena提供了经过专门针对投票机制的
优化的“专供版”模型
是导致了Llama 4的口碑急转直下
那舆论爆发之后
LMArena官方
更新了排行榜政策
要求厂商披露模型版本与配置
以确保未来评估的公平性和可重复性
并把公开的Hugging Face版本的
Llama 4 Maverick
加入了排行榜进行重新的评估
但事件依然在当时引发了
业内关于评测公平性的激烈讨论
而除了系统和技术上的挑战
LMArena的商业化
也让它的中立性受到质疑
2025年5月
LMArena背后的团队
正式注册公司Arena Intelligence Inc.
并且宣布完成了1亿美元的种子轮融资
投资方就包括了a16z、UC Investments
和Lightspeed等
这也意味着
LMArena正式从一个开源研究项目
转变为了具备商业化运营能力的企业
公司化之后
平台就可能会开始探索数据分析
定制化评测和企业级报告等商业服务
那么这一转变也开始让业界担忧
当资本介入
用户需求与市场压力叠加的时候
LMArena是否还能保持最初“开放”与“中立”
它的角色是否会从“裁判”
变成“利益相关方”呢
在LMArena之后
大模型评测似乎
就进入到了一个新的拐点
它解决了过去Benchmark静态、封闭的问题
但是却也暴露出新的矛盾
那就是当评测数据用户偏好
甚至投票机制
都可能会成为商业竞争的一部分
我们该如何去界定“公平”这两个字呢
那么究竟什么样的模型评估方式
才是当前所需要的呢
实际上LMArena的出现
并不意味着传统的Benchmark已经过时
在它之外
静态的Benchmark依然在持续地演化
最近几年来
基于传统的Benchmark
研究者们是陆续推出了难度更高的版本
比如说MMLU Pro、BIG-Bench-Hard等等
此外呢一些全新的
聚焦于细分领域的Benchmark
也在不断地被创造出来
比如说数学与逻辑领域的AIME 2025
编程领域的SWE-Bench
多智能体领域的AgentBench等等
这些新的Benchmark不再只是“考知识”
而是在模拟模型在真实世界中的工作方式
从过去单一的考试题集
演化为了一个庞大而多层次的体系
有的评推理
有的测代码
有的考记忆与交互
那与此同时
评测也在进一步走向真实世界
比如说最近
一家名为Alpha Arena的新平台
就引发了大量的关注
它由创业公司Nof1推出
在首轮活动中啊
平台选取了Deepseek、Genimi
GPT、Claude、Gork和千问等六大模型
在真实的加密货币交易市场中
进行对战
它给了每个模型相同的资金和Prompt
让它们独立决策和交易
最终以实际收益和策略稳定性
作为评测依据
结果是：DeepSeek竟然赢了
不愧是量化基金母公司
下面做出来的AI模型
虽然这个对战更多是“噱头”为主
大语言模型去预测股市
现在还是非常不靠谱的
但是Alpha Arena的这种“实战式测评”
是再一次跳出了传统的题库和问答框架
让模型在动态对抗的环境中被检验
被视为是继LMArena之后
再一次尝试让AI在开放世界中
接受考验的实验
不过Alpha Arena更偏向特定任务领域的真实验证
其结果也更难复现和量化
实际上这些Arena出现的意义
也并非是要取代静态的Benchmark
而是为这个体系提供一面镜子
试图把静态测试中
难以衡量的人类偏好和语义细节
重新引入到评测系统当中
那也就是说呢
未来的模型评估不再是静态Benchmark
和Arena之间的二选一
而更可能是一种融合式的评测框架
那么静态Benchmark负责提供可复现
可量化的标准
而Arena负责提供动态开放
面向真实交互的验证
那么两者结合
进而构成衡量智能的完整坐标系
那么在这样的一个评估体系中
目前最重要
也最具挑战的部分是什么呢
那朱邦华就认为啊
随着大模型能力提升
原有测试集“太简单”的问题愈发突出
Arena的自动难度过滤提出了阶段性解决方案
但是真正的方向
是由人类专家与强化学习环境
共同推动的高难度数据建设
大家之前比如说包括Arena在内
大家会complain（抱怨）一个问题
就大家觉得
可能比较简单的问题太多了
但随着model变得越来越强
它这个简单的定义也会变得越来越大
就可能越来越多的prompt
都都属于是easy prompt
然后所以当时Arena出了一个
这个Hard Filter Version（难度过滤版）
它会根据这个prompt的
他直接问model说这个哪一个更难
然后去filter（筛选）一些hard prompt出来
那现在随着
thinking model（具备显式思维链的模型）的引入
然后也随着
比如说大家接著用RL训练各种各样的model
原来难的prompt现在也不是特别难了
所以这个时候
可能就更需要human expert（人类专家）
去标各种各样更难的数据
作为Benchmark
那这也是我们作为model developer（模型开发者）
我们现在在做的一个事儿
如果你看Grok 4的话
它其实做了什么
他们可能做Pretraining-scale RL（预训练规模强化学习）
其实这个里面就是说
一方面你的RL data就得非常多
另一方面
如果你RL data都是用非常简单的data
那其实对model不会有任何提升
所以你需要大量的非常困难的data
包括我现在在英伟达做的一个事
也是就是说想做一个
RL Environment Hub（强化学习环境平台）
让大家去创造更多更难的这种环境进来
能让更多人用RL来去训练它
那朱邦华就谈到说
大模型评估的未来呢
不会是线性的一个改进
会是螺旋式的共演
一边是不断变强的模型
另外一边呢
是不断变难的评测
那模型的突破迫使评测体系升级
而新的评测呢
又反过来定义了模型能力的边界
而高质量的数据
成为了连接这两者的中枢
RL和这个Evaluation（评测）
或者是Training（训练）和Evaluation（评测）
就像是双螺旋的感觉
就说你一方面training不断的让model变强
然后你evaluation
就会有更难的Benchmark出来
说你这个现在model还不行
你就会提升你的training（训练）
比如说environment（环境）的难度
或者是你找更好的model architecture（模型架构）
更好的算法
然后把model capability（模型能力）再提升
你可能就需要更难的evaluation（评测）
然后现在似乎就已经到了
就说大家这两步
都得慢慢
不断的找human expert（人类专家）来去标的程度
现在大部分
其实很多这种
RL Environment Labeling（强化学习环境标注）的工作
他们都会去找
比如说PhD（博士）
他们会找这个顶尖的Math PhD
然后顶尖CS PhD去标
这个math coding data（数学代码数据）
然后这个data卖的也非常贵
这一条可能就是几千刀的水平
所以现在这个就是大家慢慢的都偏向
就是说找这种expert data（专家数据）
能够真的让GPT-5
或者是Top model（顶尖模型）都没有办法回答的data
或者回答错的这个data
然后通过这种来去构造更难的
Training data（训练数据）
和Evaluation data（评估数据）
除了数据至关重要之外呢
朱邦华还认为说
研究者不仅是要“造benchmark”
更要学会“选benchmark”
如何在成百上千个数据集中
进行筛选、组合和聚合
建立一个兼顾统计有效性
与人类偏好的聚合框架
是接下来几年重要的工作方向
正如OpenAI的研究员姚顺雨
在他的博客《The Second Half》当中写道
AI的上半场是关于“如何训练模型”
而下半场则是“如何定义与衡量智能”
如今评测不再只是AI模型性能的终点
而正在成为AI向前发展的核心科学
那究竟什么样的评估方法才是最优的呢
或许我们目前还无法下定论
但能够预见的是
这将是一场持续进行的实验
我们需要在成百上千个benchmark当中
找到那些真正有价值的任务
然后在类似于LMArena这样的竞技场当中
去捕捉人类偏好的信号
最后再将它们结合
成为一个动态、开放、可信的智能测量体系
也许在那一天
我们就不再需要问“哪个模型最强”
而是真正去探索“智能究竟是什么”
那欢迎大家给我们留言
你们觉得LMArena的方式
是不是衡量模型的最好标准呢
你们的留言点赞和转发
是支持我们硅谷101
做好深度科技和商业内容的最佳动力
我是陈茜
那我们就下期视频再见啦 bye