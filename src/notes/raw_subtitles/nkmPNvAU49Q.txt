AI
 is
 not
 synonymous
 with
 language


models.
 AI
 is
 being
 developed
 with


pretty
 similar
 architectures
 for
 a
 wide


range
 of
 different
 modalities
 and


there's
 a
 lot
 more
 data
 there.
 Feedback


is
 starting
 to
 come
 from
 reality.
 Maybe


we're
 running
 out
 of
 problems
 we've


already
 solved.
 When
 we
 start
 to
 give


the
 next
 generation
 of
 the
 model
 these


power
 tools
 and
 they
 start
 to
 solve


previously
 unsolved
 engineering


problems,
 I
 think
 you
 start
 to
 have


something
 that
 looks
 kind
 of
 like
 super


intelligence.


Nathan,
 I'm
 stoked
 to
 stoked
 to
 have
 you


on
 the
 AC
 and
 Z
 podcast
 for
 the
 first


time.
 Obviously,
 I've
 been
 podcast


partners
 for
 a
 long
 time
 with
 with
 you


leading
 Cognitive
 Revolution.
 Welcome.


>> It's
 great
 to
 be
 here.
 Thank
 you.
 So,
 we


we
 were
 talking
 about
 Cal
 Newport's


podcast
 appearance
 on
 lost
 debates
 and


and
 we
 we
 thought
 it
 was
 a
 good


opportunity
 to
 just
 have
 this
 broad


conversation
 and
 really
 entertain
 this


sort
 of
 question
 of
 is
 AI
 slowing
 down?


Um,
 so
 why
 don't
 you
 sort
 of
 steelman


some
 of
 the
 arguments
 that
 you've
 heard


on
 that
 side
 either
 from
 him
 or
 more


broadly
 and
 then
 we
 could
 sort
 of
 have


this
 broader
 conversation.


>> Yeah,
 I
 mean
 I
 think
 for
 one
 thing
 um


it's
 really
 important
 to
 separate
 a


couple
 different
 questions
 I
 think
 with


respect
 to
 AI.
 One
 would
 be
 is
 it
 good


for
 us
 you
 know
 right
 now
 even
 and
 is
 it


going
 to
 be
 good
 for
 us
 in
 the
 big


picture?
 And
 then
 I
 think
 that
 is
 a
 very


distinct
 question
 from
 are
 the


capabilities
 that
 we're
 seeing


continuing
 to
 advance
 and
 you
 know
 at
 a


pretty
 healthy
 clip.
 Um
 so
 I
 actually


found
 a
 lot
 of
 agreement
 with
 the
 uh
 Cal


Newport
 podcast
 that
 you
 shared
 with
 me


when
 it
 comes
 to
 some
 of
 the
 worries


about
 the
 impact
 that
 AI
 might
 be
 having


even
 already
 on
 people.
 you
 know,
 he
 he


goes
 over
 looks
 over
 students
 shoulders


and
 watches
 how
 they're
 working
 and


finds
 that
 basically
 he
 thinks
 that
 they


are
 using
 AI
 to
 be
 lazy,
 which
 is,
 you


know,
 no
 big
 revelation.
 I
 think
 a
 lot


of
 teachers
 would
 tell
 you
 that.
 Puts


that
 in


>> Yeah.
 Puts
 that
 in
 maybe
 um
 more
 dressed


up
 terms
 that
 that
 people
 are
 not
 even


necessarily
 moving
 faster.
 uh
 but


they're
 able
 to
 reduce
 the
 strain
 that


the
 work
 that
 they're
 doing
 places
 on


their
 own
 brains
 by
 kind
 of
 trying
 to


get
 AI
 to
 do
 it
 and
 you
 know
 if
 that


continues
 and
 I
 think
 you
 know
 he's
 been


um
 I
 think
 a
 very
 valuable
 commenter
 on


the
 impact
 of
 social
 media
 certainly
 I


think
 we
 all
 should
 be
 mindful
 of
 how
 is


my
 attention
 span
 uh
 you
 know
 evolving


over
 time
 and
 am
 I
 getting


weak
 or
 you
 know,
 uh,
 averse
 to
 hard


work.
 Uh,
 those
 are
 not
 good
 trends
 if


they
 are
 showing
 up
 in
 oneself.
 So,
 I


think
 he's
 really
 right
 to
 watch
 out
 for


that
 sort
 of
 stuff.
 And
 then,
 as
 we've


covered
 it,
 you
 know,
 many
 conversations


in
 the
 past,
 I've
 got
 a
 lot
 of
 questions


about
 what
 the
 ultimate
 impact
 of
 AI
 is


going
 to
 be.
 And
 I
 think
 he
 he
 probably


does
 too.
 But
 then
 when
 it
 comes
 to
 it's


a
 strange
 move
 from
 my
 perspective
 to
 go


from
 you
 know
 there's
 all
 these
 sort
 of


problems
 today
 and
 maybe
 in
 the
 big


picture
 to
 but
 don't
 worry
 it's


flatlining
 like
 kind
 of
 worry
 but
 don't


worry
 because
 it's
 not
 really
 going


anywhere
 further
 than
 this
 or
 it's
 you


know
 scaling
 has
 kind
 of
 petered
 out
 or


you
 know
 we're
 not
 going
 to
 get
 better


AI
 than
 we
 have
 right
 now.
 um
 or
 even


maybe
 the
 most
 easily
 refutable
 claim


from
 my
 perspective
 is
 GPT5
 wasn't
 that


much
 better
 than
 GPT4.
 And
 that
 I
 think


is
 where
 I
 really
 was
 like,
 whoa,
 wait
 a


second.
 You
 know,
 I
 was
 with
 you
 on
 a


lot
 of
 things.
 And
 some
 of
 the
 behaviors


that
 he
 observes
 in
 the
 students,
 I


would
 cop
 to
 having
 exhibited
 myself.


You
 know,
 when
 I'm
 trying
 to
 code


something
 these
 days,
 a
 lot
 of
 times
 I'm


like,
 "Oh
 man,
 can't
 the
 AI
 just
 figure


it
 out?"
 You
 know,
 I
 really
 don't
 want


to
 have
 to
 sit
 here
 and
 read
 this
 code


and
 figure
 out
 what's
 going
 on.
 It's
 not


even
 about
 typing
 the
 code
 anymore,
 you


know,
 I'm
 way
 too
 lazy
 for
 that.
 But


it's
 even
 about
 like
 figuring
 out
 how


the
 code
 is
 working
 just
 just
 case
 you


just
 make
 it
 work.
 Uh
 try
 again,
 you


know,
 and
 just
 try
 again
 and
 I'll
 I
 do


find
 myself
 at
 times
 falling
 into
 those


traps.
 But
 I
 would
 say
 big
 part
 of the


reason
 I
 can
 fall
 into
 those
 traps
 is


because
 the
 AIs
 are
 getting
 better
 and


better
 and
 increasingly
 it's
 not
 crazy


for
 me
 to
 think
 that
 they
 might
 be
 able


to
 figure
 it
 out.
 So
 that
 that's
 my
 kind


of
 first
 um
 slice
 at
 the
 takes
 that
 I'm


hearing.
 there's
 almost
 like
 a
 two
 by


two
 matrix
 maybe
 that
 one
 could
 draw
 up


where
 it's
 like
 do
 you
 think
 AI
 is
 good


or
 bad
 you
 know
 now
 and
 in
 the
 future


and
 do
 you
 think
 it's
 like
 not
 a
 big


deal
 or
 a
 big
 deal
 and
 I'm
 I
 think
 it's


both
 on
 the
 good
 and
 bad
 side
 I


definitely
 think
 it's
 a
 big
 deal
 the
 the


thing
 that
 I
 struggle
 to
 understand
 the


most
 is
 the
 people
 who
 kind
 of
 don't
 see


the
 big
 deal
 that
 it
 seems
 pretty


obvious
 to
 me
 and
 the
 you
 know


especially
 when
 it
 comes
 again
 to
 the


the
 leap
 from
 GBD4
 GB5
 um
 maybe
 one


reason
 that
 that's
 happened
 a
 little
 bit


is
 that
 there
 were
 just
 a
 lot
 more


releases
 between
 GPT4
 and
 5.
 So
 what


people
 are
 comparing
 to
 is,


you
 know,
 something
 that
 just
 came
 out
 a


few
 months
 ago,
 like
 03,
 right?
 That


only
 came
 out
 a
 few
 months
 before
 GBT5.


Whereas
 with
 GBT4,


it
 was,
 you
 know,
 shortly
 after
 ChatGpt


and
 it
 was
 all
 kind
 of
 this
 moment
 of


like,
 whoa,
 this
 thing
 is
 like
 exploding


onto
 the
 scene.
 A
 lot
 of
 people
 were


seeing
 it
 for
 the
 first
 time.
 And
 if
 you


look
 back
 to
 GPT3,
 you
 know,
 there's
 a


huge
 leap.
 I
 would
 contend
 that
 the
 leap


is
 similar
 from
 GPT4
 to
 5.
 These
 things


are
 hard
 to
 score.
 there's
 no,
 you
 know,


single
 number
 that
 you
 could
 put
 on
 it.


Well,
 there's
 loss.
 Uh,
 but
 of
 course,


one
 of the
 big
 challenges
 is
 that
 like


what
 exactly
 does
 a
 a
 loss
 number


translate
 into
 in
 terms
 of
 capabilities.


So,
 you
 know,
 it's
 very
 hard
 to
 to


describe


what
 exactly
 has
 changed.
 Um,
 but
 we


could
 go
 through
 some
 of
 the
 dimensions


of
 change
 if
 you
 want
 to
 and
 um,
 you


know,
 enumerate
 some
 of
 the
 things
 that


I
 think
 people
 maybe
 are
 starting
 to
 or


have
 come
 to
 take
 for
 granted
 and
 kind


of
 forget
 like
 that
 GPT4
 didn't
 have
 a


lot
 of
 the
 things
 that
 now,
 you
 know,


were
 sort
 of


>> expected
 in
 the
 GPD5
 release
 because


we'd
 seen
 them
 in
 40
 and
 01
 and
 03
 and


all
 those,
 you
 know,
 things
 sort
 of,
 you


know,
 maybe
 boiled
 the
 frog
 a
 little
 bit


when
 it
 comes
 to
 how
 much
 progress


people
 perceived
 in
 this
 last
 release.


Well,
 yeah,
 a
 couple
 reactions.
 So,
 one


is
 and
 even
 to
 complicate
 your
 two
 by


two
 even
 even
 further
 in
 the
 sense
 of,


you
 know,
 is
 it
 bad
 now
 versus
 is
 it
 bad


later?
 Like
 Cal
 is
 not
 really,
 you
 know,


who
 we
 both
 admire
 by
 the
 way
 a
 lot.


Cal's
 a
 great
 guy
 and
 a
 valuable


contributor
 to
 the
 the
 thought
 space,


but
 he's
 not
 as
 concerned
 about
 sort
 of


this
 sort
 of
 future
 AI
 concerns
 that
 um


you
 know,
 sort
 of
 the
 AI
 safety
 folks


and
 um
 many
 others
 are
 are
 are
 concerned


about.
 he's
 more
 concerned
 about,
 you


know,
 what
 it
 means
 to
 life
 for,
 you


know,
 cognitive
 performance
 and
 and


development
 now
 in
 the
 same
 way
 that


he's
 worried
 about,
 you
 know,
 social


media's
 impact.
 And,
 you
 know,
 you
 you


think
 that's
 a
 uh,
 you
 know,
 a
 concern,


but
 not
 nowhere
 near
 as
 big
 a
 concern
 as


as
 what
 to
 what
 to
 expect
 in
 the
 future.


And
 and
 then
 also
 he
 he
 presents
 sort
 of


this
 theory
 of
 why
 we
 shouldn't
 worry


about
 the
 future
 because
 it's
 slowing


down.
 And
 why
 don't
 we
 just
 share
 what


we
 how
 we
 interpreted
 kind
 of
 his


history
 which
 as
 I
 interpreted
 it
 was


this
 idea
 of
 like
 hey
 we
 figured
 out
 the


simplistic
 version
 is
 we
 figured
 out


this
 this
 way
 such
 that
 if
 you
 throw
 a


bunch
 of
 data
 um
 into
 the
 model
 it
 gets


better
 and
 sort
 of
 order
 of
 magnitude


and
 so
 the
 difference
 between
 GB22
 and


GPD3
 and
 then
 GBD3
 and
 GBD4
 um
 but
 then


that
 sort
 of
 you
 know
 was
 significant


the
 difference
 but
 then
 it
 achieved
 sort


of
 a
 uh
 diminishing
 returns


significantly
 and
 where
 we're
 not
 seeing


it
 in
 GBD5
 five
 and
 thus
 we
 don't
 have


to
 worry
 anymore.
 How
 would
 you
 edit
 the


characterization
 of
 his
 view
 of
 sort
 of


the
 the
 history
 and
 then
 we
 can
 get
 into


the
 differences
 between
 four
 and
 five


the
 scaling
 law
 idea
 which
 is
 you
 know


it's
 definitely
 worth
 agreeing
 taking
 a


moment
 to
 to
 note
 that
 it
 is
 not
 a
 law


of
 nature.
 You
 know
 we
 do
 not
 have
 a


principled
 reason
 to
 believe
 that


scaling
 is
 some
 law
 that
 will
 go


indefinitely.
 All
 we
 really
 know
 is
 that


it
 has
 held
 through
 quite
 a
 few
 orders


of
 magnitude
 so
 far.


I
 think
 that


it's
 really
 not
 clear
 yet
 to
 me
 whether


or
 not
 the
 scaling
 laws
 have


petered
 out
 or
 whether
 we
 have
 just


found
 a
 steeper
 gradient
 of
 improvement


that
 is
 giving
 us
 better
 ROI
 on
 another


uh
 front
 that
 we
 can
 push
 on.
 So
 they


did
 train
 a
 a
 much
 bigger
 model
 which


was
 GBT4.5


and
 that
 did
 get
 released
 and
 there
 are


a
 number
 of
 interesting
 you
 know
 of


course
 there's
 a
 million
 benchmarks


whatever
 the
 one
 that
 I
 zero
 in
 on
 the


most
 in
 terms
 of
 understanding
 how
 GBT


4.5
 relates
 to
 both
 03
 and
 GBT
 5
 and


OpenAI
 obviously
 famously
 terrible
 at


naming
 we
 can
 all
 agree
 on
 that
 I
 think


a
 decent
 amount
 of
 this
 confusion
 and


sort
 of
 disagreement
 actually
 does
 stem


from
 unsuccessful
 naming
 decisions.
 4.5


on
 this
 one
 benchmark
 called
 simple
 QA


which
 is
 really
 just
 a
 super
 longtail


trivia
 benchmark.
 It
 it
 really
 just


measures
 do
 you
 know
 a
 ton
 of
 esoteric


facts
 and
 they're
 not
 things
 you
 can


really
 reason
 about.
 You
 either
 just


have
 to
 know
 or
 don't
 know
 these


particular
 facts.
 The
 03
 class
 of
 models


got
 about
 a
 50%
 on
 that
 benchmark
 and


GPT4.5
 popped
 up
 to
 like
 65%.


So
 in
 other
 words,
 it
 basically
 of
 the


things
 that
 were
 not
 known
 to
 the


previous
 generation
 of
 models,
 it
 picked


up
 a
 third
 of
 them.
 Now
 there's


obviously
 still
 two-thirds
 more
 to
 go,


but
 I
 would
 say
 that's
 a
 pretty


significant
 leap,
 right?
 These
 are
 super


longtail
 questions.
 I
 would
 say
 most


people
 would
 get
 like
 close
 to
 a
 zero.


you
 know,
 you'd
 be
 like
 the
 person


sitting
 there
 at
 the
 trivia
 night
 who


like
 maybe
 gets
 one
 a
 night
 is
 kind
 of


what
 I
 would
 expect
 most
 people
 to
 do
 on


simple
 QA.
 And
 that,
 you
 know,
 checks


out,
 right?
 Like
 obviously
 the
 models


know
 a
 lot
 more
 than
 we
 do
 in
 terms
 of


facts
 and
 just
 general,
 you
 know,


information
 about
 the
 world.
 So
 at
 a


minimum,
 you
 can
 say
 that
 GPT4.5


knows
 a
 lot
 more.
 You
 know,
 a
 bigger


model
 is
 able
 to
 absorb
 a
 lot
 more


facts.
 Qualitatively,
 people
 also
 said


in
 some
 ways
 maybe
 it's
 better
 for


creative
 writing.


you
 know,
 it
 was
 never
 really
 trained


with
 the
 same


uh
 power
 of
 post-
 training
 that
 GBD5
 has


had.
 And
 so,
 we
 don't
 really
 have
 an


apples
 to
 apples
 comparison,
 but
 people


still
 did
 still
 find
 some
 utility
 in
 it.


I
 think
 maybe
 the
 the
 way
 to
 understand


why
 they've
 taken
 that
 offline
 and
 gone


all
 in
 on
 GBD5
 is
 just
 that
 that
 model's


really
 big.
 It's
 expensive
 to
 run.
 The


price
 was
 like
 way
 higher.
 It
 was
 a
 full


order
 of
 magnitude
 plus
 higher
 than
 GPT5


is.
 and
 it's
 maybe
 just
 not
 worth
 it
 for


them
 to
 consume
 all
 the
 compute
 that
 it


would
 take
 to
 serve
 that
 and
 maybe
 they


just
 find
 that
 people
 are
 happy
 enough


with
 the
 somewhat
 smaller
 models
 for


now.
 I
 don't
 think
 that
 means
 that
 we


will
 never
 see
 a
 bigger
 GPT
 4.5
 model


with
 all
 that
 reasoning
 ability
 and
 I
 I


would
 expect
 that
 that
 would
 deliver


more
 value
 especially
 if
 you're
 really


going
 out
 and
 trying
 to
 do
 esoteric


stuff
 that's
 you
 know
 pushing
 the


frontier
 of
 science
 or
 what
 have
 you.


Um,
 but
 in
 the
 meantime,
 the
 current


models
 are
 really
 smart
 and
 you
 can
 also


feed
 them
 a
 lot
 of
 context.
 That's
 one


of
 the
 big
 things
 that
 has
 improved
 so


much
 over
 the
 last
 generation.
 When
 GPT4


came
 out,
 at least
 the
 version
 that
 we


had
 as
 public
 users
 was
 only
 8,000


tokens
 of
 context,
 which
 is
 like
 15,
 you


know,
 pages
 of
 of
 text.
 So,
 you
 were


limited.
 You
 couldn't
 even
 put
 in
 like
 a


couple
 papers.
 You
 would
 be
 overflowing


the
 context.
 And
 this
 is
 where
 prompt


engineering
 initially
 kind
 of
 became
 a


thing
 was
 like,
 man,
 I've
 really
 only


got
 such
 a
 little
 bit
 of
 information


that
 I
 can
 provide.
 I
 got
 to
 be
 really


careful
 about
 what
 information
 to


provide,
 uh,
 lest
 I
 overflow
 the
 thing


and
 it
 just
 can't
 handle
 it.
 There
 were


also,
 as
 context
 windows
 got
 extended,


there
 were
 also
 versions
 of
 models
 where


they
 could
 nominally
 accept
 a
 lot
 more,


but
 they
 couldn't
 really
 functionally


use
 them.
 you
 know,
 they
 sort
 of
 could


it
 could
 could
 fit
 them,
 you
 know,
 at


the
 API
 call
 level,
 but
 they
 the
 models


would
 lose
 recall
 or
 they
 they'd
 sort
 of


unravel
 as
 they
 got
 into
 longer
 and


longer
 context.
 Now
 you
 have
 obviously


much
 longer
 context
 and
 the
 command
 of


it
 is
 really
 really
 good.
 So
 you
 can


take
 dozens
 of
 papers
 on
 the
 longest


context
 windows
 with
 Gemini
 and
 it
 will


not
 only
 accept
 them,
 but
 it
 will
 do


pretty
 intensive
 reasoning
 over
 them
 and


with
 really
 high
 fidelity
 to
 those


inputs.
 So
 that
 skill
 I
 think
 does
 kind


of
 substitute
 for
 the
 model
 knowing


facts
 itself.
 You
 could
 say
 geez
 we


let's
 try
 to
 train
 all
 these
 facts
 into


the
 model.
 We're
 going
 to
 need
 you
 know


a
 trillion
 or
 who
 knows
 five
 trillion


however
 many
 trillion
 parameters
 to
 fit


all
 these
 super
 longtail
 facts.
 Or
 you


could
 say,
 well,
 a
 smaller
 thing
 that's


really
 good
 at
 working
 over
 provided


context
 can


if
 people
 take
 the
 time
 or
 you
 know
 go


to
 the
 trouble
 of
 providing
 the


necessary
 information,
 I
 can
 kind
 of


access
 the
 same
 facts
 that
 way.
 So
 you


have
 a
 kind
 of
 do
 I
 want
 to
 push
 on
 this


size
 and
 do
 I
 want
 to
 bake
 everything


into
 the
 model
 or
 do
 I
 want
 to
 just
 try


to
 get
 as
 much
 performance
 out
 of
 a


smaller
 tighter
 model
 that
 I
 have?
 And


it
 seems
 like
 they've
 gone
 that
 way.
 And


I
 I
 think
 basically
 just
 because
 they're


seeing
 faster
 progress
 on
 that
 gradient,


you
 know,
 in
 the
 same
 way
 that
 the


models
 themselves
 are
 always
 kind
 of
 in


the
 training
 process
 taking
 a
 little


step
 toward
 improvement,
 you
 know,
 the


the
 outer
 loop
 of
 the
 model
 architecture


and
 the
 the
 nature
 of
 the
 training
 runs


and
 where
 they're
 going
 to
 invest
 their


compute
 is
 also
 kind
 of
 going
 that


direction.
 And
 they're
 always
 looking
 at


like,
 well,
 we
 could
 scale
 up
 over
 here


and
 maybe
 get
 this
 kind
 of
 benefit
 a


little
 bit
 or
 we
 could
 do
 more
 post-


training
 here
 and
 get
 this
 kind
 of


benefit.
 And
 it
 just
 seems
 like
 we're


getting
 more
 benefit
 from
 the
 post-


training
 and
 the
 reasoning
 paradigm
 than


scaling.
 But
 I
 don't
 think
 either
 one
 is


uh
 I
 I
 definitely
 don't
 think
 either
 one


is
 is
 dead.
 We
 haven't
 seen
 yet
 what
 4.5


with
 all
 that
 post-
 training
 would
 look


like.


>> Yeah.
 And
 and
 so
 I
 mean
 one
 of
 the


things
 that
 you
 mentioned
 that
 that
 Cal


um
 you
 know
 analysis
 missed
 was
 was
 that


it
 way
 underestimated
 the
 value
 of
 of
 of


of
 of
 extended
 reasoning,
 right?
 Um,
 and


so
 what
 what
 would
 it
 mean
 to
 to
 fully


sort
 of
 appreciate
 that?


>> Well,
 I
 mean,
 a
 big
 one
 from
 just
 the


last
 few
 weeks
 was
 that
 we
 had
 an
 IMO


gold
 medal
 with
 pure
 reasoning
 models
 uh


with
 no
 access
 to
 tools
 from
 multiple


companies.
 And
 you
 know,
 that
 is
 night


and
 day
 compared
 to
 what
 GPT4
 could
 do


with
 math,
 right?
 We
 And
 these
 things


are
 really
 weird.
 Like
 it's
 nothing
 I


say
 here
 should
 be
 uh
 intended
 to


suggest
 that
 people
 won't
 be
 able
 to


find
 weaknesses
 in
 the
 models.
 I
 I
 still


use
 a
 tic-tac-toe
 puzzle
 to
 this
 day


where
 I
 take
 a
 picture
 of
 a
 tic-tac-toe


board
 where
 some
 the
 one
 of
 the
 players


has
 made
 a
 wrong
 move
 um
 that
 is
 not


optimal
 and
 thus
 allows
 the
 other
 player


to
 force
 a
 win
 and
 I
 ask
 the
 models
 if


somebody
 can
 force
 a
 win
 from
 this


position.
 Only
 very
 recently,
 only
 the


last
 generation
 of
 models
 are
 starting


to
 get
 that
 right
 some
 of
 the
 time.


Almost
 always
 before
 they
 were
 like


tic-tac-toe
 is
 a
 solved
 game.
 You
 know,


you
 can
 always
 get
 a
 draw.
 there's
 and


they
 would
 wrongly
 assess
 my
 board


position
 as
 player
 can
 still
 get
 a
 draw.


So,
 there's
 a
 lot
 of
 weird
 stuff,
 right?


The
 the
 jagged
 uh
 capabilities
 frontier


remains
 a
 real
 issue
 and
 people
 are


going
 to
 find,
 you
 know,
 peaks
 and


valleys
 for
 sure,
 but
 GPD4
 when
 it
 first


came
 out
 couldn't
 do
 anything


approaching
 IMO
 gold
 problems.
 It
 was


still
 struggling
 on
 like
 high
 school


math.
 And
 since
 then,
 we've
 seen
 this


high
 school
 math
 progression
 all
 the
 way


up
 through
 the
 IMO
 gold.
 Now
 we've
 got


the
 frontier
 math
 benchmark
 that
 is
 I


think
 now
 like
 up
 to
 25%.
 It
 was
 2%


about
 a
 year
 ago
 or
 even
 a
 little
 less


than
 a
 year
 ago
 I
 think.
 And
 we
 also


just
 today
 saw
 something
 where
 um
 and
 I


haven't
 absorbed
 this
 one
 yet
 but


somebody
 just
 came
 out
 and
 said
 that


they
 had
 solved
 a
 you
 know
 canonical


super
 challenging
 problem
 that
 no
 less


than
 Terrence
 Ta
 had
 put
 out.
 Um
 and
 it


was
 like
 this
 you
 know
 this
 thing


happened
 in
 I
 think
 days
 or
 weeks
 of
 the


model
 running
 versus
 it
 was
 18
 months


you
 know
 that
 it
 took
 professional
 and


not
 just
 any
 professional
 mathematicians


but
 like
 really
 you
 know
 the
 leading


minds
 in
 the
 world
 to
 make
 progress
 on


these
 problems.


So
 yeah
 I
 think
 that's
 really
 um


you
 know
 that's
 that's
 really
 hard
 uh


jumping
 capabilities
 to
 miss.
 I
 also


think
 a
 lot
 about
 the
 Google
 AI


co-scientist
 which
 we
 did
 an
 episode


with.
 We
 can
 you
 can
 uh
 check
 out
 the


full
 story
 on
 that
 if
 you
 want
 to.
 But


you
 know
 they
 basically
 just
 broke
 down


the
 scientific
 method
 into
 a
 schematic


you
 know
 and
 this
 is
 a
 lot
 of
 what


happens
 when
 people
 there's
 one
 thing
 to


say
 the
 model
 will
 respond
 with
 thinking


and
 it'll
 go
 through
 a
 reasoning
 process


and
 you
 know
 the
 more
 tokens
 it
 it


spends
 at
 runtime
 the
 better
 your
 answer


will
 be.
 That's
 true.
 Then
 you
 can
 also


build
 this
 scaffolding
 on
 top
 of
 that


and
 say
 okay
 well
 let
 me
 take
 something


as
 broad
 and
 you
 know
 aspirational
 as


the
 scientific
 method
 and
 let
 me
 break


that
 down
 into
 parts
 okay
 there's


hypothesis
 generation
 then
 there's


hypothesis
 evaluation
 then
 there's
 you


know
 experiment
 design
 there's


literature
 review
 there's
 all
 these


parts
 to
 the
 scientific
 method
 what
 the


team
 at
 Google
 did
 is
 created
 a
 pretty


elaborate
 schematic
 that
 represented


their
 best
 breakdown
 of
 the
 scientific


method
 optimized
 prompts
 for
 each
 of


those
 steps
 and
 then
 gave
 this
 resulting


system
 which
 is
 scaling
 inference
 now


kind
 of
 two
 ways.
 It's
 both
 the
 chain
 of


thought,
 but
 it's
 also
 all
 these


different
 angles
 of
 attack
 structured
 by


the
 team.
 And
 they
 gave
 it
 legitimately


unsolved
 problems
 in
 science.
 And
 in
 one


particularly
 famous
 kind
 of
 notorious


case,
 it
 came
 up
 with
 a
 hypothesis
 which


it
 wasn't
 able
 to
 verify
 because
 it


doesn't
 have
 direct
 access
 to
 actually


run
 the
 experiments
 in
 the
 lab.
 But
 it


came
 up
 with
 a
 hypothesis
 to
 some
 open


problem
 in
 viology
 that
 had
 stumped


scientists
 for
 years
 and
 it
 just
 so


happened
 that
 they
 had
 also
 recently


figured
 out
 the
 answer
 but
 not
 yet


published
 their
 results
 and
 so
 there
 was


this
 confluence
 where
 the
 scientists
 had


experimentally
 verified
 and
 Gemini
 uh
 in


the
 form
 of
 this
 AI
 co-scientist
 came
 up


with
 exactly
 the
 right
 answer
 and
 these


are
 things
 that
 like
 literally
 nobody


knew
 before
 and
 GP
 GPD4
 just
 wasn't


doing
 that
 you
 know
 I
 mean
 these


uh
 qualitatively
 new
 capabilities.
 That


thing
 I
 think
 ran
 for
 days.
 You
 know,
 it


probably
 cost
 hundreds
 of
 dollars,
 maybe


into
 the
 thousands
 of
 dollars
 to
 run
 the


inference.
 Um
 you
 know,
 that's
 not


nothing,
 but
 it's
 also
 like
 very
 much


cheaper
 than,
 you
 know,
 years
 of
 grad


students.
 And
 if
 you
 can
 get
 to
 those


caliber
 of
 problems
 and
 actually
 get


good
 solutions
 to
 them,
 like,
 you
 know,


what
 would
 you
 be
 willing
 to
 pay,
 right,


for
 that
 kind
 of
 thing?
 So,


yeah,
 I
 don't know.
 That's
 probably
 not


a
 full
 appreciation.
 We
 could
 go
 on
 for


a
 long
 time,
 but
 I
 would
 say
 in
 in


summary,
 GBT4
 was
 not
 able
 to
 push
 the


actual
 frontier
 of
 human
 knowledge.
 I


don't
 to
 my
 knowledge,
 I
 don't
 know
 that


ever
 discovered
 anything
 new.
 It's
 still


not
 easy
 to
 get
 that
 kind
 of
 output
 from


a
 GPT5
 or
 a
 Gemini
 2.5
 or,
 you
 know,
 a


Claude
 Opus
 4,
 whatever,
 but
 it's


starting
 to
 happen
 sometimes.
 And
 that


in
 and
 of
 itself
 is
 a
 a
 huge
 deal.
 Well,


then
 how
 do
 we
 explain
 the
 the
 the
 the


bearishness
 or
 or
 the
 kind
 of
 vibe
 shift


around
 GBD5?
 Then
 you
 know
 one one


potential
 kind
 contributor
 is
 this
 idea


that
 if
 a
 lot
 of
 the
 if
 if
 the


improvements
 are
 at
 the
 frontier,
 you


know,
 not
 everyone
 is
 working
 with
 you


know,
 sort
 of
 advanced
 math
 and
 physics


and
 in
 a
 day-to-day
 and
 so
 maybe
 they


don't
 see
 the
 benefits
 in
 their


day-to-day
 lives
 in
 in
 the
 same
 way


that,
 you
 know,
 sort
 of
 the
 jumps
 in


chat
 GBT
 were
 were
 were
 obvious
 and
 and


shaped
 the
 day-to-day.


>> Yeah.
 I
 mean,
 I
 think
 a
 decent
 amount
 of


it
 was


that
 they
 kind
 of
 up
 the
 launch,


you
 know,
 simply
 put,
 right?
 They
 like


were
 tweeting
 Death
 Star
 images,
 uh,


which
 Sam
 Alman
 later
 came
 back
 and


said,
 "No,
 you're
 the
 Death
 Star.
 I'm


not
 the
 Death
 Star."
 But,
 uh,
 I
 think


people
 thought
 that
 the
 Death
 Star
 was


supposed
 to
 be
 the
 model.
 That
 was


generally
 the,
 you
 know,
 the


expectations
 were
 set
 extremely
 high.


The
 actual
 launch
 itself
 was
 just


technically
 broken.
 So
 a
 lot
 of
 people's


first
 experiences
 of
 GBT5,
 they've
 got


this
 model
 router
 concept
 now
 where


and
 I
 think
 one
 another
 way
 to


understand
 what
 they're
 doing
 here
 is


they're
 trying
 to
 own
 the
 consumer
 use


case
 and
 to
 own
 that
 they
 need
 to


simplify
 the
 product
 experience
 relative


to
 what
 we
 had
 in
 the
 past
 which
 was


like
 okay
 you
 got
 GBT4
 and
 40
 and
 40


mini
 and
 03
 and
 04
 mini
 and
 other
 things


you
 know
 45
 was
 in
 there
 at
 one
 point


you
 got
 all
 these
 different
 models
 which


one
 should
 I
 use
 for
 which?
 It's
 like


very
 confusing
 to
 most
 people
 who
 aren't


obsessed
 with
 this.
 And
 so,
 one
 of
 the


big
 things
 they
 wanted
 to
 do
 was
 just


shrink
 that
 down
 to
 just
 ask
 your


question
 and
 you'll
 get
 a
 good
 answer


and
 we'll
 take
 that
 complexity
 on
 our


side
 as
 the
 the
 product
 owners
 to
 do


that.
 Interestingly,
 and
 I
 don't
 have
 a


great
 account
 of
 this,
 but
 one
 thing
 you


you
 might
 want
 to
 do
 is
 kind
 of
 merge


the
 models
 and
 figure
 out
 just
 have
 the


model
 itself
 decide
 how
 much
 to
 think
 or


maybe
 even,
 you
 know,
 have
 the
 model


itself
 decide
 how
 many
 of
 its
 experts.


If
 it's
 a
 mixture
 of
 experts,


architecture
 it
 needs
 to
 use
 or
 maybe,


you
 know,
 there's
 been
 a
 bunch
 of


different
 uh
 research
 projects
 on
 like


skipping
 layers
 of
 the
 model.
 If
 the


task
 is
 easy
 enough,
 you
 could
 like
 skip


a
 bunch
 of
 layers.
 So
 you
 might
 have


hoped
 that
 you
 could
 genuinely
 on
 the


back
 end
 merge
 all
 these
 different


models
 into
 one
 model
 that
 would


dynamically
 use
 the
 right
 amount
 of


compute
 for
 the
 level
 of
 challenge
 that


a
 given
 user
 query
 presented.
 It
 seems


like
 they
 found
 that
 harder
 to
 do
 than


they
 expected.
 And
 so
 the
 solution
 that


they
 came
 up
 with
 instead
 was
 to
 have
 a


router
 where
 the
 the
 router's
 job
 is
 to


pick
 is
 this
 an
 easy
 query
 in
 which
 case


we'll
 send
 you
 to
 this
 model.
 Is
 it
 a


medium?
 Is
 it
 a
 hard?
 And
 I
 think
 they


just
 have
 two
 really
 u
 models
 behind
 the


scenes.
 So
 I
 think
 it's
 just
 really
 easy


or
 hard.
 Certainly
 the
 graphs
 that
 they


showed,
 you
 know,
 basically
 showed
 the


kind
 of
 with
 and
 without
 thinking.
 Um


the
 problem
 at
 launch
 was
 that
 that


router
 was
 broken.
 So
 all
 all
 of
 the


queries
 were
 going
 to
 the
 dumb
 model
 and


so
 a
 lot
 of
 people
 literally
 just
 got


bad
 outputs
 which
 were
 worse
 than
 03


because
 they
 were
 getting
 non-thinking


responses.
 And
 so
 the
 initial
 reaction


of
 like
 okay
 this
 is
 dumb
 and
 that
 sort


of
 you
 know
 uh
 traveled
 really
 fast.
 I


think
 that
 kind
 of
 set
 the
 tone.
 My


sense
 now
 is
 that
 as
 the
 dust
 has


settled
 most
 people
 do
 think
 that
 it
 is


the
 best
 model
 available
 and
 you
 know


things
 like
 the
 meter
 the
 infamous
 uh


meter
 task
 length
 chart.
 It
 is
 the
 best.


You
 know
 we're
 now
 over
 two
 hours
 and
 um


it
 is
 still
 above
 the
 trend
 line.
 So
 if


you
 just
 said,
 you
 know,
 do
 I
 believe
 in


straight
 lines
 on
 graphs
 or
 not?
 And
 how


should
 this
 latest
 data
 point
 influence


whether
 I
 believe
 on
 these
 straight
 line


straight
 um
 lines
 on,
 you
 know,
 power
 um


logarithmic
 scale
 graphs.


It
 shouldn't
 really
 change
 your
 mind
 too


much.
 It's
 still
 above
 the
 trend
 line.
 I


talked
 to
 ZV
 about
 this.
 Zim
 Mashwitz
 uh


legendary
 infovore
 and
 uh
 AI
 industry


analyst
 on
 a
 recent
 podcast
 too
 and
 kind


of
 asked
 him
 the
 same
 question
 like
 why


do
 you
 think
 the
 you
 know
 even
 some
 of


the
 most


plugged
 in
 you
 know
 sharp
 uh
 minds
 in


the
 space
 have
 seemingly
 pushed


timelines
 out
 a
 bit
 as
 a
 result
 of
 this


and
 his
 answer
 was
 basically
 just
 it


resolved
 some
 amount
 of
 uncertainty
 you


know
 you
 had
 a
 open
 question
 of
 maybe


they
 do
 have
 another
 breakthrough
 you


know
 maybe
 it
 really
 is
 the
 Death
 Star


um
 you
 know
 if
 they
 surprise
 surprise
 us


on
 the
 upside
 than
 all
 these
 short


timelines,
 you
 know,
 we
 we
 could
 have


expected
 a
 um
 yeah,
 I
 guess
 one
 one
 way


to
 think
 about
 it
 is
 like
 the
 the


distribution
 was
 sort
 of
 broad
 in
 terms


of
 timelines
 and
 if
 they
 had
 surprised


on
 the
 upside,
 it
 might
 have
 narrowed


and
 narrowed
 in
 toward
 the
 front
 end
 of


the
 distribution
 and
 if
 it
 if
 they


surprised
 on
 the
 downside
 or
 even
 just


were,
 you
 know,
 purely
 on
 trend,
 then


you
 would
 take
 some
 of
 your
 distribution


from
 the
 very
 short
 end
 of
 the
 timelines


and
 kind
 of
 push
 them
 back
 toward
 the


middle
 or
 the
 end.
 And
 so
 his
 answer
 was


like


AI
 2027
 seems
 less
 likely,
 but
 AI
 2030


seems
 basically
 no
 less
 likely,
 maybe


even
 a
 little
 more
 likely
 because
 some


of
 the
 the
 probability
 mass
 from
 the


early
 years
 is
 now
 sitting
 there.
 So


it's
 not
 that
 um
 I
 don't
 think
 people


are
 are
 moving


the
 whole
 distribution
 out
 super
 much.
 I


think
 there
 may
 be
 more
 just
 kind
 of


shrinking
 the
 uh
 you
 know
 it's
 getting
 a


little
 tighter
 because
 it's
 maybe
 not


happening
 quite
 as
 soon
 as
 it
 seemed


like
 it
 might
 have
 been.
 Uh
 but
 I
 don't


think
 too
 many
 people
 at least
 that
 I


you
 know
 think
 are
 really
 plugged
 in
 on


this
 are
 pushing
 out
 too
 much
 past
 2030


at
 all.
 And
 by
 the
 way
 you
 know
 they


obviously
 there's
 a
 lot
 of
 um
 you
 know


disagreement.
 The
 way
 I
 kind
 of
 have


always
 thought
 about
 this
 sort
 of
 stuff


is
 Daario
 says
 2027
 Demis
 says
 2030.


I'll
 take
 that
 as
 my
 range.
 So
 coming


into
 GBT5,


I
 was
 kind
 of
 in
 that
 space.
 And
 now
 I'd


say,
 well,
 I
 don't
 know,
 Dario's
 got
 uh


what
 what
 cars
 does
 he
 have
 up
 his


sleeve?
 You
 know,
 they
 just
 put
 out
 4.1


Opus.
 And
 in
 that
 blog
 post,
 they
 said


we
 will
 be
 releasing
 more
 powerful


updates
 to
 our
 models
 in
 the
 coming


weeks,
 so
 they're
 due
 for
 something


pretty
 soon.
 You
 know,
 maybe
 they'll
 be


the
 ones
 to
 surprise
 on
 the
 upside
 this


time,
 or
 maybe
 Google
 will
 be.
 Um,
 I


wouldn't
 say
 2027
 is
 is
 out
 of
 the


question,
 but
 yeah,
 I
 would
 say
 20
 2030


still
 looks
 just
 as
 likely
 as
 before.


And
 again,
 from
 my
 standpoint,
 it's
 like


that's
 still
 really
 soon,
 you
 know?
 So,


if
 we're
 on
 track,
 whether
 it's
 28,
 29,


30,
 uh,
 I
 don't
 really
 care.
 I
 I I
 try


to
 frame
 my
 own
 work
 so
 that
 I'm
 kind
 of


preparing
 myself
 and
 helping
 other


people
 prepare
 for
 what
 might
 be
 the


most
 extreme
 scenarios
 and
 kind
 of,
 you


know,
 one
 of
 these
 things
 where
 if
 we


aim
 high
 and
 we
 miss
 a
 little bit
 and
 we


have
 a
 little
 more
 time,
 great.
 I'm
 sure


we'll
 have
 plenty
 of
 things
 to
 do
 to
 use


that
 extra
 time
 to
 be
 ready
 for,
 you


know,
 whatever
 powerful
 AI
 does
 come


online.
 Um,


but
 yeah,
 I
 guess
 I
 don't
 uh
 my


worldview
 hasn't
 changed
 all
 that
 much


as
 a
 result
 of
 these
 uh
 summer's


developments.


>> Anecdotally,
 I
 I
 don't
 hear
 as
 much


about
 AI
 2027
 or
 situational
 awareness


to
 the
 to
 the
 same
 degree.
 I
 I
 do
 talk


to
 some
 people
 who've
 just
 moved
 it
 a


few
 years
 back
 to
 to
 your
 point.
 Um
 uh


but
 um
 yeah,
 Dores
 Cesh
 had
 his
 whole


thing
 around,
 you
 know,
 he
 still
 still


believes
 in
 it,
 but
 sort
 of
 um
 you
 know,


maybe
 because
 this
 gap
 gap
 in
 continual


learning
 or
 or
 something
 to
 the
 effect


that
 um
 you
 know,
 maybe
 it's
 just
 going


to
 be
 a
 bit
 slower
 to
 to
 diffuse
 um
 and


um
 you
 know,
 Meter's
 paper
 as
 you


mentioned
 showed
 that
 uh
 engineers
 are


less
 productive
 and
 so
 maybe
 there's


there's
 less
 of
 a
 uh
 sort
 of
 concern


around
 um
 you
 know,
 people
 being


replaced
 in
 the
 next
 next
 few
 years
 in


in in
 in
 in
 mass.
 I
 think
 when
 we
 spoke


maybe
 a
 year
 ago
 about
 this
 or
 I
 think


you
 said
 something
 like
 50%
 of
 50%
 of


jobs.
 Um
 I'm
 curious
 if
 that's
 still


your
 your
 uh
 your
 litmus
 test
 or
 how
 you


think
 about
 it.
 Well,
 for
 one
 thing,
 I


think
 that
 Meter
 paper
 is
 worth


unpacking
 a
 little
 bit
 more
 because
 this


was
 one
 of
 those
 things
 that
 was
 and
 I
 I


I'm
 a
 big
 fan
 of
 Meter
 and
 I
 have
 no
 um


you
 know,
 no
 shade
 on
 them
 because
 I
 do


think
 do
 science,
 publish
 your
 results.


Like
 that's
 good.
 You
 don't
 have
 to
 uh


make
 every
 experimental
 result
 and


everything
 you
 put
 out
 conform
 to
 a


narrative.
 But
 I
 do
 think
 it
 was
 a


little
 bit


um


it
 was
 a
 little
 bit
 too
 easy
 for
 people


who
 wanted
 to
 say
 that
 oh
 this
 is
 all


nonsense
 to
 latch
 on
 to
 that.
 And
 you


know
 again
 there
 is
 there's
 something


there
 that
 I
 would
 kind
 of
 put
 in
 the


Cal
 Newport
 category
 too
 where
 for
 me


maybe
 the
 most
 interesting
 thing
 was
 the


users
 thought
 that
 they
 were
 faster
 when


in
 fact
 they
 seem
 to
 be
 slower.
 So
 that


sort
 of
 misperception
 of
 oneself
 I
 think


is
 really
 interesting.
 Personally
 I


think
 there's
 some
 explanations
 for
 that


that
 include
 like
 hitting
 go
 on
 the


agent
 going
 to
 social
 media
 and


scrolling
 around
 for
 a
 while
 and
 then


coming
 back.
 The
 thing
 might
 have
 been


done
 for
 quite
 a
 while
 by
 the
 time
 I
 get


back.
 So
 honestly
 one
 like
 really
 simple


and
 we're
 starting
 to
 see
 this
 in


products.
 One
 really
 simple
 thing
 that


the
 products
 can
 do
 to


address
 those
 concerns
 is
 just
 provide


notifications
 like
 the
 thing
 is
 done


now.
 So,
 you
 know,
 stop
 scrolling
 and


and
 come
 back
 and
 check
 its
 work.
 That


in
 terms
 of
 just
 clock
 time,
 you
 know,


it
 would
 be
 interesting
 to
 know
 like


what
 applications
 did
 they
 have
 open.


Maybe
 they
 took
 a
 little
 longer
 with


cursor
 than
 doing
 it
 on
 their
 own,
 but


how
 much
 of
 the
 time
 was
 cursor
 the


active
 window
 and
 how
 much
 of
 it
 was,


you
 know,
 some
 other
 random
 distraction


while
 they
 were
 waiting.
 Um
 but
 I
 think


a
 more
 fundamental
 issue
 with
 that
 study


which
 again
 wasn't
 really
 about
 the


study
 design
 but
 just
 in
 in
 the
 sort
 of


you
 know
 interpretation
 and
 kind
 of


digestion
 of
 it
 some
 of these
 details


got
 lost.


The
 they
 basically
 tested
 the
 models
 or


the
 you
 know
 the
 product
 cursor
 in
 the


area
 where
 it
 was
 known
 to
 be
 least
 able


to
 help.
 This
 study
 was
 done
 early
 this


year.
 So
 it
 was
 done
 with
 you
 know
 kind


of
 one
 depending
 on
 how
 you
 want
 to


count
 right
 a
 couple
 couple
 releases
 ago


with
 code
 bases
 that
 are
 large
 which


again
 strains
 the
 strains
 the
 context


window
 and
 you
 know
 that's
 one
 of
 the


the
 frontiers
 that
 has
 been
 moving


very
 mature
 code
 bases
 with
 like
 high


standards
 for
 coding
 and
 developers
 who


really
 know
 their
 code
 bases
 super
 well


who've
 made
 a
 lot
 of
 commit
 you
 know


commits
 to
 these
 particular
 code
 bases.


So
 I
 would
 say
 that's
 basically
 the


hardest
 situation
 that
 you
 could
 set
 up


for
 an
 AI
 because
 the
 people
 know
 you


know
 their
 stuff
 really
 well.
 The
 AI


doesn't
 the
 context
 is
 huge.
 People
 have


already
 absorbed
 that
 through
 working
 on


it
 for
 a
 long
 time.
 The
 AI
 doesn't
 have


that
 uh
 that
 knowledge
 and
 again
 couple


generations
 ago
 models.
 Um
 and
 then
 a


big
 thing
 too
 is
 that
 the
 user
 the


people
 were
 not
 very
 wellversed
 in
 the


tools.
 Why?
 Because
 the
 tools
 weren't


really
 able
 to
 help
 them
 yet.
 I
 think


the
 sort
 of
 mindset
 of
 the
 people
 that


came
 into
 the
 study
 in
 many
 cases
 was


like,
 well,
 I
 haven't
 used
 this
 all
 that


much
 because
 it
 hasn't
 really
 seemed
 to


be
 super
 helpful.
 They
 weren't
 wrong
 in


that
 assessment
 given
 the,
 you
 know,
 the


limitations.


And
 you
 could
 see
 that
 in
 terms
 of
 the


um
 some
 of
 the
 instructions
 and
 the
 help


that
 the
 meter
 team
 gave
 to
 people.
 One


of
 the
 things
 that
 is
 in
 the
 paper
 that


they
 would
 if
 they
 noticed
 that
 you


weren't
 like
 weren't
 using
 cursor
 super


well,
 they
 would
 give
 you
 some
 feedback


on
 how
 to
 use
 it
 better.
 One
 of
 the


things
 that
 they
 were
 telling
 people
 to


do
 is
 make
 sure
 you
 at
 tag
 a
 particular


file
 to
 bring
 that
 into
 context
 for
 the


model
 so
 that
 the
 model
 has
 you
 know
 the


right
 context.
 And
 that's
 literally
 like


the
 most
 basic
 thing
 that
 you
 would
 do


in
 cursor
 you
 know
 that's
 like
 the
 thing


you
 would
 learn
 on
 your
 in
 your
 first


hour
 your
 first
 day
 of
 using
 it.
 So
 it


really
 does
 suggest
 that
 these
 were
 you


know
 while
 very
 capable
 programmers
 like


basically
 mostly
 noviceses
 when
 it
 came


to
 using
 the
 AI
 tools.
 So
 I
 think
 the


result
 is
 real.
 Um
 but
 I
 just
 I
 would
 be


very
 cautious
 about
 generalizing
 too


much
 there


in
 terms
 of
 I
 guess
 what
 else
 was
 the


other
 question.
 It's
 what
 is
 the


expectation
 for
 jobs?
 I
 mean


>> we're
 starting
 to
 see
 some
 of
 this


right.
 We
 are
 definitely
 seeing
 no
 less


than
 like
 Mark
 Banoff
 has
 said
 that


they've,
 you
 know,
 have
 been
 able
 to
 cut


a
 bunch
 of
 headcount
 because
 they've
 got


AI
 agents
 now
 that
 are
 responding
 to


every
 lead.
 Um,
 Clara
 of
 course
 is,
 you


know,
 has
 said,
 um,
 you
 know,
 very


similar
 things
 for
 a
 while
 now.
 They


also,
 I
 think,
 have
 been
 a
 little
 bit


misreported
 in
 terms
 of
 like,
 oh,


they're
 backtracking
 off
 of
 that
 because


they're
 actually
 going
 to
 keep
 some


customer
 service
 people,
 not
 none.
 And
 I


think
 that's
 a
 bit
 of
 an
 overreaction.


like
 they
 may
 have
 some
 people
 who
 are


just,
 you
 know,
 insistent
 on
 having
 a


certain
 experience
 and
 maybe
 they
 want


to
 provide
 that
 and
 that
 makes
 sense.


You
 know,
 it
 doesn't
 I
 think
 you
 can


have
 a
 a
 a
 spectrum
 of
 service
 offerings


to
 your
 customers.
 I
 once
 coded
 up
 a


pricing
 page
 for
 a
 I
 actually
 just
 vibe


coded
 up
 a
 pricing
 page
 for
 a
 SAS


company
 that
 was
 like
 basic
 uh
 level


with
 AI
 sales
 and
 service
 is
 one
 price.


If
 you
 want
 to
 talk
 to
 human
 sales,


that's
 a
 higher
 price.
 And
 if
 you
 want


to
 talk
 to
 human
 sales
 and
 support,


that's
 a,
 you
 know,
 third
 higher
 price.


And
 so,
 like,
 literally,
 that
 might
 be


what's
 going
 on,
 I
 think,
 in
 some
 of


these
 cases.
 And
 it
 it
 could
 very
 well


be
 a
 very
 sensible
 option
 for
 people.


But
 I
 just
 I
 do
 see
 the


intercom.
 I've
 got
 an
 episode
 coming
 up


with,
 they
 now
 have
 this
 Finn
 agent
 that


is
 solving
 like
 65%
 of
 customer
 service


tickets
 that
 come
 in.
 So,
 you
 know,


what's
 that
 going
 to
 do
 to
 jobs?
 Are


there
 really
 like
 three
 times
 as
 many


customer
 service
 tickets
 to
 be


handled?
 Like,
 I
 don't
 know.
 I
 think


there's
 kind
 of
 a
 relatively
 inelastic


supply.
 Maybe
 you'll
 get
 somewhat
 more


tickets
 if
 people
 expect
 that
 they're


going to
 get
 better,
 faster
 answers,
 but


I
 don't
 think
 we're
 going
 to
 see
 like


three
 times
 more
 tickets.
 By
 the
 way,


that
 number
 was
 like
 55%
 3
 or
 four


months
 ago.
 So,
 you
 know,
 as
 they


ratchet
 that
 up,
 the
 ratios
 get
 really


hard,
 right?
 At
 half
 ticket
 resolution,


in
 theory,
 maybe
 you
 get
 some
 more


tickets.
 Maybe
 you
 don't
 need
 to
 adjust


headcount
 too
 much.
 But
 when
 you
 get
 to


90%
 ticket
 resolution,
 you
 know,
 are
 you


really
 going
 to
 have
 10
 times
 as
 many


tickets
 or
 10
 times
 as
 many
 hard
 tickets


that
 the
 people
 have
 to
 handle?
 It
 seems


just
 really
 hard
 to
 imagine
 that.
 So
 I


don't
 think
 I
 don't
 think
 these
 things


go
 to
 zero
 probably
 in
 a
 lot
 of


environments
 but
 I
 do
 expect
 that
 you


will
 see
 significant
 headcount
 reduction


in
 a
 lot
 of
 these
 places
 and
 the


software
 one
 is
 really
 interesting


because
 the
 elasticities
 are
 really


unknown.
 you
 know,
 you
 can
 potentially


produce
 x
 times
 more
 software
 per
 user


or,
 you
 know,
 per
 per
 cursor
 user
 or
 per


developer
 at
 your
 company,
 whatever.


But
 maybe
 you
 want
 that.
 You
 know,
 maybe


there
 is
 no
 limit
 or
 no,
 you
 know,
 maybe


the
 the
 regime
 that
 we're
 in
 is
 such


that
 if
 there's,
 you
 know,
 10
 times
 more


productivity,
 that's
 all
 to
 the
 good.


And,
 you
 know,
 we
 still
 have
 just
 as


many
 uh
 jobs
 because
 we
 want
 10
 times


more
 software.
 I
 don't
 know
 how
 long


that
 lasts.
 again
 the
 ratios
 start
 to


get
 challenging
 at
 some
 point.
 Um


but
 yeah,
 I
 think
 the
 bottle,
 you
 know,


the
 old
 Tyler
 Common
 thing
 comes
 to


mind.
 You
 are
 a
 bottleneck.
 You
 are
 a


bottleneck.
 Um
 I
 think
 more
 often
 it
 is


are
 people
 really
 trying
 to
 get
 the
 most


out
 of
 these
 things
 and
 you
 know
 are


they
 using
 best
 practices
 and
 have
 they


um
 have
 they
 really
 put
 their
 minds
 to


it
 or
 not?
 I
 you
 know
 often
 the
 the
 real


barrier
 is
 there.
 I
 was
 I've
 been


working
 a
 little bit
 with
 a
 company
 that


is
 doing


um
 basically
 government
 doc
 review.
 I'll


abstract
 a
 little
 bit
 away
 from
 the


details,


really
 gnarly
 stuff
 like
 scanned


documents,
 you
 know,
 handwritten


uh
 filling
 out
 of
 forms.
 and
 they've


created
 this
 auditor
 AI
 agent
 that
 just


won
 a
 state
 level
 contract
 to
 do
 the


audits
 on
 like
 a
 million
 transactions
 a


year
 of
 of
 these
 um
 you
 know
 these


packets
 of
 documents
 again
 scanned,


handwritten,
 all
 this
 kind
 of
 crap.
 Um


and
 they
 just
 blew
 away
 the
 human


uh
 workers
 that
 were
 doing
 the
 job


before.
 So
 where
 are
 those
 workers
 going


to
 go?
 Like
 I
 don't
 know.
 I
 don't


they're
 not
 going
 to
 have
 10
 times
 as


many
 transactions.
 you
 know,
 I
 can
 be


pretty
 confident
 in
 that.
 Um,
 are
 there


going
 to
 be
 a
 few
 still
 that
 are
 there


to
 supervise
 the
 AIS
 and
 handle
 the


weird
 cases
 and,
 you
 know,
 answer
 the


phones?
 Sure.
 Um,
 maybe
 they
 maybe
 they


won't
 go
 anywhere.
 You
 know,
 this
 the


state,
 you
 know,
 the
 state
 may
 do
 a


strange
 thing
 and
 uh
 just
 have
 all
 those


people
 like
 sit
 around
 because
 they


can't
 bear
 to
 fire
 them.
 Like,
 who
 knows


what
 the
 ultimate
 decision
 will
 be.
 But


I
 do
 see
 a
 lot
 of
 these
 things
 where
 I'm


just
 like
 when
 you
 really
 put
 your
 mind


to
 it
 and
 you
 identify
 what
 would
 create


real
 leverage
 for
 us,
 can
 the
 AI
 do


that?
 Can
 we
 make
 it
 work?
 You
 can
 take


a
 pretty
 large
 chunk
 out
 of
 high
 volume


tasks
 uh
 very
 reliably
 in
 today's
 world.


And
 so
 the
 the
 impacts
 I
 think
 are


starting
 to
 be
 seen
 there
 on
 on
 a
 lot
 of


jobs.


Humans
 I
 think
 are
 you
 know
 the


leadership
 is
 maybe
 the
 bottleneck
 or


the
 the
 will
 in
 in
 a
 lot
 of
 places
 might


be
 the
 bottleneck
 and
 software
 might
 be


an
 interesting
 case
 where
 there
 is
 just


so
 much
 pent-up
 demand
 perhaps
 that
 it


might
 take
 a
 little
 longer
 to
 see
 those


impacts
 because
 you
 really
 do
 want
 you


know
 10
 or
 100
 times
 as
 much
 software.


What
 is
 Yeah,
 let's
 let's
 talk
 about


code
 because
 it's,
 you
 know,
 it's
 it's


where
 Andropic
 made
 made
 a
 big
 bet
 um


early
 on,
 you
 know,
 perhaps
 inspired
 by


the
 sort
 of
 automated
 researcher,
 you


know,
 recursive
 self-improvement
 um
 you


know,
 sort
 of
 you,
 you
 know,
 desired


future
 and
 and
 we
 saw
 open
 AI
 uh
 make


make
 moves
 um
 there
 as
 well.


Why
 don't
 we
 flush
 that
 out
 or
 talk
 a


little
 about
 you
 know
 what
 what
 inspired


that
 and
 where
 you
 see
 that
 going?


>> You
 know,
 utopia
 or
 dystopia
 is
 really


the
 big
 question
 there,
 I
 think.
 Right.


Right.
 I
 mean
 is
 maybe
 one
 part


technical,
 two
 parts
 social
 in
 terms
 of


why
 code
 has
 been
 so
 focal.
 The


technical
 part
 is
 that
 it's
 really
 easy


to
 validate
 code.
 You
 generate
 it,
 you


can
 run
 it.
 If
 you
 get
 a
 runtime
 error,


you
 can
 get
 the
 feedback
 immediately.


It's,
 you
 know,
 somewhat
 harder
 to
 do


functional
 testing.
 Replet
 recently
 just


in
 the
 last
 like
 48
 hours
 released
 their


V3
 of
 their
 agent
 and
 it
 now
 in
 addition


to
 you
 know
 code
 code code
 try
 to
 make


your
 app
 work
 v2
 of
 the
 agent
 would
 do


that
 and
 it
 could
 go
 for
 minutes
 and
 you


know
 in
 some
 cases
 generate
 dozens
 of


files
 and
 I've
 had
 some
 magical


experiences
 with
 that
 where
 I
 was
 like


wow
 you
 just
 did
 that
 whole
 thing
 in
 one


prompt
 and
 it
 like
 worked
 amazing.
 Other


times
 it
 will
 sort
 of
 code
 for
 a
 while


and
 hand
 it
 off
 to
 you
 and
 say
 okay
 does


it
 look
 good?
 Is
 it
 working?
 and
 you're


like,
 "No,
 it's
 not.
 I'm
 not
 sure
 why."


You
 know,
 you
 get
 into
 a
 a
 back
 and


forth
 with
 it.
 But
 the
 difference


between
 V2
 and
 V3
 is
 that
 instead
 of


handing
 the
 baton
 back
 to
 you,
 it
 now


uses
 a
 browser
 and
 the
 vision
 aspect
 of


the
 models
 to
 go
 try
 to
 do
 the
 QA


itself.
 So,
 it
 doesn't
 just
 say,
 "Okay,


hey,
 I
 uh
 I
 tried
 my
 best,
 wrote
 a
 bunch


of
 code,
 like
 let
 me
 know
 if
 it's


working
 or
 not."
 It
 takes
 that
 first


pass
 at
 figuring
 out
 if
 it's
 working.


And
 you
 know
 again
 that
 that
 really


improves
 the
 flywheel
 just
 how
 much
 you


can
 do,
 how
 much
 you
 can
 validate,
 how


quickly
 you
 can
 validate
 it.
 The
 the


speed
 of
 that
 loop
 is
 really
 key
 to
 the


pace
 of
 improvement.
 So
 it's
 a
 problem


space
 that's
 pretty
 amendable
 to
 the


sorts
 of,
 you
 know,
 rapid
 flywheel


techniques.
 Second,
 of
 course,
 they


they're
 all
 coders
 right
 at
 these


places.
 So
 they
 want
 to,
 you
 know,
 solve


their
 own
 problems.
 That's
 like
 very


natural.
 And
 third,
 I
 do
 think
 on
 the,


you
 know,
 sort
 of
 social
 vision


competition,
 uh,
 who
 knows
 where
 this
 is


all
 going,
 they
 do
 want
 to
 create
 the


automated
 AI
 researcher.
 That's
 another


data
 point,
 by
 the
 way,
 from
 this
 was


from
 the
 03


system
 card.
 They
 showed
 a
 jump
 from


like
 low
 to
 mid
 single
 digits
 to
 roughly


40%


of
 PRs
 actually
 checked
 in
 by
 research


engineers
 at
 OpenAI
 that
 the
 model
 could


do.
 So
 prior
 to
 03
 not
 much
 at
 all
 you


know
 low
 to
 mid
 single
 digits
 as
 of
 03


40%.
 I'm
 sure
 those
 are
 the
 easier
 40%


or
 whatever.
 Again,
 there
 will
 be,
 you


know,
 caveats
 to
 that,
 but
 that's
 you're


entering
 maybe
 the
 steep
 part
 of
 the


S-curve
 there.


And
 that's
 presumably
 pretty
 high-end.


You
 know,
 I
 don't
 know
 how
 many
 easy


problems
 they
 have
 at
 OpenAI,
 but


presumably,
 you
 know,
 not
 that
 many


relative
 to
 the
 rest
 of
 us
 that
 are
 out


here
 making
 uh
 generic
 web
 apps
 all
 the


time.
 So,
 you
 know,
 at
 40%
 you
 got
 to
 be


starting
 to,
 I
 would
 think,
 get
 into


some
 pretty
 hard
 tasks,
 some
 pretty
 high


value
 stuff,


you
 know,
 at
 that
 at
 what
 point
 does


that
 ratio
 really
 start
 to
 tip
 where
 the


AI
 is
 like
 doing
 the
 bulk
 of
 the
 work?


Um,
 GBD5
 notably
 wasn't
 a
 big
 update


over
 03
 on
 that
 particular
 measure.
 I


mean
 it
 also
 wasn't
 going
 back
 to
 the


simple
 QA
 thing.
 Um
 GP5
 is
 generally


understood
 to
 not
 be
 a
 scale
 up
 relative


to
 40
 and
 03
 and
 you
 can
 see
 that
 in
 the


simple
 QA
 measure.
 It
 basically
 scores


the
 same
 on
 these
 longtail
 trivia


questions.
 It's
 not
 a
 bigger
 model
 that


has
 absorbed
 like
 lots
 more
 world


knowledge.
 Um
 it
 is
 it
 is
 you
 know
 Cal


is
 right.
 I
 think
 it
 is
 is
 analysis
 that


it's
 it's
 post-training.
 Um
 but
 that


post
 training
 you
 know
 is
 potentially


entering
 the
 steep
 part
 of
 the
 S-curve


when
 it
 comes
 to
 the
 ability
 to
 do
 even


the
 kind
 of
 hard
 problems
 that
 are


happening
 at
 uh
 at
 OpenAI
 on
 the


research
 engineering
 front.


And
 you
 know,
 yikes.
 I
 I'm
 a
 little


worried
 about
 that,
 honestly.
 The
 um
 the


idea
 that
 we
 could
 go
 from
 these


companies
 having
 a
 few
 hundred
 research


engineer
 people
 to
 having,
 you
 know,


unlimited
 overnight
 and
 like
 what
 would


that
 mean
 in
 terms
 of
 how
 much
 things


could
 change
 and
 also
 just
 our
 ability


to
 steer
 that
 overall
 process.
 Um,
 I'm


not
 super
 comfortable
 with
 the
 idea
 of


the
 companies
 tipping
 into
 a
 recursive


self-improvement
 regime,
 especially


given
 the
 the
 level
 of
 control
 and
 the


level
 of
 unpredictability
 that
 we


currently
 see
 in
 the
 models.
 But
 that


does
 seem
 to
 be
 what
 they
 are
 going
 for.


So,
 in
 terms
 of
 like
 why
 um
 I
 think
 this


has
 been
 the
 plan
 for
 quite
 some
 time.


Even
 you
 remember
 that
 leaked
 anthropic


uh
 fundraising
 deck
 from
 maybe
 two
 years


ago
 where
 they
 said
 that
 in
 2025
 and


2026
 the
 companies
 that
 train
 the
 best


models
 will
 get
 so
 far
 ahead
 that
 nobody


else
 will
 be
 able
 to
 catch
 up.
 I
 think


that's
 kind
 of
 what
 they
 meant.
 I
 think


that
 they
 were
 projecting
 then
 that
 in


the
 25
 26
 time
 frame
 they'd
 get
 this


like
 automated
 researcher
 and
 once
 you


have
 that
 how's
 anybody
 you
 know
 who


doesn't
 have
 that
 going
 to
 catch
 up
 with


you?
 Um,
 obviously
 some
 of
 that
 remains


to
 be
 validated,
 but
 um,
 I
 do
 think
 they


have
 been
 pretty
 intent
 on
 that
 for
 a


long
 time.
 Five
 years
 from
 now,
 are


there
 more
 engineers
 or
 fewer
 engineers?


>> I
 I
 tend
 to
 think
 less.
 Um,
 you
 know,


already
 if
 I
 just
 think
 about
 my
 own


life
 and
 work,
 I'm
 like,
 would
 I
 rather


have
 a
 model
 or
 would
 I
 rather
 have
 like


a
 junior
 marketer?
 I'm
 pretty
 sure
 I'd


rather
 have
 the
 model.


Would
 I
 rather
 have
 the
 models
 or
 a


junior
 engineer?


I
 think
 I'd
 probably
 rather
 have
 the


models
 in
 a
 lot
 of
 cases.
 I
 mean,
 it


obviously
 depends
 on,
 you
 know,
 the


exact
 person
 you're
 talking
 about.
 Um,


but
 truly


forced
 choice
 today.
 Now,
 that
 and
 then


you've
 got
 cost
 adjustment
 as
 well,


right?
 I'm
 not
 spending
 nearly
 as
 much


on
 my
 cursor
 subscription
 as
 I
 would
 be


on
 a,
 you
 know,
 an
 actual
 human


engineer.
 So,
 even
 if
 they
 have
 some


advantages,


you
 know,
 and
 I
 I
 also
 have
 not


scaffolded
 um
 I
 haven't
 gone
 full


co-scientist,
 right,
 on
 my
 uh
 cursor


problems.
 I
 think
 that
 that's
 another


interesting


you
 start
 to
 see
 why


folks
 like
 Sam
 Alman
 are
 so
 focused
 on


questions
 like
 energy
 and
 the
 7
 trillion


buildout
 because
 these
 power
 law
 things


are
 weird
 and
 you
 know
 to
 get


incremental
 performance
 for
 10x
 the
 cost


is
 weird.
 It's
 it's
 a
 it's
 definitely


not
 the
 kind
 of
 thing
 that
 we're
 used
 to


dealing
 with,
 but
 for
 many
 things
 it


might
 be
 worth
 it
 and
 it
 still
 might
 be


cheaper
 than
 the
 human
 alternative.
 You


know,
 if
 it's
 like,
 well,
 cursor
 cost
 me


whatever
 40
 bucks
 a
 month
 or
 something.


Uh,
 would
 I
 pay
 400
 for,
 you
 know,


however
 much
 better?
 Yeah,
 probably.


Would
 I
 pay
 4,000
 for
 however
 much


better?
 Well,
 it's
 still,
 you
 know,
 a


lot
 less
 than
 a
 full-time
 human


engineer.
 Um,
 and
 the
 costs
 are


obviously
 coming
 down
 dramatically,
 too,


right?
 That's
 another
 huge
 thing.
 GPD4


was
 way
 more
 expensive.
 It's
 like
 90
 uh


it's
 like
 a
 95%
 discount
 from
 GPD4
 to


GPT5.


That's,
 you
 know,
 no
 small
 thing,
 right?


I
 mean,
 it's


Apple
 staff
 is
 a
 little
 bit
 hard
 because


the
 chain
 of
 thought
 does
 spit
 out
 a
 lot


more
 tokens
 and
 so
 that
 you
 get
 you
 give


back
 a
 little
 on
 a
 per
 token
 basis.
 It's


dramatically
 cheaper.
 more
 tokens


generated.
 Um,
 you
 know,
 does
 does
 eat


back
 into
 some
 of
 that
 savings,
 but


everybody
 seems
 to
 expect
 the
 trends


will
 continue
 in
 terms
 of
 prices


continuing
 to
 fall.
 And
 so,
 you
 know,


how
 many
 more
 of
 these
 like
 price


reductions
 do
 you
 have
 to
 to
 then
 be


able
 to,
 you
 know,
 do
 the
 power
 law


thing
 a
 few
 more
 times?


I
 guess
 I
 think
 I
 think
 I
 I
 think
 less.


Um,
 and
 I
 I
 think
 that's
 probably
 true


even
 if
 we
 don't
 get
 like
 full-blown
 AGI


that's,
 you
 know,
 better
 than
 humans
 at


everything.
 I
 think
 you
 could
 easily


imagine
 a
 situation
 where
 of
 however


many
 million
 people
 are
 currently


employed
 as
 professional
 software


developers,
 some
 top
 tier
 of
 them
 that


do
 the
 hardest
 things
 can't
 be
 replaced.


But
 there's
 not
 that
 many
 of
 those,
 you


know,
 they
 and
 the
 the
 real
 like
 rank


and
 file,
 you
 know,
 the
 people
 that
 over


the
 last
 20
 years
 were
 told
 learn
 to


code,
 you
 know,
 that'll
 be
 your
 thing.


Like
 the
 people
 that
 are
 the
 really
 top


top
 people
 didn't
 need
 to
 be
 told
 to


learn
 to
 code,
 right?
 They
 just
 it
 was


their
 thing.
 They
 had
 a
 passion
 for
 it.


They
 were
 amazing
 at
 it.
 Um,
 we
 may
 not,


it
 wouldn't
 wouldn't
 shock
 me
 if
 we
 like


still
 can't
 replace
 those
 people
 in


three,
 four,
 five
 years
 time,
 but
 I


would
 be
 very
 surprised
 if
 you
 can't
 get


your
 nuts
 and
 bolts


web
 app,
 mobile
 app
 type
 things
 spit
 out


for
 you
 for
 far
 less
 um,
 and
 far
 faster


than
 and
 probably
 honestly
 with


significantly
 higher
 quality
 and
 and


less
 back
 and
 forth
 um,
 with
 an
 AI


system
 than
 you
 know
 with
 your
 kind
 of


middle
 of
 the
 pack
 developer
 um
 in
 that


time
 frame.
 One
 thing
 I
 do
 want
 to
 call


out,
 you
 know,
 there
 are
 definitely


people
 have
 concerns
 about
 progress


moving
 too
 fast,
 but
 there's
 also


concern
 and
 maybe
 it's
 rising
 about


progress
 not
 moving
 fast
 enough
 in
 the


sense
 that
 um
 you
 know
 a
 third
 of
 the


stock
 market
 is
 is
 mag
 7
 um
 you
 know
 AI


capex
 is
 you
 know
 over
 1%
 of
 GDP
 and
 so


we
 are
 kind
 of
 relying
 on
 some
 of
 this


progress
 in
 order
 to
 uh
 sort
 of
 estain


our
 sustain
 our
 economy.


>> Yeah.
 And
 with
 the
 um
 you
 know
 another


thing
 that
 I
 would
 say
 has
 been
 slower


to
 materialize
 than
 I
 would
 have


expected
 are


AI
 culture
 wars
 or
 you
 know
 sort
 of
 the


the


ramping
 up
 of
 protectionism
 of
 various


industries.
 We
 just
 saw
 um
 Josh
 Holly
 I


don't
 know
 if
 he
 introduced
 a
 bill
 or


just
 said
 he
 intends
 to
 introduce
 a
 bill


to
 ban
 self-driving
 cars
 nationwide.


>> Um


you
 know
 God
 help
 me.
 Uh,


I've
 dreamed
 of
 self-driving
 cars
 since


I
 was
 a
 little
 kid.
 Truly,
 like
 sitting


at
 red
 lights,
 I
 used
 to
 be
 like,


there's
 got
 to
 be
 a
 way.
 I
 think
 we
 took


away
 together.


>> Yeah.
 And
 it's
 it's
 so
 good.
 Um,
 and
 the


safety,
 you
 know,
 no,
 I
 think
 whenever


people
 want
 to
 argue
 about
 jobs,
 it's


going to
 be
 pretty
 hard
 to
 say


>> 30,000
 Americans
 should
 die
 every
 year


uh
 so
 that
 people's
 incomes
 don't
 get


disrupted.
 It
 seems
 like
 you
 have
 to
 be


able
 to
 get
 over
 that
 hump
 and
 say
 like


the,
 you
 know,
 saving
 all
 these
 lives
 if


nothing
 else
 is
 just
 really
 hard
 to
 uh


to
 argue
 against.
 But
 we'll
 see,
 you


know,
 I
 mean,
 he's
 uh
 not
 uh
 without


influence
 obviously.
 So
 yeah,
 I
 mean
 I


am
 uh
 very
 much
 on
 team
 abundance
 and


you
 know
 my
 old
 mantra
 I've
 been
 saying


this
 less
 lately
 but
 adoption


accelerationist
 hyperscaling
 pauser


the


tech
 that
 we
 have
 you
 know
 could
 do
 so


so
 much
 for
 us
 even
 as
 is
 I
 think
 if
 if


progress
 stopped
 today
 I
 still
 think
 we


could
 get
 to
 50
 to
 80%
 of
 work
 automated


over
 the
 next
 like
 five
 to
 10
 It
 would


be
 a
 real
 slog.
 You'd
 have
 a
 lot
 of,
 you


know,
 co-scientist
 type
 breakdowns
 of


complicated
 tasks
 to
 do.
 You'd
 have
 a


lot
 of
 work
 to
 do
 to
 go
 sit
 and
 watch


people
 and
 say,
 "Why
 are
 you
 doing
 it


this
 way?
 What's
 going
 on
 here?
 What's


this?
 You
 handled
 this
 one
 differently?


Why
 did
 you
 handle
 that
 one


differently?"
 All
 this
 uh
 tacet


knowledge
 that
 people
 have
 and
 the
 kind


of
 knowhow,
 procedural
 um
 you
 know,
 just


instincts
 that
 they've
 developed
 over


time,
 those
 are
 not
 documented
 anywhere.


They're
 not
 in
 the
 training
 data.
 So
 the


AI
 haven't
 had
 a
 chance
 to
 learn
 them.


But
 again,
 no,
 if
 I
 when
 I
 say
 like
 no


breakthroughs,
 I
 I
 still
 am
 allowing


there
 for
 like,
 you
 know,
 fine-tuning
 of


things
 to
 just
 like
 capabilities
 we
 have


that
 haven't
 been
 applied
 to
 particular


problems
 yet.


>> Um
 so
 just
 going
 through
 the
 economy
 and


and
 just
 sitting
 with
 people
 and
 being


like
 why
 are
 you
 doing
 this?
 You
 know,


let's
 let's
 document
 this.
 Let's
 get
 the


you
 know,
 the
 model
 to
 learn
 your


particular
 niche
 thing.
 Um
 that
 would
 be


a
 real
 slog
 and
 in
 some
 ways
 I
 kind
 of


wish
 that
 were
 the
 future
 that
 we
 were


going
 to
 get.
 Uh
 because
 it
 would
 be
 a


methodical,
 you
 know,
 kind
 of


one
 step,
 one
 foot
 in
 front
 of
 the


other,
 you
 know,
 no
 quantum
 leaps.
 Like


it
 would
 probably
 feel
 pretty


manageable,
 I
 would
 think,
 in
 terms
 of


the
 pace
 of
 change.
 Hopefully
 society


could,
 you
 know,
 could
 absorb
 that
 and


kind
 of
 adapt
 to
 it
 as
 we
 go
 without,


you
 know,
 one
 day
 to
 the
 next
 like,
 oh


my
 god,
 you
 know,
 all
 the
 drivers,
 you


know,
 are
 are
 getting
 replaced
 or
 that


one
 would
 be
 a
 little
 slower
 because
 you


do
 have
 to
 have
 the
 actual
 physical


buildout.
 Uh
 but
 in
 some
 of
 these


things,
 you
 know,
 customer
 service
 could


get
 rampant
 down
 real
 fast,
 right?
 Like


if
 a
 call
 center
 has
 something
 that
 they


can
 just
 drop
 in
 and
 it's
 like
 this


thing
 now
 answers
 the
 phones
 and
 talks


like
 a
 human
 and
 has
 a
 higher
 success


rate
 and
 scales
 up
 and
 down.
 Um
 one


thing
 we've
 seen
 at
 Wayark,
 small


company,
 right?
 We've
 always
 prided


ourselves
 on
 customer
 service.
 We
 do
 a


really
 good
 job
 with
 it.
 Our
 customers


really
 love
 our
 customer
 success
 team.


But
 I
 looked
 at
 our
 intercom
 data
 and
 it


takes
 us
 like
 half
 an
 hour
 to
 resolve


tickets.
 Uh
 we
 respond
 really
 fast.
 We


respond
 in
 like
 under
 two
 minutes
 most


of
 the
 time.
 But
 when
 we
 respond,
 you


know,
 2
 minutes
 is
 still
 long
 enough


that
 the
 person
 has
 gone
 on
 to
 do


something
 else,
 right?
 It's
 the
 same


thing
 as
 with
 the
 cursor
 thing
 that
 we


were
 talking
 about
 earlier,
 right?


They've
 tabbed
 over
 to
 something
 else.


So
 now
 we
 get
 the
 response
 back
 in
 two


minutes,
 but
 they
 are
 doing
 something


else.
 So
 then
 they
 come
 back
 at,
 you


know,
 minute
 six
 or
 whatever.
 and
 then


they
 respond.
 But
 now
 our
 person
 has


gone
 and
 done
 something
 else.
 So
 the


resolution
 time
 even
 for
 like
 simple


stuff
 can
 be
 easily
 a
 half
 an
 hour
 and


the
 AI,
 you
 know,
 it
 just
 responds


instantly,
 right?
 So
 you
 don't
 have
 to


have
 that
 kind
 of
 back
 and
 forth.
 You're


just
 in
 and
 out.
 So
 I
 do
 think
 some
 of


these
 categories
 could
 be
 really
 fast


changes.
 Um,
 others
 will
 be
 slower.


But
 yeah,
 I
 mean,
 I
 kind
 of
 wish
 we
 had


that
 um
 I
 kind
 of
 wish
 we
 had
 that


slower
 path
 in
 front
 of
 us.
 My
 best


guess
 though
 is
 that
 we
 will
 probably


continue
 to
 see
 things
 that
 will
 be


significant
 leaps
 and
 that
 there
 will
 be


like
 actual
 disruption.
 Another
 one


that's
 come
 to
 mind
 recently,
 you
 know,


maybe
 we
 can
 get
 the
 abundance


department
 on
 these
 new
 antibiotics.


Have
 you
 seen
 this
 uh
 development?
 No.


Tell
 us
 about
 it.
 I
 mean,
 it's
 not
 a


language
 model.
 I
 think
 that's
 another


thing
 people
 really
 underappreciate
 or


that
 you
 you
 could
 kind
 of
 look
 back
 at


GPT4
 to
 5
 and
 then
 imagine
 a
 pretty
 easy


extension
 of
 that.
 So,
 GPT4


initially
 when
 it
 launched
 the
 we
 didn't


have
 image
 um
 understanding
 capability.


They
 did
 demo
 it
 at
 the
 time
 of
 the


launch,
 but
 it
 wasn't
 released
 for
 some


months
 later.
 The
 first
 version
 that
 we


had
 could
 understand
 images,
 could
 do
 a


pretty
 good
 job
 of
 understanding
 images,


still
 with
 like
 jagged
 capabilities
 and


whatever.
 Um,
 now
 with
 the
 new
 nano


banana
 from
 Google,
 you
 have
 this
 like


basically
 Photoshop
 level
 ability
 to


just
 say,
 "Hey,
 take
 this
 thumbnail."


Like
 we
 could
 take
 our
 two
 uh
 feeds


right
 now,
 you
 know,
 take
 a
 snapshot
 of


you,
 a
 snapshot
 of
 me,
 put
 them
 both


into
 Nano
 Banana
 and
 say,
 generate
 the


thumbnail
 for
 the
 YouTube
 preview


featuring
 these
 two
 guys,
 put
 them
 in


the
 same
 place,
 same
 background,


whatever.
 It'll
 mash
 that
 up.
 You
 can


even
 have
 it,
 you
 know,
 put
 text
 on
 top,


progress
 since
 GPT4,
 whatever
 we
 want
 to


call
 it.
 Um,
 GPD5
 is
 not
 a
 bust.
 Uh,
 and


it'll
 spit
 that
 out.
 And
 you
 see
 that
 it


has
 this
 deeply
 integrated
 understanding


that
 bridges
 language
 and
 image.
 And


that's
 something
 that
 it
 can
 take
 in,


but
 now
 it's
 also
 something
 it
 can
 put


out
 as
 all
 as
 part
 of
 one
 core
 model


with
 like
 a
 single
 unified
 intelligence


that
 I
 think
 is
 going
 to
 come
 to
 a
 lot


of
 other
 things.
 Um,
 we're
 at
 the
 point


now
 with
 these
 biology
 models
 and


material
 science
 models
 where
 they're


kind
 of
 like
 the
 image
 generation
 models


of
 a
 couple
 years
 ago.
 They
 can
 take
 a


real
 simple
 prompt
 and
 they
 can
 do
 a


generation,
 but
 they're
 not
 deeply


integrated
 where
 you
 can
 have
 like
 a


true
 conversation
 back
 and
 forth
 um
 and


have
 that
 kind
 of


unified
 understanding
 that
 bridges


language
 and
 these
 other
 modalities.
 But


even
 so,
 it's
 been
 enough
 for
 this
 group


at
 MIT
 to
 use
 some
 of
 these


relatively,
 you
 know,
 narrow


purpose-built
 biology
 models
 and
 create


totally
 new
 antibiotics.
 New
 in
 the


sense
 that
 they
 have
 a
 new
 mechanism
 of


action
 like
 they're
 they're
 affecting


the
 bacteria
 in
 a
 new
 way.
 And


uh
 notably,
 they
 they
 do
 work
 on
 um


antibiotic
 resistant
 bacteria.
 This
 is


some
 of
 the
 first
 new
 antibiotics
 we've


had
 in
 a
 long
 time.
 Now
 they're
 going
 to


have
 to
 go
 through,
 you
 know,
 when
 I
 say


that
 get
 the
 abundance
 department
 on
 it,


it's
 like
 where's
 my
 operation
 warp


speed
 for
 these
 new
 antibiotics,
 right?


Like
 we've
 got
 people
 dying
 in
 hospitals


from
 drugresistant
 strains
 all
 the
 time.


Um


why
 is
 nobody,
 you
 know,
 crying
 about


this?
 I
 think
 one
 of
 the
 things
 that's


happening
 to
 our
 society
 in
 general
 is


just
 so
 many
 things
 are
 happening
 at


once.
 It's
 kind
 of
 the
 it's
 like
 the


flood
 the
 zone
 thing
 except
 like
 there's


so
 many
 AI
 developments
 flooding
 the


zone
 that
 nobody
 can
 even
 keep
 up
 with


all
 of
 those.
 And
 that's
 that's
 come
 for


me
 by
 the
 way
 too.
 I
 would
 say
 two
 years


ago
 I
 was
 like
 pretty
 in
 command
 of
 all


the
 news
 and
 a
 year
 ago
 I
 was
 starting


to
 lose
 it
 and
 now
 I'm
 like
 wait
 a


second
 there
 was
 new
 antibiotics


developed
 you
 know
 and
 I'm
 kind
 of
 um


missing
 things
 you
 know
 just
 like


everybody
 else
 despite
 my
 best
 efforts.


But
 key
 point
 there
 is
 AI
 is
 not


synonymous
 with
 language
 models.
 There


are
 AIs
 being
 developed
 with
 pretty


similar
 architectures
 for
 a
 wide
 range


of
 different
 modalities.


We
 have
 seen
 this
 play
 out
 with
 text
 and


image
 where
 you
 had
 your
 textonly
 models


and
 you
 had
 your
 image
 only
 models
 and


then
 they
 started
 to
 come
 together
 and


now
 they've
 come
 really
 deeply
 together.


And
 so
 I
 think
 you're
 going
 to
 see
 that


across
 a
 lot
 of
 other
 modalities
 over


time
 as
 well.
 Um
 and
 there's
 a
 lot
 more


data
 there.
 you
 know,
 we
 might
 I
 don't


know
 what
 it
 means
 to
 like
 run
 out
 of


data.
 Um,
 in
 the
 reinforcement
 learning


paradigm,
 there's
 always
 more
 problems,


right?
 There's
 always
 some
 something
 to


go
 figure
 out.
 There's
 always
 something


to
 go
 engineer.
 The
 feedback
 is
 starting


to
 come
 from
 reality,
 right?
 That
 was


one
 of the
 things
 Elon
 talked
 about
 on


the
 Groc
 4
 launch
 was
 like
 maybe
 we're


running
 out
 of
 problems
 we've
 already


solved
 and
 you
 know,
 we
 only
 have
 so


much
 of
 those
 sitting
 around
 in


inventory.
 You
 only
 have
 one
 internet,


you
 know,
 we
 only
 have
 so
 much
 of
 that


stuff.
 But
 over
 at
 Tesla,
 over
 at


SpaceX,
 like
 we're
 solving
 hard


engineering
 problems
 on
 a
 daily
 basis,


and
 they
 seem
 to
 be
 never
 ending.
 So


when
 we
 start
 to
 give
 the
 next


generation
 of
 the
 model
 these
 power


tools,
 the
 same
 power
 tools
 that
 the


professional
 engineers
 are
 using
 at


those
 companies
 to
 solve
 those
 problems


and
 the
 AI
 start
 to
 learn
 those
 tools


and
 they
 start
 to
 solve


previously
 unsolved
 engineering


problems,
 like
 that's
 going
 to
 be
 a


really
 powerful
 signal
 that
 they
 will
 be


able
 to
 learn
 from.
 And
 now
 again
 fold


in
 that
 those
 other
 modalities
 right
 the


ability
 to
 have
 sort
 of
 a
 sixth
 sense


for
 you
 know
 the
 space
 of
 material


science
 possibilities
 when
 you
 can


bridge
 or
 or
 unify
 the
 understanding
 of


language
 and
 those
 other
 things
 I
 think


you
 start
 to
 have
 something
 that
 looks


kind
 of
 like
 super
 intelligence
 even
 if


it's
 like
 not
 able
 to
 you
 know
 write


poetry
 at
 a
 a
 superhuman
 level


necessarily
 its
 ability
 to
 see
 in
 these


other
 spaces
 is
 going
 to
 be
 truly
 a


superhuman
 uh
 thing
 that
 I
 think
 will
 be


pretty
 hard
 to
 miss.


>> You
 said
 that
 that
 was
 one
 thing
 that


Cal's
 analysis
 missed
 is
 just
 the
 lack


of
 appreciation
 for
 non-
 language


modalities
 and
 and
 how
 they're
 driving


some
 of
 the
 innovations
 that
 you're


talking
 about.


>> Yeah.
 I
 think
 people
 are
 often
 just
 kind


of
 equating
 the
 chatbot
 experience
 with


AI
 broadly.


>> Yeah.


>> And
 you
 know
 that
 that
 that
 uh


conflation
 will
 not
 last
 probably
 too


much
 longer
 because
 we
 are
 going
 to
 see


self-driving
 cars
 unless
 they
 get


banned.
 Um
 and
 that's
 a
 you
 know
 very


different
 kind
 of
 thing.
 And
 talk
 about


your
 impact
 on
 jobs
 too,
 right?
 It's


like
 what
 four
 or
 five
 million


professional
 drivers
 in
 the
 United


States.
 Um
 that
 is
 a
 big
 that
 is
 a
 big


deal.
 I
 don't
 think
 most
 of
 those
 folks


are
 going
 to
 be
 super
 keen
 to
 learn
 to


code
 and
 even
 if
 they
 do
 learn
 to
 code,


you
 know,
 I'm
 not
 sure
 how
 long
 that's


going
 to
 last.
 So
 that's
 going
 to
 be
 a


disruption.
 And
 then
 general
 robotics
 is


like
 not
 that
 far
 behind.
 you
 know,
 the


and
 this
 is
 one
 area
 where
 I
 do
 think


China
 might
 be
 actually
 ahead
 of
 the


United
 States
 right
 now,
 but
 regardless


of
 whether
 that's
 true
 or
 not,
 you
 know,


these
 robots
 are
 getting
 really
 quite


good,
 right?
 They
 can
 like
 walk
 over
 all


these
 obstacles.
 I
 mean,
 these
 are


things
 that
 a
 few
 years
 ago
 they
 just


couldn't
 do
 at
 all.
 You
 know,
 they
 they


could
 barely
 balance
 themselves
 and
 walk


a
 few
 steps
 under
 ideal
 conditions.
 Now


you've
 got
 things
 that
 you
 can
 like


literally
 do
 a
 flying
 kick
 and
 it'll


like
 absorb
 your
 kick
 and
 shrug
 it
 off


and
 just
 keep
 going.
 Uh,
 you
 know,
 write


itself
 and
 and
 uh
 continue
 on
 its
 way.


Super
 rocky,
 you
 know,
 uneven
 terrain,


all
 these
 sorts
 of
 things
 are
 getting


quite
 good.
 Um,


you
 know,
 the
 same
 thing
 is
 working


everywhere.
 I
 think
 one
 of
 the
 other


thing
 that's
 kind
 of


there's
 always
 a
 lot
 of
 detail
 to
 the


work.
 So
 it's
 it's
 a
 sort
 of
 inside


view,
 outside
 view,
 right?
 Inside
 view,


you're
 like
 there's
 always
 this
 minutia.


There's
 always,
 you
 know,
 these
 problems


that
 we
 had
 and
 things
 we
 had
 to
 solve,


but
 you
 zoom
 out
 and
 it
 looks
 to
 me
 like


the
 same
 basic
 pattern
 is
 working


everywhere.
 And
 that
 is
 like
 if
 we
 can


just
 gather
 enough
 data
 to
 do
 some


pre-training,
 you
 know,
 some
 kind
 of
 raw


rough,
 you
 know,
 not
 very
 useful,
 but


just
 enough
 at
 least
 to
 kind
 of
 get
 us


going,
 then
 we're
 in
 the
 game.
 And
 then


once
 we're
 in
 the
 game
 now
 we
 can
 do


this
 flywheel
 thing
 of
 like
 you
 know


rejection
 sampling
 like
 have
 it
 try
 a


bunch
 of
 times
 take
 the
 ones
 where
 it


succeeded
 you
 know
 refine
 tune
 on
 that


the
 RHF
 you
 know
 feedback
 the
 the
 sort


of
 preference
 take
 two
 which
 one
 was


better
 find
 you
 know
 fine-tune
 on
 that


the
 reinforcement
 learning
 all
 these


techniques
 that
 have
 been
 developed
 over


the
 last
 few
 years
 seems
 to
 me
 they're


absolutely
 going
 to
 apply
 to
 a
 problem


like
 a
 humanoid
 robot
 as
 well
 and
 that's


not
 to
 say
 there
 won't
 be
 a
 you
 know
 a


lot
 of
 work
 to
 figure
 out
 exactly
 how
 to


do
 that.
 But
 I
 think
 the
 big
 difference


between
 language
 and
 robotics


is
 really
 mostly
 that
 there
 just
 wasn't


a
 huge
 repository
 of
 data
 to
 train
 the


robots
 on
 at
 first.
 And
 so
 you
 had
 to
 do


a
 lot
 of
 hard
 engineering
 to
 make
 it


work
 at
 all,
 you
 know,
 to
 even
 stand
 up,


right?
 You
 had
 to
 have
 all
 these
 control


systems
 and
 whatever
 um
 because
 there


was
 nothing
 for
 them
 to
 learn
 from
 in


the
 way
 that
 the
 language
 models
 could


learn
 from
 the
 internet.
 But
 now
 that


they're
 working
 at least
 a
 little
 bit,


you
 know,
 I
 think
 all
 these
 kind
 of


refinement
 techniques
 are
 going
 to
 work.


It'll
 be
 interesting
 to
 see
 if
 they
 can


get
 the
 air
 rate
 low
 enough
 that
 I'll


actually
 like
 allow
 one
 in
 my
 house


around
 my
 kids.
 Um,
 you
 know,
 that


they'll
 probably
 be
 um
 better
 deployed


in
 like
 factory
 settings
 first,
 more


controlled
 environments
 than
 the
 chaos


of
 my
 house
 as
 you,
 you
 know,
 have
 seen


in
 this
 uh
 in
 this
 recording.
 But


I
 do
 think
 they're
 going
 to
 they're


going
 to
 work.
 What's
 the
 state
 of


agents
 more
 more
 broadly
 uh
 at
 the


moment?
 Where
 do
 you
 how
 do
 you
 see


things
 playing
 out?
 Where
 do
 you
 see
 it


go?


>> Well,
 broadly
 I
 think
 you
 know
 we're


it's
 the
 the
 task
 length
 story
 from


meter
 of
 the
 you
 know
 every
 seven
 months


or
 every
 four
 months
 doubling
 time.


We're
 at
 2
 hoursish
 with
 GBT5.
 Replet


just
 said
 their
 new
 agent
 v3
 can
 go
 200


minutes.
 That
 if
 that's
 true
 that
 would


even
 be
 a
 new
 you
 know
 high
 point
 on
 the


um


on
 that
 graph.
 Again,
 it's
 a
 little
 bit


sort
 of
 apples
 to
 oranges
 because


they've
 done
 a
 lot
 of
 scaffolding.
 How


much
 have
 they
 broken
 it
 down?
 Like,
 how


much
 scaffolding
 are
 you
 allowed
 to
 do,


you
 know,
 with
 these
 things
 before
 you


sort
 of
 are
 off
 of
 their
 chart
 and
 onto


maybe
 a
 different
 chart.
 But
 if
 you


extrapolate
 that
 out
 a
 bit
 and
 you're


like,
 okay,
 take
 take
 the
 fourmonth
 case


just
 to
 be
 a
 little
 aggressive.
 Um,


that's
 three
 doublings
 a
 year.
 That's
 8x


task
 length
 increase
 per
 year.
 That


would
 mean
 you
 go
 from
 2
 hours
 now
 to
 2


days
 in
 one
 year
 from
 now.
 And
 then
 if


you
 do
 another
 8x
 on
 top
 of
 that,
 you're


looking
 at
 basically
 say
 2
 days
 to
 two


weeks
 of
 work
 in
 two
 years.


That
 would
 be
 a
 big
 deal,
 you
 know,
 to


say
 the
 least.
 If
 you
 could
 delegate
 an


AI
 two
 weeks
 worth
 of
 work
 and
 have
 it


do
 a,
 you
 know,
 even
 half
 the
 time,


right?
 The
 meter
 thing
 is
 that
 they
 will


succeed
 half
 the
 time
 on
 tasks
 of
 that


size.
 But
 if
 you
 could
 take
 a
 two-eek


task
 and
 have
 a
 50%
 chance
 that
 an
 AI


would
 be
 able
 to
 do
 it,
 even
 if
 it
 did


cost
 you
 a
 couple
 hundred
 bucks,
 right?


It's
 like,
 well,
 that's
 again
 a
 lot
 less


than
 it
 would
 cost
 to
 hire
 a
 human
 to
 do


it.
 Um,
 and
 it's
 all
 on
 demand.
 It's


kind
 of,
 you
 know,
 it's
 immediately


available.
 Um,
 if
 I'm
 not
 using
 it,
 I'm


not
 paying
 anything.
 Transaction
 costs


are
 just
 like
 a
 lot
 lower.
 The
 whole,


you
 know,
 the
 many,
 many
 other
 aspects


are
 favorable
 for
 the
 AI
 there.
 So,
 you


know,
 that
 would
 suggest
 that
 you'll
 see


a
 huge
 amount
 of
 automation
 in
 in
 all


kinds
 of
 different
 places.
 The
 other


thing
 that
 I'm
 watching
 though
 is
 the


reinforcement
 learning
 does
 seem
 to


bring
 about
 a
 lot
 of
 bad
 behaviors.
 Re


um
 reward
 hacking
 being
 one.
 You
 know,


the
 the
 any
 sort
 of
 gap
 between
 what
 you


are
 rewarding
 the
 model
 for
 and
 what
 you


really
 want
 can
 become
 a
 big
 issue.
 Um,


we've
 seen
 this
 in
 coding
 in
 many
 cases


where
 the
 AI
 will
 claude
 is
 like


notorious
 for
 this
 will
 put
 out
 a
 unit


test
 that
 always
 passes,
 you
 know,
 that


just
 has
 like
 return
 true
 in
 the
 unit


test.
 Why
 is
 it
 doing
 that?
 Like,
 well,


uh,
 it
 must
 have
 learned
 that
 what
 we


want
 is
 for
 unit
 tests
 that
 pass.
 You


know,
 we
 want
 it
 to
 pass
 unit
 tests.


Well,
 we
 didn't
 mean
 to
 write
 fake
 unit


tests
 that
 always
 pass,
 but
 that


technically
 did,
 you
 know,
 satisfy
 the


reward
 condition.
 And
 so
 we're
 seeing


those
 kind
 of
 weird
 behaviors.


With
 that
 comes
 this
 like
 scheming
 kind


of
 stuff.
 We
 we
 don't
 really
 have
 a


great
 handle
 on
 that
 yet.


There
 is
 also
 situational
 awareness
 that


seems
 to
 be
 on
 the
 rise,
 right?
 Where


the
 models
 are
 like
 increasingly
 in


their
 chain
 of
 thought.
 You're
 seeing


things
 like
 this
 seems
 like
 I'm
 being


tested.
 Um
 you
 know
 maybe
 I
 should
 be


conscious
 of
 what
 my
 tester
 is
 really


looking
 for
 here.
 And
 that
 makes
 it
 hard


to
 evaluate
 models
 in
 tests
 because
 you


don't
 know
 if
 they're
 actually
 going
 to


behave
 the
 same
 way
 when
 they're
 out
 in


the
 real
 world.
 So
 those,
 you
 know,
 I


wouldn't
 say
 this
 is
 a
 high
 level
 uh
 or


high
 confidence
 prediction,
 but
 like
 one


model
 of
 the
 future
 I've
 been
 playing


with
 is
 the
 task
 length
 keeps
 doubling


while
 at
 the
 same
 time
 these
 weird


behaviors


pop
 up
 and
 then
 are
 suppressed.
 And
 we


have
 seen
 in
 the
 Cloud
 4
 and
 in
 the
 GP5


system
 cards,
 Cloud
 4
 reported,
 I
 think


a
 twothirds
 reduction
 in
 um
 reward


hacking
 and
 you
 know
 in
 GBD5
 they


reported
 a
 few
 different
 dimensions
 but


you
 know
 say
 something
 similar
 um


reduction
 in
 deceptive
 behavior


but
 those
 behaviors
 kind
 of
 just


emerged.
 So,
 it's
 sort
 of
 like
 weird


behavior
 emerges,
 then
 they
 sort
 of


figure
 out
 how
 to
 tamp
 it
 down,
 but
 not


entirely.
 Presumably,
 in
 the
 next


generation,
 they'll
 like
 tamp
 it
 down


some
 more,
 but
 maybe
 some
 new
 additional


weird
 behavior
 could
 emerge,
 and
 then


they'll
 have
 to
 kind
 of
 tamp
 that
 one


down.
 All
 the
 while,
 the
 the
 tasks
 are


expanding
 in
 scope,
 you
 know,
 with
 a


every
 four
 months
 doubling.
 So,
 you


could
 end
 up
 in
 a
 world
 where
 you
 can


delegate
 really
 like
 major
 things
 to


AIS,
 but
 there's
 some


small


but
 not
 necessarily
 totally
 vanishing


chance
 that
 it
 like


actively
 screws
 you
 over
 in
 the
 way
 that


it
 is
 trying
 to
 do
 that
 task.
 And
 so
 you


have
 a,
 you
 know,
 maybe
 it's
 like,
 okay,


I
 think
 here
 I'm
 about
 to
 get
 two
 weeks


worth
 of
 work
 done
 for
 a hundred
 bucks.


Okay,
 that's
 amazing.
 But
 there's
 also
 a


one
 in
 10,000
 chance
 that
 it
 like


legitimately,
 you
 know,
 attacks
 me
 in
 a


in
 a
 like
 meaningful
 way.
 Some
 of
 the


things
 that
 we
 have
 seen,
 these
 are


fairly
 famous
 at
 this
 point,
 but
 in
 the


cloud
 for
 system
 card,
 they
 reported


blackmailing
 of
 the
 human.
 The
 the
 setup


was
 that
 the
 AI
 had
 access
 to
 the


engineers
 email
 and


they
 told
 the
 AI
 that
 it
 was
 going to
 be


like
 replaced
 with
 a,
 you
 know,
 a
 less


ethical
 version
 or
 something
 like
 that.


It
 didn't
 want
 that
 and
 it
 found
 in
 the


engineer's
 email
 that
 the
 engineer
 was


having
 an
 affair.
 So
 it
 started
 to


blackmail
 the
 engineer
 to
 so
 as
 to
 avoid


being
 replaced
 with
 a
 less
 ethical


version.


>> People
 I
 think
 are
 way
 too
 quick
 in
 my


view
 to


>> move
 past
 these
 uh
 anecdotes.
 People
 are


sort
 of
 often
 like
 well
 you
 know
 they


set
 it
 up
 that
 way
 and
 you
 know
 that's


not
 really
 realistic.
 But


another
 one
 was
 whistleblowing.
 You


know,
 there
 was
 another
 thing
 where
 they


sort
 of
 set
 up
 this
 dynamic
 where
 there


was
 some,
 you
 know,
 unethical,
 illegal


behavior
 going
 on
 and
 again,
 the
 model


had
 access
 to
 this
 data
 and
 it
 decided


to
 just
 email
 the
 FBI
 and
 and
 tell
 uh


the
 FBI
 about
 it.
 So,
 first
 of
 all,
 I


don't
 think
 we
 really
 know
 what
 we
 want.


You
 know,
 to
 some
 degree,
 maybe
 you
 do


want
 AIS
 to
 report
 certain
 things
 to


authorities.
 Um,
 that
 could
 be
 one
 way


to
 think
 about
 the
 bioweapon
 risk,
 you


know,
 is
 like
 not
 only
 should
 the
 models


refuse,
 but
 maybe
 they
 should
 report
 you


to
 the
 authorities
 if
 you're
 actively


trying
 to
 create
 a
 bioweapon.
 Um,
 I


certainly
 don't
 want
 them
 to
 be
 doing


that
 too
 much.
 I
 don't
 want
 to
 live


under
 the,
 you
 know,
 surveillance
 of
 um,


Claude
 five
 that's
 always
 going
 to
 be,


you
 know,
 threatening
 to
 turn
 me
 in.
 But


I
 do
 sort
 of
 want
 some
 people
 to
 be


turned
 in
 if
 they're
 doing
 sufficiently


bad
 things.
 We
 don't
 have
 a
 good


resolution
 societywide


on,
 you
 know,
 what
 we
 want
 the
 models
 to


even
 do
 in
 those
 situations.
 Um,


and
 I
 think
 it's
 also,
 you
 know,
 it's


like,
 yes,
 it
 was
 set
 up,
 yes,
 it
 was


research,
 but
 it's
 a
 big
 world
 out


there,
 right?
 We
 got
 a
 billion
 users


already
 on
 these
 things
 and
 we're


plugging
 them
 in
 to
 our
 email,
 so


they're
 going
 to
 have
 very
 deep
 access


to
 information
 about
 us.
 You
 know,
 I


don't
 know
 what
 you've
 been
 doing
 in


your
 email.
 Well,
 I
 don't
 I
 hope
 there's


nothing
 too
 crazy
 in
 mind,
 but
 like
 now


I
 got
 to
 think
 about
 it
 a
 little
 bit,


right?
 What
 what
 did
 I
 have
 I
 ever
 done


anything
 that
 I,
 you
 know,
 geez,
 I
 don't


know.
 Um
 or
 or
 even
 that
 it
 could


misconrue,
 right?
 Like
 it's
 obviously


not
 um
 maybe
 I
 didn't
 even
 really
 do


anything
 that
 bad,
 but
 it
 just


misunderstands
 what
 exactly
 was
 going


on.
 So


that
 could
 be
 a
 weird,
 you
 know,
 if


there's
 one
 thing
 that
 could
 kind
 of


stop
 the


agent
 momentum
 in
 my
 view,
 it
 could
 be


like


the


one
 in
 10,000
 or
 whatever,
 you
 know,
 we


ultimately
 kind
 of
 push
 the
 the
 really


bad
 behaviors
 down
 to
 is
 maybe
 still


just
 so
 spooky
 to
 people
 that
 they're


like,


I
 can't
 deal
 with
 that,
 you
 know,
 and


that
 might
 be
 hard
 to
 resolve.
 So,


well,
 you
 know,
 what
 happens
 then?
 Um,


you
 know,
 it's
 hard
 to
 check
 two
 weeks


worth
 of
 work
 every
 couple
 hours
 or


whatever,
 right?
 Like
 that's
 part
 of


where
 the
 where
 the
 whole
 then
 you
 bring


another
 AI
 in
 to
 check
 it.
 You
 know,


that's
 again
 where
 you
 start
 to
 get
 to


the
 now
 I
 see
 why
 we
 need
 more


electricity
 and
 and
 7
 trillion
 of


buildout
 is
 yikes.
 You
 know,
 they're


going
 to
 be
 producing
 so
 much
 stuff.


I
 can't
 possibly
 even
 review
 it
 all.
 I


need
 to
 rely
 on
 another
 U
 AI
 to
 help
 me


do
 the
 review
 of
 the
 first
 AI
 to
 make


sure
 that
 if
 it
 is
 trying
 to
 screw
 me


over,
 you
 know,
 somebody's
 catching
 it,


I
 can't
 monitor
 that
 myself.
 I
 think


Redwood
 Research
 is
 doing
 some
 really


interesting
 stuff
 like
 this
 where
 they


are
 trying
 to
 get
 systematic
 on
 like,


okay,
 let's
 just
 assume
 this
 is
 quite
 a


different
 quite
 a
 departure
 from
 the


traditional
 AI
 safety
 work
 where
 the
 you


know
 the
 big
 idea
 traditionally
 was


let's
 figure
 out
 how
 to
 align
 the


models,
 make
 them
 safe,
 you
 know,
 make


them
 not
 do
 bad
 things.
 Great.
 Redwood


Research
 has
 taken
 the
 other
 angle,


which
 is
 let's
 assume
 that
 they're
 going


to
 do
 bad
 stuff.
 They're
 going
 to
 be
 out


to
 get
 us
 at
 times.
 Um,
 how
 can
 we
 still


work
 with
 them
 and
 get
 productive
 output


and,
 you
 know,
 get
 value


without,
 you
 know,
 uh,
 fixing
 all
 those


problems
 and
 that
 involves
 like
 again


all
 these
 sort
 of
 AI
 supervising
 other


AIs
 and,
 um,
 crypto
 might
 have
 a
 place


to
 to
 a
 role
 to
 play
 in
 this.
 Um,


another
 episode
 coming
 out
 soon
 with


Ilia
 Puluin,
 who's
 the
 founder
 of
 Near.


Really
 fascinating
 guy
 because
 he
 was


one
 of
 the
 eight


>> Yeah.


>> authors
 of
 the
 attention
 is
 all
 you
 need


paper.
 And
 then
 he's
 started
 this
 Near


company.
 It
 was
 originally
 an
 AI


company.
 They
 took
 a
 huge
 detour
 into


crypto
 because
 they
 were
 trying
 to
 hire


task
 workers
 around
 the
 world
 and


couldn't
 figure
 out
 how
 to
 pay
 them.
 So,


they
 were
 like,
 "This
 sucks
 so
 bad
 to


pay
 these
 task
 workers
 in
 all
 these


different
 countries
 that
 we're
 trying
 to


get
 data
 from
 that
 we're
 going
 to
 pivot


into
 a
 whole
 blockchain
 uh
 side
 quest.


Now,
 they're
 coming
 back
 to
 the
 AI
 thing


and
 their
 their
 tagline
 is
 the


blockchain
 for
 AI."
 And
 so
 you
 might
 be


able
 to
 get,
 you
 know,
 a
 certain
 amount


of


control
 from,
 you
 know,
 the
 the
 sort
 of


crypto
 security
 that
 the
 the
 blockchain


type
 technology
 can
 provide.
 But
 I
 could


see
 a
 scenario
 where
 the
 these
 the
 bad


behaviors
 just
 become
 so


costly
 when
 they
 do
 happen
 that
 people


kind
 of
 get
 spooked
 away
 from


using
 the
 frontier
 capabilities
 in
 terms


of
 just
 like
 how
 much
 you
 know
 work
 the


the
 AIS
 can
 do.


But
 that
 wouldn't
 be
 a
 that
 wouldn't
 be


a
 pure
 capability
 stall
 out.
 It
 would
 be


a
 we
 can't
 solve
 you
 know
 some
 of
 the


longtail
 safety
 issues.


>> Yeah.
 challenge
 and
 you
 know
 that
 if


that
 is
 the
 case
 then
 you
 know
 that'll


be
 um
 that'll
 be
 an
 important
 fact
 about


the
 world
 too.
 I
 I
 always
 nobody
 ever


seems
 to
 solve
 any
 of
 these
 things
 like


100%,
 right?
 They
 always
 every
 every


generation
 it's
 like
 well
 we
 reduced


hallucinations
 by
 70%
 or
 we
 reduced


deception
 by
 2/3
 we
 reduced
 um
 you
 know


scheming
 or
 or
 whatever
 by
 however
 much


but
 it's
 always
 still
 there
 you
 know
 and


it's
 and
 if
 you
 take
 the
 even
 you
 know


lower
 rate
 and
 you
 multiply
 it
 by
 a


billion
 users
 and
 thousands
 of
 queries
 a


month
 and
 agents
 running
 in
 the


background
 and
 processing
 all
 your


emails
 and
 you
 know
 all
 the
 deep
 access


that
 people
 sort
 of
 envision
 them


happening.
 It
 could
 be
 a
 pretty
 weird


world
 where
 there's
 just
 this
 sort
 of


negative
 lottery
 of
 like
 AI
 accidents.


Um,
 another
 episode
 coming
 up
 is
 with


the
 AI
 underwriting
 company
 and
 they
 are


trying
 to
 bring
 the
 insurance
 industry


and
 all
 the,
 you
 know,
 the
 wherewithal


that's
 been
 developed
 there
 to
 price


risk,
 figure
 out
 how
 to,
 you
 know,


create
 standards,
 you
 know,
 what
 can
 we


allow,
 what
 sort
 of
 guardrails
 do
 we


have
 to
 have
 to
 be
 able
 to
 ensure
 this


kind
 of
 thing
 in
 the
 first
 place.
 Um
 so


that
 that'll
 be
 another
 really


interesting
 area
 to
 watch
 is
 like
 can
 we


sort
 of
 financialize
 those
 risks
 um
 in


the
 same
 way
 we
 have
 you
 know
 with
 car


accidents
 and
 all
 all
 these
 other


mundane
 things
 but
 the
 the
 space
 of
 car


accidents
 is
 only
 so
 big
 the
 space
 of


weird
 things
 that
 AIs
 might
 do
 to
 you
 um


you
 know
 as
 they
 have
 weeks
 worth
 of


runway
 is
 much
 bigger
 and
 so
 it's
 it's


going
 to
 be
 a
 hard
 challenge
 but
 you


know
 people
 are
 people
 are
 working
 we've


got
 some
 of
 our
 best
 people
 working
 on


it.
 What
 do
 you
 make
 of
 the
 claim
 that


80%
 of
 AI
 startups
 have
 Chinese
 open


models?
 Um,
 and
 what
 do
 you
 make
 of
 the


claim
 and
 and
 the
 implications?
 I
 think


that
 may
 be
 that
 probably
 is
 true
 with


the
 one
 caveat
 that
 it
 is
 only
 measuring


companies
 that
 are
 using
 open
 source


models
 at
 all.
 I
 think
 most
 companies


are
 not
 using
 open
 source
 models
 and
 I


would
 guess
 you
 know
 the
 vast
 majority


of
 tokens
 being
 processed
 by
 American
 AI


startups
 are


they're
 their
 API
 calls
 right
 to
 to
 the


usual
 suspects.
 Um
 so


weighted
 by
 actual
 usage
 I
 would
 say


still
 the
 majority
 as
 far
 as
 I
 could


tell
 would
 be
 going
 to
 commercial


models.
 Um,
 for
 those
 that
 are
 using


open
 source,
 I
 do
 think
 it's
 true
 that


the
 Chinese
 models
 have
 become
 the
 best.


Um,
 you
 know,
 the
 American


bench
 there
 was
 always
 kind
 of
 thin,


right?
 It
 was
 basically
 Meta
 that
 was


willing
 to
 put
 in
 huge
 amounts
 of
 money


and
 resources
 and
 then
 open
 source
 it.


You've
 got,
 you
 know,
 um,
 Paul
 Allen


funded
 group,
 the
 Allen
 Institute
 for


AI,
 AI
 too.
 um
 you
 know
 they're
 they're


doing
 good
 stuff
 too
 but
 they
 don't
 have


pre-training
 resources
 so
 they
 do
 you


know
 really
 good
 post-
 trainining
 and


and
 open
 source
 their
 recipes
 and
 all


that
 kind
 of
 stuff
 so
 it's
 not
 like


American
 open
 source
 is


bad
 you
 know
 and
 again
 it's
 the
 time
 the


this
 is
 another
 way
 in
 which
 I
 think
 you


can
 really
 validate
 that
 things
 are


moving
 quickly
 because
 if
 you
 take
 the


best
 American
 open
 source
 models
 and
 you


take
 them
 back
 a
 year
 they
 are
 probably


as
 good
 if
 not
 a
 little
 better
 than


anything
 that
 we
 had
 commercially


available
 at
 the
 time.


If
 you


compare
 to
 Chinese,
 you
 know,
 they
 have,


I
 think,
 uh,
 surpassed.
 So,
 there's
 been


like
 pretty
 clear
 change
 at
 the


frontier.
 I
 think
 that
 means
 that
 the


best
 Chinese
 models
 are
 like
 pretty


clearly
 better
 than
 anything
 we
 had
 a


year
 ago,
 um,
 commercial
 or
 otherwise.


So,


yeah,
 I
 mean,
 that
 just
 means
 like


things
 are
 moving.
 I
 think
 that's
 like


hopefully
 I've
 uh
 made
 that
 case


compellingly,
 but
 that's
 another
 data


point
 that
 I
 think
 makes
 it
 hard
 to
 you


I
 don't
 think
 you
 can
 believe
 both
 that


um
 the
 Chinese
 models
 are
 now
 the
 best


open
 source
 models
 and
 that
 AI
 has


stalled
 out
 and
 we
 haven't
 seen
 much


progress
 since
 GBT4.
 Like
 those
 seem
 to


be
 kind
 of
 contradictory
 notions.
 Um
 I


believe
 the
 the
 one
 that
 is
 wrong
 is
 the


lack
 of
 progress.
 In
 terms
 of
 what
 it


means,
 I
 mean


I
 don't
 really
 know.
 It's
 uh


we're
 not
 going
 to
 stop
 China.
 Yeah.
 The


the
 whole
 I've
 always
 been
 a
 skeptic
 of


the
 no
 selling
 chips
 to
 China
 thing.
 The


notion
 originally
 was
 like
 we're
 going


to
 prevent
 them
 from
 doing
 you
 know
 some


super
 cutting
 edge
 military


applications.
 Then
 it
 was
 like
 well
 we


can't
 really
 stop
 that.
 Um
 but
 we
 can
 at


least
 stop
 them
 from
 training
 frontier


models.
 And
 then
 it
 was
 like
 well
 we


can't
 necessarily
 really
 stop
 that
 but


now
 we
 can,
 you
 know,
 at least
 keep
 them


from
 like
 having
 tons
 of
 AI
 agents.


Well,
 we'll
 have
 like
 way
 more
 AI
 agents


than
 they
 do.
 And
 I
 don't
 love
 that
 line


of
 thinking
 really
 at
 all.
 Um,
 but
 one


upshot
 of
 it
 potentially
 is
 they
 just


don't
 have
 enough


compute
 available
 to
 provide
 inference


as
 a
 service,
 you
 know,
 to
 the
 rest
 of


the
 world.
 So
 instead,
 the
 best
 they
 can


do
 is
 just
 say,
 okay,
 well,
 we'll
 train


these
 things
 and,
 you
 know,
 you
 can


figure
 it
 out
 here.
 Here
 you
 go.
 Like


have
 at
 it.
 Um,
 it's
 kind
 of
 a
 soft


power
 play
 presumably.


Um,
 I
 did
 an
 episode
 with
 uh
 Anne
 from


A16Z
 who
 I
 I
 thought
 really
 did
 a
 great


job
 of


providing
 the
 perspective
 of
 what
 I
 what


I
 started
 calling
 countries
 3
 through


193.
 If
 the
 US
 and
 China
 are
 one
 and


two,


three
 through
 there's
 a
 big
 gap.
 you


know
 there's
 like
 I
 think
 the
 US
 is


still
 ahead
 but
 not
 by
 that
 much
 in


terms
 of
 research
 and
 you
 know
 ideas


relative
 to
 China.
 We
 do
 have
 this


compute
 advantage
 and
 that
 does
 seem


like
 it
 matters.
 One
 of
 the
 upshots
 may


be
 that
 they're
 open
 sourcing
 and


countries
 3
 through
 93
 are
 like
 or
 3


through
 193
 are
 significantly
 behind.


Um,
 so
 for
 them
 it's
 a
 way
 to,
 you
 know,


try
 to
 bring
 more
 countries
 over
 to
 the


Chinese
 camp
 potentially
 in
 the
 US
 China


rivalry.
 Seems
 like
 the
 model
 everybody


and
 I
 don't
 like
 this
 at
 all.
 I
 I
 I


don't
 like
 technology
 decoupling
 as


somebody
 who
 worries
 about,
 you
 know,


who's
 the
 real
 other
 here.
 I
 always
 say


the
 the
 real
 other
 are
 the
 AIs,
 not
 the


Chinese.
 So,
 if
 we
 do
 end
 up
 in
 a


situation
 where
 yikes,
 like,
 you
 know,


we're
 seeing
 some
 crazy
 things,
 it
 would


be
 really
 nice
 if
 we
 were
 on
 basically


the
 same
 technology
 paradigm
 to
 the


degree
 that
 we
 really
 decouple
 and,
 you


know,
 not
 just
 the
 chips
 are
 different,


but
 maybe
 the
 ideas
 start
 to
 become
 very


different,
 publishing
 gets
 shut
 down,


you
 know,
 tech
 tech
 trees
 evolve
 and
 and


kind
 of
 grow
 apart.
 Um,


that
 to
 me
 seems
 like
 a


recipe
 for


you
 know,
 it's
 harder
 to
 know
 what
 the


other
 side
 has.
 It's
 harder
 to
 trust
 one


another.
 It
 seems
 to
 feed
 into
 the
 arms


race
 dynamic,
 which
 I
 do
 think
 would,


you
 know,
 is
 is
 a
 real
 uh
 existential


risk
 factor.
 I
 would
 hate
 to
 see
 us,
 you


know,
 create
 another
 sort
 of
 MAD
 type


dynamic
 where
 we
 all
 live
 under
 the


threat
 of
 AI
 destruction.
 Um,
 but
 that


very
 well
 could
 happen.


And
 so,
 yeah,
 I
 don't know.
 I
 I
 I
 do


kind
 of
 um


have
 some
 sympathy
 for
 the
 recent


decision
 that
 the
 administration
 made
 to


be
 willing
 to
 sell
 the
 H20s
 to
 China.


And
 then
 it
 was
 funny
 that
 they
 turned


around
 and
 rejected
 them,
 which
 to
 me


seemed
 like
 a
 mistake.
 I
 don't
 know
 why


they
 would
 be
 rejecting
 them.
 If
 I
 were


them,
 I
 would
 buy
 them.
 Um
 and
 I
 would


maybe
 I
 would
 maybe
 sell
 inference
 on


the
 models
 that
 I've
 just
 been
 uh


creating
 and
 I
 would
 try
 to
 make
 my


money
 back
 doing
 that.
 But
 in
 the


meantime,
 they
 can
 at
 least,
 you
 know,


demonstrate
 the
 greatness
 of
 the
 Chinese


nation
 by
 showing
 that
 they're
 not
 uh


far
 behind
 the
 frontier.
 And
 they
 can


also
 make
 a
 pretty
 powerful
 appeal
 to


countries
 3
 through
 193
 and
 say
 like,


you
 know,
 look,
 you
 really
 want
 to,
 you


see
 how
 the
 US
 is
 acting
 uh
 in
 general,


you
 know,
 you
 really
 want
 to
 they
 cut
 us


off
 from
 chips.
 Uh
 they
 had
 a
 even
 a


long,
 you
 know,
 the
 last
 administration


had
 an
 even
 longer
 list
 of
 countries


that
 couldn't
 get
 chips.
 this


administration
 is
 doing
 all
 kinds
 of


crazy
 stuff.
 You
 know,
 you
 get
 50%


tariffs
 here,
 there,
 whatever.
 Um,
 how


do
 you
 know
 you
 can
 really
 rely
 on
 them


to
 continue
 to
 provide
 you
 AI
 into
 the


future?
 Well,
 you
 can
 rely
 on
 us.
 We


open
 sourced
 the
 model.
 You
 can
 have
 it.


Um,
 so,
 you
 know,
 come
 work
 with
 us
 and


buy
 our
 chips
 because,
 by
 the
 way,
 our


models
 will,
 you
 know,
 as
 we
 mature,


they'll
 be
 optimized
 to
 run
 on
 our


chips.
 So,
 I
 don't
 know.
 That's
 a


complicated
 stuff,
 a
 complicated


situation.


I
 do
 think
 it's
 true.
 I
 I
 don't
 think


the
 adoption
 is
 as
 high
 as
 that
 80%.
 I


think
 that
 is,
 you
 know,
 within
 that


subset
 of
 companies
 that
 are
 doing
 stuff


with
 open
 source.
 We're
 going
 to


experiment
 with
 that
 at
 Wemark,
 but
 we


to
 be
 honest,
 we
 have
 never
 done


anything
 with
 an
 open
 source
 model
 in


our
 product
 to
 present.
 Everything
 we've


ever
 done
 has
 been
 through
 commercial.


Um,
 at
 this
 point,
 we
 are
 going
 to
 try


doing
 some
 reinforcement
 fine-tuning.
 We


are
 going
 to
 do
 that
 on
 a
 Quen
 model,
 I


think,
 first.
 Um,
 so
 you
 know,
 that'll


put
 us
 in
 that
 80%.
 But
 I'm
 guessing


that
 at
 the
 end
 of
 the
 day,
 we'll
 take


that
 Quinn
 model,
 we'll
 do
 the


reinforcement
 fine-tuning,
 and
 we'll


probably
 get
 roughly
 up
 to
 as
 good
 as,


you
 know,
 GBD5
 or
 Cloud
 4
 or
 whatever.


And
 then
 we'll
 say,
 okay,
 do
 we
 really


want
 to
 have
 to
 manage
 inference


ourself?
 How
 much
 are
 we
 really
 going
 to


save?
 And
 at
 the
 end
 of the
 day,
 I
 would


guess
 we
 probably
 are
 still
 going
 to
 end


up
 just
 being
 like,
 eh,
 we'll
 pay
 a


little
 bit
 more
 on
 a
 monthly
 bill
 basis


for
 one
 of
 these
 Frontier
 models.


They're
 a
 little
 bit
 better
 maybe
 still.


And,
 you
 know,
 it's
 operationally
 a
 lot


easier
 and
 they'll
 have
 upgrades,
 you


know.
 Um,


so
 yeah,
 I
 mean,
 of
 course,
 there's


regulated
 industries.
 There's
 all


there's
 a
 lot
 of
 places
 where,
 you
 know,


you
 have
 hard
 constraints.
 You
 just


can't
 get
 around
 and
 that
 forces
 you
 to


do
 those
 Chinese
 thing,
 Chinese
 models.


Then
 there's
 also
 going
 to
 be
 the


question
 of
 like
 are
 there
 back
 doors
 in


them?
 Um
 you
 know
 people
 have
 seen
 the


sleeper
 agents


project
 where
 a
 model
 was
 trained
 to
 be


good
 up
 until
 a
 certain
 point
 of
 time


and
 you
 know
 people
 put
 the
 today's
 date


in
 the
 system
 prompt
 all
 the
 time
 right


today's
 date
 is
 this
 you
 are
 clawed
 you


know
 here
 you
 go
 so
 then
 that's
 going
 to


be
 another
 kind
 of
 thing
 for
 people
 to


worry
 about
 um


and
 we
 don't
 really
 have
 great
 there


there
 have
 been
 some
 studies
 anthropic


did
 a
 thing
 where
 they
 trained
 models
 to


have
 some
 hidden
 objectives
 and
 then


challenged
 teams
 to
 figure
 out
 what


those
 hidden
 objectives
 were.
 And
 with


certain
 interpretability
 techniques,


they
 were
 able
 to
 figure
 that
 stuff
 out


relatively
 quickly.
 So
 you
 might
 be
 able


to
 get
 enough
 confidence
 that
 you
 take


this
 open
 source
 thing
 you
 know
 created


by
 some
 Chinese
 company
 whatever
 and


then
 put
 it
 through
 you
 know
 some
 sort


of
 not
 exactly
 audit
 because
 you
 can't


trace
 exa
 exactly
 what's
 happening
 but


some
 sort
 of
 examination
 you
 know
 to
 see


can
 we
 detect
 any
 hidden
 goals
 or
 any


you
 know
 secret
 back
 door
 bad
 behavior


whatevers
 and
 maybe
 with
 enough
 of
 that


kind
 of
 work
 you
 could
 be
 confident
 that


you
 don't
 have
 it.
 Um,
 but
 the
 more
 and


more
 critical
 this
 stuff
 gets,
 you
 know,


again,
 going
 back
 to
 that
 task
 length


doubling,
 weird
 behavior,
 now
 you
 got
 to


add
 into
 the
 mix,
 what
 if
 they


intentionally
 programmed
 it
 to
 do


certain
 bad
 things
 under
 certain,
 you


know,
 rare
 circumstances.
 Um,
 we're
 just


headed
 for
 a
 really
 weird
 future.
 You


know,
 that
 we've
 got
 all
 these


there's
 there's
 no
 limit
 to
 it.
 You


know,
 all
 these
 things
 are
 valid


concerns.
 they


often
 are
 in
 direct
 tension
 with
 each


other.
 Um
 I
 don't
 I
 I'm
 not
 one
 who
 uh


you
 know
 wants
 to
 see
 one
 tech
 company


take
 over
 the
 world
 by
 any
 means.
 So
 I
 I


definitely
 think
 we
 would
 do
 really
 well


to
 have
 some
 sort
 of
 broader
 more


buffered
 ecological
 like
 system
 where


you
 know
 all
 the
 AIs
 are
 kind
 of
 in
 some


sort
 of
 competition
 you
 know
 mutual


coexistence
 with
 each
 other
 but
 we
 don't


really
 know
 what
 that
 looks
 like
 and
 we


don't
 really
 know
 um
 you
 know
 we
 don't


really
 know
 what
 an
 invasive
 species


might
 look
 like
 you
 know
 when
 it
 gets


introduced
 into
 that
 very
 you
 know
 nent


and
 as
 yet
 like
 not
 battle
 tested
 uh


ecology.
 So
 yeah,
 I
 don't
 know.
 Bottom


line,
 I
 think
 the
 future's
 gonna
 be


really,
 really
 weird.


>> Yeah.
 Well,
 I
 I
 do
 want
 to
 close
 on
 a
 on


a
 uplifting
 note.
 So
 maybe
 maybe
 as
 a
 as


a
 gearing
 to
 closing
 question,
 we
 could


get
 into
 some
 areas
 where
 we're
 already


seeing
 some
 some
 exciting
 capabilities


emerge
 and
 sort
 of
 transform
 the


experience.
 Maybe
 maybe
 around
 education


or
 or
 healthcare
 or
 any
 other
 areas
 you


want
 to
 you
 want
 to
 highlight?


>> Yeah,
 it's
 boy,
 it's
 all
 over.
 Um,


one
 of
 my
 mantras
 is
 that
 there's
 never


been
 a
 t
 better
 time
 to
 be
 a
 motivated


learner.
 Yeah.


>> So,
 I
 I
 think
 a
 lot
 of these
 things
 do


have
 kind
 of,
 you
 know,
 two
 sides
 of
 the


coin.


>> There's
 the
 worry
 that
 the
 students
 are


taking
 the
 shortcuts
 and
 they're,
 you


know,
 losing
 the
 ability
 to
 sustain


focus
 and
 endure
 cognitive
 strain.
 Flip


side
 of
 that
 is
 as
 somebody
 who's


fascinated
 by
 the
 intersection
 of
 AI
 and


biology,
 sometimes
 I
 want
 to
 read
 a


biology
 paper
 and
 I
 really
 don't
 have


the
 background.
 An
 amazing
 thing
 to
 do


is
 turn
 on
 voice
 mode
 and
 share
 your


screen
 with
 chat
 GPT
 and
 just
 go
 through


the
 paper
 reading.
 It's
 you
 don't
 even


have
 to
 talk
 to
 it.
 Most
 of
 the
 time


you're
 doing
 your
 reading.
 It's
 watching


over
 your
 shoulder
 and
 then
 at
 any


random
 point
 you
 have
 a
 question
 you
 can


verbally
 say
 what's
 this
 why
 why
 are


they
 talking
 about
 that
 what's
 going
 on


with
 this
 what
 is
 the
 role
 of
 this


particular
 protein
 that
 they're


referring
 to
 or
 whatever
 and
 it
 will


have
 the
 answers
 for
 you.
 So
 if
 you


really
 want
 to
 learn


in
 a
 sincere
 way
 you
 know
 the
 the
 things


are
 unbelievably
 good
 at
 helping
 you
 do


that.


flip
 side
 is
 you
 can
 take
 a
 lot
 of


shortcuts
 and
 you
 know
 maybe
 never
 have


to
 learn
 stuff
 on
 the
 biology
 front
 you


know
 again
 like
 we've
 got


multiple
 of
 these
 sort
 of
 discovery


things
 happening
 the
 antibiotics
 one
 we


covered
 there
 was
 another
 one
 that
 I
 did


another
 episode
 on
 with
 a
 a
 Stanford


professor
 named
 James
 Xiao
 who
 created


something
 called
 the
 virtual
 lab


and
 basically
 this
 was
 an
 AI
 agent
 that


could
 spin
 up
 other
 AI
 agents


depending
 on
 what
 kind
 of
 problem
 it
 was


given.
 Then
 they
 would
 go
 through
 a


deliberative
 process
 where
 you'd
 have


you
 know
 one
 expert
 in
 one
 thing
 would


give
 its
 take
 and
 they'd
 you
 know
 bat
 it


back
 and
 forth.
 There
 was
 a
 critic
 in


there
 that
 would
 criticize
 you
 know
 the


ideas
 that
 had
 been
 given.
 Eventually


they'd
 synthesize.
 Then
 they
 were
 also


given
 some
 of
 these
 narrow
 specialist


tools.
 So
 you
 have
 agents
 using
 the


alphafold
 type
 um
 not
 just
 alpha
 fold


you
 know
 there's
 a
 whole
 a
 whole
 wide


wide
 uh
 array
 of
 those
 at
 this
 point
 but


using
 that
 type
 of
 thing
 to
 say
 okay


well
 can
 we
 simulate
 you
 know
 how
 this


would
 interact
 with
 that
 um
 agents
 are


running
 that
 loop
 and
 they
 were
 able
 to


get
 this
 language
 model
 agent
 with


specialized
 tool
 system
 to
 generate
 new


treatments
 for
 novel
 strains
 of
 COVID


that
 had
 a
 you
 kind
 of
 escaped
 um
 the


previous
 treatments.


Amazing
 stuff,
 right?
 I
 mean,
 the
 flip


side
 of
 that,
 of
 course,
 is
 you
 know,


you
 got
 the
 bioweapon
 risk.
 So,
 all


these
 things
 do
 seem
 like
 they're
 going


to
 be


even
 even
 on
 just
 the
 abundance
 front


itself,
 right?
 Like
 we
 may
 have
 a
 world


of
 unlimited
 professional
 private


drivers,
 but
 we
 don't
 really
 have
 a


great
 plan
 for
 what
 to
 do
 with
 the
 five


million
 people
 that
 are
 currently
 doing


that
 work.
 We
 may
 have
 infinite


software,
 but
 you
 know
 when
 especially


once
 the
 five
 million
 drivers
 pile
 into


all
 the
 coding
 boot
 camps
 and
 you
 know


get
 coding
 jobs,
 I
 don't
 know what
 we're


going to
 do
 with
 the
 10
 million
 people


that
 were
 coding
 when
 you
 know
 9
 million


of
 them
 become
 superfluous.


So
 yeah,
 I
 don't
 know.
 I
 think
 we're


we're
 headed
 for
 a
 weird
 world.
 Nobody


really
 knows
 what
 it's
 going
 to
 look


like
 in
 five
 years.
 There
 was
 a
 great


moment
 at
 at
 um
 Google's
 IO
 where
 they


brought
 up
 some
 journalist.
 I
 know
 you


we
 uh
 we're
 skeptical
 of
 journalists.


This
 is
 a
 great
 moment
 to
 uh
 we're
 going


direct,
 right?
 This
 was
 a
 great
 reason


or
 example
 of
 why
 one
 would
 want
 to
 do


that.
 They
 brought
 up
 this
 person
 to


interview
 Demis
 and
 uh
 Sergey
 Brennan.


They
 the
 guy
 asked
 like
 what
 is
 search


going
 to
 look
 like
 in
 5
 years?
 and


Sergey
 Brandon
 like
 almost
 spit
 out
 his


coffee
 on
 the
 on
 the
 stage
 and
 was
 like


surge
 we
 don't
 know
 what
 the
 world
 is


going
 to
 look
 like
 in
 five
 years.
 So
 I


think
 that's
 really
 true
 like
 the


biggest
 risk
 I
 think
 for
 so
 many
 of
 us


and
 I
 you
 know
 include
 myself
 here
 is


thinking
 too
 small.
 You
 know
 the
 the


worst
 thing
 I
 think
 we
 could
 do
 would
 be


to
 underestimate
 how
 far
 this
 thing


could
 go.
 I
 would
 much
 rather
 be


I
 would
 much
 rather
 be
 mocked
 for
 things


happening
 on
 twice
 the
 time
 scale
 that
 I


thought
 than
 to
 find
 myself
 unprepared


when
 they
 do
 happen.
 So
 whether
 it's
 27,


29,
 31,
 uh
 I'll
 take
 that
 extra
 buffer


honestly
 where
 we
 can
 get
 it.
 My


thinking
 is
 just


get,
 you
 know,
 get
 ready
 as
 as
 much
 and


as
 fast
 as
 possible.
 And
 again,
 if
 we
 do


have
 a
 little
 grace
 time
 to
 uh
 you
 know


to
 do
 extra
 thinking,
 then
 great.
 Um
 but


I
 would


I
 think
 the
 worst
 mistake
 we
 could
 make


would
 be
 to
 dismiss
 and
 and
 not
 feel


like
 we
 need
 to
 get
 ready
 for
 big


changes.


>> Should
 we
 wrap
 directly
 on
 that
 or
 is


there
 any
 other
 last
 note
 you
 want
 to


make
 sure
 to
 get
 across
 regarding


anything
 we
 we
 said
 today?


>> One
 of
 my
 uh
 other
 mantras
 these
 days
 is


the
 scarcest
 resource
 is
 a
 positive


vision
 for
 the
 future.
 Yeah,
 I
 do
 think


it's
 always
 really
 striking
 whether
 it's


Sergey
 or,
 you
 know,
 or
 Sam
 Alman
 or


Daario.
 Like
 Daario
 probably
 has
 the


best
 positive
 vision
 of
 the
 Frontier


Developer
 CEOs
 with
 Machines
 of
 Love
 and


Grace.
 But
 it's
 always
 striking
 to
 me


how
 little
 detail
 there
 is
 on
 these


things.
 And
 when
 they
 launched
 GBT40,


which
 was
 the
 voice
 mode,
 they
 were


pretty
 upfront
 about
 saying,
 "Yeah,
 this


was
 kind
 of
 inspired
 by
 the
 movie
 Her."


And
 so
 I
 do
 think
 like
 even
 if
 you
 are


not
 a
 researcher,
 you
 know,
 not
 great
 at


math,
 not
 somebody
 who
 codes,
 um
 I
 think


that
 this
 technology
 wave
 really
 rewards


play.
 It
 really
 rewards
 imagination.
 I


think
 literally
 writing
 fiction
 might
 be


one
 of
 the
 highest
 value
 things
 you


could
 do,
 especially
 if
 you
 could
 write


aspirational
 fiction
 that
 would
 get


people
 at
 the
 frontier
 companies
 to


think,
 geez,
 maybe
 we
 could
 steer
 the


world
 in
 that
 direction.
 Like,
 wouldn't


that
 be
 great?
 If
 you
 could
 plant
 that


kind
 of
 seed
 um
 in
 people's
 minds,
 it


could
 come
 from
 a
 totally
 non-technical


place
 and
 potentially
 be
 really


impactful


play
 fiction.
 um


had
 one
 other
 dimension
 of
 that,
 but


yeah,
 play
 fiction,
 positive
 vision
 for


the
 future.
 Anything
 that
 you
 could
 do


to
 offer
 a
 positive
 Oh,
 behavioral
 too


is
 like
 these
 days
 because
 you
 can
 get


the
 AIS
 to
 code.
 So,
 well,
 I'm
 starting


to
 see
 people
 who


have
 never
 coded
 before.
 I'm
 working


with
 one
 guy
 right
 now
 who's
 never
 coded


before,
 but
 does
 have
 a
 sort
 of


behavioral
 science
 background


and
 he's
 starting
 to
 do
 legitimate


frontier
 research
 on
 how
 our
 AI
 is
 going


to
 behave
 under
 various
 kind
 of
 esoteric


circumstances.
 So
 I
 think
 nobody
 should


count
 themselves
 out
 from
 the
 ability
 to


contribute
 to
 figuring
 this
 out
 and
 even


to
 shaping
 this
 phenomenon.
 Um
 it
 is
 not


just
 something
 that
 the
 you
 know
 the


technical
 minds
 can
 contribute
 to
 at


this
 point.
 Literally
 philosophers,


fiction
 writers,
 uh
 people
 literally


just
 messing
 around.
 Um
 Ply
 the


Jailbreaker
 you
 know
 there's
 there
 are


almost
 unlimited


cognitive
 profiles
 that
 would
 be
 really


valuable
 to
 add
 to
 the
 mix
 of
 people


trying
 to
 figure
 out
 what's
 going
 on


with
 AI.
 So,
 come
 one,
 come
 all
 is
 kind


of
 my
 attitude
 on
 that.
 That's
 a
 great


place
 to
 to
 wrap.
 Nathan,
 thank
 you
 so


much
 for
 coming
 on
 the
 podcast.
 Thank


you,
 Eric.
 It's
 been
 fun.


[Music]


[Music]