大家好，这里是最佳拍档，我是大飞
昨天
DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp
在之前 DeepSeek-V3.1-Terminus 的基础上
通过持续训练集成了一种全新的稀疏注意力机制
也就是DeepSeek 稀疏注意力
简称DSA
摇身一变成为了一款实验性的稀疏注意力模型
再搭配上一个高效的闪电索引器
V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升
尤其是在处理长上下文场景的时候
这种效率优势会更加明显
这期视频我们就从架构、训练、评估这几个核心维度
给大家拆解一下这款模型的细节
首先来看架构部分
这次DeepSeek-V3.2-Exp在架构上唯一的修改
就是通过持续训练
引入了DSA
那这个DSA到底是由什么构成的呢？
它主要包含两个关键组件
一个是闪电索引器
另一个是细粒度的token选择机制
我们先来了解一下闪电索引器
它的核心作用是计算查询token（query token）和前面的某个token之间的索引分数It,s
然后通过这个分数来决定
查询token要选择哪些token进行后续的注意力计算
这个索引分数的计算公式是这样的
这里面有几个关键参数需要跟大家解释清楚
其中Hᴵ代表的是索引器头（indexer heads）的数量；
qt,jᴵ和wt,jᴵ是一个实数
是从查询token ht中推导出来的；
而ksᴵ是从前面的token hs中推导出来的
可能有朋友会问
为什么选择ReLU作为激活函数呢？
这里主要是出于吞吐量的考虑
ReLU函数在计算效率上有一定的优势
能够更好地配合闪电索引器实现高效计算
而且还有一个很重要的点
闪电索引器的头数比较少
同时还可以用FP8精度来实现
这就使得它的计算效率非常突出
不会因为引入新的组件而导致计算成本大幅增加
这也是后续模型整体效率提升的一个重要基础
有了闪电索引器计算出的索引分数之后
接下来就要靠细粒度的token选择机制来发挥作用了
对于每个查询token ht
这个机制会根据索引分数
只筛选出索引分数排在前k位的token所对应的键值对（key-value entries）
然后，注意力输出ut的计算
就是让查询token ht和这些经过稀疏筛选后的键值对
用{cs}表示，进行注意力机制的运算
具体公式是这样的
这样一来
模型就不需要对所有的token都进行注意力计算
只针对筛选后的关键token进行处理
从而在保证效果的前提下
大幅降低计算量
不过，DSA并不是孤立存在的
它是在DeepSeek在2024年提出的MLA架构的基础上实例化来的
为什么要选择这样的方式呢？
主要是考虑到DeepSeek-V3.2-Exp是从DeepSeek-V3.1-Terminus进行持续训练得到的
基于MLA来实例化DSA
能够更好地保证训练的连贯性和兼容性
而且在kernel层面
为了提升计算效率
每个键值对都必须在多个查询之间共享
这一点是袁等人在2025年的研究中提到的关键结论
基于这样的需求
研发团队选择了在MLA的MQA模式
也就是多查询注意力（Multi-Query Attention）的基础上实现DSA
这种MQA模式是Transformer作者之一诺姆·沙泽尔（N
Shazeer）在2019年提出的
在这种模式下，MLA中的每个潜在向量
都会在查询token的所有查询头（query heads）之间共享
这样就能很好地满足键值对共享的需求
进一步提升计算效率
关于DSA基于MLA的具体架构
大家可以参考原文中的图1
里面清晰地展示了整个注意力架构的细节
尤其是绿色部分
直观地呈现了DSA如何根据索引器筛选出前k个键值对的过程
另外
为了让大家能够更清晰地了解具体的实现细节
研发团队还提供了DeepSeek-V3.2-Exp的开源实现
大家如果想深入研究代码层面的逻辑
这个地址会非常有帮助
同时
原文的附录A还详细说明了MLA的MQA模式和MHA模式（Multi-Head Attention
多头注意力）之间的区别
感兴趣的朋友也可以去附录部分进一步阅读
讲完了架构，咱们再来看训练流程
DeepSeek-V3.2-Exp的训练是从DeepSeek-V3.1-Terminus的基础checkpoint开始的
这个基础checkpoint的上下文长度已经扩展到了128K
为后续处理长上下文任务打下了很好的基础
整个训练过程分为两个主要阶段
持续预训练和后训练
每个阶段又有各自细分的步骤和目标
咱们一步步来看
首先是持续预训练阶段
这个阶段又分为两个训练步骤
分别是密集预热阶段和稀疏训练阶段
需要先说明的是
这两个阶段所使用的训练数据分布
是完全和DeepSeek-V3.1-Terminus在扩展128K长上下文时所用的数据保持一致的
这样做的目的是为了保证数据的连贯性
减少因数据分布差异对模型训练效果产生的干扰
第一个步骤是密集预热阶段
这个阶段的核心目标是初始化闪电索引器
在这个阶段
模型仍然保持着密集注意力的计算方式
也就是说
此时还没有启用稀疏筛选的机制
同时
除了闪电索引器之外
模型的其他所有参数都处于冻结状态
不进行更新
那为什么要这么做呢？
主要是为了让索引器的输出能够和主注意力的分布保持一致
为后续的稀疏训练做好铺垫
具体怎么实现这种对齐呢？
对于第t个查询token
研发团队首先会将主注意力的分数在所有注意力头上进行求和
然后沿着序列维度对这个求和结果进行L1归一化
得到一个目标分布p
然后，基于这个目标分布p
团队设置了KL散度损失作为闪电索引器的训练目标
具体的损失函数公式是这样的
其中Softmax函数是为了将索引器计算出的索引分数转换为概率分布
以便和目标分布p进行比较
通过最小化两者之间的KL散度
让索引器的输出尽可能接近主注意力的分布
在这个预热阶段
所使用的学习率是10的负3次方
训练步数非常少，只训练了1000步
每一步包含16个序列
每个序列有128K个token
咱们可以算一下
整个密集预热阶段总共训练的token数量是1000步 × 16序列/步 × 128K token/序列
约等于2.1B个token
这样的训练量能够在短时间内完成索引器的初始化
同时避免过度训练导致其他问题
完成了密集预热之后
就进入到了持续预训练的第二个步骤
稀疏训练阶段
在这个阶段
研发团队引入了前面提到的细粒度token选择机制
开始让模型适应DSA的稀疏模式
并且此时会对模型的所有参数进行优化更新
而不再是只训练索引器
在这个阶段
仍然需要保持索引器的输出和主注意力分布的对齐
但和密集预热阶段不同的是
此时只考虑经过筛选后的token集合Sₜ
这个集合St的定义是所有满足It,s属于It,:中前k个分数的s所构成的集合
因此
KL散度损失函数也相应地进行了调整
也就是只计算筛选后的token在目标分布和索引器输出分布之间的KL散度
这里有一个非常关键的技术细节
研发团队将索引器的输入从计算图中分离了出来
进行单独的优化
这意味着什么呢？
简单来说
索引器的训练信号只来自于刚才提到的KL散度损失
而主模型的优化则完全依据语言建模损失（language modeling loss）
两者的优化过程相互独立
这样做能够更好地平衡索引器和主模型的训练
避免相互干扰
保证各自训练目标的实现
在稀疏训练阶段
所使用的学习率是7.3×10⁻⁶
每个查询token会选择2048个键值对token进行后续的注意力计算
训练步数比预热阶段多很多
总共训练了15000步
每一步包含480个序列
每个序列同样是128K个token
咱们再来计算一下这个阶段的总训练token量
15000步 × 480序列/步 × 128K token/序列
约为943.7B个token
如此大的训练量能够让模型充分适应稀疏模式
保证模型在稀疏注意力机制下的性能
持续预训练完成之后
就进入到了后训练阶段
这个阶段的目标是打造最终的DeepSeek-V3.2-Exp模型
需要强调的是，在后训练阶段
模型仍然采用和稀疏持续预训练阶段相同的方式来使用稀疏注意力
这样可以保证训练过程的一致性
为了严谨地评估引入DSA之后对模型的影响
研发团队在DeepSeek-V3.2-Exp的后训练过程中
保持了和DeepSeek-V3.1-Terminus完全相同的后训练流水线、算法以及数据
这样一来
两者在后训练阶段的差异就只来源于是否引入了DSA
从而能够更准确地判断DSA对模型性能和效率的影响
后训练阶段主要包括两个核心环节
分别是专家蒸馏（Specialist Distillation）和混合RL训练（Mixed RL Training）
先来看专家蒸馏环节
这个环节的思路是
针对每个特定的任务或领域
先开发一个专门的模型
也就是“专家模型”，
这些专家模型都只专注于自己所对应的领域
所有的专家模型都是从同一个预训练的DeepSeek-V3.2基础checkpoint进行微调得到的
这样可以保证专家模型之间的基础能力处于同一水平
具体涵盖了哪些领域呢？
除了常见的写作任务和通用问答任务之外
还包括五个专门的领域
包括数学、竞赛编程（competitive programming）、通用逻辑推理（general logical reasoning）、智能体编程（agentic coding）以及智能体搜索（agentic search）
每个专家模型的训练都投入了大规模的强化学习（RL）计算资源
确保专家模型在各自领域内能够达到较高的性能水平
而且
为了让训练数据更贴合不同的推理和生成需求
研发团队还使用了不同的模型来生成两种类型的训练数据
一种是用于长链式推理（long chain-of-thought reasoning）的训练数据
对应的是“思考模式”（thinking mode）；
另一种是用于直接响应生成（direct response generation）的训练数据
对应的是“非思考模式”（non-thinking mode）
当这些专家模型训练完成之后
它们就会被用来生成特定领域的数据
这些数据会用于最终DeepSeek-V3.2-Exp模型checkpoint的训练
从实验结果来看
使用这些蒸馏后的数据训练出来的模型
性能只比那些专门的领域专家模型略低一点
而且通过后续的RL训练
这个性能差距能够被有效的消除
这就意味着，通过专家蒸馏
DeepSeek-V3.2-Exp能够在多个领域同时具备接近专家模型的能力
而不需要为每个领域单独训练一个模型
大大提升了模型的通用性和效率
接下来呢
是混合RL训练环节
在DeepSeek V3.2 EXP的后训练中
研发团队仍然采用了分组相对策略优化GRPO作为RL训练的算法
不过，和之前的DeepSeek模型相比
这里有一个重要的改进
那就是之前的模型采用的是多阶段强化学习训练
而DeepSeek-V3.2-Exp则将推理训练、智能体训练以及人类对齐训练这三个原本分开的阶段合并成了一个RL阶段
这种合并带来了两个显著的好处
一方面
它能够在不同的领域之间实现更好的性能平衡
避免某个领域因为训练阶段的侧重不同而导致性能过强或过弱；
另一方面
它成功规避了多阶段训练范式中常见的“灾难性遗忘”问题
所谓灾难性遗忘
就是模型在学习新任务或新领域知识的时候
会忘记之前已经学会的知识
而单阶段训练则很好地缓解了这个问题
保证了模型在各个领域知识的稳定性
在奖励设计方面
研发团队也考虑得非常细致
针对不同类型的任务设置了不同的奖励机制
对于推理任务和智能体任务
采用了基于规则的结果奖励（rule-based outcome reward）、长度惩罚（length penalty）以及语言一致性奖励（language consistency reward）
基于规则的结果奖励主要是根据任务的客观结果来判断模型输出是否正确
比如推理任务是否得出正确答案
智能体任务是否完成指定目标；
长度惩罚则是为了避免模型生成过长但无意义的内容
鼓励简洁有效的输出；
语言一致性奖励则是保证模型输出的语言在逻辑、风格等方面保持一致
提升输出质量
而对于通用任务
研发团队采用了生成式奖励模型（generative reward model）
并且为每个提示词都设置了专门的评估标准（rubrics）
这种奖励模型能够更灵活地评估通用任务中模型输出的质量
因为通用任务的评价标准往往不像推理或智能体任务那样客观明确
需要更细致的评估维度
在整个奖励设计过程中
研发团队重点平衡了两个关键的权衡关系
第一个是“长度与准确性”的权衡
也就是既要避免模型为了追求准确性而生成过长的内容
也要防止为了缩短长度而牺牲准确性；
第二个是“语言一致性与准确性”的权衡
即保证语言一致性的同时
不影响模型输出的准确性
确保模型在多个评估维度上都能有出色的表现
讲完了训练流程
咱们再来看大家最关心的评估结果
毕竟一款模型的好坏
最终还是要靠实际的评估数据来证明
DeepSeek-V3.2-Exp的评估主要从三个方面展开
分别是模型能力、推理成本
以及未来的真实场景验证计划
首先是模型能力的评估
研发团队选择了一系列涵盖不同能力的基准测试（benchmark）
将DeepSeek-V3.2-Exp和DeepSeek-V3.1-Terminus进行了全面的对比
具体的对比数据大家可以参考这张表
从整体结果来看
尽管DeepSeek-V3.2-Exp在长序列处理的计算效率上有了显著提升
但是无论是在短上下文任务还是长上下文任务中
它都没有出现明显的性能下降
这一点非常关键
因为很多时候模型效率的提升往往会伴随着性能的损失
而DeepSeek-V3.2-Exp很好地兼顾了效率和性能
不过，在个别基准测试中
DeepSeek-V3.2-Exp的性能确实比DeepSeek-V3.1-Terminus略低一些
比如在GPQA-Diamond（Pass@1）、HLE（Pass@1）以及HMMT 2025（Pass@1）这几项测试中
那是什么原因导致了这种性能差异呢？
研发团队经过分析发现
主要是因为DeepSeek-V3.2-Exp生成的推理token数量更少
而有趣的是
当使用那些生成的token数量
与DeepSeek-V3.1-Terminus相近的中间checkpoint进行测试时
这种性能差距就消失了
这说明
性能差异并不是因为DSA机制本身导致的
而是和模型生成内容的长度相关
只要调整好生成token的数量
DeepSeek-V3.2-Exp完全能够达到和之前版本相当的性能水平
除了基准测试的静态结果之外
研发团队还对比了两款模型的强化学习训练曲线
其中图（a）是BrowseComp任务的训练曲线
图（b）是SWE Verified任务的训练曲线
从曲线可以看出，在整个训练过程中
DeepSeek-V3.1-Terminus和DeepSeek-V3.2-Exp在这两项任务上的准确率都在稳步提升
而且两条曲线的走势非常接近
几乎是同步上升的
同时
图中的虚线代表模型输出的平均token数量
两款模型的输出token数量也保持着相似的变化趋势
这种接近的训练曲线充分说明了DSA机制并没有影响模型的训练稳定性
DeepSeek-V3.2-Exp在训练过程中能够像之前的版本一样稳定地学习和提升性能
接下来是推理成本的评估
这也是DeepSeek-V3.2-Exp的核心优势所在
咱们先从理论层面来看
DSA机制到底是如何降低推理成本的
在传统的密集注意力机制中
主模型的核心注意力计算复杂度是O(L²)，
其中L是序列的长度
这意味着当序列长度大幅增加时
计算量会呈平方级增长
这也是长上下文处理效率低的主要原因之一
而引入DSA之后
核心注意力计算复杂度降低到了O(L×k)，
其中k是每个查询token选择的键值对token数量
而且k远小于L（k≪L）
这样一来，当处理长序列时
计算量的增长速度就从平方级变成了线性级
大幅降低了计算成本
可能有朋友会问
那闪电索引器的计算复杂度呢？
确实
闪电索引器的计算复杂度仍然是O(L²)，
但是它的计算量和DeepSeek-V3.1-Terminus中MLA的计算量相比
要少得多
一方面是因为闪电索引器的头数少
另一方面是因为它可以用FP8精度实现
这些都使得闪电索引器的实际计算成本非常低
再加上研发团队针对DSA做的优化实现
最终使得DeepSeek-V3.2-Exp在长上下文场景下实现了显著的端到端加速
为了让大家更直观地感受到这种成本差异
研发团队在H800 GPU集群上部署了实际的服务
并且对两款模型的推理成本进行了 benchmark 测试
测试时GPU的租赁价格按照每小时2美元计算
具体的成本对比数据可以参考这张图
其中图（a）是预填充（Prefilling）阶段的成本
图（b）是解码（Decoding）阶段的成本
从图中可以清晰地看到
随着序列中token位置的增加
比如从0K到128K
DeepSeek-V3.2-Exp的每百万token成本始终低于DeepSeek-V3.1-Terminus
而且在token位置达到128K这种长上下文场景下
成本优势更加明显
另外
还有一个细节需要跟大家提一下
在处理短序列预填充的时候
研发团队专门实现了一种掩码MHA（masked MHA）模式来模拟DSA的效果
这样做的目的是在短上下文条件下
也能让模型达到更高的效率
避免在短序列场景下因为DSA的稀疏机制反而导致效率下降
充分考虑了不同序列长度场景下的效率优化
最后，关于未来的验证计划
研发团队也表现出了非常严谨的态度
虽然目前的内部评估结果显示DeepSeek-V3.2-Exp的表现很有前景
但是团队并没有就此止步
而是计划在真实世界场景中进行更大规模的测试
为什么要这么做呢？
因为实验室环境下的基准测试往往无法完全覆盖真实场景中的各种复杂情况
真实场景中可能会遇到各种意想不到的问题
比如不同类型的长上下文数据、不同的推理任务需求等
通过大规模的真实场景测试
研发团队希望能够发现稀疏注意力架构可能存在的潜在局限
从而进一步优化模型
让DeepSeek-V3.2-Exp在实际应用中能够有更稳定、更出色的表现
到这里
DeepSeek-V3.2-Exp的核心内容就给大家介绍得差不多了
这款模型通过引入DSA稀疏注意力机制
成功在长上下文处理的效率上实现了突破
同时又保持了和之前版本相当的性能水平
为长上下文AI任务的应用提供了一个更高效、更经济的选择
无论是架构设计中的细节考量
还是训练过程中的严谨优化
再到评估阶段的全面验证
都能看出DeepSeek-AI团队在这款模型上投入的心血和专业能力
当然
AI模型的发展是一个持续迭代的过程
DeepSeek-V3.2-Exp作为一款实验性模型
未来还有很大的优化空间
比如进一步提升稀疏筛选的准确性、在更多真实场景中验证性能等
相信随着后续的不断改进
这款模型会在更多领域发挥重要作用
也期待DeepSeek团队能够带来更多像这样兼顾性能和效率的优秀模型
感谢收看本期视频，我们下期再见