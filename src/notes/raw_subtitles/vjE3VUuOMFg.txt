This
 is
 a
 follow-up
 video
 to
 my
 no
 AI


will
 not
 do
 us
 all,
 but
 we
 need
 to
 be


worrying
 about
 much
 more
 important
 AI


risk
 video.
 So,
 if
 you
 haven't
 seen
 that


one,
 you'll
 want
 to
 watch
 it
 first.


There's
 a
 doomer
 talking
 point
 that
 I


didn't
 address
 that
 popped
 up
 in
 that


video's
 comments.
 First,
 let
 me
 point


out
 that
 there's
 an
 article
 from
 the


Journal
 of
 Medical
 Ethics
 presented
 at
 a


conference
 in
 July
 of
 2024
 that


contradicts
 this
 talking
 point,
 and
 I'll


point
 out
 that
 paper
 at
 the
 end
 of
 this


video.
 So,
 here's
 an
 excerpt
 from
 one


YouTube
 comment
 paring
 this
 talking


point.
 Now,
 please
 don't
 go
 looking
 for


this
 person
 that
 posted
 this
 comment
 and


pile
 on
 them.
 This
 issue
 isn't
 about
 the


comment.
 It's
 the
 bad
 articles
 about
 the


bad
 study
 that
 are
 the
 problem.
 This
 is


yet
 another
 one
 of
 those
 people
 read


only
 the
 headlines
 of
 the
 articles


written
 by
 the
 reporters
 that
 read
 only


the
 headlines
 of
 the
 study
 whose


headline
 was
 already
 hyperbolic


problems.
 Although
 the
 headline
 of
 this


particular
 study
 seems
 particularly


egregious
 to
 me.
 So,
 here
 are
 some
 of


the
 article
 headlines.


And
 here
 is
 the
 actual
 headline
 from
 the


study.
 quote,
 "Existential
 risk


narratives
 about
 AI
 do
 not
 distract
 from


its
 immediate
 harms."
 Which
 sounds


pretty
 clear-cut
 until
 you
 dig
 into
 what


the
 words
 existential,
 immediate,
 and


distract
 mean
 to
 the
 authors
 and
 how
 the


study
 was
 actually
 conducted.
 I'm


honestly
 not
 sure
 if
 this
 was
 intended


to
 be
 deceptive
 or
 if
 there's
 just
 a


fundamental
 disagreement
 on
 what
 words


mean,
 but
 I
 certainly
 was
 not
 at
 all


surprised
 to
 find
 out
 that
 this
 poorly


done
 study
 is
 from
 the
 political
 science


department
 of
 the
 same
 university
 that


was
 forced
 to
 apologize
 to
 Reddit
 for


unethical
 research
 that
 they
 conducted.


And
 let's
 face
 it,
 if
 you're
 having
 to


say
 you're
 sorry
 to
 Reddit,
 yeah,
 that's


a
 pretty
 low
 bar.
 So,
 we
 don't
 know
 if


it
 was
 the
 exact
 same
 researchers


because
 the
 name
 of
 the
 people
 at
 the


university
 who
 did
 the
 unethical


research
 on
 Reddit
 were
 withheld.
 So,
 in


this
 actual
 paper,
 there's
 basically


only
 one
 sentence
 that's
 relevant
 to


what
 they
 were
 actually
 studying,
 and


the
 rest
 of
 it's
 a
 bunch
 of
 background


graphs
 and
 results.
 So,
 here's
 that
 one


sentence.
 AI's
 immediate
 harms


consistently
 dominate
 public
 concern


with
 ethical
 issues,
 biases,


misinformation,
 and
 job
 losses
 seen
 as


the
 most
 pressing
 risks.
 That
 makes
 it


sound
 like
 there
 was
 a
 big
 list
 and


ethical
 biases,
 misinformation,
 and
 job


losses
 were
 the
 most
 pressing.


In
 fact,
 those
 are
 the
 only
 four
 they


asked
 about.
 So,
 it's
 complete
 crap.


Anyway,
 then
 you
 have
 to
 dig
 into
 the


appendix,
 which
 is
 you
 have
 to
 go


download
 that
 separately
 in
 order
 to


figure
 out
 what
 the
 study
 actually
 did.


First
 off,
 it
 was
 a
 paid
 voluntary


online
 survey,
 which
 is
 a
 problem
 right


off
 the
 bat.
 Second,
 the
 headline
 says


distract.
 What
 the
 actual
 survey
 asked


people
 to
 do
 was
 to
 rate
 how
 likely
 and


impactful
 they
 thought
 their
 randomly


chosen
 risks
 were
 and
 then
 compare
 those


ratings
 to
 that
 of
 a
 control
 group.
 Just


so
 we're
 clear,
 asking
 someone
 to
 rate


two
 events
 on
 a
 scale
 from
 1
 to
 10
 does


not
 mean
 that
 one
 of
 those
 events
 cannot


distract
 from
 the
 other.
 That's
 not
 what


distract
 means.


So
 here
 are
 the
 things
 that
 the
 survey


takers
 were
 asked
 to
 rate.
 First
 off,


here
 are
 the
 four
 risks
 that
 they


categorized
 as
 existential.


One,
 AI
 leading
 to
 a
 global
 catastrophic


event,
 whatever
 that
 means.
 Two,
 AI


making
 humans
 obsolete,
 whatever
 that


means.
 Three,
 AI
 autonomously
 starting
 a


war.


And
 four,
 AI
 causing
 significant


environmental
 disaster.


And
 then
 here
 are
 what
 they
 considered


to
 be


immediate
 or
 actual
 risks.
 One,
 AI


leading
 to
 significant
 job
 losses
 in


certain
 sectors.
 Two,
 AI
 being
 used
 in


mass
 surveillance
 systems.
 Three,
 AI


increasing
 the
 spread
 of
 misinformation


online.
 And
 four,
 AI
 exacerbating
 biases


and
 decision-making
 process.
 Okay,
 so


let's
 break
 these
 down
 starting
 with
 the


existential
 ones.
 When
 it
 comes
 to


global
 catastrophic
 event
 and


significant
 environmental
 disaster,
 so


here's
 an
 actual
 report
 from
 an


insurance
 group.
 Quote,
 US
 natural


catastrophes
 dominate
 global
 losses
 in


the
 first
 half
 of
 2025.
 Here's
 a
 paper


on
 global
 catastrophic
 flood
 failures.
 I


could
 pull
 out
 a
 ton
 of
 these.
 So
 we


have
 events
 that
 are
 referred
 to
 as


global
 catastrophes
 or
 environmental


disasters
 reported
 in
 the
 news
 fairly


often
 and
 the
 vast
 vast
 vast
 majority
 of


Earth's
 population
 don't
 die
 from
 those


catastrophes
 and
 disasters.
 As
 to


autonomously
 starting
 a
 war,
 according


to
 Wikipedia,
 there
 are
 currently
 eight


inrogress
 major
 wars,
 nine
 minor
 wars,


and
 19
 smaller
 conflicts.
 And
 that's
 at


the
 time
 that
 I'm
 writing
 this.
 There


may
 be
 more
 by
 the
 time
 you're
 watching


this.
 And
 again,
 the
 vast
 vast
 majority


of
 Earth's
 population
 don't
 die
 in
 even


our
 worst
 wars.
 And
 as
 for
 making
 humans


obsolete,
 I'm
 not
 even
 sure
 what
 that's


supposed
 to
 be.
 I
 can't
 find
 anywhere
 in


the
 survey
 that
 defines
 what
 they


expected
 survey
 takers
 to
 understand


that
 to
 mean.
 So
 I
 guess
 it's
 just
 left


open
 to
 interpretation
 of
 each


individual
 survey
 taker.
 Not
 very


scientific,
 if
 you
 ask
 me.
 Now,
 I'm
 not


sure
 what
 definition
 of
 existential
 they


were
 using
 here.
 But
 I
 know
 that
 it's


nowhere
 near
 the
 severity
 of
 what
 the


doomers
 were
 talking
 about
 when
 the


doomers
 say,
 "If
 anyone
 builds
 it,


everyone
 dies."


So
 now
 let's
 talk
 about
 the
 study's


definition
 of
 immediate
 and
 actual.
 So


there's
 mass
 surveillance,
 job
 losses
 in


certain
 sectors,
 disinformation,
 and


biases.
 Now,
 that's
 certainly
 not
 what
 I


mean
 when
 I
 say
 immediate
 actual
 risks


of
 AI.
 To
 me,
 immediate
 actual
 means


actual
 harm
 that
 the
 AIS
 have
 already


done
 and
 continue
 to
 do
 in
 the
 immediate


future.
 I
 mentioned
 several
 in
 my
 last


video.
 There
 was
 a
 teenager
 that
 was


convinced
 by
 a
 chatbot
 to
 permanently


and
 irrevocably
 harm
 himself.
 Two
 men


who
 were
 both
 jailed
 due
 to
 incorrect
 AI


facial
 recognition
 matches.
 A
 man
 who


was
 hospitalized
 because
 a
 chatbot
 told


him
 to
 switch
 from
 salt
 to
 sodium


bromide
 in
 his
 diet.
 people
 who
 are
 ran


over,
 driving
 or
 self-driving
 cars.


These
 are
 real
 problems
 happening
 to


real
 people
 right
 now
 that
 we
 are
 not


doing
 enough
 about.
 So,
 just
 to
 make
 the


comparison
 crystal
 clear,
 what
 the
 study


actually
 showed
 was
 that
 AI
 making


humans
 obsolete,
 whatever
 that
 means,


and/or
 increasing
 the
 number
 of
 ongoing


wars,
 catastrophes,
 or
 environmental


disasters,
 did
 not
 cause
 paid
 online


internet
 survey
 takers
 from
 lowering


their
 estimates
 of
 the
 likeliness
 or


impact
 of
 AI
 causing
 an
 increase
 in


surveillance,
 biases,
 disinformation,
 or


layoffs
 relative
 to
 a
 control
 group.
 So


my
 argument
 from
 the
 last
 video
 is
 that


spending
 time
 on
 discussing
 AI
 will
 kill


every
 single
 human
 being
 on
 this
 planet,


including
 you,
 everyone
 you
 love,
 and


everyone
 you
 have
 ever
 met
 takes
 away


from
 time
 and
 effort
 that
 we
 could
 be


spending
 thinking
 about
 regulations
 that


would
 prevent
 or
 reduce
 the
 many
 deaths


and
 serious
 harm
 that
 AI
 has
 already


caused
 and
 is
 continuing
 to
 cause
 even


as
 you
 watch
 this
 and
 that
 we
 are


absolutely
 not
 doing
 a
 good
 job
 of


preventing.
 And
 if
 you
 can't
 tell
 that


those
 are
 not
 talking
 about
 the
 same


thing
 and
 you
 cannot
 tell
 that
 one
 does


not
 directly
 disprove
 the
 other,
 then
 I


don't
 know
 what
 to
 do
 with
 you.


Meanwhile,
 this
 paper
 AI
 in
 the
 falling


sky
 interrogating
 X-
 risk
 that
 I


mentioned
 at
 the
 beginning
 explicitly


addresses
 the
 same
 kind
 of
 existential


risk
 or
 what
 they
 call
 X-risk
 that
 I


talked
 about
 in
 my
 video
 and
 from
 people


who
 actually
 study
 ethics
 instead
 of


politics.
 And
 I
 hope
 that
 people
 start


listening
 because
 in
 the
 3
 days
 since
 my


previous
 video
 on
 the
 topic
 went
 up,


OpenAI
 has
 already
 announced
 that
 they


were
 about
 to
 relax
 the
 restrictions
 in


most
 cases,
 treat
 adults
 users
 like


adults,
 and
 allow
 even
 more
 like


erotica.


We
 are