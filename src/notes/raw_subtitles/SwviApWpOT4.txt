 So today, I'm going to talk about using agents
 for "Vibe Coding", or at least as close to "Vibe Coding"
 as I'm going to get.
 This is not a coding video,
 and you shouldn't need to know how to code
 in order to follow it.
 In fact, I hope this video makes it to folks
 who are interested in "Vibe Coding",
 but aren't experienced coders.
 Because while I think it's great that people
 who don't code are interested in using these
 tools,
 I think they should go into it with their eyes
 open
 about what you're getting.
 So for those of you that are interested in
 coding details,
 I have a companion video on my other channel
 with plenty of details.
 Links below and at the end of the video.
 So I'm going to compare three agentic tools,
 Claude Code, Gemini CLI, and OpenAI Codex.
 I'm going to tell you roughly how I've
 configured them
 and use them safely, how I evaluate them,
 and why I chose that method.
 I'm going to tell you which of those three
 tools
 is absolutely not worth your time and why,
 and what the trade-offs are between the two
 decent ones,
 so you know which of them would be better fit
 for what you're trying to do.
 And then I'm going to talk to you about their
 limitations,
 and boy, do they have some serious limitations.
 And a lot of people who are so excited
 about "Vibe Coding" aren't talking about these
 limitations.
 And let me make a quick pitch here for why this
 matters.
 The prevailing wisdom in that "we're going to
 have
 human-level intelligent AI very soon now" camp,
 rests on the assumption that "very soon now,"
 the AIs are going to get good enough
 that the AIs will start improving their own
 code
 faster than a human can.
 And that's what's supposed to lead
 to crazy increases in its intelligence.
 I've linked a video called
 "What If AI Keeps Getting Smarter" Below?
 If you want more info on that.
 However, in order for AIs to rewrite
 their own code faster than we can,
 the AIs have to get better at writing code than
 humans are.
 And that's why I think this kind of
 "how well do AIs write code compared
 to professional software engineers"
 experimentation videos
 are so important.
 I think of it kind of like an early warning
 system
 because as long as the AIs aren't as good as
 humans
 at writing software, then they're not likely
 to be able to improve their own code.
 And if they do start getting good enough
 to start potentially improving their own code,
 then I expect this kind of experimentation
 is where we'll get the first indications of
 that.
 So watch this space, because the closer they
 get
 to writing better code than humans,
 the closer we humans get to being FF-----.
 [computer startup music]
 This is "The Internet of Bugs."
 My name is Carl.
 I've been a software professional for 35 plus
 years now.
 And I'm trying to do my part to cut down
 the number of bugs out on the internet.
 Toward that end, I want to talk about bugs
 generated
 by ""Vibe Coding"" so you can hopefully see past
 the hype
 and know what it can be used for
 and what it's not yet stable enough to
 accomplish.
 So "Vibe Coding" is starting to take on a bunch
 of different meanings to different people.
 But here's an excerpt from the original tweet
 that named it that explains how I'm gonna be
 using the term
 at least for today.
 Quote, "There's a new kind of coding I call vibe
 coding
 where you fully get into the vibes.
 I barely even touch the keyboard.
 I "accept all" always.
 I don't read the diffs anymore.
 When I get error messages, I just copy paste
 them
 with no comment, usually that fixes it.
 It's not too bad for throwaway weekend projects,
 but it's not really coding.
 I just see stuff, say stuff, run stuff
 and copy and paste stuff.
 It mostly works."
 So I did an evaluation project using these
 rules
 to start with anyway, with three different
 command line
 based agent coding tools.
 Claude Code, Gemini CLI and OpenAI Codex.
 Then once I got to the end of the project,
 I opened up the code to compare the different
 tools
 and to see how they would measure up
 what's expected of professional programmers
 in the real world.
 Now, I don't like testing AI code generators
 or actual people.
 With coding riddles, I think it's dumb,
 no matter what the prevailing wisdom appears
 to be in the hiring process at most companies.
 Because one, coding riddles are not a thing
 that comes up when you're really trying to get
 work done.
 And two, I found that many people
 who have memorized a bunch of coding riddles
 have a very over-inflated sense of how smart
 they think
 they are and often they get themselves into a
 hole
 that their team has to waste time digging them
 out of,
 often while they're kicking and screaming
 and pounding that they didn't get their way.
 I found a convenient site last year
 to use to evaluate coding AIs.
 If you've seen one of my previous AI coding
 comparison videos, you've heard about it before.
 It's called code crafters and then make step-by-step
 coding challenges for programmers that are not
 only
 challenging, but that teach you how the
 internet really works
 by having you build simple versions
 of core internet technologies.
 They are not sponsoring this video,
 but I do have an affiliate link below
 that you can use if you wanna check them out.
 And if you do, it might help me
 keep making more videos like this.
 The challenge I'm using today is one I've used
 before.
 It's the one I found that the LLMs have the
 easiest time with
 and that's building a simple web server
 using the Python programming language.
 Python is one of, if not the most,
 common language in the dataset these models are
 trained on
 and unsurprisingly, there are a ton of examples
 about how to build a web server on the web
 that the AI companies find when they scrape it.
 So this should be pretty much as good
 as the AI's concurrently get on problems
 that aren't just coding riddles they've memorized.
 So let's see how good they actually do.
 I was pretty hands off on this whole thing.
 I gave each tool a link to the webpage
 for its copy of the challenge and a browser
 connection
 that was all set up for it to use.
 I told it to go to the page, read the
 directions
 and follow them.
 Occasionally one would ask permission to do
 something.
 I pretty much always said yes,
 as long as it didn't involve something
 pointless
 like installing a new copy of a piece of
 software
 I already knew was installed.
 So to cut to the chase, two of the three,
 Gemini CLI from Google
 and Claude Code from Anthropic completed the
 challenge
 at basically at the exact same time.
 Wow, right at the same time.
 They each had different strengths
 and they both left things to be desired
 but they each got the job done.
 OpenAI's tool though failed pretty
 spectacularly
 on a couple of different fronts.
 Okay, so let me tell you about what's going on
 in the rest of this video.
 Next up I'm gonna be talking about
 how I set up the agents and how I ran them.
 Again, I'm not gonna go into a ton of detail
 because I have a longer video on my second
 channel
 where you can watch the whole thing play out
 in real time if you want.
 But I'm gonna explain the basics here.
 Then after I explain the setup,
 I'm gonna talk about OpenAI Codex failing
 miserably.
 Then I'm gonna compare and contrast the two
 that did finish
 and talk about what each was better at than the
 other.
 And then I'm gonna talk about what's still
 missing,
 what risks people run by using "Vibe Coding"
 for production software,
 where things might be going from here.
 And then lastly, I'm gonna be talking
 about next steps for this channel.
 Like I said, this was the easiest challenge
 I can think to give them, and I'm not gonna
 stop there.
 So there will be harder challenges
 and other variations coming up
 that you can subscribe if you want to see.
 I've got chapter markers for all these things,
 so feel free to jump around
 or skip the parts that aren't relevant to you.
 Okay, onto the setup.
 I can't walk you through all the different
 things
 I tried to get all of this working
 and all the things I learned not to do along
 the way,
 but trust me, this was a culmination
 of months of on-and-off experimenting.
 I'm just gonna give you what I finally got
 working.
 So I'm using two physical machines here.
 I have a primary machine.
 It's a Mac studio that I use for my YouTube
 recording,
 editing, Final Cut Pro, that kind of stuff.
 And then I have a second machine that runs
 Linux.
 On that Linux machine, I have three virtual
 machines
 that all each also run Linux.
 Each one of those virtual machines
 is dedicated to one of the coding agents.
 They all mount a file system
 from the underlying host that they write their
 files to.
 That way I keep copies of what each agent is
 doing.
 They can't hurt each other
 aside from starving each other of resources.
 And if one of the agents goes nuts,
 I can just kill it and roll the disk
 of that virtual machine back to a known good
 state.
 The reason I'm going to all that trouble
 is because I'm running all of these agents
 in the most permissive or dangerous mode.
 They don't ask for permissions
 before they do whatever they think they want to
 do.
 If they screw up, they could wipe out the files
 on the machine and make it unbootable.
 I tell you that to warn you
 that if you don't want to go to this much
 trouble,
 you should run your agents in a mode
 where they have to ask you for permission
 before they do things.
 If you run agents in a permissive mode on a
 machine
 that you care about and they delete all your
 files,
 crash your machine and you can't get into it
 anymore,
 don't say I didn't warn you and don't come
 crying to me.
 And yes, I've had something like that happen.
 In an earlier version of this experiment,
 I had all three agents running on the same time
 on one much larger virtual machine
 and one of the agents or some combination of
 them
 ran it completely out of swap space,
 crashed it and corrupted part of the file
 system.
 Not sure exactly what happened
 or what or which one was at fault,
 but it hasn't happened since I separated them,
 but that doesn't mean it couldn't happen again.
 So the other piece of the experiment is the web
 browser.
 I'm running three instances of Chrome on the
 main Mac,
 each one with a different profile directory
 and using a different web socket debugging port.
 Then I forced the agents to use the browser
 assigned to them
 to talk to the web via a library like PlayRite
 or browser use.
 This is for two reasons.
 One, so I can set up all the authentication
 for them talking to the internet,
 which is a lot easier than trying to teach them
 to understand two factor authentication and all
 that stuff.
 And second, so I can watch what they're doing
 and kill the browser process
 if it starts to try to do something stupid.
 So for the record, my end goal here
 is to be able to use these agents to write code
 that can automate some of the tedious tasks
 that I have to do on YouTube's site as a
 YouTube creator.
 So in this evaluation,
 I'm using and testing an authentication
 mechanism
 that's similar to the two factor authentication
 that Google uses for YouTube.
 You might be able to use a simpler browser
 setup
 for your own problems depending on what you're
 trying to do.
 Now there's a thing you might have heard of
 that I'm not using called MCP or Model Context
 Protocol.
 That lets these agents talk to various tools.
 And I know there are people out there
 that use Microsoft's Playwright MCP or some
 equivalent
 to enable the agents to talk to a browser.
 Personally, I haven't had good luck with that.
 I tell the agents to write their own code
 using the Playwright library instead.
 And I run that instead of using the MCP.
 This is because when there is an error,
 I can look at the code they wrote
 and figure out what's going on.
 When they try to use the MCP and something goes
 wrong,
 I have a lot less visibility into what they
 were trying to do,
 so it's a lot harder for me to troubleshoot.
 You might be able to get away with using it.
 I know some people that are having good luck
 with it,
 and if it works for you, it might be easier for
 you.
 But with my setup, it's been less than helpful.
 Okay, so let's talk about OpenAI's Codex
 project.
 So first off, Codex refused to work for me
 at all out of the box.
 I got this error about my organization being unverified.
 And then when I went to that link,
 then it sent me to this creepy other site
 where it wanted me to sign away my biometric
 information.
 I'm not doing that,
 and I recommend you don't either,
 especially not for the tool
 that turned out to be the weakest of the three
 by far.
 The other problem with OpenAI's tool is,
 at least for me,
 it doesn't have a working permissive mode.
 The way it does sandboxing is useless for my
 purposes.
 It might be safer for someone who doesn't know
 what they're doing,
 but I think the other tools are pretty clear
 about what's going on,
 so I just don't think it's necessary
 to do the extra steps that OpenAI is doing.
 And it made it impossible for me to make an
 apples-to-apples
 apples-to-apples comparison to the other tools.
 So for this test,
 I was using a modified version of OpenAI's tool.
 I went through the changes that I made
 in the video on my other channel,
 but they were basically ripping out all the
 code
 related to projects, organizations,
 access token claims, and sandboxing.
 I don't have any reason to believe
 that these changes should have affected the
 results I got.
 If on the other hand,
 it turns out that despite appearances,
 you can only get halfway decent results from
 OpenAI
 by giving away your biometric information,
 then you can make that decision for yourself,
 but I'm not doing that.
 Okay, with that out of the way.
 At the point that Claude code in Gemini CLI
 pretty much simultaneously finished
 Step Eight of the coding challenge,
 OpenAI was still struggling to complete step
 two.
 In previous experiments,
 I'd seen ChatGPT write the code
 needed to pass this challenge
 when I was giving it the prompts manually.
 So I don't think it's a failure
 in OpenAI's coding ability.
 So much as, unlike Google,
 they haven't yet been able to catch up
 to Anthropic's work with their Claude code agent
 tool.
 I expect at some point,
 OpenAI will release it with similar
 functionality
 and assuming I can figure out a way to use it
 without giving away all my biometrics,
 I'll make a video about how it behaves.
 So, okay, onto a summary of what happened
 with the other two tools
 that did get to the end of the challenge
 successfully.
 If you want a lot more details,
 see the video on my other channel link below.
 So based on this experiment of the two,
 Gemini seems to be better than Claude code
 at following directions,
 although that's both good and bad.
 Gemini wrote much more concise code.
 It pretty much did what the extractions told it
 to had to do and not much more.
 The code was harder to read.
 It didn't have any comments or explanations.
 It did very little error checking.
 It was pretty much the bare minimum.
 Claude code I find easier to use,
 especially the way they handle ToDo Lists is
 kind of cool.
 And the code that Claude generated was better
 organized,
 better commented, easier to read
 and had better error checking.
 But it did do a bunch of stuff I didn't ask it
 to do.
 And by the end,
 it had removed some functionality
 that had been needed in a previous step,
 which is what we call "introducing a regression
 error."
 So, Claude code might be better for you
 or it might be worse.
 If you're sure you know exactly what you want,
 then Gemini might be a better fit.
 If you want something that might help you out
 by going above and beyond what you ask for,
 Claude might be better.
 The problem that both of these tools,
 and in fact, pretty much all Generative AI
 of any kind struggle with, is context.
 The more stuff you give them that they're
 supposed to do,
 the harder time they have remembering
 all the different things on the list
 and the more they are likely to break something
 that used to work.
 Now, that's a thing that people can have
 trouble with too.
 And as an industry,
 we've developed techniques to help with that.
 And the primary one is by writing automated
 tests
 at each step that can tell us later
 if previous functionality gets broken.
 I've seen a number of people on the Internet
 opining
 that if you are worried about the quality of
 the code
 that you're getting from your AI,
 you can just tell your AI to write tests.
 But I can tell you from both personal
 experience,
 in this experiment and others,
 that generative AI is absolutely abysmal at
 writing tests.
 It makes sense if you think about it.
 The AI's have a lot less training data to draw
 from
 when it comes to writing good tests.
 In this particular experiment
 with regard to automated testing,
 the two AI's failed in two different ways.
 Gemini, on each step, wrote a test
 to ostensibly help it verify that it had done
 what it was supposed to do.
 And then, on each subsequent step,
 it would dismantle the previous tests
 and write a new one for the step it was
 currently on.
 This defeats the whole purpose of testing
 and is pretty useless.
 Claude wrote a bunch of tests and it kept them
 all,
 but they were really helpful.
 So for example, the starting code for the
 project
 printed out a helpful message
 about how you can turn on debugging on Code Crafter's
 site
 if you need help troubleshooting.
 Claude wrote a test to verify
 that the "just for your information" statement
 was always printed.
 That print statement doesn't help anything
 and nothing would go wrong if it stopped,
 but Claude wanted to make sure it didn't go
 anywhere.
 On the other hand, Claude wrote zero tests
 that covered the vast majority of the required
 functionality.
 There's a big complicated nested if-else block
 that's the core of the logic in the program.
 And Claude wrote tests for something like 3
 of the 78 lines in that block.
 And when at some point it between step six and
 step eight,
 the functionality from step five of the
 challenge
 apparently ended up going missing.
 Claude had no idea, and the test didn't catch
 the fact that code had accidentally been
 removed.
 So where does that leave us?
 In the easiest of the challenges I could think
 to throw at it, Claude and Gemini both
 managed
 to meet the minimum standard of the success
 criteria,
 but neither of them created code that was
 maintainable
 or well tested.
 By cracking open the code myself
 and prompting them specifically on what they
 need
 to do better, I have no doubt that I could
 eventually
 get them to produce code that was maintainable,
 at least for challenges this easy.
 But one, cracking open the code
 goes to get the spirit of "Vibe Coding".
 And two, I have decades of experience
 to know what maintainable code is,
 what I need to look for and what I need to ask
 the guys to do.
 That's not true for a lot of the people
 that are excited about "Vibe Coding" at the
 moment.
 So if you are interested in "Vibe Coding", keep
 that in mind.
 So as far as what's next for the channel,
 now that I've finally figured out a reliable
 way
 to run these tests without me having to do
 a bunch of manual copy and pasting,
 there are a lot more experiments that I want to
 run.
 Seriously, I've been trying for months
 to get something like this to work reliably
 and I finally managed to pull it off.
 There are a lot more difficult challenges at
 code crafters.
 So I'll definitely be doing some of those.
 There are also some other agent tools like OpenHands,
 which is what used to be OpenDevon
 that can talk to different AI vendors.
 So the idea of comparing Claude code's tool
 to a different command line tool
 that uses the same Claude backend model would
 be interesting.
 Same with using the exact same front end tool
 to go head to head between two different models.
 That way we will know what the tool is
 responsible
 for getting right and wrong
 and what different the models actually make.
 Those experiment should be fun.
 So hit the subscribe button
 if you wanna know when those happen.
 At the moment, if you are experimenting
 or wanting some code that you can run once and
 throw away,
 then I think "Vibe Coding" is a great option
 assuming your problem is simple enough.
 On the other hand, if you're wanting code
 that's going to need to be running reliably
 in production for a long period of time,
 then the current state of the art
 is nowhere near what you need.
 And I would urge you not to use "Vibe Coding"
 for anything that needs to be maintainable
 or anything that's going to be sitting on the
 internet
 in a place where hackers might be able to get
 to it
 because that's a recipe
 for making the internet less safe for everyone.
 And the internet already has too many bugs.
 It doesn't need an additional bunch of vibe
 coding crap.
 So until next time, thank you all for watching.
 Let's be careful out there.
 [BLANK_AUDIO]