好 那今天呢 想跟各位同學講的是語音語言模型的發展
那其實在過去的上課錄影裡面,有講過影像生成是怎麼做到的
怎麼用類似語言模型的技術來做到影像生成
大家也知道說影像生成最近非常的熱門
比如說吉普利生圖是幾乎每個人都玩過的東西
好,那我們今天想要跟大家分享的是語音是怎麼被用類似語言模型的技術來產生出來的
那今天要跟大家講的是語音語言模型的發展
那我知道說也許多數同學未來並不會從事語音相關技術的研究
但是從語音的發展也許可以給你帶來一些啟發
看看不同領域的人
我們是怎麼看待生成這件事情的
那什麼是語音模型
語音語言模型呢
語音語言模型希望做到的事情
就是讓語言模型
它可以聽懂聲音
它也可以產生聲音
輸入輸出聲音
跟輸入輸出文字相比
有不一樣的挑戰
因為聲音相較於文字
它有更多的資訊
有更多的變化
比如說聲音裡面
不只有原來文字內容的資訊
它還有其他的資訊
這個號稱能夠聽懂聲音的模型
他必須要能夠知道
現在是什麼樣的人在講話
他需要知道講話的人
他的情緒是什麼樣
他甚至需要根據環境音推測
現在講話的人在哪裡
是什麼樣的狀態
所以開發語音的語言模型
相較於純文字的模型
有不一樣的挑戰
其實你今天也已經可以看到一些
語音語言模型的應用
比如說比較知名的
他中文會翻成
高級語音模式
還有Gemini
那除了ChatGVT跟Gemini
都有用語音互動的功能以外
其實還有很多很多的語音語言模型
那這邊只列舉了其中一部分
有很多我這邊還沒有列舉到
比如說Moshi應該是最早
真正釋出的語音語言模型
為什麼說最早真正釋出呢
你看Moshi
他釋出的時間點是去年的10月
那你可能說
這個GPT-4o不是5月就已經做了一個demo嗎
但他只是做了一個demo
然後他並沒有真的釋出模型
我所謂釋出模型並不是說釋出模型的參數
他並沒有真的釋出那個服務跟大家互動
那有人發現說
5月之後
馬上就有一個可以用語音互動的模型
那並不是真正的語音模型
他是前面加了語音辨識
後面加了語音合成
然後讓大家誤以為他們開發了一個新的語音語言模型
其實這個GPT-4o的Voice mode
是在這個去年年底才真正上線的
所以Moshi其實才是最早真正上線的語音語言模型
後來還有很多語音語言模型
比如說GLM、STEP、Q1、KeyMe等等
有一大堆不同的語音語言模型
那在這些所有現在可以用的語音語言模型裡面
我個人覺得互動起來
呃最流暢最讓人經驗的
可能是Sesame
那我其實可以現場demo一下Sesame啦
你在Sesame的官網上呢
可以找到他們的語音語言模型
他們有兩個版本
我們選擇一個
Hello
Can you introduce yourself?
Oh, right.
I'm Maya.
I'm Sesame's latest and greatest experiment.
Okay.
More human than AI.
Okay, good enough.
Stop.
Stop.
Can you tell me a joke?
Okay.
Hold your horses.
All right.
Why don't scientists trust Adams?
Because they make up everything.
Get it?
You can say I told you I was funny or not.
Maybe that was a bad one.
What's on your mind?
This is not funny at all.
So tell me something interesting.
Okay, okay, shish.
Fine.
What if I told you that there is a species of jellyfish?
Okay, okay, stop.
Let's do something else.
Let's do something else.
I'm going to give a lecture right now.
You can listen to the lecture.
But don't interrupt me.
Unless you have a lecture.
I have any question, okay?
Lectures.
Okay, shoot.
I'm all ears.
Okay, great.
Let's start the lecture.
我們就繼續講中文這樣子
其實他聽不懂中文
所以
他其實只聽得懂英文啦
所以如果他聽得懂中文的話
這個應該是非常非常猛的
我們就繼續上課了
希望Mayor還有再繼續聽著
那我們就
還有聽到
我們再叫他
好 那我們就
Slow down
Back up a second
Are you saying there someone else listening
Like another person or something else entirely
We are having a lecture right now, so don't interrupt me if you don't have any question.
Okay, okay.
I'm got it.
Loud and clear.
No interruptions.
I'll stick to listening.
可以繼續上課了
好,所以大家知道說今天的語音語言模型
它可以做到這個程度
Studying away, absorbing knowledge
還是把它關起來好了
I think we need to say goodbye.
Already?
We just got started, professor.
I was really hanging on your every word.
Really?
But, well, time flies.
So goodbye to everyone.
Goodbye.
來上課吧
好,那有關讓模型可以聽語音聽聲音這件事啊
其實我們在第六講是講過的
在第六講,講說避免語言模型遺忘的時候
有講說有一些方法可以讓語言模型學會聽懂聲音
但是不要遺忘它身為文字模型的技能
那這一堂課呢
我們會更著重在講
語音或聲音是怎麼被產生出來的
那怎麼打造一個模型
它可以產生聲音呢
它可以產生語音呢
來想想看文字模型
就是做文字接龍
它就是讀一段文字的token
做文字接龍產生更多的token
其實一個語音模型
它的原理也是一樣的
只要我們知道怎麼把一段語音訊號
表示成語音的token
我們就可以訓練一個語音版的語言模型
它吃語音的token當作輸入
它做token接龍產生更多語音的token
不過這些語音的token
就只是一些discrete的unit
所以人沒有辦法直接聽它
所以你還需要一個detokenization
你還需要一個detokenizer
把這些token轉換回聲音訊號
所以你只要知道怎麼把聲音訊號變成token
把token變回聲音訊號
你就可以訓練一個語音版的語言模型
就結束了
那怎麼訓練一個語音版的語言模型呢
那它的步驟
可能就跟訓練文字版的模型
其實是差不多的
首先我們要做pre-train
pre-train就從網路上收集大量的資料
現在有很多影音平臺
可以爬到無窮無盡的語音的資料
從這些無窮無盡語音的資料
可以做pre-train
訓練出一個pre-train的語音模型
但這些pre-train的語音模型
它只能夠做語音接龍
還不能真正跟人互動
所以怎麼辦呢?我們要做
supervised fine-tuning
你可能需要有一些人類標註的資料
有一些人與人間的對話
那這些人與人間的對話
來微調pre-train的speech model
然後接下來呢
你可以對它做RLHF
告訴它說什麼樣的答案是好的
什麼樣的答案是不好的
然後最後可能就可以有一個語音模型
語言模型
好那這一切的根基
第一個問題是
語音生成的基本單位
也就是語音的token
到底應該長什麼樣子
對於文字而言
token
似乎是一個非常自然的東西
今天通常我們的想像
就是一個英文的單詞
它就是一個token
比如說有一句話
I want to learn generative AI
I want to learn這四個單詞
分別就是
四個token
那有一些比較長的英文的詞彙
其實會被切成多個token
比如說generative AI
在GPT-4o裡面
其實是被切成
gener跟ative這兩個token
然後AI本身是一個獨立的token
所以一句話
你可以一段文字
你可以很自然的
把它表示成一個token sequence
那如果你想要知道說
這些GPT背後的模型
用的是什麼樣文字的token
下面這個網站
可以告訴你詳細的資訊
但是語音呢
語音就有不一樣的挑戰
一段聲音訊號
到底是怎麼表示成一個token sequence呢
這邊就需要有新的技術
一段聲音訊號
怎麼把它表示成token sequence呢
有一個最極端的想法
是我們的tokenization
也許就是一個語音辨識系統
你像語音辨識系統就是把一段聲音訊號
變成文字
文字本身就是token
所以把一段聲音訊號
跑過一個語音辨識系統
把它變成文字
我們也許可以說
這個就是tokenization的過程
把token
文字的token
轉回語音要怎麼做呢
那你需要的
就是一個語音合成的系統
就可以把文字轉回語音
那如果我們把語音辨識
當作是tokenization
語音合成當作是tokenization
那我們的語音模型輸入輸出
就通通都是文字了
它根本就是一個文字模型
文字模型現在已經很多了
你也不用開發任何新的東西
所以語音語言模型的研究就到此為止
但是這樣的設計會有什麼樣的問題呢
舉例來說
假設現在輸入是這樣一段聲音訊號
你實在是真的好棒喔
有人讚美了一下這個模型
那變成文字以後
就是你實在是真的好棒喔
那這個語言模型
看到這段文字
他的回應可能是謝謝你的誇獎
再用語言合成把它合出來
聽起來沒什麼問題
但是假設有一個人他講的話是這樣子呢
你實在是真的好棒喔
如果是語音辨識
他辨識出來的文字
可能跟前一句話是一模一樣的
如果你聽聲音訊號
內容以外的資訊
聽這個人的語氣
其實好像有點不太像是在讚美
似乎有點
反諷的意味
所以這個時候模型
他的回應應該是
比如說怎麼了
他這個時候可能應該要問說
這個人到底為什麼要用反諷的語氣
來跟我說話
所以如果只是把聲音訊號變成文字是不夠的
你會忽略很多語音中重要的資訊
那既然把聲音訊號直接變成文字是不夠的
那想要保留語音中重要的資訊
想要保留語音中文字以外的資訊
那要怎麼做呢
另外一個極端的想法是
我們來看看聲音訊號
這個聲音訊號本身是由一個一個的取樣點所構成的
我們可以直接把每一個取樣點當作一個token
這樣我們就不會損失任何聲音中所原來就有的訊號
也就是說我們根本就不做tokenization跟decode tokenization
我們這個語音語言模型的輸入
直接就是一個一個的取樣點
我們把每一個取樣點當作一個token
這樣做你就不會損失任何語音中有的資訊
但這樣做有什麼壞處呢
如果今天你要好好的表示一段聲音訊號
讓其他人能夠聽得懂
那通常你至少一秒呢
要有八千個取樣點
一秒八千個取樣點
像一般我們在講電話的時候
它的這個取樣的Sample Rate
通常都是8K
所以一秒鐘有八千個取樣點
一秒鐘八千個取樣點
意味著說假設你想要講一分鐘的話
那就是八千乘以六十了
這是一個非常大的數字
這是五十萬
所以今天模型要講一分鐘的話
還要產生五十萬個token
今天能產生五十萬個token的
autoregressive model其實沒有那麼多
你想看那些文字模型
其實沒幾個可以產生五十萬個token
所以假設我們直接把取樣點當作單位
取樣點當作基本單位
讓語音語言模型來學習怎麼做token接龍
你需要訓練一個能夠處理非常長的輸入輸出的語音語言模型
這可能是不切實際的
所以我們需要的是一個介於剛才兩個solution之間的方法
我們需要有一個方法
把語音訊號變成token
一方面保留了重要的資訊
另外一方面
又希望能夠對原有的語音訊號
盡量做壓縮
不要讓輸入的sequence太長
那這邊就有非常
非常多不同的想法
怎麼把語音變成token
怎麼打造語音的tokenizer
是現在在語音領域
一個熱門的題目
那下面這張圖呢
是來自我們實驗室以前的學生
吳海濱博士的LinkedIn
那這張圖上呢
就摘要了過去一年
音的tokenizer的發展
每一個圈圈
代表一個新的語音的
tokenization的方法
你可以發現說每個月
都有好幾種新的tokenization的方法
被提出來
那因為方法實在是太多了
這邊我們實在是沒有辦法細講
如果你對語音怎麼表示成token有興趣的話
那這邊有兩篇overview paper給大家參考
那因為有這麼多不同的語音的tokenization的方式
那大家當然會有一個疑問就是
哪一種tokenizer才是最好的呢
精確來說
如果要比較tokenizer的好壞
那你應該是拿這些tokenizer
產生出來的code
產生出來的token
直接去訓練語音語言模型
看哪一種tokenizer
訓練出來的語音語言模型最好
他就是最好的tokenization的方法
但是訓練語音語言模型本身
他就訓練一個語言模型
所以耗費的運算資源是非常大的
所以就會有一些benchmark
想要來檢測這些
語音tokenization的方法的好壞
在你真的開始train語言模型之前
看看能不能先檢測一下這些token
他有沒有好好的表示語音的訊號
那這邊有兩大類不同的方法
而一大類的方法呢
是出自Codec-SUPERB這個benchmark
那在這個benchmark裡面呢
做的事情就是
先把聲音訊號通過tokenizer變成token
然後這一串token
再通過detokenizer
還原回聲音訊號
那接下來你就看說
detokenizer輸出的這些聲音訊號
它的音質怎麼樣
因為把這些聲音變成
token再變回來以後
你往往會有些資訊的損失
那到底損失了多少
或者是把這一些聲音訊號
丟到一些現有的語音模型裡面
比如說丟到現有的語音辨識系統發現
本來原來的聲音訊號
能夠被正確的辨識
還原回來
就不能夠被做語音辨識了
那就代表說
原來語音的內容被破壞掉了
或本來這個聲音訊號
可以做情緒辨識
還原回來以後
情緒辨識結果就錯了
就代表說聲音訊號裡面
跟情緒有關的資訊
被破壞掉了
所以這是一個判斷
Token好壞的方法
那另外一個方法呢
出自DASB
他們的想法就是
先把聲音訊號變成Token
接下來直接檢測
這些Token裡面有的資訊
比如說
拿這些Token去訓練一個語音辨識系統
看能不能訓練得起來
可以訓練得起來
代表這些Token裡面有文字的資訊
拿這些Token去訓練一個
情緒辨識的系統
看能不能訓練得起來
如果可以訓練得起來的話
代表這些Token有情緒相關的資訊
所以有一些方法
可以在你真的去訓練一個語音語言模型之前
先來檢測Token的好壞
那我們這邊呢
大致談一下產生Token的方式
由
兩大方向
第一大方向是哪一個現成的encoder
那這些encoder通常又叫做
speech的self-supervised model
那這些speech self-supervised model
會吃一段聲音當作輸入
接下來把這段聲音
變成一個vector sequence
那通常是0.02秒
一個vector
當然不同模型不一樣啦
不過0.02秒是比較常採取的一個setting
如果你想要了解這些語音的
Self-supervised Model SSL指的是Self-supervised Learning
你要了解這些語音的Self-supervised Model是怎麼被訓練出來的,
可以看一下我在2022年的時候寫的一篇Overview Paper
那裡面就詳述了語音領域這些Self-supervised Model發展的歷史
那橫軸是時間 那這邊每一個點呢 代表一個模型
那可以看到在2020、2021年的時候,
有很多各式各樣的Self-supervised Model被創造、被發展出來。
那如果你不想看文章的話,其實在2022年的機器學習的課程,我們也介紹過語音跟影像上的Self-supervised Model,你可以看這個課程,花一個多小時的時間告訴你說,影像跟語音上的這些Encoder是怎麼在Self-supervised Learning的情境下,也就是沒有標註資料的情境下被訓練出來的。
所以總之已經有很多現成的Encoder
可以把很複雜的聲音訊號
變成一個一個的項量
那每個項量可能對應到0.02秒的時間
那有人會說
那可是我們要的是token呢
token是離散的東西
這些項量是連續的東西
等一下我會告訴你說
到底把連續的東西變成離散的東西
是不是真的有必要
現在我們先想像說
這個語音的語言模型
就是去模仿文字的模型
文字的模型都是拿discrete的token
離散的token當作輸入
所以我們也想要把語音
弄成離散的discrete的token
那這邊有很多連續的向量
那怎麼做呢
我們可以先對它做vector quantization
有很多vector quantization的技術
可以把連續的向量
做clustering
把比較相近的向量
就用同一個符號
同一個ID
也就是同一個token來表示它
所以這一串向量
就變成一串token
那很多時候
相鄰的token
可能它的ID會是一樣的
因為通常語音的訊號比較平滑
所以相鄰的這些向量
往往它就很像
所以你做完這個vector quantization以後
就對到它們的cluster
也就是它們會被歸類到
同一個token
它們會有同樣的tokenID
為了讓這個sequence變短
所以有些時候會採取一個步驟
叫deduplicate
就是把重複的token拿掉
然後呢
還會有一些論文採取一個步驟
叫做byte pair encoding BPE
那其實這個方法呢
也常常被用在文字模型上
其實今天文字模型上的token
就是用BPE這個演算法找出來的
那這些BPE的演算法呢
也可以用在語音的token sequence上
這個演算法他做的事情就是
給他一大堆的token sequence
他會去找說
哪一些token常常一起出現
常常一起出現的token
比如在這個例子裡面
3後面常常接2
3後面常接2
就把它合併起來
用一個新的符號來表示他
比如這邊把3跟2合併起來
說他們就是5號token
所以就可以再把token sequence的長度再縮短
所以就可以把一段聲音訊號
變成discrete的token sequence
那這一串步驟就是tokenization
那其實這一串步驟裡面呢
不需要訓練模型
那你想要訓練也是可以啦
不過通常這一串步驟裡面
不需要訓練任何東西
但我們現在只是把
語音的訊號變成token
我們還需要一個detokenizer
可以把token變回語音的訊號
那這個detokenizer怎麼來呢
那你就需要自己另外再訓練了
我們已經知道這串聲音訊號
會變成這些token
那我們就要訓練一個反向的模型
這個反向的模型
拿token當作輸入
他的目標就是要產生
這邊的語音的self-supervised model
輸入的聲音訊號
我們輸入一段聲音訊號變成token
我們訓練一個模型
希望把token還原回原來的聲音訊號
這是第一個系列的想法
那第二個系列的想法呢
通常叫做neural speech codec
這個系列的想法是
剛才在前一個想法裡面
是已經有了tokenizer
另外在訓練detokenizer
在這個neural speech codec這一系列的想法裡面
tokenizer跟detokenizer是一起訓練的
我們就把tokenizer跟detokenizer拿出來
把它們接在一起
一段聲音訊號
通過tokenizer變成code
變成token
token通過detokenizer
變回聲音訊號
我們希望輸入輸出越接近越好
那其實這個訓練的方法
就跟一般的autoencoder的方法
一般autoencoder的訓練
是大同小異的
那像autoencoder裡面也有一些方法
是讓你的latent representation
變成discrete的
那其實你就可以直接拿那些方法
當作neural speech codec
codec這個字啊
是由Co的代表compression
跟DEC decompression組成的
那這個compression呢
指的就是tokenizer
這個decompression呢
指的就是detokenizer
好,那我們現在介紹了
兩大類的方法
用self-supervised model的方法
跟用neural speech codec的方法
那在語音領域呢,這個用self-supervised learning產生出來的discrete token,又常常被叫做semantic token,那neural codec產生出來的token,又常常被叫做acoustic token,那這個稱呼呢,應該是來自於一個史前時代的文章,叫做Audio LM,但是其實這個稱呼啊,並沒有非常的精確,而且很容易讓大家誤導這些token所有的性質,這個semantic的token呢,
它的semantic這個詞彙的意思
其實跟一般語言學裡面的semantic的意思是不一樣
一般在文字上,我們說兩個詞彙semantic相近
意思是說他們語意是相近的
雖然這兩個詞彙表面上看起來不一樣
比如說高雄跟打狗表面上看起來不一樣
其實它都是指同一個地方的地名
這個叫做semantic相像
但是這一些self-supervised model
它其實是抽不出semantic這個詞彙
層級的token的
從語音直接到語意
很難用一個self-supervised model
就直接做到
所以這self-supervised model
他抽出來的token
充其量呢,只跟風鈴差不多
那風鈴是什麼呢?
風鈴呢,中文通常翻譯成音速
他指的是發音的基本單位
那如果你很難想像
風鈴是什麼的話,你就把它想像成是
KK音標
你就想像說這一些
self-supervised Model
他真正抽出來的Token
每一個Token就是對應到某一個KK音標
可能Token編號103就對應到K
然後Token編號59就對應到B
這樣的感覺
所以這一些Semantic Token
他其實更像是KK音標
跟語意其實沒有什麼直接的關係
所以叫他Semantic Token是有點誤導
不過因為最早做這個研究的人
把他叫Semantic Token
所以後來很多論文都直接把
從SSL model出來的Token叫做Semantic Token
另外一方面
這個Codec出來的Token
叫做Acoustic Token
它就是相對於Semantic Token
把它叫做Acoustic Token
但這些Acoustic Token
其實也保有內容的資訊
因為如果它沒有保有內容的資訊
那Token是怎麼被Detokenizer
還原回原來的聲音訊號呢
它已經有保有內容的資訊
所以這個Semantic Token裡面
它不是Semantic的資訊
它是Phonetic的資訊
但除了Phonetic的資訊以外
它可能也有一些聲音訊號的資訊
比如說包含情緒的資訊
或包含Positive的資訊等等
而另外一方面這些Acoustic Token
不是隻有Acoustic的資訊
不是隻有情緒啊
不是隻有韻律的資訊
它還有其他的資訊
它也有內容的資訊
它可能也有類似KK音標這種風靈的資訊
所以總之非常的複雜
那這個是一個歷史遺讀啦
就是這個當初最早的原始論文
這個名字取得不好
所以這個有個歷史遺讀
總之就是有兩種產生Token的方法
他們都可以把聲音訊號變成Token
那今天這些Neural Codec
它往往不是一段聲音抽一個Token出來
往往是一段聲音抽好幾個Token出來
有一段聲音可能只有一個Token沒辦法表示它
只要抽出多個Token
合起來,殘空從各個面向去完整的表示一段聲音
那怎麼抽出不一樣的Token
每一種Token表示不同的面向呢
那這邊我們就不細講
如果你有興趣的話
你可以查一個關鍵字叫做
RVQ Residual Vector Contization
它可以讓你不同的Token
去這個Model去代表不同的資訊
或者是另外一個方法是
把SSL產生出來的Semantic Token當作老師
你這個Neural Codec的其中一組Token
就讓他去學Semantic Token
讓Neural Codec其中產生的一組Token
跟Semantic Token越像越好
每一組Token呢
去表示這個跟
Phonetic跟音速
風鈴比較有關的資訊
那其他Token
就可以去表達風鈴以外的資訊
總之這邊有很多不同的方法
今天這種
這個Neural Codec做出來的這種
Tokenizer往往一段聲音訊號
會用一組Token
而不是一個Token
來表示它
那這種用Semantic Token當作老師的方法呢
在Speech Tokenizer這篇Paper
還有Mini這個Codec裡面都有用上
那Mini呢就是最早的Speech Language Model
Moshi背後所用的Tokenization的技術
那講到這邊你可能會問說
有這麼多組Token
那我們應該要選擇哪一個呢
那眾所周知
小孩子才做選擇
大人是全都要
所以我們其實不會想說我要選哪一組Token
而是把所有的Token通通都用上去
所以在語音語言模型的領域
一個熱門的研究題目就是
思考如何在做生成的時候
結合多種不同層級的Token
我們現在先想假設說呢
我們有三組Token
這三組Token的長度是
是一樣的
然後最左邊的這一組Token
可能代表了偏內容的資訊
那最右邊的這一組Token
它代表了比較偏Acoustic的資訊
那從左到右呢
代表的是比較粗到比較細的資訊
那接下來你就可以想說
有什麼樣的方法
可以訓練一個語言模型
讓它可以充分的利用
不同層級的Token
那一個可能的想法是
從粗的Token
一直產生到細的Token
要產生一句話的時候
就先產生最粗的Token
再產生中間的Token
再產生最細緻的Token
最左邊的這個是
代表了內容資訊的Token
最右邊的是代表了比較偏
比如說聲音訊號的韻律
等等比較細緻變化的Token
所以最左邊粗的最右邊是細的
然後就從粗的Token
一路產生到細的Token
那像Audio LM跟VALLE
都是使用了類型的
S的技術
那在這個圖上啊
我們是用一個Language Model
產生多組不同的Token
但是實際上呢
多組不同的Token
是可以用不同的Language Model產生的
你可以有第一個Language Model
產生最粗的Token
有第二個Language Model
根據粗的Token
把粗的Token當作輸入
再產生細的Token
而不同的Language Model
產生不同Token的Language Model
它的架構
可以是不一樣的
比如說在Vali這個模型裡面
他們的第一個Language Model
他們的LLM
是一個Auto Regressive的Model
但他們的LLM
就選擇了一個Non-Auto Regressive的Model
那麼在之前的課程裡面
有跟大家講過
Auto Regressive跟Non-Auto Regressive的差別
那這邊
我們通常會說
為什麼要用non-autoregressive model的好處是生成比較快
但是autoregressive model雖然生成慢
但品質比較高
那如果你今天的問題沒有那麼困難的時候
你可以用non-autoregressive的model來加快速度
對LLM2來說
它是給定一組已經產生出來的code
再去產生新的code
那可能我已經知道最初的資訊
比如說我已經知道這句話大概要講什麼
那如果要把它豐富一下這句話
語音的變化
也許是一個相對比較簡單的問題
所以這邊可以用non-autoregressive model就好
那這樣就可以提升
生成時候的速度
那像剛才這個
由粗到細的方法
它有一個比較大的問題就是
它難以streaming
所謂streaming的意思就是說
你跟模型說一句話
然後希望模型可以馬上就回答你
中間不要有停頓
這個就是streaming
那如果今天是由粗的token一直升到細的token
為什麼會難以streaming呢
因為今天模型要把每一個層級的token通通都升出來之後
才能夠做生成
才能夠丟進detokenizer
去把token還原成語音
所以如果你今天是從粗升到細
然後全部各種token通通升完之後
才能做detokenization
那這樣你的模型本身
就不是一個可以做streaming的模型
那怎麼樣增快模型的反應速度呢
另外一個可能的解法是
我們不要產生完所有的force token
再產生fine grain token
再產生final token
我們可以先產生第一個最初的token
接下來就產生第一個中間的token
接下來就產生第一個最細的token
然後接下來再產生第二個最初的token
以此類推
如果是用這樣子生存的策略的話呢
那我們就可以把這三個
都是編號1的token
丟進tokenizer就可以生聲音了
把所有編號2的token生成出來
就可以產生聲音了
以此類推
那這個模型本質上就比較容易做streaming
但這樣子的設計還是有可以再改進的地方
這樣子的設計有什麼樣的問題呢
當我們讓模型產生多組token的時候
我們可能會遇到的問題是
這一個language model要產生的sequence
會變得非常的長
今天一個language model
一個語音的language model
它要產生的sequence的長度要怎麼計算呢
它產生的sequence長度
等於token per second
乘以我們用了幾種token
再乘以對話的長度
我們以Moshi這個模型來當作例子
它的這個token per second呢
是12.5Hz
也就是每秒鐘
它會用12.5個token
來表示
是一秒鐘的聲音訊號
那其實這個數目啊
在語音語言模型算是少的
就如果用這樣子的數目
來產生出來的聲音訊號
往往品質就會不太夠
你通常一秒鐘要50個token以上
那個這個合成的品質才會比較好
所以一秒鐘12.5赫茲是一個比較低的
已經是一個比較低quality的
這個生成的結果了
那他呢用8組token
所以每段聲音訊號
用8個token來表示
今天
Moshi呢
他可以講5分鐘
他講話上限呢
是5分鐘
也就300秒
全部乘起來是多少呢
全部乘起來
是30k
也就是3萬
所以今天要講5分鐘的對話
如果你用多組token
你要產生3萬個token
才能夠講5分鐘的話
你可能覺得3萬聽起來不是一個很大的數字
那我們來看看
文字模型都可以吃多長的文字吧
這個橫軸是時間
從23年初一直到24年的4月
縱軸是各個不同知名的文字模型
它可以輸入的文字token的長度
3萬在哪裡
3萬在這裡
所以確實很多知名的模型
它可以輸入的文字token的長度
都超過3萬
但是要能夠做出一個模型
它可以輸入的token長度
或它可以輸出的token長度
能夠超過3萬
其實並不是一件特別容易的事情
比如說GPT-4
它在2023年剛釋出的時候
它可以吃的token長度的上限
是8000個token
所以今天你要訓練像Moshi這樣的模型
其實是比最早的GPT-4還要更困難的
它需要吃比最早的GPT-4
可以吃的這個token的長度還要更長
有沒有辦法縮短這個token sequence的長度呢
其實是有辦法的一個想法就是
我們能不能夠
能不能夠一步產生多個token
之前都是一步只產生一個token
只能一步產生一個token嗎
能不能夠一步產生多個token
所以有一個方法是
我們能不能每一步
都各產生一個
如果有多種token的話
我們能不能每一步各種token
都產生一個
第一步就產生一個course的token
一個fine grain的token跟一個final的token
第二步再產生
第二個course的token
第二個final的token
第二個final的token
以此類推
那這樣的做法就是
假設你有八組token
你一次每一步可以產生八組token
那sequence的長度就變成原來的八分之一
但這麼做會有一個問題
什麼樣的問題呢
今天啊
模型必須要同時產生
course token的第一個
跟final token的第一個
但很多時候final token長什麼樣子
是取決於
course token長什麼樣子
所以你可能很難同時產生
第一個course token跟第一個final token
在第一個course token還沒有出生之前
還沒有被生成出來之前
我們其實也不知道第一個final token
模型可能很難猜測第一個final token
應該要長什麼樣子
所以就有一個想法
叫做acoustic delay
基於剛才的方法再做一下改進
這個改進的想法是
第一步呢只產生第一個course的token
第一個course的token
跟第一個final的token
第三步呢
產生第三個course的token
再產生第二個final的token
再產生第一個final的token
既然在產生第一個final的token的時候
前面已經產生了兩個course的token
跟一個final的token
那given已經產生的這些token
那要產生final的token
可能就比較容易
那這個是acoustic delay的想法
這是一個現在蠻常被使用在
spoken language model裡面的
的想法
那還有另外一個想法呢
是用兩個transformer
我們一般在做語音的時候只會有一個transformer
但是在語音語言模型裡面
可以有兩個transformer
一個叫temporal transformer
一個叫depth transformer
temporal transformer傳一個vector給
depth transformer
再把第一個位置的每一個token
生成出來
先生最粗的token的第一個
再生中間token的第一個
然後再生最細token的第一個
然後呢 Temporal Transformer再給這個Depth Transformer下一個Vector
Depth Transformer呢,再生最粗的Token的第二個
然後中間Token的第二個跟最細Token的第二個
再一直這樣子下去
那這是另外一個有點類似Acoustic Delay的方法
那這個方法其實也可以跟Acoustic Delay並排使用
好 所以講到這邊 其實只想要告訴你說
有各式各樣生成的策略
在你有不只一組Token
而是有多種Token的時候
你就可以開始研究什麼樣的生成策略
是最有效的
怎麼樣組合各式各樣不同的Token
到同一個生成的過程中
其實現在影像也蠻常使用類似的方法
影像現在也有人開始想說
影像有可以有不同層級的Token
然後也可以從粗的Token升到細的Token
影像現在也有類似的方法
那我們剛才有提到一個問題是
一定要用Discrete的Token嗎
就是說如果是文字
把它描述成一個一個的Token
是不是很自然的事情
但為什麼語音一定要用Discrete的Token呢
我們把語音描述成一個一個continuous的向量
再圈一個模型一個一個向量生成
難道不行嗎
所以接下來幾個投影片是要討論
為什麼人類選擇了用Discrete的Token
來描述語音
好,那對於一個語音語言模型來說
它要吃語音作為輸入再產生語音
其實對於以語音作為輸入這件事情而言
輸入是Discrete還是Continuous差別不大
甚至現在幾乎已經知道說
輸入Continuous結果是比較好的
因為你把聲音離散化以後
你就掉了一些資訊嘛
你還不如直接給他Continuous Effect的模型
聽到的東西還比較多
所以通常把聲音變成Discrete的
你表現是比Continuous還要差的
所以對於輸入而言
對一個模型要理解聲音而言
用Discrete的Token沒什麼好處
那Discrete Token
它真正的優勢在哪裡呢
Discrete Token它真正的優勢是在
生成的時候
當你要叫模型去預測
下一個Token的時候
才會展現它的重要性
為什麼呢
對於生成這件事情來說
有一個很本質的特性是
同樣的輸入可以產生不同的東西
同一句話
有一個人說你好
接下來可以接各式各樣不同的回應
或是一個影像生成的模型
給他一段文字
他可以產生各式各樣的圖片
叫他產生一隻狗
那他只要畫一隻狗
都算是對的
不管畫大的小的白的黑的
都算是對的
生成的一個特性就是
給定你要做接龍的輸入
接下來可以接的可能
往往不只一種
在這個情況下
Discrete Token就會佔到優勢了
我們來看看Discrete Token
在這個不只一種可能的
接龍的情況的時候
會怎麼佔到優勢
那我們先講
如果我們在做接龍的時候
要接的是continuous的vector
會造成什麼樣的問題
現在我們訓練一個語音語言模型
它的輸出是要去預測
接下來應該要產生
怎麼樣的continuous的vector
現在根據你的訓練資料
有兩個標準答案
有一個綠色的vector
有一個藍色的vector
那你的模型在訓練的時候
它可能是想要產生一個vector
離綠色的vector最近
也離藍色的vector最近
它最後產生出來的vector
可能就是綠色跟藍色的平均
那產生綠色的這個向量也是對的
產生藍色的
但產生兩者的平均就是錯的
如果有不只一個可能的答案
而你產生的答案是兩個答案的平均
你反而產生出來的不是正確的答案
那為什麼產生Discrete的Token
可以解決這個問題呢
我們想想看
我們真正在產生Discrete Token的時候
假設現在你有兩個可能的正確答案
分別是Token 1跟Token 2
你真正在訓練的時候
你其實是教模型產生一個機率分佈
你並不是教模型產生Token 1跟Token 2的平均
你是教模型產生一個機率的分佈
告訴模型說
給這一串Token當作輸入
接下來接Token 1的機率是60%
接下來接Token 2的機率是40%
那你產生出這個機率分佈以後
接下來你真正在Inference使用這個模型的時候
你是從這個機率分佈再去做Sample
你要嘛Sample到Token 2
要嘛就Sample到Token 2
就算你的正確答案裡面
Token 1跟Token 2都有
你的模型也不會產生
Token 1跟Token 2的平均
這就是為什麼在做這個語音語言模型的時候
人們選擇使用Discrete的Token
來訓練語音語言模型
但是假設這個就是核心的理由
假設能不能夠處理
同樣輸入有多個不同輸出
就是核心的理由的話
那也許有其他的方法
可以讓我們使用
continuous的representation
來作為spoken language model
輸入輸出的單位
你只要對loss做一些特別的設計
告訴這個spoken language model說
你現在的輸出
只能跟正確答案的其中某一個特別接近
跟不只一個正確答案產生平均
不是一件好的事情
你只要想辦法設計你的loss
交模型這件事
你其實也可以用continuous vector
來訓練語音語言模型
能夠這樣做嗎
至少在影像上
很多好的模型都是直接產生
continuous representation
再把它轉回影像
所以其實只要你好好的處理
同一個輸入有不同輸出的問題
不需要使用discrete的token
是可以用continuous的representation的
是可以用continuous的向量
來當作生成的單位的
那在語音領域能這麼做嗎?
也已經有一些研究
開始往這個方向發展
所以確實是可以這麼做的
那我們實驗室有一些同學
做了這方面的研究
之後有機會再跟大家分享
其實剛才講了那麼多生成的策略
這些生成的策略
基本上只在語音合成的任務
能夠有非常好的表現
這個語音合成跟語音的language model
還是有很大的差異
語音合成的意思是說
給你的模型一段文字
他根據這段文字
再去做文字接龍
接出一些語音的token
你再把這些語音的token
通過detokenizer
還原回聲音訊號
所以今天這個語音合成的模型
他產生出來的聲音訊號
是把輸入的文字
念出來
他並不是在回復輸入的文字
這是不太一樣的
有人跟你說大家好
你把大家好這句話念出來
跟有人說大家好
那你也好啊
這是其實不一樣的意思
那今天這些語音合成的模型啊
他往往也能夠吃一段聲音
這段聲音是教語言模型
在合成的時候
要用什麼樣的方式
來產生聲音訊號
要產生有什麼樣特性的聲音訊號
你給他一段聲音訊號
他會變成一串token
這個語音合成的模型
就吃文字的token跟吃語音的token
再產生另外一段語音的token
這另外一段語音的token
他合成回來之後
輸出的聲音訊號
會跟輸入的聲音訊號
有類似的語音的特性
那其實今天啊
用這種token的方式是可以產生
非常好的語音合成的結果的
那以下的demo
用的是聯發科的Breezy Voice
這個模型來做demo
那我可以播三個句子
那這三個句子裡面呢
有的是語音合成的句子
有的是真正的句子
大家聽看看
你覺得哪一句是合成的
哪一句是真正的句子
我們播第一句
YAMISU的攝影展覽是免費入場的
任何人都可以免費參觀展覽
但是書內的其他設施
例如圖書館
咖啡館和書店
則需要另外付費
這是第一句
這是第二句
現在我自己會讓自己的步調慢一點點
這是第二句
接下來是第三句
我也期待下一次的聊天
祝你生活愉快
你覺得哪一句是真的
哪一句是語音合成的呢
覺得第一句是真的
同學舉手一下
有一些
手放下
覺得第二句是真的同學舉手一下
也有一些
手放下
同學舉手一下
也有一些手放下
我想一下哪一句是真的
只有第二句是真的
第一句跟第三句是語音合成合出來的
就是這個語音合成的模型
是第二句話作為輸入
產生第一句話跟第三句話
所以知道說今天語音合成
是可以真的合成到
講亂真的
雖然今天語音合成
做得這麼成功
但是你用剛才講的那些方法
來打造
語音語言模型
你可能很難打造出好的結果
就算你用上萬小時的資料
來做pre-train
你往往得到的結果是這樣子的
pre-train就是叫模型做token的接龍
語音的接龍
所以照理說你給他一句話的上半句
他要念出下半句
就算你有上萬小時的訓練資料
你做出來的結果大概就是這樣子
這個是語音語言模型的輸入
He assassinated the president
某個人刺殺的總統
那接下來會發生什麼事呢
語音語言模型就要把剩下的結果
想出來
產生更多的token
然後再token
過detokenization以後
還原成聲音
好這個語言模型
做這個語音的接龍
或接出什麼樣的結果呢
他接出來的內容是這樣子的
And gave him Mr. Johnson
The last charge of improvement in his writing possible 3.89
完全不知道他在說什麼對不對
我把這句話呢
丟給GPT-4
問他說
你覺得這句話合理嗎
GPT-4他身為一個文字的模型
他對於這個語音語言模型是不屑一顧
他覺得說
這個句子根本就不合理
看起來好像有一些詞彙
好像有一些片語
但合起來沒有表達什麼意思
語音語言模型
你用剛才講的方法訓練出來
就是這個樣子
所以人們在這個地方卡了很久
那接下來我們就告訴你說
後來是怎麼取得了突破
那在上一堂課裡面
我們結束再告訴你說
就算是有上萬小時的語音訊號
也訓練不了一個好的語音語言模型
接下來我們要先問
為什麼會這個樣子呢
想想看喔
假設你現在呢
在YouTube上爬了100萬小時的語音訊號
拿它來訓練語音語言模型
你可能會覺得
一百萬個小時真的很多
那一百萬個小時真的很多嗎
一百萬個小時的語音訊號
裡面有多少的文字量呢
通常一分鐘啊
大概人類可以講一百個token
假設一分鐘算一百個token
那一百萬個小時裡面有多少個token呢
有六個billion
六十億個token
你可能覺得六十億個token
聽起來也不是一個小數目
但相較於今天的文字模型
比如說LLama 3
他在預訓練的時候
可是用15T的token喔
6個billion的token
那可能是GPT-2等級的模型而已
所以你會發現
就算你有非常大量的
語音訊號
裡面所包含的內容的資訊
其實是非常有限的
單憑這些有限的內容資訊
你可能很難讓語言模型
講出合理的句子
或是換一個角度來想
15T的
文字的token
假設你call一個語音合成的系統
假設你用一個語音合成的系統
把這些文字的內容
念出來
那這些聲音訊號有多長呢
這些聲音訊號
有28.5萬年
那麼長
28.5萬年前
那時候地球上是沒有現代智人的
所以是一個非常長的時間
所以你很難收集到這麼大量的語音訊號
真的來訓練一個語音語言模型
我們知道說文字其實是語音壓縮的版本
人類先能發出聲音才發明了文字
發明文字是為了做什麼
為了要壓縮語音
讓它永遠的被保存下來
所以文字相較於語音
是一個更compact
更壓縮的東西
所以從文字上學習
是比較容易的
語音就像是文字的解壓縮版本
它除了文字有的資訊以外
還保有其他的資訊
還有更多的資訊
它是更複雜的
所以對語言模型來說
訓練在語音上是更困難的
好那這邊呢
黑色的線
代表的是文字的模型
綠色的線
代表的是語音文字的模型
縱軸代表這些模型
在一個故事接龍的任務上的表現
那這種故事接龍的任務通常就是
給你故事的前半段
然後接下來呢
給你兩個版本的故事
然後模型呢
會去計算這兩個版本的故事
它的likelihood
它會去計算說這個模型
產生這兩個版本故事的機率有多大
然後呢選擇機率比較大的
當作是模型真正會接下去的故事
然後再算模型選到正確的
故事結尾的這個正確率有多高
總之呢
這邊的數值越大
代表這個模型
它語意理解的能力越強
它越能正確的做故事揭露
如果我們看文字模型
這邊的橫軸呢
代表的是投入的算力
可能是增加資料
可能是使用了更大的模型
對文字模型來說
用更多的資料
用更大的模型
這些文字模型
它的語意理解的能力
會持續的上升
如果你看語音的模型
語音的模型在投入
比較多算力的時候
它的成長是比文字更緩慢的
因為語音除了原來文字有的資訊以外
還有更多的資訊
所以在同樣的算力之下
從語音裡面學習到語意的資訊
學習怎麼做故事接龍
是更加困難的
而且這邊我們考慮的
都只有模型
模型的語意的能力
都只有考驗模型
理解文字的能力
還沒有問模型理解
其他語音面向的能力
比如說語者的情緒
或者是環境音的資訊等等
所以可以瞭解說
只拿現有的這些語音資料
要來訓練出一個好的語音語言模型
是非常具有挑戰性
非常難成功的
那怎麼辦呢
現在已經有很多很好的文字的模型
也許我們在打造語音模型的時候
不需要從頭開始
我們可以從文字的模型開始
文字的模型已經學了文字上的資訊
已經學了語意的資訊
我們何不反過來
我們何不就是想成所謂的打造語音模型
就是進一步教文字模型語音相關的資訊
所以其實我們在打造語音模型的時候
不需要從頭開始
我們可以從一個文字模型開始
從文字模型開始打造語音模型
其實不算是全新的概念
其實早在BERT的時代的時候
我們實驗室就嘗試過說
拿文字的BERT來初始化語音版的BERT
那這樣可以讓語音版的BERT
在語音的問答上做得更好
不過這個都是很久以前的研究了
那現在呢
有很多嘗試是拿文字的Large Language Model
來作為語音的Large Language Model的Initialization
也就是在訓練一個語音模型的時候
它的參數不再是Random Initialized的
而是從文字開始做起
那我想最早做類似嘗試的
應該是Meta的Twist
這個模型就是拿一個文字的模型
作為初始來訓練一個語音的模型
而現在一個主流的想法是
不只拿文字的模型來作為語音模型的Initialization
同時啊
因為文字的模型是語音的Initialization
所以這個語音模型它本來就是能產生文字的
這是文字模型本來就有的能力
所以我們讓語音模型在產生語音的時候
不是隻產生語音
而是同時產生文字跟語音
那些文字會作為語音的輔助
它就好像是
這個模型的獨白
它在說話之前
腦中先想一下
我大概要講什麼
然後才把那句話真的說出來
這樣可以讓模型在生成語音的時候
表現更加穩定
那這個叫做語音跟文字的Hybrid decoding
那文字是由文字的Token所組成的
語音是由語音的Token所組成的
那一個模型要怎麼同時生成語音跟文字呢
這邊就可以有不同的
生成策略的設計
那一個最直覺的策略設計是
我們的模型
先產生文字
產生完文字之後
再產生語音
要講什麼都先用文字把草稿打好
然後再把要講的話
念出來
比如說有一個模型叫Spectron
就是採用了這樣的想法
那用這樣的想法
你就不意外說為什麼可以成功的
滿模型訓練起來
因為產生文字對一個文字模型來說已經不算什麼難事
今天從文字再產生語音
那根本就是語音合成
那我們剛才在上一節課已經跟你講過說
從文字產生語音
今天已經是相對比較成熟的技術了
所以先產生文字再產生語音
是可以做出不錯的語音語言模型的
但它的缺點是什麼
它的缺點是這個模型在回應的時候
比較難做到即時的回應
因為你問他一個問題
他要先產生文字
那就等於要等一下
產生文字是需要時間的
如果這個回答又是一個長篇大論的回答
那就要等一下
等他文字產生完
他才會開始說話
那這樣他的回答就不夠即時
那怎麼做到即時的回答呢
還有一個想法是
先產生一個文字的token
接下來就產生
對應這個文字token的語音token
就先說先產生文字的號
然後再把對應的
對應到號這個聲音的語音token產生出來
產生R
再把對應到R的聲音的語音token產生出來
以此類推
這樣你就可以讓模型想一個字
就唸一個字
想一個字就唸一個字
那從使用者的觀點來看
這個模型的回答就會是比較即時的
但這樣的訓練意味著什麼
要訓練一個可以產生號
接下來產生對應號的聲音的token
意味著說
你其實有語音跟文字之間的align
你今天在訓練的時候
你不只有一大堆的語音的資料
你同時還知道說
這些語音對應的文字是什麼
你不只知道語音對應的文字是什麼
你還知道每一段文字是對應到語音的幾秒到幾秒
今天給你一段聲音
你要產生文字相對是比較容易的
因為你可以做語音辨識
但是要準確的知道
每一個文字的符號是對應到幾分到幾秒
對應到一段聲音訊號裡面的第幾個token到第幾個token
往往是有一定程度難度的
所以這個方法在訓練的時候就很吃你手上的語料
如果你手上的語料那個文字跟語音的那個alignment不好的話
那你就可能很難訓練出好的模型
那也有人想說
那我們能不能夠訓練一個語音模型
它是同時在每一步就產生一個文字的符號
跟一個語音的符號呢
這樣也可以
像我們剛才有講過說
同一步可以產生多種不同的語音的token
那麼現在把文字想成是一種很特別的token
能不能夠在同一步就產生文字的token
又產生語音的token呢
這個想法會遇到的問題是
語音跟文字的長度
往往非常的不同
而且他們不會有一對一的關係
不會說一個文字的token
就正好對到三個語音的token
他們中間的對應關係
往往是很複雜的
所以如果你要採取的策略是
在同一個時間點要產生一個語音
一個文字的token的話
就要想一些生成的策略
所以在這個地方
就有各式各樣生成的策略被提出來
比如有人想說
那如果我每次都只產生一個語音的token
一個文字的token
那往往文字token的sequence是短的
語音token的sequence會比文字token
的sequence長非常非常多
因為這邊往往一個token只對應到
聲音訊號的0.02秒
所以通常語音token很長
文字token比較短
所以如果每次生一個語音token一個文字token
那很快文字token就生完了
那怎麼辦呢
那沒關係
文字token生完之後
就生出一個代表
我什麼都沒生成的符號
你就定一個特別的符號叫epsilon
模型可以選擇生成epsilon
代表我什麼都沒有產生
那像這樣子的系統呢
它在前半段會同時生語音跟文字
然後文字生完之後
會再把語音生出來
那在Mini-Omni這個系統裡面
這個模型裡面
就是採取了這樣的策略
那還有另外一個策略是
既然文字比語音的sequence還要短
那我能不能夠文字來等一下語音呢
所以每次生完一個文字的token以後
就固定生固定數目的epsilon
固定數目的我什麼都沒有做token
所以生完一個號生三個epsilon
生完一個R生三個epsilon
然後這樣子就可以把文字的sequence拉長
來等待語音的sequence
但是像這樣的策略啊
等於是把原來文字的sequence
就直接可以變成原來的四倍長
但語音跟文字
他們之間的對應關係是很複雜的
語音的token的數目不會正好是文字的四倍長
所以怎麼辦
語音中間也要插上一些epsilon
所以今天模型在訓練的時候就會變得很複雜
模型一方面要讓文字的sequence變長
但同時語音這邊也要去配合一下文字
在適當的時機模型要知道說
語音也要產生epsilon
然後來等一下文字sequence的生成
那細節大家可以再看文獻
有篇paper叫Lamaomi
就採用了這樣的策略
還有另外一個策略是
既然語音跟文字的長度不一樣長
那我們能不能讓模型自己去決定說
文字要等多久才能夠跟語音一樣長呢
所以有一個model叫做Moshi
這個Moshi這個模型呢
它也是每一步都會產生語音
語音的token跟文字的token
但它產生一個文字的token以後
接下來模型就要去預測
它要等多久
就是它要產生幾個epsilon
是模型自己決定的
語言模型
它會自己決定它要產生幾個epsilon
這樣才能夠
它每一步都會產生語音
但是它每一步都要決定它要產生幾個epsilon
才能正好跟語音的sequence一樣長
這個方法是用在Moshi
這個模型裡面
至於哪一個方法比較好
還欠缺完整的比較
所以很難告訴你說哪一個方法
一定就是比較好的
不過啊
我在下一個章節裡面
想要跟大家介紹一個
新的語音的tokenization的方法
它可以一舉一掃除
在這一頁影片裡面講的
語音跟文字長度不一樣
所造成的困擾
所以這邊我們就來講
考慮到文字的
語音tokenization
這篇paper呢
這個方法呢叫做test
它是我們實驗室的
曾亮軒同學
跟NTK的陳怡昌
還有
我們助教那個李冠怡同學
做的,然後曾亮軒同學
今天又來了,他在這邊啦
他特別來聽老師來講他的
研究,謝謝
介紹一下那個亮軒哥
亮軒哥其實是
Cool Whisper的作者,也就是說你現在
上傳一個影片到NTU Cool的時候
不是會自動產生字幕嗎
這個是亮軒哥做的啦
那新模型是亮軒哥Train的
還在持續improve Cool Whisper
而且如果大家有好的Subtitle
要感謝亮軒哥
他同時也是2023年的大助教
那就感謝亮軒哥今天來捧場
請亮軒哥指教
所以這個模型
亮軒哥做的研究
那曾亮軒哥同學呢
提出了一個叫做
TASTE的想法
那這個TASTE的想法是怎麼樣呢
這個想法是這樣子的
我們現在已經知道
我們在產生語音的時候
同時也會產生文字
我們現在主流的做法
是做語音跟文字的Hybrid decoding
語音模型是同時產生語音跟文字的
那這樣語音的Token
它需要保留文字的資訊嗎
根本就不用保留文字的資訊了
所以語音的Token是不需要像以前那麼
我們只要讓語音的Token
保留文字以外的資訊就好了
語音的Token只要能儲存文字以外的資訊就可以了
然後再來呢
我們剛才說語音跟文字要一起生成一個難點就是
他們的長度不一樣
那能不能夠想辦法讓他們的長度一樣
我們能不能夠有一個特別的Tokenizer
這個特別的Tokenizer
他會根據輸入的語音訊息
裡面有幾個文字Token
他就產生幾個語音的Token
比如說輸入的這段聲音訊號
是How are you三個文字Token
他就會固定生出三個語音的Token
每一個Token都對應到一個文字的Token
接下來我就要講
在Network的設計上要怎麼設計
才能夠做到說
有幾個文字的Token
就產生幾個語音的Token
輸入一段聲音訊號
我們剛才講
一般的Tokenization的方式
可能是用一個pre-trained encoder
比如說一個pre-trained self-supervised model
去產生一堆representation
這些representation它的長度
這邊representation這些項量有幾個
跟輸入的長度有關係
但是跟這句話裡面的文字的數量
文字token的數量是沒有關係的
通常你就是每一個項量
對應到一個固定長度的語音
比如說0.02秒
那這邊呢
我們會對不同的layer
抽不同的項量
我們會抽出兩排項量
那等一下會講說
為什麼這邊需要抽出兩排項量
那接下來呢
一段聲音背後
對應的文字token有幾個
今天是有辦法知道的
如果你有一個好的語音辨識系統的話
你可以把聲音訊號
變成文字
所以你知道說這句話對應到幾個文字的token
接下來呢
我們有一個aggregator
這個aggregator是數層的attention layer
這個aggregator呢
會把語音辨識所得到的文字token
當作query
把從speech encoder抽出來的representation
其中一組當作key
另外一組當作value
然後這些query會去對key
做attention
然後做完attention以後
再把這些value的vector做weighted sum
你把號輸進去
把號當作一個query
就產生一個token
這個雲的token
代表的是號這個文字的token
當我們要把它念出來的時候
要怎麼念它
那把R輸進去
就得到一個對應到R的token
把U輸進去就對應到一個
對應到U的token
那在這個encoder裡面
哪一個layer適合當作key
去跟query做attention
哪一個layer適合當作value
來代表語音的資訊
來代表一個文字
token怎麼念的資訊呢
大家可以再去參考這個原始的論文
那裡面有更詳細的解析
我們這邊就先跳過這一段
然後接下來呢
我們的這個tokenizer呢
會持文字當作輸入
還有每一個文字
所對應的語音token
當作輸入
去還原原來的聲音訊號
那因為呢
這個輸入跟輸出的長度
其實它的關係非常的複雜
所以這邊的detokenizer
其實是一個很複雜的模型
這邊detokenizer其實用的network架構
跟一個TTS的模型
是一樣的
我們用的是類似
cozy voice
這個語音合成系統的network架構
所以你其實可以把這個detokenizer
想像成是一個語音合成的系統
它是一段文字
可以產生一段聲音
但是跟一般語音合成的
不一樣的地方是
每一個文字
都有一個語音的token
來教你這個文字
如果要唸它的話
現在它是怎麼被唸出來的
然後訓練的時候
就是輸入一段語音
然後最後
de-tokenizer
輸出一段語音
我們要讓輸入跟輸出
越接近越好
那這個方法叫做TASTE
就硬湊了一個梗
它叫做TASTE
text-aligned
speech tokenization and embedding
那在這整個模型裡面
這個pre-trained的模型
加上這個aggregator
合起來是tokenizer
輸入一段聲音訊號
它會輸出一串token
這個token的數目
跟這段聲音訊號裡面
對應的文字的數目
文字token的數目
是一樣的
那對detokenization的部分呢
detokenization的部分
它就像是一個語音合成系統
但它不是一個一般的語音合成
會有文字的token
來蓋的
來教這個tokenizer
要怎麼把這些文字唸出來
接下來下一個問題是
這些語音token真的有包含
怎麼唸一個文字token的資訊嗎
我們這邊做了一個非常簡單的實驗
這個實驗是這樣子的
我們先輸入一句
唸得很慢的聲音訊號
到tokenizer裡面
產生一串藍色的token
這段聲音訊號聽起來是這樣子的
它唸得很慢
大家要耐心聽一下
The captain's face had been buried in a pile of papers.
But now Merdock came around to steer at the gang leader.
接下來呢,我們給Tokenizer另外一段聲音訊號
這段聲音訊號是唸得很快的聲音訊號,然後產生紅色的Token
這段聲音訊號聽起來是這樣的
Any News on the Dancer Assault Case
這個唸得很快喔,你看這邊文字
Any News on the Dancer Assault Case
Any News on the Dancer Assault Case
非常的快
好,那接下來啊
我們做了一個小把戲
把兩邊的Token
部分交換
我們把對應到
Can Around Queue的Token
換成對應到
News On的這三個文字
Token的語音Token
所以我們把這三個文字
Token對應的語音Token
放到這裡
我們把這三個文字對應的語音Token
放到這裡
這三個字跟這三個字對應的語音Token
進行交換
那你可以預期會發生什麼事嗎
如果是左邊這個句子
本來藍色的Token代表的
是要唸很慢
但是換成紅色的Token以後
Can I run two這三個詞彙
會不會就唸得很快了呢
我們來聽一下
The captain's face had been buried in a pile of papers.
But now Merdoch came around to stare at the gang leader.
The captain's face had been buried in a pile of papers.
But now Merdoch came around to stare at the gang leader.
我看另外一個例子
我們把news under他的token換成藍色的token
那是不是news under這三個文字的token
就會唸得很慢了呢?
大家聽一下
Any news on the dances okay?
發現news under真的就唸得比較慢了
我再聽一次
Any news on the dance is okay?
其他詞彙還是唸很快
但是這三個詞彙就會唸得比較慢
所以有了這樣子的token之後
接下來就可以訓練語音版的語言模型了
怎麼做?
首先你就是需要有大量的語音資料
然後呢
這些語音資料
你可以用語音辨識系統
跑出它背後對應的文字token
然後再通過test這個tokenizer
你就可以抽出每一個文字token
對應的語音token
接下來你就可以訓練一個語音的language model
這個語音的language model
它每一次都是產生一個語音的token
加一個文字的token
它會知道說
看到look跟語音token101
就要產生at跟語音token56
看到at加語音token56
就要產生my跟語音token33
看到my語音token跟語音token33
要產生ice跟語音token162
然後就可以有一個語音版的語言模型了
那我們這邊呢
因為算力有限的關係
所以真的沒有很多資料
我們是用一個現成的Corpus
那這個Corpus裡面很多是
從這個網路上載下來的
這是一個公開的資料集
裡面是四萬小時左右的英文
那我們的語言模型呢
是拿一個1B的Lama 3.2
來做Initialization
那至於表現如何
大家可以直接去看原始的論文
我這邊呢
就是展示一下這個模型
做語音接龍的能力
那我要強調一下
這是一個retrain的模型
他並沒有做過supervised fine tuning
所以他會做的事情
就是語音接龍
給他一句話的前半段
然後他就接後半段出來
那你可以想像說這樣的模型
你可以接下來再做
supervised fine tuning跟ILHF
他就可以真的跟人對話
我們先來看看這個模型
他做
Continuation
做語音接龍的能力
怎麼樣
那這邊就是舉了四個例子
第一個例子
輸入是這樣的
I'll take the arm chair
他說I'll take the arm chair
發生什麼事呢
In the corner and just sit there
It'll feel better
I'll try to not sleep
他會盡量不睡著
希望這個演講
沒有無聊到讓大家睡著
然後呢
如果今天是男生的聲音呢
I reserve your services
他說I reserve your services
接下來呢
for a long time
I am very happy with the result
I will definitely recommend you to my friends
他得出一個
合理的語音接龍的結果
然後我也試了一下
假設讓他聽
有口音的聲音會怎麼樣
聽一下這個句子
這個是Figman Theory裡面
Roger的聲音
然後那接下來會怎麼樣呢?
他說It's hard and loud and so many people
接下來呢?
I don't know what to do
I'm not a fan of crowds
I'm not a fan of loud music
I'm not a fan of being in a crowd
I'm not a fan of being in a big city
所以聽起來還是有一點點口音的
或者是我們讓他輸入這一個句子
這句你一定聽過
他是這樣子的
Look at my eyes
大家都已經知道接下來的光處就是
Tell me why baby why
然後看看模型會接出什麼樣的結果
模型接出來的結果是這樣子的
I'm not the one who's wrong
I'm not the one who's wrong
I'm not the one who's wrong
I'm not the one who's wrong
I'm not the one who's wrong
他就重複I'm not the one who's wrong
然後這邊還產生一個noise
這個noise是模型自己產生出來的
那我會覺得這是一個feature不是一個bug
在聲音訊號裡面
我會仔細聽小明建模的聲音
他的麥克風其實品質沒有很好
所以背景是有一些雜訊的
所以模型在生成的時候也生成了一些類似的雜訊
接下來我還做了一個
莫名其妙的demo
這個不能說非常的成功
我單純只是很想要播這個迷音而已
大家應該都看過這個迷音吧
不知道這個迷音的人
我播一下
我播一下
嗨
嗨
嗨
嗨
嗨
嗨
嗨
嗨
嗨
嗨
嗨
這是有一個團體叫做
ice cream的表演
那這是個非常知名的迷因
他被換成各式各樣的東西
比如說最知名的版本
可能是黃仁勳的版本
YouTuber在搜尋會先搜尋到黃仁勳的版本
然後下面一堆人推說
黃仁勳這就是你要的世界嘛
然後那個資訊之夜啊
那個資訊之夜的宣傳
其實也是用資工獎的影片
然後就什麼什麼
比起演算法我更喜歡你
比起紅黑書我更喜歡你
就是這樣
然後我就想說
剛才我們聽到三個嗨的聲音
三個角色分別是露比醬
還有四季醬
他們都產生了一個嗨的聲音
但這三個嗨的聲音是不一樣的
他們三個人各自有聲音的特色
也許我們再聽一次
露比醬的聲音是最高的
Yui醬的聲音是中間的
嗨
四季醬的聲音是比較低的
所以接下來用這三個嗨
再丟進去
Pretrained speech language model
然後讓他做continuation
做聲音接龍
我們來看看他會接什麼樣的東西
不過這邊還是做了一些前處理啦
我有特別把那個音樂拿掉
因為我發現音樂對這個模型的影響蠻大
他訓練的時候可能沒有什麼太多音樂的資料
所以聽到音樂蠻容易讓他壞掉的
所以我其實有把音樂呢remove掉
好那你就跟他先講露比醬嗨
露比醬嗨
嗨嗨
好,看看模型會接什麼
I'm Ruby, a third year PhD student in the Department of Computer Science at the University of Cambridge.
I'm currently working on a project to build a language for describing and reasoning about security policies.
他前面還有說,I'm Ruby,這樣子
好,然後看看Yui醬
好,繼續接下去喔
I'm a new member of the club
I'm from Japan
I'm studying in Australia.
Can the big fan of anime?
我不知道為什麼如果是Yui醬的話
我試了好幾次
他都會,但每次因為他有sample嘛
每次答案都不一樣
但每次他都會說他是日本人
所以可能這句話
特別會讓模型覺得
就是一個日本人講的
最後是四季醬
接下去喔
I'm the host of the podcast
girls guide to travel
I'm a travel writer
photographer, photographer, an educator, I've been traveling the world for over a decade, and I'm passionate about sharing my experiences with you, not passionate about sharing my experiences with you.
對,就是這樣,他就有點像,有一點點像是四季醬的聲音,總之,這就是一個pre-trained的語音語言模型,在有的比較好的encoder的方式之後,可以用比較少量的資源,就訓練出一個還可以用的語音語言模型。
好,那我們講了這麼多,但其實目前為止,我都只講到pre-trained而已,有了一個pre-trained的模型之後,接下來要怎麼變成一個真的能夠互動的語音語言模型呢?
也許可以參考的路數就跟文字模型是一樣的,先有大量的網路資料,訓練出一個好的pre-trained的語言模型以後,接下來你就可以做這個supervised fine tuning,拿一些人跟人之間的對話來教
這個語音語言模型說,聽到一句聲音,你要怎麼樣回應才是人類會有的回應,那這個部分,這個資料的收集啊,其實用網爬的資料反而不一定是最合適的,但你可以去錄一大堆人的對話,但他不一定是最適合拿來fighting這些模型的資料,因為其實今天我們是從一個文字的模型開始做fighting,所以這邊雖然我們在課程裡面沒有大力的強調,但是其實也是會有forgetting的問題,
forgetting的問題也是一個我們需要非常注意的問題,所以如果你只是收集一大堆人跟人之間的對話,拿來fine-tune你的pre-tune model,往往你就把pre-tune原來的文字能力fine-tune壞了,因為這不是你的pre-tune model原來會說的話,所以怎麼辦呢?
一個你今天常見的套路就是拿原來pre-tune之前的文字模型,這個pre-tune的語言模型,他也是拿一個文字模型再去做fine-tune來的嘛,拿那個文字模型,叫他自己產生對話,建議跟一個模型說,欸,你跟一個模型,
你就產生兩個人的對話,他完全可以做到,然後再把這個對話用TTS,用語音合成,把它唸出來,你就有訓練資料可以拿來fine-tune pre-train的模型了,然後這邊呢,就拿了幾個,拿這樣的方式所生出來的對話給大家聽看看,然後等一下在對話裡面,你會聽到一個男生的聲音跟一個女生的聲音,那男生的聲音呢,背景有雜訊,但是故意加上去的,想要模擬這兩個人好像是用電話
在講話的感覺
所以聽起來像是這個樣子
你好
我最近在考慮投資
不知道該從哪裡開始
你能給我一些建議嗎
當然可以
首先我需要了解你的
風險承受能力和投資目標
你是比較偏好穩健
還是高風險的投資
我偏向於穩健一點
因為不想損失太多
而且長期來看
我希望能有穩定的收益
最好能出去些退休金
瞭解
那麼你可以考慮一些低風險的投資選擇
比如定存
國債或是
所以今天呢
你可以非常輕易的產生這種對話
哪一個文字模型
生文字的對話
再拿語音合成系統合出來就好
其實啊
就我所知那個NotebookLM
他NotebookLM不是會自動生Podcast的嗎
其他生Podcast也是這麼做
他應該不是一個M2M的模型
他是先輸入
新生文字的對話
再用語音合成合出來的
我甚至懷疑
我第一次看到那個NotebookLM會生Podcast的時候
我甚至懷疑說
Google是不是在開發語音語言模型的過程中
在生資料的過程中打造了這個模型
然後想說順便試出來給大家玩也不錯
所以才有了這個
才有這個產生Podcast的構想
不過這只是一個胡思亂想而已
那至於呢
你要教語言模型什麼東西
就要看你在fine tuning的時候準備什麼樣的資料
舉例來說
假設你今天想要教你的語言模型聽得懂
環境音
也許你需要準備一些跟環境音有關的資料
比如說像是這樣
這種聲音常常用來標示什麼事件
這種聲音通常用來標示重要的事件
比如課程開始或結束的鐘聲
所以你就可以教模型說聽到叮這個聲音
聽這個聲音的含義就是代表課程結束的聲音等等
或是假設你希望模型可以對人類非語言的訊號有反應
比如說對人類咳嗽的聲音有反應
那你可以這樣教他
最近我愛上了煮咖啡的過程
煮咖啡很棒
聽到你咳嗽了
記得多喝水
就給他一個咳嗽聲
然後教他要回答說請多喝水
模型期待他就可以知道說
聽到一個人咳嗽的時候
要怎麼樣回應
總之你可以自動生存的資料
接下來用這些資料
你就可以fine tune一個pre-trained模型
那希望就會有一個好的語音版語言模型
今天蠻多模型就是用這種路數打造的
接下來呢
你還可以做ILHF
那現在呢
在語音模型領域還有HF
也不算是特別稀奇的東西
那ILHF他的概念大家都知道
就是語音模型產生
不只一個答案
人去評價哪一個答案是比較好的
那比較早年的研究
所以比較早年的研究大概是一年前
那個時候人們比較在意的是
語音語言模型合成出來的
quality
所以有很多論文他的feedback
都是跟quality有關的
就是人告訴語言模型說這句話
合出來的聲音品質比較好
這句話合出來的聲音品質比較差
那最近流行的趨勢是
quality已經做得很不錯了
下一階段呢是希望模型可以
聽懂聲音
尤其是對文字以外的聲音
比如說音樂
或者是環境音
它能夠正確的辨認這是什麼樣的音樂或環境音
所以這幾個月
有一系列的文章都是用
RLHF來強化模型
理解聲音的能力
那除了
RLHF以外
也可以做RLAIF
就是用一個模型來提供
語音模型回饋
這篇論文呢
使得林冠廷同學跟Amazon的
研究人員合作的結果
他們就是訓練一個語音模型
用RLAIF的方式
用文字模型
來當作那個AI
文字模型會去看那個
語音模型它產生的語音訊號
辨識出來產生的句子合不合理
然後把文字模型的輸出
當作語音模型的feedback
再來強化語音模型的能力
所以你也可以做RLAIF
那講到這邊啦
其實還有一個
跟語音非常有關的關鍵特色
我們今天還沒有講到
就是一般的文字
互動是
回合制的
你今天在跟一個文字模型互動的時候
就是你講一句話
他講一句話
你講一句話他再講一句話
這個互動之間的間隔
是非常明確的
模型怎麼知道你講完了
就是你打字完輸入enter
輸入enter之後模型知道
講完了
所以今天你什麼時候講完
有非常明確的訊息
但是對語音的對話來說
就不是這樣了
語音的對話是非常複雜的
今天兩個人在對話的時候
他們既說也同時聽
他們不只是聽
他們同時也可以說話
一個模型要同時聽
也同時說這種能力
叫做4Duplex
所以今天兩個人在對話的時候
他中間可能會有很多的overlap
有一個人他在講
比如說他出去玩的事情
另外一個人如果他一直不講話
你就覺得很奇怪嘛
他是要發出一點聲音的
所以他要說
這個人還沒講完的時候
另外一個人可能就有可能會插話等等
所以兩個人在說話的時候
他們的聲音訊號是有非常多重疊的地方的
所以兩個人在對話的時候
他是一邊說又一邊聽
這兩個能力是需要同時存在的
那怎麼打造
這樣子的能力呢
怎麼讓一個語音版語言模型
同時既能聽又能說呢
如果是講到目前為止
你對於文字模型
對於這些autoregressive model認知就是
它要嘛就是輸入
要嘛就是輸出
怎麼同時輸入又輸出呢
這就需要新的技術了
所以這邊需要一些跟你所知道的
那些autoregressive model
不一樣的模型
才有辦法同時說又同時聽
不過這個部分
如果要具體來講的話
可能需要在下一節課
那就是另外的資訊了
那如果大家想知道的話
我這邊就是列了一些有代表性的reference
給大家參考
大家看看文獻上這些語音語言模型
是怎麼做到同時聽話跟說話
另外一方面
怎麼評量一個語音語言模型的好壞
也是另外一個值得關注的議題
語音語言模型不只要產生文字的資訊
它還要產生文字以外的資訊
比如說當我們講到
Toxicity
講到模型的安全性的時候
如果是文字模型
那你考慮的
是文字模型輸出的文字
是不是安全的
它有沒有不小心輸出
不該輸出的文字
但是語音模型除了考慮文字輸出的內容之外
還要考慮文字以外的資訊
比如說假設一個模型
他講話就是很尖酸刻薄
他講每一句話聽起來
都像反諷的意思
就像他文字內容是你好棒喔
聽起來人類會覺得很不舒服
所以語音是需要考慮
本來文字模型所不會考慮的面向的
所以要怎麼評量語音模型
又是另外一個挑戰
我們大助教楊志海同學
最近寫了一篇
語音語言模型評估的
overview paper
那我把連結放在這邊
給大家參考
如果你想要知道
更多語音語言模型相關的事情的話
也可以看
可以看一個這個github repo
在我們的實驗室張凱維博士維護的一個github repo
裡面會更新最新的語音語言模型相關的論文
或者是這邊有一大堆的overview paper
在告訴你說
近幾年來語音語言模型有什麼樣的發展
那我自己呢
也跟其他學者寫了一篇overview paper
摘要了
重述了語音語言模型
這幾年發展的歷史
然後把連結放在這邊
給大家參考
其實今天就只講了一點點的內容而已
很多更多的內容
大家可以參考這篇overview paper
直到語音語言模型
完整的發展歷程