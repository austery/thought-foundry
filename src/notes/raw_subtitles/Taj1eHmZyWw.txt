好那我們就開始來上課吧
今天這堂課啊
是要來一堂課搞懂機器學習和深度學習的基本概念
到目前為止呢
我們已經告訴大家說生成式人工智慧的基本原理
就是我們有一個函式
這個函式呢會把未完成的句子當作輸入
它會輸出下一個 Token
那我們把這個函式叫做 F
未完成的句子叫做 X
輸出就是 F of X
那在過去的課程裡面
我們花了很多時間講說這個 F 呢
它是怎麼運作的
我們也跟大家剖析 F 內部的結構長什麼樣子
但我們一直沒有講 F 是怎麼被找出來的
那今天我們就是要講這個生成式人工智慧裡面最核心的這個 F
可以輸入未完成的句子
預測下一個 Token 的這個 F
是怎麼透過資料被尋找出來的
那透過資料找出一個函式的技術
就統稱為機器學習 Machine Learning
那今天這堂課呢
會分成上下兩部分
我們會先講機器學習的原理
那接下來呢還是會有一些實作
讓大家更能夠瞭解上課所講的內容
好那我們就從原理開始講起
雖然說在生成式 AI 裡面呢
我們要找的函式是一個語言模型
輸入一句話輸出下一個 Token 的函式
但是實際上同樣的技術
可以用在各式各樣的地方
我們可以用同樣的技術找出各式各樣的函式
所以在今天這堂課裡面
我並不打算拿語言模型當作例子
我們換一個例子
用一些跟語言模型沒有半毛錢關係的例子
讓你知道說機器學習這種找函式的技術
它是可以用在各式各樣的領域的
那今天要找什麼樣的函式呢
今天要找的是一個莫名其妙的函式
這個莫名其妙的函式它輸入呢
是李宏毅老師做的投影片
所以我們的 X 是一份投影片
Y 是什麼
Y 是根據這份投影片
李宏毅老師如果要上課的話
他會講多久
輸出 F of X 是一個數字
這個數字代表了這堂課的長度
那像這種啊
輸入一個東西
輸出一個數字的這種任務
就假設你要找的函式
它的輸出是一個數字
那這種任務呢
在機器學習裡面叫做 Regression
那這個函式有什麼用呢
這個函式可以回答一個非常關鍵的問題
這個關鍵的問題
我相信在上課的時候
會時時刻刻問自己的
就是老師什麼時候要下課
所以我們今天有了這個函式之後
你就可以把現在這份投影片丟進這個函式
這個函式就會告訴你說
這堂課呢
老師大概會講多久
好那怎麼找一個函式呢
機器學習找函式的步驟啊
我們這邊寫做 3 加 1
我們先講前面的三個步驟是什麼
等一下再講加 1 指的是什麼
好那找函式呢
基本上是三個步驟
第一個步驟是
你要問
我要找什麼樣的函式
然後第二個步驟是
我現在可以選擇的函式有哪些
那有了第一步跟第二步之後
你就可以進入第三步
根據第一步跟第二步
找做第三步
第三步就是找出一個最好的函式
那第一步跟第二步呢
他們並沒有先後關係
那我在過去的課程裡面
通常是先講我有哪些選擇
才講我要找什麼
那助教在講解作業的時候
也是先講我有哪些選擇
再講我要找什麼
不過後來我發現說
先講我要找什麼
這步驟可能比較順一點
所以我們這邊是
先說我們要找什麼
再說我們有哪些選擇
有了一跟二之後
再進入第三步
選一個最好的函式
好第一步
第一步是你要先訂好目標
知道你到底要找什麼樣的函式
好那步驟一、步驟二、步驟三
這三步驟合起來
就是學習、learning
或者又叫訓練、training
好那步驟一是我要找什麼
你要知道說給我一個函式 F
它到底是不是我要的
我們還不知道要怎麼找一個 F 出來
但是假設有人給我一個 F
我要能夠去評估說這個 F
到底是不是我要找的
講到評估這件事
大家記不記得在上週的課程裡面
我們已經講了
怎麼評估一個生成式人工智慧
我們已經講了
生成式人工智慧的能力檢定
然後我們說這個能力檢定
評估一個模型做得有多好的方式
通常是這個樣子的
你準備一些輸入丟給模型
然後它給你一些輸出
你有標準答案
你會去計算輸出跟標準答案的
某種距離或某種相似度
那把這些距離或相似度
全部平均起來得到一個數值
那我們叫做 Evaluation Metric
這個數值就代表這個函式、這個模型
它的表現有多好
事實上在第一個步驟裡面
我們要做的事情
跟 Evaluation 可以說是一樣的
這就是為什麼在這一門課裡面
我們是先講 Evaluation
才進入機器學習的原理
我們現在終於要進入這個課程裡面
比較深水區的地方
我們要來講人工智慧是怎麼被訓練的
但是在人工智慧訓練的三個步驟中
第一個步驟其實就是 Evaluation
所以這就是為什麼我們先講 Evaluation
才講機器學習的原理
好,那怎麼知道一個函式好不好呢
這邊就根據我們剛才講的
預測一段投影片要講多長這個任務來做說明
我們有一些老師過去上課的投影片
比如說機器學習 2021 的投影片
我們也知道根據這份投影片
老師會講了多久
你就上 YouTube 頻道去看一下
每堂課講多久
那你就知道每堂課講了多長
那我們現在呢
有了這些投影片之後
把這些投影片丟進某一個函數 F
就有人隨便給我一個函數 F
這個函數 F 是可以把投影片當作輸入的
但我們等一下會再講說
把投影片當作輸入是怎麼回事
假設這個函式 F 就是可以把投影片當作輸入
它的輸出就是一個數值
它代表說這個函式預測這份投影片
李宏毅老師會講的時間的長度
有了這個預測的時長
我們就可以去計算跟真實的正確答案的差距
比如說給機器學習 2021 第一堂課投影片
實際上上課時長是 100 分鐘
這個 F 輸出 99 分鐘
那它的差距就是 99-100
我們這邊取平方等於 1
那對於其他課程你都可以做一樣的事情
可以計算預測時長跟上課時長之間差距的平方
然後呢我們再把這些差距的平方呢
把它平均起來
那這個平均後的數值就代表了這個函數的好壞
代表它預測的有多精準
那這邊因為我們算的是距離
所以這個數值越小
代表這個函式它的預測是越精準的
那如果你今天啊
定出來的這個數值是越小越好
那像我們這邊的例子就是越小越好
那這個數值呢就被叫做 Loss
或者是叫做 Cost
那如果這個數值定出來是越大越好的話
那它就被叫做 Objective
不過越小越好跟越大越好
跟等一下的說明都是沒有影響的
那在以下的課程裡面
我們會假設我們用的是 Loss
也就是說我們計算的是某一種距離
這個距離是越小越好
這個距離的數值越小
代表這個 F 它是一個越好的 F
那我們這邊計算的是預測的時長
跟真正的上課時長之間差距的平方
那這個差距的平方呢
它叫做 Mean Squared Error
它的縮寫是 MSE
那等一下在課程的後面講到 Loss 的時候
我們就會直接說我們的 Loss 是 Mean Squared Error
我們的 Loss 是 MSE
那這個 Loss 就是定義了一個函式的好壞
那這個過程啊幾乎跟 Evaluation 是一模一樣的
那你可以想說
所以 Loss 就等於是 Evaluation Metric 喔
那有時候 Evaluation Metric 是越大越好
所以你也可以說 Objective 就等於是 Evaluation Metric 喔
這個問題啊
是也不是
就是在今天的這一堂課裡面
你可以想成 Evaluation Metric 就是我們的 Loss 或者是 Objective
但是其實這兩者之間有可能會有微妙的區別
那在這一堂課裡面
你先假設兩者其實就是一樣的
那在下週的課程
我會再跟大家講說這兩者什麼時候有可能會是不一樣的
總之這邊我們就說
算 Loss 的過程其實跟 Evaluation 是一樣的
好那今天呢我們要計算出這個 Loss
我們就需要有一些資料
這些資料讓我們可以把這個 Loss 計算出來
那這些資料定義了這個 Loss 長什麼樣子
那這些定義 Loss 長什麼樣子的資料
這些輸入的投影片還有輸出的
還有這個真正的上課時長合起來呢
就叫做訓練資料
叫做 Training Data
所以拿來定義 Loss 的這些資料
就是所謂的 Training Data
就是所謂的訓練資料
好那我們現在走完第一步了
我們知道怎麼定義出 Loss 的
接下來我們進入第二步
第二步是我們有哪些選擇
我們有哪些函式是我們可以選的
我們要定出一個候選的函式集合
那怎麼定這個候選的函式集合呢
我們先想看看喔
現在的函式的輸入是一個投影片
我們用 x 來表示
但是函式的輸入只能是數字啊
我們沒有辦法直接把投影片當作一個函式的輸入
所以我們需要把一頁投影片表示成一些數字
這些數字才可以當作是某一個函式的輸入
比如說我們可以說投影片 x
它裡面的頁數
頁數總是一個數字啦
頁數叫做 x1
那這個投影片裡面總共的字數叫做 x2
那這個投影片標題的長度叫 x3
這個投影片裡面有沒有提到 learning 這個字叫做 x4
那 x4 就是只有 0 跟 1 兩個可能
0 就代表沒有提到 learning
1 就代表有提到 learning
那你可以把一份投影片表示成一串數值
這些數值有機會作為函式的輸入
那把一個東西表示成一些數值
這些數值要當作函式的輸入
這些數值就叫做 Feature
那我們在這份投影片會用下標來代表一個完整的東西的一部分啦
所以 x 是一個投影片
x1 代表的是投影片裡面的頁數
希望你可以瞭解這個符號的用法
好那這個函式 F 應該長什麼樣子呢
我們這邊呢先做一個直覺的假設
我們假設輸入的 x 跟輸出長度的這個關係
這邊長度的單位我們可能就分中作為單位吧
今天輸入跟輸出的關係有這樣子的一個關係
這個 x1 也就是投影片的頁數
乘上 w1 加 b
就會等於輸出上課的長度 y
為什麼直覺可以這樣想呢
因為你想想看課程的長度應該跟投影片的頁數乘某種比例關係
投影片越長顯然這一堂課就會講得越久
這是非常直覺的
所以呢最後課程的長度 y 應該要跟輸入的投影片頁數 x1 乘正比
我們只是不知道這個比例的關係到底有多大
到底老師平均是一頁投影片講一分鐘還是兩分鐘還是三分鐘
我們不知道
所以我們先保留意見
先不說這個 w1 的數值應該是多少
但我們知道 x1 跟 y 有這樣一個乘上 w1 的關係
那這邊呢再多加了一個數字叫做 b
那可能老師每次上課前面都會有一個開場
所以稍微多了一點時間
所以投影片的頁數乘上 w1 以後
可能需要固定加一些額外的時間
所以這邊加了一下叫做 b
那這個 w1 跟 b 的數值呢我們是不知道的
在一個函式裡面數值我們不知道的這一些東西啊
我們就叫它參數
它的英文呢是 parameter
那現在呢如果我們做的是 regression 的問題
那我們把我們的函式寫成 y 等於 w1x1 加 b
那這種 regression 呢叫做 linear 的 regression
那這個 y 等於 w1x1 加 b 啊
當我們寫出這個式子的時候
它其實真正代表的是一個函數的集合
因為 w1 跟 b 呢它的數值是未知的
這些參數的數值是未知的
所以這個式子其實代表的是一個函式的集合
那這個函式的集合我們可以怎麼樣來描述它呢
你可以想成有一個二維的平面
這個二維平面的橫軸代表 w1 的不同數值
它的縱軸代表 b 的不同數值
那這個平面上的每一個點都是一個函數
比如說這個點代表的是 w1 為 -1 b 是 0.5
這個 w1 代 -1 b 代 0.5 的時候的那一個函式
在這邊是 w1 等於 1 b 等於 0.5 的那個函式
這個點代表的是 w1 等於 0.5 b 等於 -0.4 這個函式
那這個二維平面上每一個點都是一個函式
這些點全部集合起來就是一個函式的集合
那這個就是我們劃定的要來搜尋最好的函式的範圍
那你可能會想說為什麼我們是劃定這樣一個範圍呢
為什麼我們是做 linear regression 呢
為什麼 y 等於 w1x1 加 b 呢
為什麼不能長其他樣子呢
它當然可以長其他樣子
這個函數的範圍是你人類去劃定的
所以你完全可以把函式的範圍劃成別的樣子
你可以說那我要 y 等於 w1x1 加 w2x2 加 b
我要把這個投影片的總字數也考慮進來
這樣可不可以呢
當然可以
你要把標題長度考慮進來也可以
雖然我覺得標題長度應該跟 y 沒有什麼關係吧
應該跟上課時長沒有什麼關係吧
你也可以設計非常複雜的函式集合
比如說你覺得呢這個 x2 跟 x4 應該有某種關係
當這個課程裡面沒有提到 learning 這個字的時候
總字數就不重要
有提到 learning 這個字的時候總字數就要被考慮
所以 x2 應該乘 x4
那是不是這樣呢
不知道感覺應該不太可能是這樣
然後跟標題並不是成正比
而是跟標題長度的平方成正比
有可能是這樣子嗎
你可以定各式各樣的函式的集合
然後裡面有一些未知的參數
然後我們在第三步會把這些未知的參數找出來
那至於為什麼會定 y 等於 xw1x1 加 b 呢
那其實是出於我們人類對這個任務的理解
也就是我們人類的 domain knowledge
那其實在做機器學習的時候
如果你想把人類對這個任務對這個問題的理解
放到訓練的過程中
一個最合適的階段就是在第二步
把它放到你定的函式集合裡面
因為我們認為頁數應該跟長度
成正相關
所以我們定出了這麼一個式子
那事實上我自己在準備投影片的時候
我確實相信頁數跟長度是非常有關係的
我通常會假設一頁投影片大概是講一分鐘左右
那我看著投影片就可以大概知道說
這堂課會講多長
所以我相信 y 等於 w1x1 加 b
應該可以非常好的描述 x 跟 y 的關係
但是這就是出於人類對這個問題的理解啦
那如果你對這個問題有不一樣的理解
你可以找不一樣的函式的範圍
那這個人類對問題的理解
其實就叫做模型
叫做 model
這個函式的範圍其實就是所謂的模型
那實際上模型這個詞彙被用得非常地氾濫
有時候模型指的是一個函數的範圍
一個函數裡面有未知數
有時候它指的是一個函式本身
指的是單一個函式
這邊用詞在機器學習領域 model 這個詞彙的用詞
是蠻混亂的
不過很多時候模型指的就是一個函式的範圍
其實就是所謂的模型
你從模型這個字也可以大概看出來它的意思
它指的就是真實的世界非常複雜
但我有一個比較簡單的方法來理解它
這個就是模型
所以這個人類定出來的函式也是一樣
也許 x 跟 y 之間的關係非常複雜
但是人類相信用這個函式
其實應該在某種程度上可以精確地描述 x 跟 y 之間的關係
好那我們現在走了第二步
第二步我們把人類對問題的理解
放入了學習的過程中
有了第一步跟第二步之後
我們就進入了第三步
第三步就是
我們要找一個 loss 最低的函式
我們已經定出了怎麼算 loss
我們已經定出了搜尋的範圍
接下來就是在我們定出了搜尋範圍裡面
找一個 loss 最低的函式
那怎麼在我們定的搜尋範圍內
找一個 loss 最低的函式呢
在做這件事之前
我們先把 loss 的數學式
先把它寫出來
loss 的數學式長什麼樣子呢
假設呢我們現在
把 2021 年機器學習這門課的投影片
它的頁數呢都統計出來
X 下標 1
代表一份投影片裡面的頁數
那我用上標呢
來代表是第幾堂課
那我們有 n 堂課
第一堂第二堂到第 n 堂
那在這一堂課裡面呢
上標代表的是不同的東西
我們有多個類似的東西
我們用上標呢來表示
那這個符號要怎麼用呢
都是看個人啦
我這邊就是告訴你我符號的用法
讓你接下來聽課的時候
可以更容易一點
好那我們呢就把
我們這邊還有上課的實際時長
那我們這邊呢用 hat
來代表說它是一個實際的東西
不是模型預測的東西
那有人可能會覺得說
模型預測的東西應該要用 hat
你可能有在教科書上看過這樣的寫法
其實都可以啦
這個符號呢要怎麼用
都是看個人高興
你只要讓其他人能夠看得懂就好了
首先就告訴你說
真正的東西真正的答案
我們就是加一個上標 hat
來代表真正的答案
好那我們現在呢
把這個
X 下標 1 上標 1
X 下標 1 上標 2
X 下標 1 上標 n
通通丟到 F 裡面
得到預測的時長
這邊用 Y1 Y2 Yn 來表示
接下來我們會計算
Y1 跟 Y1 hat 的平方
Y2 跟 Y2 hat 的平方
Yn 跟 Yn hat 的平方
我們把這些距離平均起來
就是我們的 loss
所以我們的 loss 寫做 n 分之 1
summation i 等於 1 到 n
Yi 減 Yi hat 的平方
但我們又同時知道說這個 F 啊
它的長項一定寫做
Y 等於 W1 X1 加 B
所以我們可以把這個 Yi
代換掉
我們可以把這個 Yi 代換成
W1 X 上標 i 下標 1 加 B
這個 Yi 等於
X 上標 i 下標 1 乘上 W 加 B
那我們把這個項呢
代入 Yi 裡面
這個就是 loss 完整的式子
所以我們知道啊
其實 loss 跟 W1 和 B 是有關係的
你選擇了不同的 W1 跟 B
你就有不同的函式
你就算出不同的 loss
所以 loss 本身呢
你也可以把它看作是一個函式
它是什麼函式
它是你參數的函式
當你選擇不同函數的時候
不同參數的時候
那你算出來的 loss 就會不一樣
那我們現在的目標呢
是選一個最好的
也就是 loss 最低的函式
這句話其實就等同於
投影片中的這一個式子
我們要找到
這個式子什麼意思呢
這個式子就是
我們要找到一組 W1 跟 B
這組 W1 跟 B 呢
代到 loss 這個函式以後
可以讓 loss 這個函式的值最小
那這個可以讓 loss 函式值最小的
W1 跟 B 呢
我們給它一個特別的名字
叫 W1 上標 star
還有 B 上標 star
好那這個式子啊
就是我們現在在步驟三呢
真正要解的問題了
那這種找一組參數
讓某一個目標函數
讓某一個 loss 最大或者是最小
這種事情呢
就要做 optimization
所以這是一個 optimization 的問題
是我們要來解的問題
那怎麼解這個數學式呢
假設你對 optimization 一無所知
那你至少有一個最糟最暴力的做法
就是暴力算出所有候選函式的 loss
也就是 Mean Squared Error (MSE)
我們就暴搜所有可能的 W1
跟暴搜所有可能的 B
暴搜所有 W1 跟 B 的組合
一個一個帶進這個 L 裡面
看哪一組 W1 跟 B 可以讓 L 最小
那我們就成功解了
這個 optimization 的問題了
好所以這個暴力的方法呢
在這個題目裡面還是可以勉強可以用的
因為我們只有兩個未知的參數
但是這兩個未知的參數
它的範圍可以從負無限大到正無限大
要從負無限大到正無限大都暴搜是不可能的
所以我們得設定一個範圍
W1 我們就從 0 一直搜尋到 3
B 呢我們就從 0 一直搜尋到 20
那這邊當然需要猜測
這個 W1 跟 B 可能會有的範圍了
這邊就是根據人對這個問題的理解
猜測一個範圍
好那我們就窮舉這個 W1 跟 B
在這兩個範圍間的所有的組合
每一個組合都算出它的 loss
就是在這一個三維圖的 Z 軸的這個方向
你就可以得到這樣的一個曲線
我們就可以知道說
欸看起來對 W 而言呢
它對 loss 的影響非常的大
但如果在 B 這個方向上
看起來對 loss 的影響呢就比較小
那像這種圖啊
我們常常會畫成 loss 的等高線圖
畫三維圖比較麻煩
所以常常畫成二維的圖
然後第三維就用顏色用等高線來表示
那像這種圖呢叫做 loss 的 surface
叫 loss surface
好在這張圖上呢
橫軸是不同的 W1
縱軸是不同的 B
然後顏色比較淺的
代表的是 loss 比較大的區域
顏色比較深的
代表是 loss 比較小的區域
那在暴搜所有的值之後發現呢
如果 W1 設 1.67
然後 B 設 4.85
那我們可以得到最低的 loss
就這個圖上紅色點的這個位置
那在這個圖上你就可以看出說
假設你是改變 W1
看起來對 loss 的影響比較大
那這個呢非常符合直覺
因為我們這個把 W1 呢乘上 X1
然後呢今天這個 W1 如果有變化的話
那應該對 Y 的影響呢是比較劇烈的
然後呢你會發現這個 W1 算出來是 1.67
那就代表說呢
平均一頁投影片呢
我會講 1.67 分鐘
有了這個結果以後才知道
原來我一頁投影片平均講 1.67 分鐘啊
然後呢 B 呢是一個額外的數值
這個 B 呢它是 4.85
那它對結果的影響呢就小很多
但是也不是完全沒有影響的
另外你會發現說呢
W 跟 B 之間也是有一些
互相影響的關係的
你看這邊這個曲線
它是有一點斜的
也就是這個 loss 特別小的地方
其實同時要考慮 W1 也是要同時考慮 B
它是一個有點斜的這個範圍
好總之我們在這一個特別的問題上
特別簡單的問題上
可以暴力窮舉出所有的候選函式
把他們 loss 都計算出來
選一個最小的
那有的同學呢可能會想說
這個問題這個 optimization 的問題
應該有 closed-form solution 吧
如果今天我們用的 loss 是 MSE
如果我們的函式集合是寫成這樣
也就是我們在做 linear regression
事實上這個 optimization 的問題
是有 closed-form solution 的
簡單來說就是有公式解啦
如果你修過線性代數的話
這個題目有公式解
直接代完公式就結束了
好但是因為我們現在講的是機器學習
我們要考慮的 loss 可能是更複雜的
我們只是在這堂課裡面
舉一個簡單的例子告訴你說
這個是我們 optimization 的對象
但是實際上在真實的情境下
你的函式的集合可能寫得更複雜
你的 loss 可能不是 MSE
在這個情況下
我們要怎麼做 optimization 呢
所以等一下要講的是一個更通用的做法
這個通用的做法
幾乎適用於所有的 loss 跟所有的 model
這個做法叫做 gradient descent
它的中文通常翻成梯度下降法
那這個方法是怎麼運作的呢
現在我們為了要簡化我們的討論
我們先假設我們只考慮一個參數 W1
雖然我們實際上有兩個參數
但我們先假設我們只考慮參數 W1
當我們改變 W1 的時候
我們的 loss 顯然會跟著改變
我們把 W1 的 error surface
它是一個一維的東西
我們把它畫在這個投影片上面
gradient descent 這個方法
它的精神就是隨便找一個起始點
隨便找一個點開始
那這邊畫一個二郎
因為餓狼下坡
大家知道餓狼下坡第三季一拳超人
最知名的經典畫面
餓狼下坡
所以講到梯度下降法
我們就用餓狼來當作代表一個模型
代表一組參數
那我們先隨便從一個 W1 的數值開始
你就隨機選一個 W1 的數值
這邊寫成 W1 上標 0
從這個位置開始
那這個位置呢
你就往左右各踏一小步
往左邊踏一小步
往右邊踏一小步
看往哪一邊可以讓 loss 下降
那在這個例子裡面呢
往右踏一步
可以讓 loss 下降
如果往右踏一步可以讓 loss 下降
那就往右真的踏出一大步
那之前只是踏一小步作為試探
試探完之後呢
就踏一大步
然後就從 W0 的地方跳到 W1 的地方
我們就到了一個新的位置
那在這個新的位置呢
就重複剛才的步驟
左右各踏一小步作為試探
發現說還是往右可以讓 loss 變小
就再往右再踏出一大步
這個步驟就一直反覆持續下去
什麼時候會停止呢
當你發現往左往右
都不會讓 l 變小的時候
比如說你走到這個地方
這個地方呢
是一個山谷中的凹點
所以往左邊踏一小步
往右邊踏一小步
發現 loss 都只會變大不會變小
那就無路可走了
就停下來
那從這個例子裡面
你也可以很明顯的看出
梯度下降法的劣勢
因為在這個例子裡面
loss 最低的點顯然在這個地方
loss 最低的點
我們叫做 global 的 minimum
這才是我們真正的目標
但是二郎呢
他只從這個坡呢
這樣子滑下來
他是走不到這個地方的
因為當他走到這個點的時候
往左往右都不會讓 loss 變小
他就會停下來了
那這種往左往右 loss
都不會再變小
讓訓練讓 gradient descent 停下來的點呢
叫做 local 的 minimum
好那這邊呢
是用比較科普的方法來講一下 gradient descent
實際上你並不會真的往左踏一步
往右踏一步這樣太麻煩了
怎麼估算出往左往右
哪一步會讓 loss 變小呢
你要去計算在這一個點
對這個 loss 所形成的曲線的
切線斜率
如果你計算出這個切線斜率以後
接下來如果你發現
切線的斜率是負的
也就是左邊高右邊低
那就代表右邊 loss 比較小
就往右走
那如果算出來
切線斜率是正的
左邊低右邊高
那
那就是往左走
因為左邊的 loss 是比較小的
所以實際上你真正做的事情
並不是往左往右各踏一小步
而是每次呢
你在每一個位置
就會計算切線斜率
然後決定要往哪裡走
計算切線斜率
再決定要往哪裡走
再計算切線斜率
再決定要往哪裡走
當你算出來切線斜率是零的時候
那代表無路可走了
那就停下來
這個就是 gradient descent
那怎麼計算切線斜率呢
那我告訴你
切線斜率
就是你現在考慮的這個參數 W1
對你的 loss L 的
偏微分
所以你知道怎麼計算這一項
你就知道
怎麼計算切線斜率
這一項就是切線斜率
那你說這個微積分我都忘了
這個我要怎麼計算這一項呢
那我告訴你
就算你微積分忘了
也都不用擔心
因為現在這種計算切線斜率這件事
沒人類什麼事情
深度學習的套件
PyTorch、TensorFlow
通常都能夠自動算切線斜率
所以你在使用這些深度學習套件的時候
那到時候那個作業裡面呢
助教也會跟大家講
怎麼用這些深度學習的套件
當你在使用這些深度學習套件的時候
你就是把第一步 loss 定好
第二步函式的範圍選好
接下來用 gradient descent
去找最好的那個 function
這個步驟裡面如果要計算切線斜率的話
在這些深度學習套件裡面
就都是一行程式碼而已
你完全不需要去煩惱
怎麼計算斜率這件事情
這些套件都是能夠自動計算切線斜率的
好所以實際上整個運作過程是這個樣子的
好我們在 W0 這個位置
我們在 W 上標 0 這個位置
我們在 W1 等於 W 上標 0 下標 1 這個地方
我們會計算 W1 對 L 的切線斜率
計算出來以後
計算出這個偏微分也就得到切線斜率之後
然後我們就會往右走一步
那再來就是這一大步到底要踏多大步呢
這一步的步伐是等於兩個東西相乘
第一個東西叫做 learning rate
第二個東西就是我們算出來的偏微分的結果
也就是切線斜率
也就是說切線斜率越大
也就是這個山坡越陡
我們就踏越大步
如果很平滑就踏小步一點
那我會定一個 learning rate
這個 learning rate 代表學習的速度
從它的名字上就可以看出來
它就是學習的速度
當你把 learning rate 設大一點的時候
步伐踏大一點學的就比較快
那這個 learning rate 設小一點
步伐就小一點
那這個學習就比較慢
講到這邊你可能會問說
那誰要學習比較慢呢
誰都要學習比較快啊
等下告訴你說學習快有什麼樣的壞處
所以 learning rate 你也是不能夠設太大
好那有 learning rate
有這個切線斜率
決定了這一步要跨多大
那你就從 W 上標 0 下標 1
跨到 W 上標 1 下標 1
然後在這個位置
你再計算 W1 在這個位置
對 L 的切線斜率
然後再往右踏一步
到了 W 上標 2 下標 1
那你再計算這個位置
W1 對 L 的切線斜率
就這樣反覆這個步驟
直到切線斜率算出來接近 0 為止
好那這邊呢
我們實際上需要考慮的有兩個參數啦
不是只有 W1
我們還得考慮 B
所以實際上
整個訓練方法運作起來是這個樣子的
首先呢
先隨便找一個 W1 跟 B 呢
作為起始
那這個起始的數值呢
我們都在右上角標 0
代表是起始的數值
隨機找一個起始的數值
隨便找個地方開始
好然後就接下來就計算
這個在 W1 上標 0 跟 B0 的這個位置
W1 對 L 的偏微分
還有 W 上標 0 下標 1
還有 B0 的這個位置
B 對 L 的偏微分
那這些偏微分集合起來啊
它有一個名字就叫做 gradient
這就是 gradient descent 裡面的 gradient
那就像我剛才說過的
你不用煩惱你不會算 gradient
在這個 deep learning 套件
比如說 PyTorch 裡面
計算 gradient 就是一行指令而已
那之後大家可以在作業裡面看到
好的計算出這個 gradient 之後
然後你就要去更新我們的參數
所以 W 上標 0 下標 1
就會加上 learning rate
就會減掉 learning rate 乘上 gradient
然後就更新成 W 上標 1 下標 1
然後 B 上標 0
就會減掉 learning rate
乘上 gradient 裡面的這個數值
然後就更新成 B1
然後這個步驟就反覆繼續下去
你有 W 上標 1 B 上標 1 以後
你會再去計算 gradient
然後再更新這些參數
就反覆持續下去
那這邊是考慮只有兩個參數的情況
那我們可以考慮更通用的情況
比如說在一個語言模型裡面
通常你的參數量不是 5 個 10 個
而是 10 億個 100 億個那麼多
那這些大量的參數
我們可以把它全部集合起來
用個 Theta 來表示它
Theta 是一個非常巨大的向量
它的每一個維度就代表了一個參數
那你現在要解的問題
當我們用 gradient descent
來 optimize 這個 L of Theta
找到一個 Theta
可以讓 L of Theta 最小值的時候
你用 gradient descent 的方法
跟前面只有兩個參數
其實是沒有什麼太大的差別的
我們就把我們剛才看過的東西
再重複講一次
我們就先從 Theta 上標 0 開始
我們就隨機選一個 Theta
我們把它叫做 Theta 上標 0
然後去計算
每一個參數 Theta 1 Theta 2 Theta 3
在 Theta 上標 0 這個位置
對大 L 的偏微分
把這些數值全部集合起來
其實這個向量
這個向量叫做 gradient
那這個 gradient
我們這邊表示成 G 上標 0
然後接下來
你計算出這些數值之後
再把 Theta 上標 0 下標 1
Theta 上標 0 下標 2
減掉這些 gradient 的數值
乘上 learning rate
然後你就可以更新你的參數
用這個方法
你就可以反覆的更新你的參數
那計算 gradient 這個式子
寫起來非常的複雜
所以在檔上
你常常看到寫法是
直接用一個倒三角形
當做 gradient
所以當有人說
他現在在計算倒三角形
L of Theta 0 的時候
意思就是他在計算
代表 gradient 的這個向量
得到 G0 就是代表 gradient 的向量
那我們有這個 G0 以後
把這個 G0 乘上 learning rate
然後再把 Theta 0 減掉
learning rate 乘上 G0
就得到更新後的參數
叫做 Theta 1
然後你就會反覆這個步驟
所以從 Theta 0 開始
然後計算出 gradient
然後把 gradient 乘上 learning rate
把 Theta 0 減掉 gradient 乘上 learning rate
得到 Theta 1
然後這每更新一次參數
就叫做一個 iteration
或叫一次 update
然後現在新的參數變成 Theta 1
所以在 Theta 1 這個地方
再計算一次 gradient
得到 G1
然後再把 Theta 1 減掉
G1 乘上 learning rate
得到 Theta 2
然後在 Theta 2 這個地方
再計算一次 gradient
得到 G2
就反覆這個步驟
把 Theta 2 減掉
G2 乘上 learning rate
然後更新成 Theta 3
所以我們就從隨便一個地方開始
然後計算 gradient 更新
然後祈禱最後
你可以找到一個很不錯的 Theta
它可以讓 loss 的值很低
這個概念講起來也是很簡單
但我告訴你實際上做起來其實也沒那麼容易
我們就拿剛才的只有兩個參數的例子
來做一下 gradient descent 吧
我們現在初始參數是  w_1  設定成 1
 w  呢設定成 15
我們從這個地方開始走起
這個是我們初始的位置
那你就在這個初始的位置計算一下 gradient
順著 gradient 的方向走一步出去
那我們的 learning rate 呢
我們設 0.001 吧
就隨便設個值
那踏一步
哇這個一步走這麼遠
一步走很遠
這個 learning rate 其實 0.001 在這個問題裡面算是蠻大的
所以一步從這邊跨過整個山谷就踏到右邊去了
一步走太大會有什麼樣的壞處呢
一步走太大
你再接下來下一步你就會看到問題了
如果再走一步會發生什麼事
再走一步
哇就跑出這個地圖外了
你就會發現說這個模型的參數呢
開始有巨大的變化
很快的你就會看到 NaN 整個訓練就壞掉了
根本沒有辦法訓練出任何東西來
所以 learning rate 不能設太大
learning rate 設太大
你可能會跑得太遠
然後就再也回不來了
所以怎麼辦
你不能那麼急功近利
我們把 learning rate 設小一點
變成 0.001
好設 0.001
那走兩步
我們現在離這個穀底比較接近了
走兩步現在是不夠的
走個 100 步
走到這裡卡住了
這邊的這個微分
這邊的這個 gradient 已經非常小了
所以走的非常的慢
尤其是我們 learning rate 設的比較小
所以現在走的非常的慢
100 步剛下到穀底
1000 步剛開始轉彎
這邊有很多點
這邊有 900 多個點通通都擠在這邊
因為這邊的這個 gradient 已經非常小了
所以走的非常慢
然後你現在是不是開始懷念比較大的 learning rate
但是如果你把 learning rate 調大
又很容易暴走
又很容易突然飛出這個穀外
飛出地圖外又回不來
所以 learning rate 太大不好
太小也不好
是一個很難調的東西
那 1000 顯然是不夠的才走到這邊
那如果設個 10000
可以走到這邊
大幅有不錯大幅的進展
不過離終點還有非常長的距離
終點大概在這個位置左右吧
那當然今天在這個例子裡面
如果單就這個例子而言
因為只有兩個參數
我們可以暴搜所有的 Loss
我們可以把 error surface 畫出來
所以我們等於是開了天眼
開了天眼有很多好處
你知道說走到這邊 Loss 已經夠低了
其實也沒必要再往下走了
走到這邊就好
但等一下你會看到
假設你的參數非常多的時候
你就沒有辦法暴搜所有的參數
你就沒有辦法畫出這種 Loss 的 surface
沒有辦法畫出 Loss surface
那你實際上觀察到的
實際上當你發現說
現在移動非常緩慢的時候
你真正觀察到的是 Loss 幾乎沒有下降
那 Loss 幾乎沒有下降的情況
那到底要停下
比如說你可能已經走到了
一個疑似 local minima 的地方
還是應該要繼續向前
這個時候就非常難決定了
那我們在下週會講更多
跟 optimization 有關的事情
不過在這週我就是告訴你說
這個訓練模型真的是很困難
你很難調出一個非常理想的 learning rate
好那如果你覺得參數更新太慢的話
有一個方法
這個方法是這樣
為什麼有時候我們會覺得參數更新太慢呢
因為每一次你在計算 gradient 的時候
你都是要去計算所有參數
對這個 Loss 的 gradient
對 Loss 的偏微分
而這個 Loss
雖然就算你不知道這個 gradient 要怎麼計算
你在計算出這個 gradient 之前
你得先把這個 Loss 計算出來
而這個 Loss 呢
是你所有資料算出來的
某種 distance 的平均
所以要計算出這個大  L
你得掃過所有的資料
你得對所有的資料進行運算
這可能是一件非常花時間的事情
但在今天我們的這個例子裡面
我們的這個訓練資料沒有多少
才二十幾筆而已
因為 2021 年機器學習也上了二十幾堂課
所以才二十幾筆而已
但是如果你今天在更真實的問題上
比如說訓練一個語言模型
那你的資料可能是海量的
網路上爬下來的資料
那你要對所有的資料去計算出這個大  L
可能是根本不可行的
所以怎麼辦呢
在實作上有一個更常用的方法
就是迫不及待的更新參數
什麼意思呢
假設你所有的資料有大  N  筆
我們先把資料呢
切成一個一個的區域
那每一個區域呢
叫做一個 batch
那每個 batch 裡面有  B  筆資料
當然這個大  B  呢
它的數字會比大  N  小了
我們就把大  N  的資料分成一個一個 batch
那本來用大  N
你可以算出真正的 Loss 叫做大  L
但我們現在沒有辦法真的算大  L
因為算大  L  實在是太耗時間了
尤其假設大  N  代表整個網路資料的話
你甚至根本不可能算出大  L
所以我們只拿每一個 batch 裡面的資料
來算出我們的 Loss
第一個 batch 算出來的 Loss
我們就叫做  L_1
我們算出  L_1  之後
就拿  L_1  去算出 gradient
然後我們就拿  L_1  算出來 gradient 去更新參數
接下來再拿第二個 batch 出來
我們得到了第二個 batch 的資料給我們的  L_2
我們對  L_2  去計算 gradient
然後再用  L_2  算出來 gradient 去更新參數
然後再拿第三個 batch 出來
依此類推
那這樣子我們每次在更新的時候
就不是看了所有的資料才更新
而是每看完一個 batch
用一個 batch 的資料就可以更新一次
那在做機器學習的時候
尤其在訓練模型的時候
有一個常常出現的詞彙叫做 Epoch
Epoch 的意思就是
當我們把資料裡面所有的 batch
都看過一次的時候
就叫做一個 Epoch
那在一個 Epoch 裡面
其實參數會被更新很多次
參數會被更新幾次呢
那就取決於大  N  跟  B  的比例
參數會在一個 Epoch 裡面
參數會被更新  N  /  B  次
那今天假設我們有一千筆資料
那你的大  B  是 100
每個 batch 有 100 資料
那在一個 Epoch 裡面
你的模型就是被更新了 10 次
那用 batch 有什麼樣的好處跟壞處呢
我們剛才看到說
我們的訓練其實是有點緩慢的
今天假設我們說
我們的 Epoch 的數目設定為 100
假設所有的資料都在同個 batch 裡面
也就是沒有做這種切 batch 的概念
如果沒有切 batch
通常又叫做 Full Batch
在 Full Batch 的 case
100 個 Epoch 只夠我們從初始的位置
剛好走到山谷下麵
但是如果我們今天把 batch size 設小一點
我 batch size 設 1
那假設我們的資料總共有 20 筆
那一個 Epoch 呢
我們就可以 update 20 次參數
所以本來在 Full Batch 的情況下
你只能夠 update 100 次參數
但是如果 batch size 設 1
那我其實在 100 個 Epoch 裡面
我已經更新了 2000 次參數
假設我們總共有 20 筆資料的話
那有 2000 次參數的更新
那你看我們已經從起點這邊
一路走到星星的這個位置
我把最後走到的位置
加一個黃色星星的符號
我們已經從這裡走到了這裡
我們其實是比 Full Batch 還走得更遙遠
但你一看這個圖
你就可以看出
當我們有用 batch 的時候
到底會發生什麼樣的事情
因為我們今天在計算 gradient 的時候
不是再拿全部的資料來計算
而是只 sample 了一小部分的資料
就進行計算
所以我們今天在 update 參數的時候
我們 update 的方向是會不穩定的
是會不斷改變的
所以就發現說從這邊開始
你的模型的參數
在兩個山壁之間
劇烈劇烈的擺盪
然後到 100 個 Epoch 的時候
大概擺盪到這裡
雖然比 Full Batch 的狀況走得更遠
但是其實離最低的這個穀底
反而有一些偏離
這個就是 batch size 帶來的好處跟壞處
它可以讓你在固定的資料量下
固定的算力的情況下
update 比較多次參數
但是它走得比較不穩定
那 batch size 設成 1 的情況
又叫 Stochastic Gradient Descent
它的縮寫是 SGD
當然這個 Full Batch 跟 Stochastic Gradient Descent SGD
是兩個極端的狀況
你的 batch size 可以設一個不大不小的值
比如說 batch size 設成 5
那 batch size 設大一點
那就不會那麼不穩定
你看這邊在兩筆之間
這個震盪的幅度非常的劇烈
那 batch size 設大一點
每個 batch 資料多一點
它的更新就比較穩定
那同時呢
因為有設 batch size
所以在每一個 Epoch 裡面
還是 update 比較多次參數
所以相較於 Full Batch
還是更新的比較快
那至於 batch size 要設多大才好呢
就現在 batch size 設的太小會有問題
設的太大也有問題
你需要一個不大不小的值
要設多少呢就不好說了
所以總之除了 learning rate 之外
又多了一個可以調的 hyperparameter
這些你沒有辦法透過訓練得到的數值
你需要人手來調
來決定要設多少的數值
就叫 hyperparameter
learning rate 是一個 hyperparameter
那這邊我們有 batch size
是另外一個 hyperparameter
那在使用到 batch 這個技術的時候
另外一個需要跟大家提醒的點
就是你需要使用一個叫做 shuffle 的技術
所謂 shuffle 的意思是說
假設在第一個 Epoch 裡面
1 跟 2 組成一個 batch
3、4 組成一個 batch
5、6 一個 batch
7、8 一個 batch
那在下一個 Epoch 裡面
你希望幫他們換組
不要一直是同樣的資料
放在同一個 batch 裡面
比如說第二個 Epoch 裡面
你可能重新打亂 Epoch 裡面的成員
把 5、7 一組
1、4 一組
3、2 一組
8、6 一組
這樣可以避免在訓練的時候
模型反覆看到一樣內容的 batch
可以增加更多的隨機器
讓你在 sample 的時候 sample 出來的結果
它的 distribution
更接近原來用全部資料的 distribution
所以這個 shuffle 是你在訓練的時候
常常會用到的一個技巧
好
那總之呢
我們把三個步驟都走完了
然後最後假設我們算出來最好的  w_1
也就是  w_1  star
是 1.67
最好的  b
也就是  b  star
是 4.85
好
那我們就知道說
我們選出來的函式叫做  y  =  1.67x_1  +  4.85
然後如果我們在訓練資料上定義出來的這個 Loss
訓練資料的 Loss 上面算一下的話
我們在訓練資料上面的 Loss 是 240
好
那走完這三個步驟就結束了嗎
走完這三個
那我們來先來看一下我們算出來的結果
我們看一下
在這邊這個圖上
每一個點代表的是一堂課
橫軸代表的是那一堂課的投影片的頁數
縱軸代表的是課程的時長
那我們找出來的
可以根據投影片頁數去預測課程時長的函式
叫做  y  =  1.67x_1  +  4.85
但這個函式它的預測並不是完美的
你可以發現有很多課程
它的預測並不是非常的精準
不過從所有課程的分佈來看
這個預測看起來好像還可以
然後計算出來
我們的 MSE 是 240
那 240 其實不是一個非常大的數值
要記得這個是 Mean Square Error
所以這個數值呢
其實是正確答案跟預測答案差距的平方
240 接近 15 的平方
所以現在的預測大概差 15 分鐘
所以根據投影片預測課程的時長
我們目前用這一個 linear regression
我們有 15 分鐘左右的誤差
好了那現在找出這個函式了
我們課程可以到這邊結束了嗎
我們可以把這個課程直接拿來做測試
我們可以把這個函式直接拿來使用
直接拿來測試在沒有看過的資料上了嗎
比如說我們可以看一下
今天這堂課有幾頁投影片
然後就代入這個式子
看看我們計算出來的結果是否準確
我們可以開始這樣做了嗎
還不行
在你真正要測試一個函式好壞的時候
你要做一個額外的步驟
這就是我講的 3+1 的 +1 的這個步驟
這個步驟叫做驗證
它的英文通常寫做 validation
那這個驗證跟測試啊
如果要講比喻的話
就是測試是真正的大考
它是指考或者是會考
那驗證就是你在學校考的模擬考
那真正的大考非常的重要
你不能夠出錯
你可能只有一次機會
但驗證是模擬考
你有很多次機會
然後考壞了也沒有關係
考壞了就重新再回去把書念好就可以了
好那我們現在呢
來做一下驗證吧
那要怎麼做驗證呢
我們要準備另外一個驗證的資料集
那我們最終的目標
是要拿來測試今天這個
我們課堂的第五講
根據投影片能不能夠準確的預測
上課的時長
那我們得要有一些
validation 驗證的資料
我們也許可以拿這一門課
這門課目前已經上了第 0 講到第四講
所以上過 5 次課了
我們把第 0 講到第四講的內容
拿出來當做驗證的 validation 的資料集
好那我們知道說過去上的 5 堂課
第 0 講到第五講投影片的頁數
我們也知道上課的時長
這個都是真實的數字
然後呢我們把這些數字
帶到我們找出來的函式裡面
我們把  x_1  帶 27、35、97
看會得到什麼數字
分別得到 50、63 跟 167
你看這個結果就發現
這個預測的時長跟真正上課的時長
顯然有一些誤差的
看起來這個函式都高估了
他根據投影片的頁數
都高估了上課的時長
如果你算 MSE 一算
哇不得了很大
算出來 1143
剛才在訓練資料上算出來才 200 多
現在在 validation set 上
我們的 Loss 一舉上升了 5 倍
變得超過了以前
結果非常的糟糕
所以可以預期說在測試資料上
在我們今天這一堂課上
如果直接用這個函式
你也會得到類似的糟糕的結果
所以怎麼辦呢
今天當你做完驗證之後
如果結果好
那就沒什麼事
那我們也許就可以進入測試的階段
但是如果結果不好
那你就要回頭去檢視前面那三個步驟
到底發生了什麼事情
那我們就來看看
我們怎麼檢視前面這三個步驟
好現在驗證的結果不好
那到底是哪裡出了問題呢
到底是步驟一還是步驟二
還是步驟三出了問題呢
我們還不知道
所以要一個一個的來想
先想有沒有可能是步驟一出了問題
步驟一是要定出我們的目標
定出我們的 Loss
但有沒有可能步驟一
我們根據訓練資料定出來的目標
跟實際上在驗證的時候
validation set 上面算出來的
這個 evaluation matrix 結果不一樣呢
因為我們在這個機器學習 2021 上面
算出來的 MSE
跟在這一堂課
現在已經上完的課程上算的 MSE
會不會其實這兩個 MSE
有非常大的本質上的差異呢
為了要真的思考這個問題
瞭解這個問題
我們把機器學習 2021 的課程
投影片頁數跟課程時長的關係
還有今天這一門課
生成式 AI 和機器學習導論
2025 投影片頁數和課程時長的關係
劃在這個投影片上面
藍色的點是機器學習 2021
綠色的點是今天這一堂課
一看你就發現
哎呀難怪預測這麼差
綠色的點顯然都比藍色點的數值要小啊
所以代表說在同樣的投影片頁數上
今天我們這一堂課
生成式 AI 與機器學習導論
老師上課的時間是比較短的
或者是換另外一個說法
如果要上到同樣的時長
老師是要做比較多頁投影片
才能夠上到一樣的長度的
這個也是非常合理的
你想今天這個是一個導論的課程
其實導論的課程呢
相較於更深的課程
講起來其實對老師的壓力是更大的
你可能會覺得說比較簡單的課程
老師講起來應該比較輕鬆
其實不是難的課程講起來才快樂
這個比較專業的課程裡面
可以整個數學定理
滿滿的一頁都是數學室
一個數學室可以講個半個小時
多開心
投影片不用做太多頁
就可以講很長的時間
如果是導論的課程
為了想把每一件事講清楚
不敢放太多數學室
要多做很多投影片
試圖把觀念講清楚
所以一頁投影片
上課的時長是比較短的
所以導論的課程
要比較多頭一頁投影片
才能上到一樣的時長
所以我們今天在機器學習這門課上
算出來 Loss
要直接用到生成式 AI 與機器學習導論
顯然是沒有辦法的
所以我們現在第一步定的目標
根本就是錯的
我們在錯誤的目標上面做 optimization
根本就是緣木求魚
不可能得到好的結果
所以怎麼辦
我們得更換訓練資料
我們不該拿機器學習 2021 當作訓練資料
我們改成生成式 AI 導論 2024 這門課
當作訓練資料
這門課的課程的名稱裡面
也有導論這兩個字
所以顯然它應該跟我們這一門
生成式 AI 與機器學習導論的性質比較接近
所以拿生成式 AI 導論 2024 的課程來訓練
然後找到模型
預測現在這一門課
應該是一個更好的選擇
所以我們剛才已經抵出一個 bug
第一步就已經有問題了
所以我們要換訓練資料
把第一步做好
讓我們預期的目標
跟實際的目標更接近一點
當我們把課程換成生成式 AI 導論
2024 的時候發生什麼事情呢
在這個投影片上
藍色的點是生成式 AI 導論
綠色的點是今天我們這一門課程
你會發現生成式 AI 導論
投影片頁數跟課程時長的關係
還有我們這一門課
投影片頁數跟課程時長的關係
確實是比較接近的
那訓練完之後
在生成式 AI 導論這一門課上面
我們計算出來的模型是  y  =  0.78x_1  +  12.85
所以在導論的課程上
一頁投影片上不到一分鐘
只能上 0.78 分鐘而已
那訓練完之後
在訓練資料上面
我們的 Loss 是 71
然後在 validation set 上面
在生成式 AI 導論已有的課程上
我們算出來的 Loss 是 122
所以你看相較於用機器學習 2021
當訓練資料
當把資料換成生成式 AI 導論 2024 的時候
我們的 validation Loss 跟 training Loss 是比較接近的
我們的 validation Loss 算出來不再是上千的數值
而是壓到 100 多
不要忘了
這個是 square
這個是 Mean Square Error
所以如果把它開根號的話
大概是 11 點多
現在預測的誤差是 11 點多分鐘
那能不能夠做得更好呢
我們再想想看步驟二
有沒有什麼可以改進的
我們已經改進了步驟一
那我們現在來改進步驟二
好 有沒有可能是步驟二的選擇太少呢
為什麼我說步驟二的選擇太少呢
在步驟二裡面
我們說我們的函式一定是  y  =  w_1x_1  +  b
這意味著什麼
這意味著說
你的這個函式啊
如果你劃在一個橫軸是  x_1
縱軸是  y  的這個圖上
它就一定是一條直線
那如果你選不同的  w_1
這個線可能會有不同的斜率
如果你選不同的  b  這個線
可能會上下平移
但它就是一條直線
如果說今天  x  跟  y  之間真正的關係是
這樣一條曲線
在這個函式範圍裡面
我們不管怎麼找
都找不出這麼一條線
所以看來
這一個函式選擇的範圍
有點太狹隘了
我們應該劃一個更大的函式範圍
那接下來我要講的就是
有沒有可能
有沒有機會
我們直接劃一個函式的範圍
這個函式的範圍
有可能包含所有的函數呢
那現在呢, 如果隨便在這個 XY 的平面上畫一個函數出來
那我們有辦法就框個範圍去把它包住嗎？
這邊的方法是這個樣子的
我們可以在這個曲線上面點一些點
然後呢, 把這些點連起來
那這種鋸齒狀的線段呢
我們叫做 Piecewise Linear Curve
然後我們要用這個綠色的曲線來逼近黑色的實線
那你可以想說這個綠色的曲線跟黑色的實線感覺差太多了吧
有很多地方不一樣
那是我們點的點不夠多
只要你點的點夠多
那你就可以讓這個 Piecewise Linear Curve
這個綠色的曲線跟黑色的實線
越來越接近
那接下來我要告訴你說
所有的 Piecewise Linear Curve
都可以看作是一個常數項
加上一大堆這個看起來像是山坡形狀的函式
我們就叫這個藍色的曲線「山坡函式」吧
我們可以把 Constant 加上一大堆的山坡函式
就可以組成任何 Piecewise Linear 的曲線
怎麼說呢？
也就是說這個 Piecewise Linear Curve
不管你是畫這樣
還是畫成這樣
還是畫這樣
只要它是很多線段組成的
我們都可以把它看作是一個常數
加一大堆這個山坡形狀的函式
當然如果越複雜的 Piecewise Linear Curve
你需要的這個藍色的函式就越多
那這邊就是舉一個例子
告訴你說隨便畫一個 Piecewise Linear 的曲線
我們怎麼用山坡的藍色的函式
去組出這個紅色的 Piecewise Linear 的曲線
那我們假設這個 Piecewise Linear Curve
是從 X1 等於 0 開始
我們就不考慮 X1 等於 0 以前發生的事情
首先呢
你可以有一個 Constant
那這個 Constant 呢
就放在 X1 等於 0 的地方
就看說這個 X1 等於 0 的時候
這個紅色的這個 Curve
它 Y 的數值是多少
那我們就把 Constant 常數項放這個數值
接下來呢
我們要產生這個第一個線段
怎麼產生這個線段呢？
你就拿一個這個藍色的山坡形狀的函式過來
這個山坡形狀的函式呢
它的兩個轉折點
就在這一個線段開始跟結束的地方
那這個山坡函式這個斜線啊
這個斜面的地方
要正好跟這個線段的斜面呢
它的斜率是一樣的
那你把 0 跟 1 加起來
你就有這一段了
接下來要怎麼弄第二段出來呢
那你就在第二段的開頭跟結尾的地方
放一個山坡函式
它的兩個轉折點
正好就是開頭跟結尾的地方
所以讓它這邊對這邊
這邊對這邊
然後斜率是一樣的
然後接下來
如果要產生這個線段的話呢
就再放個山坡的函式在這邊
它的轉折點是從這裡開始的
然後你只要把 0、1、2、3 這四項全部加起來
就變成紅色的 Curve
所以一個 Piecewise Linear 的 Curve
是可以由一個 Constant
加一大堆藍色長這個形狀的函式
疊起來所構成
那再來我要告訴你說
這個藍色的函式啊
它的數學式要怎麼把它寫出來
這個藍色的函式呢
它可以看作是兩個綠色的函式的組合
這個綠色的函式呢
都只有一個轉捩點
它可以這樣子轉
它也可以這樣子轉
那這些綠色的函式呢
我們可以不失一般性的把它寫成
C 乘上 max(0, w1x1 + b)
你可以把輸入 (橫軸是 x1)
乘上某一個數值 w1 再加上 b
然後看跟 0 誰比較大
那如果它小於 0
那輸出就等於 0
如果它大於 0
那輸出就是 w1x1 + b
然後前面乘上 C
就可以來描述這個綠色的線段
綠色的函式
那這個 C 的正負號呢
取決於這個轉折是往上的還是往下的
那 w1 決定了它的斜率
w1 跟 b 決定了這個轉折點在什麼位置
好 所以我們現在知道
所有的 Curve
都可以用 Piecewise Linear 的 Curve 去逼近它
Piecewise Linear 的 Curve
是一個 Constant 加上一堆藍色的函式
那藍色的函式
又是兩個綠色的函式組起來
所以我們可以說
所有的 Curve
我們都可以拿一個 Constant
加一大堆綠色的函式拼湊起來
可以去逼近任何的函式
所以我們已經知道說
我們可以用這個方法來逼近任何的函式
再來就只是把它的 formulation
把它的數學式寫出來而已
我們已經知道綠色的這個東西
我們可以寫成 C max(0, w1x1 + b)
那如果帶不同的 C 的值
w1 的值 b 的值
那它就長得不一樣
那這邊呢
我們假設這個綠色的函式
我們有大 H 個
那要把這個式子寫出來
就會變成
y 等於這個 Constant
Constant 用 b 來表示 b
加上我們現在有大 H 個綠色的函式
所以這邊是 summation over 1 到大 H
那每一個綠色的函式都可以寫成這樣
有一項 C 有一項 W 有一項 b
但這邊有大 H 個函式
所以每一個綠色函式裡面的 C、W1、b
它們的參數 C、W1、b
我們得給它不同的名字
所以我們就加一個 i 來代表說
它是屬於不同的綠色的函式
所以我們用 i 來代表說
它是第 i 個綠色的函式
所以這邊就可以寫成 summation
ci 乘上 max(0,
wi1x1 + bi)
就這樣子
所以我們知道說
所有的 Curve
其實都可以用這個 Curve 來逼近
你只要 W、b 跟 C 帶不同的值
你就可以把它變成任意的 Curve
任意的函式
這個函式我們可以把它圖像化一下
我們的輸入是 x1
x1 要乘上某一個 W
然後再加上某一個 b
然後再通過 max(0, ...) (註: max(0, wx+b))
然後這個函式就是
如果輸入小於 0
那輸出就是 0
如果輸入大於 0
那就是輸入等於輸出
把它的輸出用 a 來表示
那其實這個式子
它是有一個名字的
它叫 Rectifier Linear Unit
它的縮寫就叫做 ReLU
那如果熟悉深度學習的人
講到這邊
你就大概知道我要講什麼了
這個東西就是 ReLU
然後這個 a 前面要再乘上某一個 C
那這邊有好幾個函式
那我們這邊假設大 H 等於 3
假設我們有 3 個函式
所以 x1 會乘上另外一個 W 加上另外一個 b
然後過 ReLU 這個函式
然後得到 a2 再乘上 c2
然後 x1 會再乘另外一個 W 加另外一個 b
再過 ReLU 再得到 a3 再得到 c3
然後接下來
把這些數值全部加起來
再加上這個 Constant b
你就得到最終的 y
那這邊這個圖其實沒有多弄什麼事
我們只是把上面這一個函式把它圖像化而已
那這個輸入可以不用只有 x1
在很多時候我們考慮的 Feature
不會只用一個
我們可能會用多個 Feature
那至於要用哪些 Feature 比較好
那這個要看你對於這個問題的理解
在作業裡面 (註: 原字幕 "座位" 應為 "作業")
你需要做 Feature Selection
才會得到比較好的結果
所以除了 x1 以外
我們還有另外一個輸入
叫做 x2
這個 x2 可能是
比如說投影片上面的總字數等等
好 然後接下來
我們也要把 x2 乘上不同的 W
乘上不同的 W
然後跟原來 x1 乘上不同的 W 加起來
一起去加上 b 一起去過 ReLU 這個函式
那如果我們要加多餘的額外的 Feature 的話
你就用這個方式
把額外的 Feature 乘上不同的 Weight
乘上不同的 W
再跟原來的數位加起來就可以了
那你也可以用這個矩陣的方式
來描述這一整個流程
我們可以說 x1 跟 x2
把它放在一起
它就是一個向量
x1 它前面乘的這 3 個 W
跟 x2 前面這 3 個 W
它們合起來
當做一個矩陣
x1 會乘這 3 個 W
會乘這 3 個 W
x2 會乘這 3 個 W
這整個過程其實就是一個向量
乘上一個矩陣
那我們這邊會加上 b1, b2, b3
等於加上一個向量
裡面的數字是 b1, b2, b3
我們會過 ReLU
這個 ReLU 呢
我們用這個函式 sigma 來表示它
我們用 sigma 括號 b + W 乘 x
代表說這個結果
會通過 ReLU 這個 activation function
然後我們得到 a1, a2, a3
a1, a2, a3 合起來也是一個向量
然後 a1, a2, a3 要乘 c1, c2, c3 再加 b
就得到最終的 y
所以這整個過程
也可以看作是一大堆的向量
跟矩陣的相乘和相加
那我們這邊
用比較簡單的方法來表示這些向量
我們就一個長方形
還有我們就用一個長方形
來表示向量或矩陣
所以 x1, x2 它是一個長方形
這邊全部 W 這些集合起來
我們可以看作是一個矩陣
用大 W 來表示
b1, b2, b3 合起來用 b 來表示
那這邊用粗體字代表說它是一個向量
a1, a2, a3 用粗體的 a 來表示
然後呢這個粗體的 a 這個向量
要乘上 c1, c2, c3 所組成的向量
那這個向量要做一個 transpose
因為它是一個倒下來的向量
加上一個長數 b
就得到最終的 y
總之一切都是矩陣的相加相乘而已
那我們剛才說
這邊做的運作
就是把 x 乘上 W 再加上 b 再過 ReLU
得到 a 這個向量
那這件事情呢可以做不止一次
我們可以把這個 a 呢
當作是一種新的輸入
我們可以把這個 a 乘上另外一個矩陣
加叫作 W'
我們用 W' 表示這是另外一個矩陣
然後呢再加上另外一個向量 b'
然後再過 ReLU 得到 a'
那如果我們圖示化的方法就是
這個 a 呢 a1, a2, a3
會乘上不同的數值集合起來
再加上 b 乘上不同的數值集合起來再加上 b
乘上不同的數值集合起來再加上 b
過 ReLU 得到 a1, a2, a3 再乘 c1, c2, c3 再加 b
就得到最終的 y
所以乘上 W 加上 b 呢
這一個操作可以做不止一次
你可以把輸入的 x1, x2 做一次這樣的操作
得到 a1, a2, a3
再做另外一次這樣的操作
得到另外一組
這邊忘了加 prime
應該說是 a1 prime, a2 prime, a3 prime
得到最終的 y
那這一整個操作的過程呢
如果你覺得它看起來不夠厲害的話
那我們就幫它取一些新的名字
我們把這個 ReLU 這個函式
跟它前面的這個 b
跟它前面的這些 Weight 集合起來
我們就叫它 Neuron
它的中文就是一個神經元
多個 Neuron 合起來
我們就叫它 Neural Network
也就是類神經網路
你有沒有覺得這整個方法
突然之間聽起來都厲害起來了呢
那一堆 Neuron 排成一排
也就是乘上一個 W 再加 b 這一個操作
我們叫它一個 Layer
那有時候呢
你會在 Layer 前面加 Hidden
為了要跟輸入區隔
就輸入的部分叫它 Input Layer
然後其他的 Layer 呢
叫它 Hidden Layer
那這為了跟輸入做區隔
所以 Layer 前面呢
有時候會加上 Hidden
那如果你有很多 Hidden Layer
就是深度學習
就是 Deep Learning
這個就是 Deep Learning
好 那我們現在已經告訴你說
Deep Learning 呢
其實就是一個劃更大的區域
來包含更多函數的方法
我們也告訴你說
Deep Learning 有機會可以涵蓋所有的函數
如果你今天的這個大 H 呢
這邊大 H 是只設 3 啦
那所以如果你把它組成
Piecewise Linear Curve 的話
它只能有有限的轉折點
但是如果你的這個大 H 設的非常大
它就可以描述
非常複雜的 Piecewise Linear Curve
那非常複雜的 Piecewise Linear Curve
有非常多轉折點
它可能就可以表示
非常多不一樣的函數
所以在理論上
如果你有無窮無盡的神經元
深度學習
可以模擬任何函數
可以涵蓋任何函數
但實際上因為神經元數目是有限的
所以你也沒辦法真的涵蓋任何函數
但是相較於 Linear 的 Model
現在我們有了這一個 Neural Network
它顯然涵蓋了更多不同的函數
那有關訓練 Neural Network 的過程呢
其實一樣是用 Gradient Descent
跟我們剛才講的 Gradient Descent
只用兩個參數做的 Gradient Descent
其實沒有本質上的差異
但是因為一個 Neural Network 裡面
它的參數非常的多
所以你需要一個比較有效率的演算法
來幫助你計算出 Gradient Descent
那這一個有效率的
計算 Gradient Descent 的演算法
叫做 Backpropagation
所以 Backpropagation 其實就是 Gradient Descent 的
一種方法而已
它跟 Gradient Descent 沒有本質上的差異
那如果你想要知道什麼是 Backpropagation 的話
可以參考我過去的課程
那我把課程的錄影放在這邊
給大家參考
那現在如果你在實作上
你可能不容易碰到 Backpropagation 這件事了
因為 Backpropagation 這件事情
都是在 Deep Learning Framework 裡面
有幫你 implement 的
所以我這邊就先暫時不講
怎麼做 Backpropagation
如果你有興趣的話
課程錄影在這邊
留給大家自己研究
好
那接下來呢
我們就來看看這個 Deep Learning Neural Network
能不能夠發揮它的威力吧
那我們剛才有說
如果是一個 Linear 的 Model
Training Loss (註: 原字幕 "Turning the loss")
我們做到 71
好
現在弄一個
有一個 Hidden Layer 的 Neural Network
它的式子如果要寫出來的話
長這個樣子
那 H 呢
我們就設 100
然後到現在這個地方
因為參數量已經非常多了
所以我們沒有辦法
在到處的參數
畫一個 Loss 的 surface 給你看
我們現在唯一能做的
是畫一個東西
叫做 Loss 的 Curve
也就是我們每次 update 完參數
我們 update 完以後
會計算這一組參數
在我們訓練資料上面給我們的 Loss
我們會把這個 Loss 記錄下來
所以你可以看到 Loss 的變化
那就會看到說
隨著 Epoch 越來越多
那我們這邊沒有做
Stochastic Gradient Descent
我們這邊是做 full-batch
所以每個 Epoch update 一次參數
但每 update 一次參數
Loss 都會有一些下降
在前面幾個 Epoch
Loss 下降比較多
然後接下來 Loss 下降的程度
就逐漸趨緩了
那最後我們可以拿到
多少的 Training Loss
在訓練了上萬個 Epoch 之後
我們得到的 Training Loss 是 80
我們得到 Training Loss 居然是 80
你覺得這合理嗎？
你不覺得不可能
這絕對不可能
我們的 Training Loss
怎麼會比 Linear 的 Model
還要更低呢？
你想想看
Linear 的 Model
是一個比較小的範圍
我們現在用了一個 Neural Network
它的範圍是比較大的
那個 Linear 的 Model
它能夠弄的函式
它能夠表達的函式
都是這個 Neural Network
可以表達的
那如果今天在這個比較小的範圍內
找出來最好的函式
它的 Loss 都已經有 71 了
那在這個比較大的範圍內
我們怎麼還能
找出來的函式
它的 Loss 怎麼還會比 71 高呢？
最差也應該就是 71 而已
怎麼會找出比 71 高的 Loss 呢？
所以接下來
我們就要進入
再下一個步驟
我們現在
框出了一個比較大的範圍
但結果你發現
框出一個比較大的範圍
居然沒用
你沒有找出一個
比較低的 Loss
到底是發生了什麼事
找出比較大的範圍
而且還把之前的範圍也包含進去
怎麼可能 Loss 比之前要低呢？
所以
有別的地方出了問題
什麼地方出了問題呢
也許是第三步出了問題
在選一個
最好的函式的時候
出了一個問題
為什麼選最好的函式
為什麼做 Optimization 的時候
有可能會出錯呢
我們在剛才在講
Gradient Descent 的時候已經告訴你說
Gradient Descent 很明顯的一個問題
就是有時候
你在訓練的時候
你只能找到 Local Minima
你找不到 Global Minima
那這可能就是
為什麼當我們有一個
Neural Network 明明有比較大的範圍
我們卻找不出更好的函式
因為你沒有找到 Global Minima
但是其實還有很多很多的理由
會讓你的 Optimization 做得不好
比如說
除了會卡在 Local Minima 以外
訓練的參數
訓練的時候
參數還有可能
會卡在 Saddle Point
什麼叫 Saddle Point？
Saddle Point 就是
微分算出來是 0
但是
不是 Local Minima 的地方
這是一個平臺
然後它微分算出來
就是 0
所以你根本不知道
要往左走還是往右走
當然你現在是開了上帝視角
你知道左邊高右邊低
但是如果你沒有開上帝視角
你只看得到你腳下
你發現它是平坦的
雖然你往右跨一大步
你就會知道說
你很快 Loss 就會下降了
但是你四周都是平坦的
所以你根本不知道往哪裡走
事實上
更多的時候
你今天訓練會失敗
單純就是走到某一個地方
它的 Gradient 太小了
因為它的 Gradient 太小了
所以 Loss 下降的非常慢
然後你以為已經卡在 Local Minimum
或 Saddle Point
所以你就不想再等下去
你就把訓練停止了
那我剛才說
像這樣子的一個 Training Curve
你會看到說 Loss 逐漸下降
逐漸下降
當它停止
當它收斂
當它 Loss 不再下降的時候
或下降非常慢的時候
這個時候你就會開始考慮
我是不是應該把這個 Training 停下來了
我是不是不應該再跑下去了
當你看到很小的 Gradient 的時候
會讓你開始心生懷疑
而你永遠不會知道
會不會在多 Training 的幾天之後
Loss 就突然下降
誰知道呢
所以在這個地方
你會非常的猶豫
到底是要停止
還是要繼續向前
有時候你選擇了停止
你就找不到更好的 solution
你就找不到更好的 Optimization 的結果
所以總之 Optimization 是非常容易失敗的
尤其是你在做 Neural Network 的時候
你非常容易找不到一組好的參數
可以讓你的 Loss 夠低
但無論如何,現在,憑藉著我們的直覺
我們知道你的 `training loss` 絕對不能夠比線性的模型低
你可能會想說,當我們有一個 `neural network` train 下去
我們什麼時候知道說我們的 `loss` 夠低
什麼時候知道說我們的範圍已經夠大
只是 `optimization` 的方法不好
所以才找不到更好的結果
而不是因為我們的範圍還是太小了
這個就必須要憑藉著你對於問題的理解
那我通常會建議大家在實作一個新的問題的時候
你可以先從一個比較簡單的模型開始做起
比如說我這邊從一個 `linear` 的 `model` 開始做起
那 `linear` 的 `model` 它的 `optimization` 是比較容易的
我知道 `linear` 的 `model` 它的 `loss` 在 `training data` 上可以降到 70 幾
那就意味著說假設我兜出了一個類神經網路
它的 `training loss` 絕對不可以高於 70 幾
如果高於 70 幾就代表 `optimization` 是有問題的
我們必須要解決 `optimization` 的問題
而不是開一個更大的類神經網路
所以什麼時候是你的範圍不夠大
什麼時候是 `optimization` 出了問題
需要取決於你對問題的理解
那如果你從一個比較小的模型開始實作
那可以給你比較好的 `insight`
知道說實際上一個大模型的 `loss` 至少可以做到多少
所以現在我有信心
`Neural network` 它的 `training loss` 絕對要比 70 低
那怎麼讓它的 `loss` 比 70 低呢
想辦法改進 `optimization` 的過程
那通常所謂改進 `optimization` 的過程
就是爆調 `hyperparameter`
爆調 `learning rate`
爆調 `epoch` 數目
爆調 `batch size`
把每一個你可以調的東西都爆調一下以後
經過一番猛如虎的操作
我可以把 `loss` 呢壓到 41
所以果然是可以做得比 70 還要低的
但能不能做到更低呢
不知道
因為反正只要比 70 低
我就覺得心滿意足
我就覺得這個答案是合理的
到底能不能做得更低
那就難以預測了
好那做到了 40 之後
我們也許可以把這個 x 跟 y 的關係
一樣畫在這個二維的平面上
所以現在橫軸呢是投影片的頁數
縱軸呢是課程的時長
我們現在找到的函式不再是一條斜直線
它是由一個複雜的 `neural network`
所產生出來的曲線
那這個曲線你可以看到在這個地方
有一個神秘的轉折點
看起來它覺得說
如果投影片的頁數少於某一個範圍
我們用這個函式來預測
但是投影片的頁數大過某一個範圍
應該用這一條線來預測
那這邊有一些怪怪的東西
不過沒差反正沒有這麼少頁數的投影片
我們找到了一個比較複雜的函式
來描述投影片的頁數
跟課程時長之間的關係
但是我們可能已經到了某種極限
光看投影片的頁數顯然是不夠的
因為你會發現說有很多課程
他的投影片頁數是一樣的
但課程時長是不同的
所以我們需要找到更多其他的 `feature`
才能預測課程的時長
我們剛才的輸入只有多少頁投影片
我們也來考慮有什麼樣其他的 `feature` 可以用吧
比如說一個可以用的 `feature` 可能是
投影片中總共有多少字
因為很直覺的如果字數越多
這個投影片應該需要講得越久吧
所以我們來看看
如果把投影片的字數當作第二個 `feature`
能不能做得更好
做下去
加上投影片的字數作為額外的 `feature`
我的 `loss` 從 41 降到 40
降的有點少
好像沒有帶來太大的幫助
後來我轉念一想
也許用投影片中總共的字數
並不是一個好的想法
為什麼用投影片中總共的字數不是一個好的想法呢
因為投影片的頁數越多
它的字數本來就會越多
所以如果我們是把投影片中總共的字數
當作第二個 `feature`
第一個 `feature` 跟第二個 `feature`
其實沒有提供非常不一樣的資訊
也許真正重要的是
每一頁投影片平均有多少字
代表一個投影片
它的內容的濃度
這個一頁投影片的內容越多
它的字數越多
那可能應該講得越久
那所以呢
我們應該把投影片中的總字數
除以投影片的頁數
來當作新的 `feature`
這個 `feature` 就是投影片每一頁的平均字數
那你可能會想說
這個我的輸入已經有投影片的頁數
也有總字數啦
難道 `network` 沒有辦法在裡面自動學到
把這兩者相除
當算出投影片的平均字數嗎
也許可以
但是那就需要仰仗更多的 `optimization`
更大的 `network`
那我今天既然都知道這是一個好的 `feature` 了
何必仰賴 `network` 自己發現呢
我直接告訴它有這個 `feature` 可以用
不就好了嗎
所以我給它一個新的 `feature`
這個 `feature` 是每頁投影片平均的字數
一做下去
有顯著的效果啊
一做下去
`training` 的 `loss` 降到 22
現在有一個新的類神經網路
它的輸入是有兩個 `feature`
我們來看看這個類神經網路
在 `validation set` 上的表現吧
剛才用 `linear` 的 `model`
`training` 是 71
`validation` 是 122
我們能不能比 122 做得更好呢
現在 `training` 都已經降到 22 了
一做下去
不得了
`validation set loss` 是 1300
太慘了
結果非常的差
這個差距非常的驚人
雖然 `training`
我們從 71 降到了 22
但是 `validation`
從 122 突然暴增到 1300
為什麼會這樣呢
為什麼有時候
我們在 `training` 的 `data` 上得到很低的 `loss`
但是在 `validation set` 上
卻沒有得到一樣低的 `loss`
甚至反而得到更高的 `loss` 呢
當你發現說啊
你 `training` 的 `loss` 跟 `validation` 的 `loss`
有巨大差距的時候
這個現象叫做 `overfitting`
那 `overfitting` 的來源是什麼呢
`overfitting` 的其中一個來源就是
當你劃定的選函式的範圍越大
就越容易發生 `overfitting` 的現象
什麼意思呢
我們先來假定一個實驗
這個實驗是這樣子的
我劃了一個非常非常巨大的函式的範圍
我的範圍是能包刮世界上一切的函式
那你說你剛才講 `deep learning`
可以涵刮一切世界上的函式
那只是理想上啊
這個 `deep learning` 的 `neuron` 的數目畢竟是有限的
所以沒辦法真的涵刮所有的函式
現在講一個極端的狀況
我不管用什麼樣的方法
我就是可以涵刮世界上一切的函式
那結果會怎樣呢
我會得到一個很好的訓練結果嗎
可能不會
為什麼
你想想看了
假設我的訓練資料有這幾筆
這三張投影片
他們正確的時長分別是 10、20、30
假設世界上一切的函數都可以選
那我們有沒有可能選這樣一個函數
這個函數我叫做 F 下標 lazy
因為它是個爛函數
啥事也沒幹
這個函數它會的事情就是
輸入訓練資料的這三份投影片
就輸出訓練資料裡面這三堂課
正確的時長
輸入其他的東西
只要是它在訓練資料裡面
沒看過的投影片
它的輸出通通都是零
那這個函數顯然沒有什麼用
你可能想說你怎麼可以選
這麼樣的函數呢
你怎麼可以選這種沒有用的函數呢
那你想想看
這個函數有違反我們前面
任何的選擇的過程嗎
它是一個在訓練資料上
`Loss` 為零的函數
在訓練資料這三筆資料上
它會完全答對
所以它 `Loss` 為零
它的 `Loss` 非常低
而我們現在劃定的函數範圍
是包含全世界所有的函數
所以這個 Lazy 函數
它也是我們可以選的函數
我們說我們要在我們
所有可以選的函數裡面
選一個 `Loss` 最低的
所以你很有可能
真的會選到 Lazy 函數
它是你可以選的
你真的會選到的一個函數
那選到這個函數
當然你在驗證資料上
你就沒有辦法預期有什麼好結果
因為驗證資料是
它沒看過的投影片
它一律輸出零
你在驗證資料上的 `Loss`
就會大爆炸
好那剛才只是
舉了一個實驗告訴你說
假設函數劃定的範圍是無限大
你很有機會找到一個函數
它是訓練資料上面 `Loss` 為零
但是在驗證資料上面
一點用處都沒有的函數
但是我還沒有講
為什麼範圍越大
就越容易找到不好的 讓訓練跟
這個驗證資料差距
`Loss` 差距很大的函數
這件事情講起來就很麻煩了
大概一個小時以上 有可能講不清楚
才有可能講清楚
所以我就把它留在
過去的上課錄影裡面
如果大家想要知道
在理論上分析
為什麼劃定的範圍越大
`Validation Set` 跟 `Training Set`
它的 `Loss` 就有可能
差距越大的話
請參見機器學習
2022 年的錄影
把錄影的連結
留在這邊給大家參考
這邊再舉一個例子
說明 `Overfitting` 的現象
我們說 `Overfitting` 就是
選擇越多
訓練跟驗證的差距越大
我們這邊就講一個故事
不知道大家有沒有考過
汽車駕照
在考汽車駕照的時候
你可能要先去駕訓班
做訓練
然後接下來才真的去考駕照
訓練的過程就對應到
機器學習裡面的訓練過程
考駕照的過程
就是驗證的過程
一般正常你在駕訓班
開車的時候
正常的學習方法
應該就是看著道路來開車
看著路況來開車
但是你知道
在考駕照的時候
有一些題目是特別難的
比如說倒車入庫等等
如果你沒有太多開車經驗
你可能很難把這件事情做好
這個時候
因為人類可以用的資訊
不是只有看著前面的道路而已
還有很多其他的資訊
所以那個時候
在駕訓班的時候
我就學會了一個招數
就是看著汽車的後照鏡
有人在旁邊的欄杆
貼了一些貼紙
那顯然是故意貼的
我就學到說
當今天貼紙出現在
後照鏡的正中間的時候
方向盤就左打四分之一
當另外一張貼紙
出現在後照鏡的中間的時候
方向盤就右打三分之一
這一招就可以讓我順利的
在駕訓班的場地做好倒車入庫這件事
但是你知道
這個時候只有在驗證的時候
也在同一個場地
你才有辦法開車
換了一個場地
基本上就不能開車了
就是這麼樣一個故事
不過現在會不會開車
也沒那麼重要
自駕車就要來了
所以以後有沒有開車
都不打緊了
這個就是個
`Overfitting` 與駕訓班的故事
好 那所以呢
我們知道有時候
我們會在訓練資料上面
得到很低的 `Loss`
但是進行驗證的時候
卻發現 `Loss` 非常的高
那這個時候怎麼辦呢
你可能得再回頭過去
改一下你的訓練步驟
你訓練了三個步驟
你要回頭來檢視
有沒有辦法改一改
讓你在測試驗證集上
可以找到一個函式
在驗證集上
可以得到低的 `Loss`
而且啊
如果你劃定的範圍越大
如果你的範圍是無窮大的時候
你就非常可能
找到一個訓練資料上
`Loss` 很低
驗證資料上面
`Loss` 很高的函式
這就是為什麼
我們在步驟二
一定要劃一個範圍
而這個範圍非常的重要
它不能太小
如果太小的話
你就沒有包含到好的函式
不能太大 如果太大的話
就會 `Overfitting`
要怎麼劃定這個範圍
我們下周會再討論
但是總之
這不是一件很容易的事情
你需要反反復復的
在三個步驟和驗證間進行操作
劃定一個範圍
驗證一下
發現不好
再重劃一個範圍
再驗證一下
又發現不好
再重劃一個範圍
再驗證一下
又發現不好
你要反反復復的
做很多次訓練跟 `Validation`
你才會找出一個
在 `Validation Set` 上
`Loss` 也低的函式
那其實今天一般在訓練的時候
你會非常常去使用
你的 `Validation Set`
舉例來說
你甚至有可能在訓練的時候
每一次 `Update`
也就是每過一個 `epoch`
你都去 `Validation Set` 上面
量一下
現在模型的表現如何
那剛才在訓練 `Neural Network` 的時候
我其實也做了一樣的事情
藍色的這條線
是訓練的 `Loss`
隨著 `epochs` 的進行
訓練的 `Loss`
越來越低
那橙色的這條線呢
橙色這條線是
`Validation` 的 `Loss`
也就是我每訓練出一個模型
我就拿那個模型
去在 `Validation Set` 上
算一下 `Loss`
看看我們可以得到什麼樣的 `Loss`
那如果你今天一直訓練
一直訓練
`epochs` 真的設非常的大
那你會得到
非常高的 `Validation Loss`
但在這整個過程中
你卻發現
其實你只要把 `epochs` 設少一點
其實你是可以得到
很不錯的 `Validation Loss` 的
我發現 `epochs` 在 20 幾的地方
我可以得到一個 `Validation Loss`
它的數值是 12
非常低 比我們看到的
所有數值都還要低
所以今天
很多時候你把 `epochs` 設少一點
其實也是一個
防止 `Overfitting` 的方法
那這一招呢
叫做 `Early Stopping`
其實在助教的
這個作業裡面
也有要求大家
做 `Early Stopping`
這是一個可以幫助你
避免你 `Overfitting` 的方式
好
總之我們剛才講說
在驗證與訓練之間
你可能會來來回回
無數次
但是這個來來回回
是有代價的
這個來來回回
有什麼樣的代價呢
這個來來回回的代價是
最終
你有可能
對 `Validation Set`
做 `Overfitting`
什麼意思
假設我們可以
無限的使用驗證資料
最終
你有可能可以找到一個函式
假設你根本不知道
什麼叫做訓練
你就是每一次都隨便弄個函式
隨便弄個函式
你的函式是隨機產生的
但是
只要你能夠驗證的次數夠多
也許你可以正好抽到
另外一個 Lazy Function
我們叫 Lazy Function 2
這個 Lazy Function
哎呀
它在驗證資料上面
看到驗證資料這幾份投影片
答案跟驗證資料正確答案
正好一模一樣
但是
在其他資料上
比如說測試資料上面
它的答案都是亂給的
比如說看到
其他投影片
不在驗證資料裡面的
他的輸出仍然是零
這樣子的lazy function
它仍然是一個
沒有用的 Function
但是當你可以
無限制的使用驗證資料
去找一個在驗證資料上面
`Loss` 最低的函式的時候
最終
你的下場可能就是這樣
你找到一個
驗證資料上 `Loss` 很低的函式
你 `overfit` 在驗證資料上
但仍然是一個
沒有用的函式
好那你說
那如果在驗證資料上面
不斷的反覆做
最終可以找到一個爛的函式
那怎麼辦呢
也許也不要緊吧
我就把驗證資料上面
找出來的函式
拿去測試資料上面
真的測測看
看看它結果好不好
那如果我不小心
`overfit` 在 `Validation Set` 上
那測試資料會告訴我
測試資料會告訴我說
這仍然是一個壞的函式
我發現它是一個壞的函式
我會再回去改訓練的步驟
期待找到一個
`Validation Set` 上好
最後測試也好的函式
但是這邊你要注意
假設你可以做無限制的測試
最終會發生什麼事情
你也有可能
`overfit` 在測試資料上
如果你可以無窮無盡的
不斷的使用測試資料
最終你會 `overfit`
在測試資料上
你會找到一個
在訓練資料
在 `Validation Set` 上
在測試資料上面
都表現很好的函式
但是在其他的輸入上面
表現都不好的函式
你有可能找到這樣的函式
就是因為這些測試資料
很多時候 實際上
是可以做
非常多次的測試的
這就解釋了為什麼
在這些benchmark上
benchmark 就是一些測試的資料集
你拿來檢驗人工智慧能力的資料集
在這些 `Benchmark` 上
人工智慧往往可以大幅打敗人類
但是實際上並沒有辦法做到
舉例來說
有一個 `Benchmark` 叫做 `SQuAD`
你看這邊的時間
這都是 2019 年上傳的結果
在 2019 年的時候
那個時候還不流行 LLM
有一個模型叫做 `BERT`
它得到的 Exact Match 的分數
是 87%
這邊是做那種閱讀測驗
讓模型讀一個文章
問它一個問題
看它能不能答對
而人類呢
人類只有 86.8% 的正確率
居然還比最好的人工智慧
2019 年的時候的人工智慧
正確率還要更低
當然你不會相信
在 2019 年的時候
那個時候的語言模型
可以做得比人類更好
但是在 `Benchmark` 上
它就是會顯示出
比人類更好的數值啊
為什麼會這樣
那就是因為 這些benchmark
其實你還是可以做大量測試的
雖然實際上建構這些 `Benchmark`
可能會說
也許會設置說
你每天只能夠測試一定的次數
他們甚至會不公佈測試資料
所以就沒有辦法無窮無盡
用那些測試資料來測試
你必須要把模型上傳到他們的平臺
由他們幫你測試
它可能甚至會規定說
你每天最多測一次
免得你每天都爆測個 10 萬
20 萬
20 萬次
然後就 `overfit` 在訓練資料上
但就算是這樣
只要一個 `Benchmark` 存在的夠久
你每天只能測一次
但你測個 1000 天
2000 天
測個 10 年
終究有一天會overfit在測試資料上
這是為什麼人工智慧
常常在這些 `Benchmark` 上
有很好的結果
但是實際上
它並沒有那麼厲害
而人類
你人類在這個 `Benchmark` 上
你只會做一次啊
所以跟機器
會無窮無盡的
不斷在這個 `Benchmark` 上實驗
做差了
就調模型再做一次
再差
再調模型
再做一次
最終模型終究能夠
人工智慧終究能夠超越人類
這個就好像在鬼滅之刃裡面一樣
鬼是殺不死的
人類只有一次機會
人中一刀就會死掉
但是鬼只要不砍他的頭
它就可以無限的再生
這個是一樣的道理
那為了避免 `overfit` 在測試資料上
所以多數機器學習的競賽
包括今天我們的作業
都有這樣的設置
就是我們會把測試資料
分成兩半
一半叫 `Public Set`
一半叫 `Private Set`
那你只能看到
`Public Set` 上面的分數
你看不到 `Private Set` 上面的分數
在做驗證的時候
你的驗證資料
可能是你自己的資料
你會從你的訓練資料裡面
我們提供給你的
有標註的資料裡面
抽一部分出來
當作 `Validation Set`
那你要用 `Validation Set` 呢
跑多少次的循環
這邊沒有人管你
所以非常有可能最後就overfit在你的
`Validation Set` 上
總之你要用幾次 `Validation Set`
那是你自己的資料
所以都可以
但是在 `Public Set` 上
你能夠做的測試次數是有限的
你當然可以訓練完一個模型
然後把它上傳到我們的 `Leaderboard`
然後呢
看看你在 `Public Set` 上
得到的分數是多少
得到的分數如果不好
你當然會
你當然不會說得到的分數不好
就這樣算了
不好就算了
你會回過頭去
然後去改一下
你的訓練的三個步驟
看看有沒有辦法在public set上做得更好
但是為了避免你 `overfit`
在 `Public Set` 上
`Public Set` 可以上傳的次數
是有限的
我們會規定每天
你只能上傳多少次
那因為每一個作業的期間
是有限的
所以你沒有辦法
無窮無盡的
使用這個 `Public Set`來做vaildation
所以你可以說Public set的testing data其實他是一個
有限次數的驗證集
它是一個真正的驗證集
可以避免你 `overfitting` 到
你原來的驗證集上
當然因為你這個 `Public Set`
你還是可以重複實驗
所以你也有可能
`overfit` 在 `Public Set` 上面
所以最後見真章的是
`Private Set`
`Private Set` 只有一次機會
你沒有辦法知道說
你在 `Private Set` 上得到的結果
你只能說
我在 `Public Set` 上
選一個 `Public Set` 上
表現比較好的模型
那麼讓你選兩個
所以你其實等於
有兩次測試的機會就是了
在 `Private Set` 上
把那兩個模型測一下
結果多少就是多少
只有一次機會
那在 `Private Set` 上
在 `Private` 的 Testing Set 上
呈現出來的效果
可能更接近你的模型
真實能夠呈現的效果
好講到這邊
我們就把原理的部分
講到一個段落
接下來我們要進入
實作的階段
把剛才我們在講機器學習基本原理的時候舉過的例子
實際上跑一遍給你看讓你更有感覺一點
好那今天我們要做的任務
就是根據投影片的資料
比如說頁數字數等等
來預測李宏毅老師這堂課講課的時間
我們先匯入必要的函式庫
然後接下來
我們已經收集了李宏毅ll老師過去的上課的資料
然後有統計出在機器學習 2021 這門課裡面
每一堂課投影片的頁數
還有那一堂課講了多長
所以這邊有機器學習這門課的資料
還有生成式人工智慧導論 2024 的資料
還有我們現在這一堂課
這一門課從第 0 講到第 14 講
總共 5 堂課的資料
那這邊記載的每一個數字
就是這個投影片的頁數
然後這邊記載的每一個數字
就是課程的時長
那你直接在我的課程網站上
都可以找到這些資訊
那收集這些資訊呢
還是需要花一點時間的
我本來是想要直接用 ChatGPT 的 agent 來做這件事情
但是它都給我一些奇奇怪怪的錯的答案
所以最後老實說我是人工做的
所以這個是人工花一些時間
收集標註了這些資料
得到投影片的頁數
跟它的課程的時長
還有投影片的總字數
好那我們有了這些資料以後
接下來我們就開始訓練吧
我們先假設我們用機器學習 2021 這門課呢
當做訓練資料
然後呢我們用 2025 現在這一門課呢
當做驗證資料
那我們把訓練資料的輸入
叫做 X_train 訓練資料的這個正確答案
標準答案叫做 Y_train
驗證資料的輸入叫做 X_val
然後測試資料叫做 Y_val
所以我們現在就把 2021 的投影片頁數
交給 X_train
然後 2021 的課程長度只給 Y_train
2025 這一門課的投影片長度
只給 X_validation
然後這個長度只給 Y_validation
好有了這些訓練資料以後
接下來還記不記得機器學習的三個步驟是什麼呢
第一個步驟是先定好我們要什麼
所以我們要先定義好我們 Loss 的計算方式
那這邊呢我們使用 Mean Squared Error
MSE 中文是均方誤差
那我們希望函式的預測值
跟真實值之間的差距要越小越好
我們就定了這個 Mean Squared Error 的函式
它的輸入就是兩個向量
第一個向量 Y_predict
代表某一個模型輸出的數值
Y_true 代表真實的正確的答案
那它裡面做的事情就是把 Y_predict 跟 Y_true
做相減把模型的輸出跟正確答案做相減
去平方再平均就這麼簡單
如果你要使用這個函式呢
比如說我假設我現在有一個很爛的模型
它的輸出就是隨機預測的
這邊就是假設有一個爛模型的輸出
這些數值都是隨機打上去的
那我想要知道這個模型
它輸出的這個 Predict Random 這些數值
到底根據 Mean Squared Error 計算起來有多大
那你就把 Mean Squared Error 第一個輸出
放這個 Random 的 Prediction
第一個輸出放 Y_true
也就是 Training Set 上的正確答案
然後得到的這個 Loss
再把它輸出就可以了
而 Loss 輸出有 6 萬不得了
所以這個隨機的輸出得到的 Loss 是非常巨大的
假設有一個函式它的輸出跟正確答案一模一樣
我們今天假設有一個函式的輸出叫做 Predict Perfect
它就跟正確答案也就是 Y_true 一模一樣
所以這個時候當你計算 Mean Squared Error 的時候
第一個輸入是 Predict Perfect
第二個輸入是 Y_true
那輸出出來的 Loss 會是多少呢
就是 0
那 Mean Squared Error 算出來最小的就是 0
好那接下來我們進入機器學習的第二步
就是劃出選擇的範圍
那我們就假設我們是一個線性的模型
Y 等於 W1 X1 加 B
Y 是課程長度
X1 是投影片的頁數
W1、B 是要找的未知的參數
那這個 W1 跟 B 設定不同的值
我們就得到不同的函式
那我們假設說呢
我們有定義一個函式叫做 Linear Model
它的輸入呢就是 X1、W1 跟 B
就我們要給它輸入跟給它我們的參數
然後它就幫我們做這個 Linear Model 的運算
把 W1 乘 X1 再加上 B
那我們可以劃出一些
在我們的這個可以選的函式集合裡面的不同的函式
我們今天如果改變 W1 跟 B 的值
改變 W1 跟 B 的值
我們就得到不同的函式
所以我們這邊就列舉了幾個不同的 W1
列舉了幾個不同的 B
然後呢再產生不同的輸入
這邊這個 X_PLOT NP.LineSpace 0, 100
100 的意思就是從 0 到 100 之間產生 100 個點
然後呢我們就重取所有的 W1 跟 B 的組合
然後每個組合把它的這個輸入跟輸出的關係劃出來
你就可以知道說
我們現在選擇的函式範圍大概是長這樣
反正通通就是直線
只是這些直線呢
它們有不同的斜率
然後它們可能會有一些上下平移的關係等等
好再來就進入最關鍵的一步
第三步找出最好的函式
我們要找出一個 W 跟 B 的組合
讓 Loss 越低越好
怎麼做呢
我們先嘗試最笨最直觀的方法
也就是暴力窮舉
我們把一定範圍內所有 W1 跟 B 的組合都暴搜一次
然後去計算它對應的 Loss
就是把每一個 W 跟 B 的組合去計算出它對應的 Loss
再把這些 Loss 劃出來
你就可以看到一個 Error Surface
那我們要找的就是這個 Error Surface 的最低點
好那我們來暴搜所有的範圍吧
那你就會先建立你要窮舉的 W1 跟 B
那 W1 的值呢
我們就是在這個 0 到 3 之間取 100 個點
所以你其實也沒辦法真的窮盡所有的值啦
但是你可以在 0 到 3 之間取 100 個點
W1 有 100 個不同數值來做測試
然後 B 呢在 0 到 20 之間
我們取 300 個不同的點來做測試
那為什麼是 0 到 3
為什麼 0 到 20
就是憑著對這個問題的理解知道說最好的值呢
這個 W1 應該落在 0 到 3 之間
最好 B 應該落在 0 到 20 之間
然後來設定這個範圍
好然後接下來呢
我們先建立一個叫做 losses 的矩陣
這個 losses 的矩陣呢
它的目標就是要存我們等一下算出來的 loss
那接下來呢
我們就是窮舉所有的 B、窮舉所有的 W
把每個 W 跟 B 呢
都帶到這個 linear 的 model 裡面
然後根據 X_train
你可以得到 predict 出來的 prediction
然後每個 predict 出來的 prediction 都去算一下 Mean Squared Error
就可以計算出
當我們 W 跟 B 設成不同數值的時候
得到的 loss 是多少
然後再把這個 loss 存到 losses 這個矩陣裡面
那印出來呢就像是這樣
有點難看哦
就是有一個矩陣
這個矩陣呢
其中一個軸代表了各種不同的 W
另外一個軸代表了各種不同的 B
然後把所有 W 跟 B 的組合拿出來算 loss
這個有點不知道這個矩陣在幹嘛
但是我們可以把這個矩陣的 3D 立體的圖
跟 2D 的等高線圖畫出來
那至於實際上怎麼畫
這個再留給大家自己研究
所以我們就可以畫出 3D 立體的圖
在這個 3D 立體的圖裡面
你就可以知道說
當這個 W1 有變化的時候
當 B 有變化的時候
那我們的 loss 呢
會有什麼樣的變化
那可以發現說
我們的這個 loss 呢
這個 loss 的 surface 呢
它是一個非常深的峽穀
那在這個峽谷的穀底
其實也是有些斜度的
只是相較於這兩邊陡峭的山坡
中間的斜度呢
相對而言呢
是小很多的
好那這個是畫成這個等高線圖
然後最好的點呢
有被點出來
那這邊就是
通過暴力搜尋法
找到最佳解釋
如果 W 代 1.67
B 代 4.88
那得出來的 loss 是最小的
大概是 240 左右
所以你可以看到
在這個 Error Surface 上面
最低的點就是在紅色的點這個地方
那如果是 W1 代 1.67
那 B 代 4.88 的時候
這個值是最小的
好那剛才呢
就是用暴力搜尋的方法啦
那其實你看根據暴力搜尋的方法
你也可以知道說這個 Error Surface 啊
是一個相對沒有非常複雜的
Error Surface
所以照理說
我們隨便找一個地方開始
然後用 Gradient Descent 的方法
沿著下坡這樣滑下來
慢慢應該就可以走到最低的點的地方了吧
所以感覺 Gradient Descent 用在這邊
應該是會有不錯的結果的
真的是這樣嗎
好我們就來實作一下 Gradient Descent 吧
所以你要先有 W1 跟 B 的初始參數
我這邊就隨便設 1 跟 15 這個值呢
你通常是用個隨機亂數產生
你可以隨便設
那因為我們想要記錄 W1 跟 B 的變化
所以我這邊開了一個叫
W1_history 跟 B_history 的 list
我是為了要存 W1 跟 B 的變化
我也想存 Loss 的變化
所以我這邊開了一個叫 Loss_history 的 list
我要存那個 Loss 的變化
那我先把最開始的 W1
放到 W1_history 裡面
把最開始的 B 放到 B_history 裡面
我也想知道這個最開始的時候 Loss 是多少
所以我用最開始的 W1 跟 B
根據我們現在的訓練資料的輸入
去計算出輸出
然後根據輸出跟正確答案
我可以計算出現在 Loss
把現在的 Loss 放到 Loss_history 裡面
所以我有現在
在什麼事都還沒有做剛開始的時候
我們起始的參數跟起始的 Loss
再來就進入了一個看似簡單
實際上非常關鍵的一步
這一步就是要設定超參數
比如說我們要設定 Learning Rate
這個 Learning Rate 太大也不好
太小也不好
沒人知道應該要設多少
所以這邊我們就先亂設一個 0.0018
看看會怎麼樣
然後再來就是要訓練幾個 Epoch
那這個要訓練幾個 Epoch 才會好呢
也說不準
先試個 100 個試試看
然後接下來就開始進入訓練了
那這邊有一個 for 迴圈
在 for 迴圈之前
我們這邊先訓練
先計算出我們總共的訓練資料有幾筆
那這個是等一下在做 Gradient Descent 的時候
會運用的上這個數值
進入 for 迴圈
for 迴圈就是跑一個一個 Epoch
在這個 Epoch 裡面
第一步
在這個 Epoch 裡面
第一步是要先計算 Gradient
也就是計算偏微分
怎麼計算 W1 對到 Loss 還有 B 對到 Loss 的偏微分呢
那這個我已經事先算好了
反正就是長這個樣子
那在這兩個式子裡面
你都需要
在這兩個式子裡面
你都需要用到 Y prediction 的結果
你都需要用到 Y prediction 的結果
所以你需要先根據現在的 W1 跟 B
先算出根據現在的 W1 跟 B
給 X1
然後預測的 Y 是多少
然後你要有這個數值
才能去計算 Gradient
因為這個 W1 跟 B 的 Gradient 裡面
都用得上根據現在的 W1 跟 B
做出來的 Y prediction 的結果
那有了這個以後其他都很簡單啦
就是把 Y_prediction - Y_train
乘上那個 X1 的數值
這個就是 W1 微分後的結果
那前面要乘上
前面乘上 2/n
然後把 Y_prediction - Y_train
對所有資料做加總
那就得到 B 的微分的結果
好這個是把 Gradient 算出來
把 Gradient 算出來之後呢
接下來要更新參數
這邊根據 Learning Rate 還有 Gradient 來更新參數
所以 W1 要減掉 Learning Rate 乘上 W1 的 Gradient
然後得到新的 W1
然後把這個 B 減掉 Learning Rate
乘上 B 的 Gradient
得到新的 B
好我們就更新完參數了
那接下來更新完參數以後
我想要記錄一下
現在這個新的參數
它的數值跟 Loss
所以我把新的參數放到 history 裡面
把新的參數放到 history 裡面
然後呢我用這個新的參數
這個 W1 跟 B 是已經更新過的
雖然我這個 notation 沒有變
但它們是已經更新過的
我拿更新過的參數去計算出 prediction
根據這個 prediction 計算出現在的 Loss
把現在的 Loss 存起來
好所以我們每做一次 update
我都會記錄下更新以後的參數
跟更新以後的 Loss
然後呢我會把那個更新以後的 Loss
還有更新以後的參數把它印出來
然後呢最終訓練完畢的時候
我會印出最終的 W1 跟最終的 B
那我們來看看結果怎麼樣吧
100 個 Epoch、Learning Rate 0.01
行不行啊
哎呀不行
你看
整個這個 W1 跟 B 的數值都大爆炸
Loss 也大爆炸
一切都大爆炸
沒辦法
所以看起來 0.01
0. 而且我們設多少啊
0.001 太大了
改小一點嗎
0.0001 嗎
0.0001
看起來還可以
剛才窮舉的那個 Loss 呢是 240
所以我們現在算出來 Loss 是 263
其實離 240 呢也是有一點接近了
那我們其實可以把那個 Error Surface
跟參數 update 的那個過程都畫出來
那這種畫大家再自己研究
我們這邊就是把 Loss 下降的過程把它畫出來啊
然後把這個參數 update 的過程啊
把它記錄下來
所以可以看到說
現在才走到這裡而已
所以看起來呢還有一段距離
我們現在設 100
那如果我再加個 10 倍呢
怎麼還在 261 的地方啊
怎麼 Loss 還是 261 啊
只走了一點點這樣不行
設 1 萬
希望不要讓大家等太久
那你說怎麼不 Learning Rate 設大一點呢
但你忘了嗎剛才 Learning Rate 設稍微大一點
就已經會噴飛出去了
所以 Learning Rate 不能設更大
爆跑一下
從 261 到 249
剛才最好答案是 240 啦
所以看起來還是差一點點
阿你看這個learning rate走得非常的慢
Learning rate設小的時候
就有可能走得非常慢
但是設大一點的又會飛起來
所以這招不是一個非常好的招數
有沒有更好的招數呢
這個我們留著下週再講
好不管怎樣就做到這邊了
好那我們再講一下 Batch Size 的概念
在剛才我們在計算 Gradient 的時候啊
你會發現我們都有用一個 np.sum
也就是我們對所有的資料算出
我們對所有的資料都考慮進去
我們對所有的資料都進行了加總
我想要拉到前面一點給大家看一下說
這邊都有 np.sum
都有 np.sum
就是對所有的資料進行了相加
當然因為我們現在資料很少啦
所以對所有資料進行相加
花不了多少時間
但是很多時候
假設你資料非常大的時候
你可能會想用這個 Batch
你可能會想讓你的模型
每看到一部分的資料的時候
就 update 一次參數
那至於用 Batch 的好處跟壞處
那我們剛剛上課的時候也已經跟大家剖析過了
那在這個 Colab 裡面呢
我們是把它好處跟壞處再講了一遍
好, 那我們來實作一下 Batch 吧
那前面這邊都是一樣的
W1、B 要有一個初始化的數值
然後我們要有一些 History 的 List
來記錄參數的變化跟 loss 的變化
這個都是剛才已經有的事情
然後呢, 這邊 Learning Rate 就跟剛才一樣都設 0.001 吧
那 Batch Size 呢
Epoch 呢, 我們先設 1, 我們先跑一個 Epoch 就好
那 Batch Size 呢, 我們總共有 20 筆資料
所以如果我 Batch Size 設 20, 那其實就是 Full-Batch
就等於是沒有達成那個 Batch 的效果
那如果設小一點, 比如說設個 5
那有 20 筆資料, 那就是 4 個 Batch
好, 那我們就開始進行訓練
那先知道總共有多少筆資料
然後計算出有幾個 Batch
那 Batch 的量就是資料的量除掉 Batch 的 Size
然後呢, 接下來呢, 就進入 Epoch, 那在每一個 Epoch 裡面會做什麼事情呢?
首先會做一件事情是 Random Shuffle, 我們剛才有講過說呢, 我們希望每一個 Batch,
在同一個 Batch 的資料是不一樣的, 所以這邊呢, 會先打亂資料的順序, 然後再去分 Batch。
所以你不會每次都是同樣的資料在同一個 Batch 裡面。
好, 然後接下來呢, 我們還會有另外一個 for 迴圈
在這個比較小的 for 迴圈裡面, 我們才是把 Batch 一個一個的去看過
所以前面這是一個 Epoch, 在一個 Epoch 裡面, 我們會看過好多個 Batch
把所有的 Batch 都看完, 才叫做走完一個 Epoch
然後在每一個 Batch 的 iteration 裡面呢, 我們會把那個 Batch 的資料讀出來
我們把那個 Batch 的資料先讀出來
然後呢, 我們再計算 Gradient Descent
那我們這邊計算 Gradient Descent 的方法呢
其實跟前面是一模一樣的
這邊的式子
如果你跟剛才的式子做比對的話
是一模一樣的
唯一不同的地方只是
當我們在做這個 Summation 的時候
不是針對所有的訓練資料去做 Summation
而是只針對一個 Batch 裡面的資料
去做 Summation
那我們的訓練資料量也變了
本來是大 N, 現在變成 n_batch
現在變成 n_batch
好, 然後呢, 計算出 Gradient 以後
我們就可以更新參數
然後呢, 我們會把更新後的參數存起來
我們也會把更新後的參數
計算出來的 loss 存起來
然後我們會把那個
Error Surface 把它畫出來
我們也會畫那個 loss 下降的曲線
來看一下做得怎麼樣喔
再 update 一次, 就只走這麼一點點喔
就只走這麼一點點
那如果你 Batch Size 設小一點
那就一個 Epoch 裡面就走多步一點
Batch Size 設 1, 那就可以走 20 步
有 20 筆資料就走 20 步
那也發現說呢, 這個模型走的呢
就是有點歪歪斜斜的
走的有點歪歪斜斜的
但是你可以在一瞬間呢
在同樣的一個 Epoch 裡面
就可以走出比較多的步數
那有時候對訓練是比較有利的
Batch Size 要開多少
你才能夠得到最好的結果
這個就要問你自己
你看這邊我們已經
如果我們看這個 Loss 的最低點
我們這邊已經跑到 100 以下
那如果你只有一個
如果你是用 Full-Batch 的話
只有一個 Epoch
你應該是跑不到 100 以下了
你看只有一次 update 的時候
我們的 Loss 大概是 450 左右
所以如果你有用 Batch 的概念
然後在這個例子裡面還是有用的
好
那做了這麼多
我們來驗證一下我們的模型做的怎麼樣吧
我們剛才呢
是有把 W1 我們剛才訓練最好的結果
跟 B 我們訓練最好的結果把它存下來
所以呢
我們現在就把 X1 在 Validation Set 上面的數值
跟 W1 還有 B 一起帶進 Linear Model
得到我們現在訓練出來的模型
在 Validation Set 上 Prediction 的結果
再計算在 Validation Set 上 Prediction 的結果
還有 Validation Set 上真正的結果的差異
算一下
哇 一算出來這個 Loss 挺大的
快將近 1000 感覺非常的糟糕
然後再來怎麼辦呢
你就要去檢視說到底是哪一步出了問題
那有可能是第一步出了問題
也許我們的訓練資料跟測試資料本來就是長得很不一樣
也就是他們是不匹配的
所以我們應該換一下訓練資料
所以我們把訓練資料從 2021 年的機器學習換成 2024 年的生成式人工智慧導論
那我們的 Validation Set 是一樣的
好來換一下
換一下
好
然後接下來訓練的過程跟剛才是完全一樣的
因為你甚至都不用改那個參數的名字
因為我們現在是拿同樣的名稱來代表訓練資料
我們只是把指導訓練資料的資料把它改變而已
所以這個 code 幾乎不需要改就可以開始訓練了
改了訓練資料以後
那我們就來開始訓練吧
這些參數跟剛才都是一樣的
每個東西都是一樣的
我們來看看做起來會不會更好一點
剛才我記得 Loss 是 200 多
現在 Loss 降到 78
結果還不錯
看起來換了訓練資料
但這個跟訓練資料有沒有匹配是沒有什麼關係
就是說這個現在我們用 2024 年的這個訓練資料
他的 Loss 是可以算出來比較低的
那真正重點是在 Validation Set 上有沒有比較好
剛才用 2021 年的機器學習在 Validation Set 上得到 Loss 將近 1000
那現在我們換了訓練資料
再重新跑一次 Validation Set 上的結果看起來有沒有差別呢
現在結果就好一些了
雖然 Loss 還是很大
算出來將近 350
仍然不是非常理想
但是總比 Loss 高達 1000 還要好的一些
好那接下來呢
就是我們想要畫一個更大的 Function Set 的搜尋範圍
也許我們第一個可以做的事情是
剛才的輸入只有 X1
這個 X1 是投影片的頁數
也許我們可以加一些額外的輸入
比如說這個投影片上有多少的字來做更豐富的考慮
來考慮更多的資訊
花一個更大的範圍的函數
呃所以我們需要定義另外一個 Linear Model
我們叫它 Linear Model 2
它的輸入是 x1, x2, w1, w2 跟 b
這個 w1, w2 跟 b 它的參數 x1 跟 x2 是輸入
那它的輸出就是 w1 乘 x1 加 w2 乘 x2
然後再加上 b
好 我們有這個第二個 Linear Model
它可以吃兩個輸入
好 那我們現在呢
要有第二個輸入
我們叫它 x2 train
x2 train 是 2024 年的頭影片上的字數
每份投影片上的總字數
然後有 x2 validation 就是 2025 年的投影片
每份投影片上的總字數
我們需要把這個新的訓練資料把它加進來
好那開始訓練吧
其實這個訓練的過程跟剛才的過程呢
沒什麼不同
我們現在就是多了 w2 要考慮啦
剛才只有 w1 嘛現在多了 w2
所以你要給 w2 的你要的這個初始參數
那從現在開始我們就不記錄參數的變化啦
因為有三個參數的變化
所以你沒辦法在二維平面上呈現啦
所以我們就不記錄參數的變化
我們只記錄 loss 的變化
好, 然後接下來呢, 這個 w
我們現在多的其實就是計算 w2 的 Gradient
其實 w2 的 Gradient 算起來的方式跟 w1 其實是一模一樣
唯一不同的地方就是你需要把 x1 換成 x2
這邊 x1 train 換成 x2 train 而已
更新的時候你也要更新 w2
然後記錄下當前的參數更新之後的 loss
然後印出來
好, 然後呢, 我們就來跑一下這個訓練吧, 看看能不能跑起來
哎呀, 都是 NaN, 怎麼回事
看起來這個 Learning Rate, 剛才我們只有一個參數的時候, 這個 Learning Rate能做
但現在如果是兩個參數的時候, 這個 Learning Rate 看起來不行, 太大了, 再弄個小一點的
欸 不 還是 NaN 再弄更小一點
哎呀 還是不行
哎呀 你這個到底要弄 你這個到底要多小 Learning Rate 你才跑得起來啊
太過分了吧
啊 設了一個很小 Learning Rate 勉強跑起來啦
但是因為 Learning Rate 太小了
所以 這邊看起來呢
他這個 loss 下降的是非常緩慢的
緩慢的
好
總之 Loss 現在算出來是
69
好了一點點了
剛才我記得是 77 開頭嘛
所以現在 loss 呢
加了一個額外的
這個 w2 以後呢
loss 還是下降了一點點
但是啊
我們剛才有說
這個直接算字數不是一個好方法
應該要算平均字數
而不是算總字數
所以我們把總字數除掉
投影片的頁數
得到平均字數
當做新的 feature
那我們把這個新的 feature 再拿來做訓練吧
其實這個新的 feature 會好訓練很多
為什麼這個新的 feature 會好訓練很多
你看剛才發現說我們的 loss
我們的 Learning Rate 要設得非常小
一個可能的原因是因為
今天如果你考慮的是總字數
總字數的數值跟頁數的數值差距太大了
當你的兩個輸入數值差距非常大的時候
會導致訓練非常的困難
所以當你換成平均字數, 它跟頁數比較接近的時候
那訓練起來其實會更容易一點
那加上平均字數其實又是一個比較合理的, 比較容易學的東西
所以我們可以預期說當我們把總字數換成平均字數的時候
應該是比較容易訓練的
好, 總之我們改了 x2 train 跟 x2 validation 的定義
然後再重新來訓練一次吧
那訓練的過程跟這個程式碼呢
跟剛才完全是一模一樣的, 不需修改
重新訓練一下
看起來 loss 又再低了一點
現在 loss 可以進步到 65
不過到目前為止
我們用的都是一個 Linear 的模型
我們可以驗證一下
驗證一下看看有沒有好一點
我們把我們現在新的模型
拿來做這個 Validation
那我記得剛才只有一個 feature 的時候
Validation 算出來的數值是 300 多
現在有兩個 feature
能不能好一點呢?
有好一點!
我們的 Validation 算出來是
124 還不錯
從 300 多進步到 124
那現在如果
因為這是 Mean Squared Error
所以如果你開根號以後呢
也是接近 11
所以我們現在的誤差呢
預測的誤差大概是 11 分鐘左右
好, 到目前為止
都是用 Linear 的 model
來改成用一個 Neural Network 吧
所以這邊呢
先定義了一個
簡單的 Neural Network 的函式
這個 Neural Network 的函式呢
它的輸入有 X
這是我們的訓練資料
有第一個 Layer 的 W
有 B, 有 W' 跟 B'
這個我們剛才課程裡的
用的符號呢
是一樣的
輸入 X 以後, X 會被乘上 W
這個小老鼠的
符號呢
代表的是矩陣相乘
在 Python 裡面呢
所以把 x 跟 w 做矩陣相乘
再加上 b 這個向量
我們得到了 z
然後把 z 再通過 ReLU 這個 activation function
那把 ReLU 定義在這邊
ReLU 就是輸入一個東西 x
那它會幫你去算說
如果這個 x 是小於 0
那我們就輸出 0
如果是大於 0 就輸出 x 本身
把 z 過 ReLU 得到 a
然後呢再把 A 乘上下一層的參數 WP
再加上 BP 得到 Prediction 的結果
然後呢我們就把 Prediction 的結果回傳
那除了回傳 Prediction 的結果以外
我們會把一些中間產物
包括這個 Z 跟 A 呢也回傳回去
我們把它放在 Cache 裡面回傳回去
那至於為什麼要這麼做呢
這是我們等一下訓練的時候會用到的
如果你只是要做測試
用這個 network 它參數都有了
你其實是不需要這個 Cache 的
但是因為我們等一下訓練的時候
會需要用到這個資訊
總之這邊就是劃定了我們訓練的範圍
我們要訓練的就是一個類神經網路
它這邊看起來平平無奇
但是它是一個類神經網路
那未知的東西就是 W, B, WP, BP
它們是要透過訓練資料被找出來的
我們先來實際使用看看
這個類神經網路
我們先把 W, B, WP 跟 BP 都給它一些隨機的數值
然後真的給這個類神經網路一個輸入
看看它能不能夠給我們一些輸出
那這個類神經網路呢
在它內部沒有定義它的 Hidden Layer 的 size
也就是它的中間的隱藏層有幾個 neuron
那這邊是定義在外部的
你給它 W, B, WP, BP 的形狀的時候
就決定了它的 Hidden Layer 的大小
Hidden Layer Size 我們就直接設個 100
那 W 就是一個 100x2 的矩陣
B 是一個 100x1 的矩陣
WP 是一個 1x100 的矩陣
BP 是一個 Constant
所以就是 1x1 的矩陣
然後我們把我們的訓練資料有兩個 feature 丟進去
把這邊 Random Initializer 它裡面的數字都隨機的
W, B, WP, BP 也都帶進去
然後進行運算
然後把 Prediction 的結果印出來
那就可以得到 Prediction 的結果
至少知道說
給這個類神經網路參數的時候
這個函式是可以順利運作的
裡面沒有什麼
項量、矩陣
它的 dimension 不匹配的這種問題
它是一個可以順利運作的類神經網路
好
接下來就要進入訓練的階段了
這邊類神經網路的訓練
是用 Python 直接實作的
當然你在作業裡面還有真實的應用裡面
你現在都會用深度學習的套件
沒有人會在手刻這個 deep learning 了
那在作業裡面
那主教範例程式也可以説明你
使用 PyTorch 的套件
順利的完成一個內神經網路
所以我這邊的程式並不值得你參考
好
那我們來訓練吧
來訓練一個內神經網路吧
那我們要怎麼訓練呢
首先呢
我們要初始化
我們要找參數
給他一個初始的數值 W, B, WP, BP
在前面幾個例子裡面這個初始的數值啊
我都直接寫好了什麼 1.0 啊 15 啊之類的
但現在這個 W 啊這些數值裡面
它是一個非常大的矩陣
所以它的數值我沒有辦法直接手寫
所以我們就是隨機的生成它
好定個 Learning Rate
然後定要訓練幾個 Epoch
然後 Loss History 呢
我們要把它記錄下來
然後計算有幾筆訓練資料量
好接下來呀
我們就要進入內神經網路的 update 了
這邊呢
如果你知道 Backpropagation 的話
我們需要 forward-backward 這兩個 pass
所以我們先做
forward pass
然後再做
backward pass
backward pass 呢
它內部的運作
寫在這個地方
有一點點複雜
那裡面會需要用到 ReLU 的 Gradient
把它寫在這邊
那這個我們就不細講
那總之我們就是對一個類神經網路計算出它的 Gradient
所以我們每一個參數 W, B, WP 跟 BP 它的 Gradient 都算出來了
我們就把 Learning Rate 乘上 W 的 Gradient
我把這些參數前面加個 d 代表是它的 Gradient
然後我們就把每一個參數減掉 Learning Rate 乘上它的 Gradient
得到更新後的參數
好, 有了更新後的參數呢, 你就可以跑一下這個內神經網路, 得到它的 Prediction,
得到它 Prediction 以後, 你就可以算一下 loss, 把 loss 存起來。
好, 那到底訓練不訓練得起來呢?
我們這個 Learning Rate 呢, 是 0.0001, Epoch 是 1 萬。
很意外
喔
85
我們剛才不是都跑到 Loss 已經可以低到 6 開頭了嗎
怎麼只有 85 而已 這個 Loss
太高了 內神經網路一定可以做得更好
不可以只有這樣的 Loss
再重新跑一次
哎呀 這一次更糟 這一次 238
為什麼每次跑都不一樣呢
因為每一次初始的參數不一樣
初始的參數不一樣
你最後會走到不一樣的地方
所以 Loss 就會不一樣
那因為類神經網路非常複雜
所以往往初始的參數對結果
也是會有一定程度影響的
重新跑一次
結果更差
再跑一次
哎呀 還是 200 多
哎呀 怎麼這個 Loss 壓不下去呢
這個就是你訓練類神經網路的時候
最容易遇到的問題啦
這個 Loss 壓不下去
而且因為參數量非常多
你又沒有辦法畫 Error Surface
所以往往你根本不知道問題出在哪裡
所以這個時候你能憑藉的是什麼呢
憑藉的第一個就是
你對於類神經網路的信仰
你相信它的 Loss 一定可以壓到更低
你相信它的 Loss 絕對不只這樣
所以你開始爆條參數
而另外一個你要憑藉的就是大量的運算資源
開始爆試各種不同的 hyperparameter
直到試到一個 Loss 夠低的為止
然後今天我們是因為訓練資料量非常小
內神經網路非常小
所以每按一次執行只花了幾分鐘的時間
如果今天每按一次執行要花一天
你就是欲哭無淚
那怎麼辦呢
接下來就是一頓操作猛如虎
接下來就是做了大量的參數的調整之後
另外我改了 update 參數的方式
我們現在是用一個叫做 Adam 的方式來 update 參數
那這個我們下週才會講
我本來不想用這一招
本來不想在這個時候用 Adam 的
但是看起來這邊不用 Adam
Loss 真的是壓不下去啊
所以哎呀
只好用個 Adam
好總之他這邊有比較複雜的參數 update 的方式
我們先不管他這個下週的事情
然後這個 initialization 也不可以隨便 initialize
要有特別的 initialization 的方法
才可以得到有可能比較好的結果
總之一頓操作之後
一樣的類神經網路, 一樣用 Gradient Descent
但是現在不同的初始而值
不同的 Learning Rate 的計算方式
這個 Adam 可以給我們一個動態的 Learning Rate
每個參數的 Learning Rate 都不一樣
也會隨時間不一樣
來看看有沒有救啊
欸你看這個 Loss 就可以壓到 21
所以這個還是有用的
這個接下來呢就要在 Validation Set 上再試一下啦
所以我們趕快剛才訓練出來的這個類神經網路
拿到 Validation Set 上再跑一下
看看結果會怎樣
剛才我們的 Loss 可是壓到 21 喔
Validation 的 Loss 呢
哎呀 1300 超過 1000
大炸裂
怎麼辦呢怎麼辦呢
其實呢你在訓練 Network 的過程中
你就已經可以挑 model 去在 Validation Set 上做 evaluation 了
所以我們這邊呢多加了一行
這一行是什麼
就剛才呢是每次我們訓練出
每次 update 參數之後
我們都會重新計算一下 loss
Training Data 上的 loss
現在我們不只算 Training Data loss
我們把 Validation Set 上的 loss 也把它算出來
然後最後呢
我們會畫圖展示training loss的變化
還有validation set的loss的變化
好, 這邊的參數, Epoch 500, 那就這樣吧
跑一下
好, 你會發現說呢, 我這個 Training 的 Loss 是下降的
Validation Loss 是先降後升
在某一個地方, 我們其實是有比較低的 Validation Loss
那要有多低呢?
其實我們可以把它輸出出來
我們這邊把那個 Validation Set 最低的那個低點的數值把它輸出出來
所以 Validation Set 它的 Loss 是 11 點多
在 Epoch 23 的時候
有最低的低點
好
那我們來
我們現在知道說
如果 Epoch 23 在 Validation Set 上
會有一個大概 11 點多的 Loss
那這樣這個差距已經落到 3 分鐘左右了
那我們就真的拿這個模型
來在測試資料上做測試吧
首先測試資料就是今天的課程
就是課程最後的高潮
測試完我們就下課
好我們先只跑 23 個 Epoch 得到一個 Loss 不錯低的模型
好真的來測試吧
這個測試呢就是我們需要輸入投影片的總字數
我們先算一下正確答案
就今天這一堂課呢到底花了多少分鐘
我們剛才是 2 點 23 分的時候開始上課
然後呢, 我們在 3 點 35 分的時候
2 點 23 分的時候開始上課
我們在 2 點 25 分的時候
然後下課
這個是 1 個小時又 2 分鐘
所以是 62 分鐘, 這是第一堂課
那我們不算下課的時間
也不算跑 Colab 實作的時間
第二堂課呢
這邊寫 3 點 35 分開始上課
上到幾點呢?
上到 4 點 20 分
所以總共多長呢?
總共 45 分鐘
好, 我們總共上了 40
所以兩堂課加起來總共 107 分鐘
這是我們的正確答案
好, 那接下來就要看說
今天到底能不能夠成功的
我們訓練出來的模型
到底能不能夠成功的預測
今天上課的時長呢?
其實我心裡也沒個底
好 今天投影片的頁數有多少呢
有 84 頁
84 頁
好 投影片的頁數
是 84 頁
好 有多少總字數呢
我剛才握著算了一下
我們投影片裡面的總字數是
3388 個
好
緊張的時刻來了
用這個類神經網路
它預測出來的數值
跟 107 會有多接近呢
我們來跑一下吧
啊 真的是 107 天啊
這個我真的沒有塞好
這個只是純粹運氣好而已
總之這個就是我們測試的結果
非常的精準
雖然我覺得只是單純運氣好而已
那我們今天上課其實就上到這邊
謝謝大家