[Music]


Hi
 everyone,
 my
 name
 is
 Ethan.
 I'm
 the


CEO
 and
 co-founder
 of
 TexQL
 and
 we
 build


agents
 uh
 for
 really
 really
 really
 big


data.
 This
 presentation
 is
 called


building
 AI
 that
 can
 query
 100,000


tables
 with
 pedabytes
 of
 data
 and
 zero


configuration
 because
 uh
 I
 I
 was
 told


that
 being
 very
 specific
 generally
 helps


to
 make
 people
 understand
 what
 you


actually
 do.
 And
 so
 what
 do
 we
 actually


do?
 Historically,
 there
 are
 kind
 of
 like


two
 types
 of
 like
 AI
 for
 data
 that


everyone's
 kind
 of
 heard
 of,
 right?


There's
 like
 uh
 there's
 like
 the
 classic


like
 historical
 infrastructure
 for
 like


really
 big
 data.
 This
 is
 data
 like
 that


can't
 fit
 on
 your
 computer.
 We're


talking
 about
 roughly
 at
 around
 like
 16


gigs
 is
 when
 like
 your
 computer
 would


shut
 down
 if
 you
 tried
 to
 do
 really
 big


manipulation
 on
 that.
 And
 so
 you
 need


very
 specialized
 systems
 uh
 Oracle
 like


ERP,
 CRM,
 HR
 systems,
 data
 warehouses,


big
 like
 you
 know
 uh
 BI
 tools.
 All
 these


things
 have
 very
 very
 high
 configuration


costs.
 You
 have
 to
 like
 migrate
 and
 like


when
 they
 offer
 an
 E1
 AI
 product
 that


only
 works
 for
 the
 data
 within
 their


environments.
 And
 then
 there's
 the
 other


class
 of
 uh
 data
 products
 and
 AI


products
 that
 everyone's
 probably
 played


with
 today
 here
 uh
 which
 are
 you
 know


cla
 notion
 glean.
 These
 are
 platforms


that
 like
 do
 really
 well
 with


powerpoints
 and
 presentations
 and
 uh
 and


spreadsheets
 and
 PDFs
 and
 word
 docs
 that


fit
 on
 your
 computer
 and
 uh
 do
 not
 let


you
 really
 access
 data
 that
 is
 pabytes


because
 there's
 a
 lot
 of
 work
 that
 goes


into
 that.
 And
 so
 that's
 what
 we
 try
 to


do.
 We
 try
 to
 make
 it
 possible
 to
 deploy


AI
 that
 you
 can
 ask
 questions
 like
 uh


can
 you
 connect
 my
 CRM
 data
 to
 my


snowflake
 data
 to
 like
 all
 the
 emails


that
 I
 sent
 last
 week
 and
 tell
 me
 like


which
 of
 my
 customers
 are
 most
 likely
 to


churn
 or
 can
 you
 validate
 that
 all
 these


numbers
 are
 correct
 and
 then
 you
 know


even
 commit
 like
 a
 PR
 to
 GitHub
 to
 make


sure
 that
 the
 uh
 that
 the
 SQL
 migrations


that
 we're
 doing
 are
 all
 like
 correct.


Um
 this
 is
 a
 very
 very
 hard
 amount
 of


overhead.
 We
 work
 with
 like
 very
 big


companies.
 Uh
 we
 work
 with
 like
 very
 uh


compliant
 like
 environments.
 we
 deploy


into
 like
 hospitals,
 financial
 services


is
 a
 super
 onrem
 like
 infra
 heavy
 like


environment
 and
 we
 work
 with
 some
 of
 the


largest
 AI
 labs
 because
 uh
 that
 that's


how
 hard
 it
 is
 to
 build
 out
 all
 this


infrastructure
 more
 use
 cases
 we
 do


financial
 services
 and
 healthcare
 like


predominantly
 and
 and
 we
 do
 a
 lot
 of


like
 transformations
 um
 around
 uh
 making


sure
 people
 can
 build
 pipelines
 move


data
 around
 really
 like
 work
 with
 big


data
 for
 lack
 of
 a
 better
 word.
 You


might
 be
 thinking
 why
 do
 I
 care
 about


this?
 Well,
 right
 now
 if
 the
 unit
 cost


of
 like
 you
 doing
 like
 a
 data
 request


and
 asking
 like,
 hey,
 can
 you
 like,
 you


know,
 figure
 out
 what
 the
 most
 likely


customer
 to
 turn
 is?
 Uh,
 if
 that
 costs


you
 a
 month
 of
 time
 of
 a
 Stanford
 PhD
 in


data
 science,
 like
 you
 pay
 $300,000
 a


year
 to,
 that
 sets
 a
 kind
 of
 a
 floor
 on


how
 valuable
 the
 opportunity
 has
 to
 be


for
 you
 to
 analyze
 that.
 Obviously,
 if


you
 can
 bring
 down
 the
 cost
 of
 that


analysis,
 if
 you
 can
 train
 an
 agent
 to


do
 that
 for
 one/10enth,
 100th,
 1/1,000th


of
 the
 cost,
 um,
 it
 unlocks
 a
 lot
 more


opportunities,
 right?
 It
 lets
 you
 like


take
 analysis
 that
 you're
 doing
 today
 at


like
 the
 like
 you're
 trying
 to
 figure


out
 like
 which
 of
 our
 customers
 is
 going


to
 churn
 per
 month
 per
 state
 and
 you


want
 to
 do
 that
 analysis
 really
 like
 per


city
 per
 week
 per
 uh
 per
 zip
 code
 per


day.
 Uh
 really
 like
 per
 per
 LinkedIn


post
 from
 your
 champion
 at
 a
 particular


account
 who
 is
 leaving
 and
 says
 I'm
 very


sad
 and
 you
 read
 from
 that
 that
 they


were
 fired
 and
 therefore
 all
 their


initiatives
 are
 going
 to
 shut
 down
 and


you
 want
 to
 like
 anticipate
 that
 in


advance
 or
 something.
 That's
 like
 a
 very


very
 low
 latency
 thing
 that's
 very
 hard.


It's
 very
 expensive.
 That's
 kind
 of
 what


we
 do.
 So
 to
 walk
 you
 through
 an
 example


uh
 let's
 say
 uh
 there's
 a
 finance
 guy
 on


your
 team
 uh
 Dylan
 who
 is
 in
 the


audience
 here
 with
 us
 today
 who
 uh
 we


have
 to
 do
 some
 board
 reporting.
 It's


five
 minutes
 until
 the
 board
 meeting
 and


he
 sent
 me
 this
 uh
 thing
 and
 he
 says
 uh


can
 you
 look
 at
 this
 like
 thing
 uh


spreadsheet
 there's
 an
 image
 on
 it.
 I


have
 no
 idea
 where
 these
 numbers
 came


from.
 Can
 you
 double
 check
 that
 these


are
 correct
 and
 I
 don't
 happen
 to
 have
 a


Stanford
 PhD
 that
 I
 can
 like
 accelerate


a
 month
 of
 work
 into
 two
 20
 minutes
 for.


So
 this
 is
 actually
 the
 uh
 this
 is
 like


slightly
 like
 this
 is
 our
 real
 like
 prod


data.
 Uh
 this
 is
 on
 top
 of
 a
 data


warehouse
 of
 I
 think
 about
 like


10,000ish
 tables.
 Uh
 it
 worked
 the
 first


time
 with
 no
 configuration.
 Now
 it's


kind
 of
 cached
 so
 it
 won't
 be
 as
 sexy.


Um
 so
 if
 I
 take
 this
 and
 I
 go
 to
 I
 need


a
 prepar
 meeting
 and
 I
 go
 uh
 hey
 Anna


can
 you
 double
 check
 if
 this
 is
 true.
 Uh


Matt
 told
 me
 this
 is
 correct.
 Uh
 the
 man


on
 my
 team.
 Um
 and
 uh
 and
 uh
 I'm
 going


to
 be
 really
 mad
 if
 it's
 not
 correct


because
 the
 board
 is
 going
 to
 fire
 me.


um
 uh
 tell
 me
 if
 they
 will
 be
 happy
 or


not
 and
 if
 not
 um
 what
 I
 can
 do
 about
 it


to
 drive
 the
 usage
 to
 go
 up.
 I
 used
 to


run
 a
 data
 team
 uh
 at
 a
 startup
 uh
 with


like
 10
 people.
 I
 was
 responsible
 for


every
 single
 one
 of
 these
 requests.
 If
 I


if
 I
 got
 a
 request
 like
 this,
 I
 would


want
 to
 gouge
 my
 eyes
 out
 because
 uh


there's
 no
 context.
 There
 is
 a
 image.
 I


have
 no
 idea
 what
 table
 this
 came
 from.


There's
 no
 lineage.
 There's
 no


infrastructure.
 There's
 no
 metadata


documentation
 whatsoever.
 I
 have
 no
 idea


what
 table
 they
 pulled
 it
 from.
 I
 don't


know
 if
 it
 was
 like
 segment
 blogs
 or


snowflake
 or
 something
 else.
 But
 uh


you'll
 see
 Anna
 start
 to
 reply
 and
 start


telling
 me
 the
 work
 that
 she
 is
 doing.


Uh
 so
 if
 I
 go
 into
 the
 environment,
 what


you'll
 see
 is
 uh
 you'll
 see
 the
 uh
 agent


has
 received
 this.
 Um
 I
 can
 you
 know
 ask


follow-up
 questions
 and
 it'll
 like
 once


it's
 done
 replying
 it'll
 like
 generate
 a


report,
 send
 it
 back
 to
 Slack
 and
 uh
 I


actually
 didn't
 think
 this
 far
 ahead


because
 if
 I
 had
 I
 would
 have
 realized


now
 we're
 going
 to
 stare
 at
 this
 run
 for


a
 little
 bit
 maybe
 to
 like
 walk
 through


like
 how
 we
 managed
 to
 do
 this
 and
 like


the
 kind
 of
 things
 that
 it's
 happening


under
 the
 hood.
 um
 how
 can
 I
 like
 do


this
 over
 hundreds
 of
 thousands
 of


tables
 without
 knowing
 anything
 about


your
 data
 warehouse?
 Well,
 the
 first


thing
 it
 does
 is
 uh
 it
 looks
 at
 a


context
 repository
 that
 we
 maintain.
 Uh


that's
 actually
 um
 a
 GitHub
 uh
 context


uh
 repo
 that
 we
 like
 let
 it
 like
 write


PRs
 to.
 And
 this
 way
 it
 kind
 of
 has
 this


like
 memory
 concept
 um
 that
 is
 like
 you


can
 scale
 this
 to
 infinite
 amounts.


It'll
 just
 run
 like
 50
 uh
 if
 anyone's
 an


engineer
 here
 like
 GPS
 uh
 against
 the


thing
 to
 try
 to
 find
 like
 all
 the
 like


keywords
 that
 you're
 looking
 for
 um


without
 using
 ve
 there's
 no
 vectors.
 is


entirely
 like
 syntactically
 oriented.
 So


it
 can
 like
 you
 know
 be
 very
 like
 meiy


in
 its
 approach
 to
 finding
 its
 content.


Um
 and
 like
 it
 has
 a
 ton
 of
 like
 you


know
 infinitely
 complex
 calculations
 or


anything
 else
 like
 built
 into
 it
 example


queries
 all
 the
 cache
 stuff
 and
 at
 the


end
 of
 any
 session
 you
 can
 tell
 the


agent
 just
 save
 this
 to
 your
 context


make
 sure
 it's
 always
 there
 and
 it'll


write
 a
 PR
 and
 uh
 you
 can
 merge
 that
 and


uh
 you
 will
 kind
 of
 just
 keep
 getting


smarter.
 Now
 when
 it
 gets
 into
 the
 SQL


mode
 uh
 this
 is
 kind
 of the
 thing
 that


we
 really
 do
 very
 differently
 from
 every


other
 company
 in
 the
 world
 uh
 who
 claims


to
 have
 something
 like
 this
 which
 is
 we


don't
 try
 to
 oneshot
 the
 SQL.
 What
 that


means
 is
 we
 don't
 try
 to
 write
 SQL
 hit


run
 and
 like
 make
 sure
 the
 query
 is


perfectly
 correct
 because
 anybody
 who
 in


the
 room
 has
 who
 has
 ever
 written
 SQL


before
 knows
 that
 if
 I
 give
 you
 10,000


pages
 of
 documentation
 and
 a
 SQL


terminal
 and
 I
 tell
 you
 hey
 can
 you
 help


me
 figure
 out
 which
 of
 our
 customers
 are


going
 to
 turn
 uh
 and
 you
 can
 only
 hit


the
 run
 button
 once
 you're
 going
 to
 get


the
 wrong
 answer.
 It
 is
 like
 literally


impossible.
 It
 is
 not
 how
 you
 go
 about


generating
 SQL.
 And
 so
 for
 us,
 we
 have


the
 agent
 explore
 the
 database.
 It
 it


it's
 looking
 at
 the
 information
 schema


for
 anyone
 who
 doesn't
 isn't
 SQL


literate.
 What
 this
 means
 is
 it
 is


literally
 saying
 tell
 me
 every
 table


table
 you
 have.
 Um
 and
 then
 tell
 me


every
 column
 you
 have.
 Um
 and
 then
 like


it's
 going
 to
 scan
 through
 the
 whole


thing
 and
 then
 like
 anything
 that's
 not


relevant,
 it's
 going
 to
 dump
 out
 of


context.
 Um
 and
 so
 it
 can
 like
 run
 for


an
 incredibly
 long
 amount
 of
 time
 as
 it


like
 tries
 tries
 to
 step
 through
 all
 of


this
 stuff.
 Now,
 um
 I
 guess
 while
 it's


running
 this,
 yeah,
 it's
 like
 like


pulling
 the
 it's
 pulling
 the
 data.
 It's


forming
 hypotheses.
 It
 might
 like
 look


at
 a
 daytime
 field
 like
 data
 warehouses


are
 incredibly
 messed
 up
 environments.


Every
 enterprise
 has
 like
 50
 of
 them.


Every
 enterprise
 has
 another
 30


different
 BI
 tools
 from
 like
 the
 past


like
 35
 years.
 Um
 three
 different
 ERPs,


17
 different
 CRM.
 And
 so
 there's
 a
 lot


of
 like
 hypothesis
 testing
 that
 like
 the


agent
 steps
 through
 as
 it's
 exploring.


It
 has
 to
 like
 go,
 oh,
 like
 plot
 the


daytime
 columns.
 Oh,
 cool.
 Uh
 there's
 a


drop
 here.
 There's
 a
 data
 pipeline
 that


was
 probably
 broken.
 Um
 so
 it's
 a
 lot
 of


it's
 a
 lot
 of
 like
 hypothesis
 testing
 so


it
 can
 like
 recover
 from
 all
 of
 the


things
 and
 all
 the
 pieces.
 Um
 I
 think


it's
 a
 written
 some
 like
 uh
 you
 know


written
 some
 plots,
 written


visualizations.
 Uh
 eventually
 now
 it's


like
 referencing
 our
 CRM
 uh
 and
 so
 it


can
 directly
 plug
 into
 any
 other
 data


sources
 that
 you
 have
 uh
 that
 is
 very


fast
 to
 configure
 because
 well
 it's


making
 an
 API
 call
 the
 exact
 same
 way


that
 any
 one
 thing
 can
 make
 an
 API
 call.


Um,
 and
 yeah,
 while
 it's
 doing
 all
 this,


I
 can
 show
 you
 uh
 what
 happens
 after
 I


complete
 this,
 which
 is
 um
 I
 would
 take


like
 a
 completed
 playbook
 or
 like
 a


report
 or
 something
 like
 for
 example


this
 daily
 customer
 upsell
 analysis
 that


tells
 me
 which
 customer
 I
 should
 reach


out
 to
 and
 ask
 them
 for
 more
 money.
 Uh,


and
 once
 I
 know
 that,
 you
 know,
 it
 steps


through
 like
 50
 steps
 and
 it'll
 give
 me


a
 good
 result,
 I
 can
 schedule
 this
 to


run
 every
 day,
 every
 week,
 every
 time
 a


customer
 sends
 me
 an
 email
 um
 to
 give
 me


a
 new
 report
 like
 this.
 Right?
 This
 is


how
 you
 um
 kind
 of
 go
 from
 like
 the
 uh


ask
 questions
 and
 it'll
 tell
 you
 what
 is


this
 number
 to
 eventually
 ask
 questions.


It'll
 tell
 you
 why
 did
 this
 number
 go
 up


and
 eventually
 uh
 ask
 questions
 why
 did


this
 go
 up?
 Okay,
 given
 why
 that
 you


know
 why
 it
 went
 up
 tell
 me
 what
 I
 can


do
 to
 make
 the
 number
 go
 up
 or
 down


depending
 on
 if
 it's
 a
 good
 number
 or
 a


bad
 number.
 And
 that's
 like
 a
 very
 very


useful
 thing
 because
 once
 you
 do
 that


you
 make
 way
 more
 money
 having
 the
 thing


uh
 I
 guess
 like
 you
 know
 run
 on
 a
 loop


and
 like
 just
 find
 you
 more
 money.
 Okay,


it's
 done.
 So
 it's
 finished
 the
 final


report.
 it
 came
 back.
 There's
 like
 a


whole
 thing.
 It
 says
 uh
 you
 know
 the
 uh


the
 the
 expected
 stuff
 is
 all
 correct.


There's
 a
 like
 3%
 delta
 I
 think
 because


we
 changed
 the
 way
 we
 count
 something.


Um
 and
 also
 the
 August
 number
 was
 like


way
 off
 because
 uh
 this
 this
 like
 thing


was
 like
 presented
 to
 me
 in
 August.
 If


you
 read
 the
 text,
 it
 says,
 you
 know,


chart
 displayed
 August
 to
 date
 for


August
 represented
 approximately
 August


13th,
 which
 is
 probably
 correct.
 If
 I
 go


back
 and
 check
 when
 he
 sent
 me
 this
 uh


or
 I'm
 going
 assume
 it's
 correct
 because


there's
 no
 reason
 for
 it
 to
 not
 be


correct.
 Um
 and
 then
 it'll
 tell
 me
 like


uh
 present
 actually.
 Okay,
 fine
 fine


fine.
 I'll
 check
 I'll
 double
 check
 that


Matt
 uh
 Matt
 Matt
 actually
 sent
 me
 this


at
 that
 time.
 This
 is
 uh
 August
 14th.


It's
 we're
 off
 by
 a
 date.
 It
 said


approximately
 13.
 It's
 actually
 14.
 Um


and
 the
 uh
 and
 the
 and
 the
 thing
 comes


out
 to
 uh
 revenue
 growth
 ACU
 usage
 uh


something
 something
 corresponds
 to
 the


thing.
 Uh
 we
 should
 tell
 Matt's
 data
 is


partially
 correct,
 incomplete.
 I
 likely


working
 with
 midmon
 data,
 exceptional


performance,
 celebration
 worthy
 data,


something
 something.
 This
 is
 great.
 I'm


not
 going
 to
 get
 fired.
 Uh
 and
 uh
 now


I'm
 really
 happy.
 And
 yeah,
 the
 that's


approximately
 uh
 the
 demo.
 Uh
 like
 we
 uh


for
 people
 who
 like
 wanted
 are
 curious


about
 how
 deeper
 how
 much
 deeper
 we
 go


like
 there's
 a
 lot
 of
 infrastructure


that
 we
 had
 to
 build
 out
 to
 make
 this


possible.
 We've
 like
 rebuilt
 this


product
 for
 the
 first
 two
 and
 a
 half


years
 of
 this
 company.
 We
 rebuilt
 this


product
 seven
 times
 over,
 right?
 We


tried
 like
 cataloges,
 fine-tuning,
 SQL


terminals,
 vector
 stores
 um
 like like


cataloges
 with
 vector
 stores
 and


sandboxes
 and
 SQL
 stores
 and
 all
 these


things
 like
 we
 added
 like
 ductb
 we
 added


polers
 we
 added
 um
 like
 we
 started
 with


the
 sandbox
 so
 we
 could
 execute
 and


we're
 as
 as
 I've
 been
 recently
 reminded


uh
 we're
 the
 first
 company
 to
 publicly


release
 a
 version
 of
 code
 interpreter
 uh


that
 wasn't
 in
 beta
 and
 uh
 and
 then
 we


were
 like
 oh
 the
 data
 is
 getting
 really


big
 okay
 well
 we
 need
 something
 that's


not
 just
 pandas
 in
 python
 it's
 going
 to


be
 polars
 then
 we
 were
 like
 okay
 the


data
 got
 even
 bigger
 okay
 we
 need
 like


DB
 got
 even
 bigger
 passed
 down
 to
 the


data
 warehouse
 got
 even
 bigger
 and
 we


needed
 joints
 on
 the
 fly
 serverless


click
 house
 infrastructure
 then
 we
 had


to
 roll
 a
 semantic
 layer
 for
 because


some
 people
 wanted
 deterministic
 queries


to
 come
 out
 every
 time
 then
 some
 of


those
 queries
 took
 a
 really
 long
 time
 so


we
 added
 caching
 at
 acceleration
 and


then
 some
 people
 had
 Tableau
 and
 looker


and
 powerbi
 and
 all
 those
 things
 so
 we


had
 to
 build
 our
 own
 MCP
 servers
 for
 all


those
 platforms
 uh
 because
 uh
 data


bricks
 and
 snowflake
 and
 Tableau's
 MCP


servers
 are
 moderately
 maybe
 not
 the


most
 usable
 things
 in
 the
 world
 in


summary
 uh
 if
 If
 you
 if
 you
 want
 to
 roll


this
 yourself
 at
 your
 own
 company
 uh


company
 like
 ramp
 or
 with
 exceptional


engineers
 or
 something
 you
 should
 do


that
 uh
 this
 is
 totally
 doable
 now
 with


today's
 technology
 in
 GPD5
 all
 you
 have


to
 do
 is
 build
 your
 own
 uh
 compute


format
 uh
 table
 format
 uh that
 is


natively
 integrated
 with
 your
 compute


format
 that
 can
 do
 this
 across
 multiple


different
 large
 data
 sources.
 Build
 your


own
 MCP
 servers
 for
 the
 Tableau's
 17


different
 ways
 of
 ingesting
 data.
 Four


out
 of
 four
 of
 them
 break.
 We
 actually


found
 like
 a
 roundabout
 way
 to
 actually


get
 this
 to
 work.
 So
 much
 so
 that
 even


the
 Tableau
 team
 currently
 tax
 us
 into


accounts
 to
 try
 to
 I
 guess
 like
 to


figure
 out
 their
 AI
 things
 for
 customers


who
 want
 to
 use
 Tableau
 AI
 but
 can't
 get


it
 to
 work.
 We
 rolled
 our
 own
 semantic


layer.
 It's
 compilable,
 transpilable


from
 like
 all
 existing
 semantic
 layers


um
 like
 look
 ML
 cube
 metric
 flow
 and
 uh


and
 and
 like
 a
 really
 cool
 like
 guey


that
 looks
 kind
 of
 wild
 when
 you
 put
 in


over
 a
 thousand
 objects
 onto
 the
 same


thing
 uh
 at
 the
 same
 time.
 and
 uh
 and


our
 own
 agent
 builder
 which
 you
 know


this
 is
 like
 very
 different
 from
 like


because
 we're
 running
 like
 a
 more


non-deterministic
 agent
 uh
 like
 uh
 like


glide
 code
 modality
 wise
 it
 looks
 very


very
 different
 like
 the
 right
 UI
 builder


for
 something
 that's
 going
 to
 be
 less
 if


else
 statements
 and
 more
 use
 best


judgment
 and
 run
 for
 like
 2
 hours
 or
 10


hours
 as
 like
 test
 time
 compute
 like


kicks
 in
 for
 like
 the
 latest
 like


generation
 of
 models
 you
 generally
 want


like
 more
 logic
 on
 the
 trigger
 way
 more


uh
 instructions
 on
 the
 prompt


environment
 and
 then
 like
 an
 easier


iteration
 cycle
 for
 testing
 because


you're
 going to
 be
 running
 these
 models


for
 like
 hours
 and
 hours.
 Now,
 this
 is


not
 the
 best
 UI
 in
 the
 world,
 but
 um


generally
 pretty
 bearish
 on
 um
 I
 guess


like
 the
 canvas
 based
 like
 UI
 for
 like


agent
 building.
 We
 rolled
 our
 own
 I


guess
 we
 we
 have
 a
 semantic
 layer
 which


means
 we
 need
 tools
 to
 query
 the


semantic
 layer.
 We
 rolled
 our
 own


version
 of
 um
 we
 we
 took
 like
 lookers


like
 metric
 explorer
 and
 like
 palunteers


object
 explorer
 and
 combined
 them
 onto


something
 that
 works
 on
 the
 same
 surface


area
 because
 sometimes
 business
 users


want
 to
 ask
 get
 me
 all
 the
 blanks
 like


customers
 or
 something
 with
 all
 these


attributes
 and
 that's
 like
 an
 object


question
 because
 every
 row
 is
 a
 noun
 and


sometimes
 people
 ask
 show
 me
 how
 x
 y
 and


z
 metrics
 change
 over
 time
 and
 uh
 break


it
 out
 by
 17
 different
 objects
 that
 come


from
 35
 different
 tables
 and
 that's


going
 to
 be
 also
 a
 horrible
 thing
 but


that's
 a
 totally
 different
 type
 of
 math


um
 to
 get
 correct.
 I
 know
 a
 lot
 of


people
 in
 the
 world
 offer
 the
 uh
 offer


like
 AI
 for
 SQL,
 AI
 for
 big
 data,
 AI
 for


data
 warehouses.
 We
 work
 with
 a
 lot
 of


customers
 who've
 been,
 you
 know,
 two
 and


a
 half
 years
 in
 on
 building
 semantic


models
 for
 Snowflake
 and
 they
 can
 get
 it


to
 work
 for
 exactly
 20
 tables.
 Um,
 and


you
 know,
 we
 can
 for
 anyone
 here
 who


cares
 about
 asking
 questions
 of
 their


data
 and
 getting
 decision
 intelligence


and
 all
 that
 stuff
 and
 building
 AI
 that


can
 work
 with
 really
 big
 data.
 Like
 if
 I


can't
 make
 sense,
 if
 our
 agent
 can't


make
 sense
 of
 your
 data
 warehouse
 with,


you
 know,
 over
 100,000
 tables
 even
 um,


in
 any
 data
 environment
 with
 no


configuration
 time
 and
 it
 can't
 build


its
 own
 semantic
 model
 and
 like
 do
 all


the
 data
 integration
 for
 itself
 in
 the


same
 loop
 and
 like
 do
 all
 the
 PRs
 and


change
 your
 DBTs
 and
 everything
 else


required
 to
 like
 get
 it
 to
 a
 place
 where


it
 can
 analyze
 all
 your
 data.
 I
 will
 buy


your
 entire
 whole
 team
 data
 team
 uh,
 NOU


with
 our
 VC
 dollars.
 Uh
 we've
 made
 this


offer
 I
 think
 like
 20
 times.
 Uh
 nobody's


taken
 up
 up
 on
 it.
 And
 uh
 maybe
 one
 day


uh
 we
 will
 come
 across
 a
 horrible


horrible
 I'm
 guessing
 it's
 probably


going
 to
 be
 like
 a
 manufacturing
 company


with
 like
 IoT
 devices
 um
 like
 with
 like


infinite
 like
 joint
 complexity.
 Uh
 but


uh
 until
 then
 uh
 yeah
 I'd
 love
 to
 you


know
 thank
 thank
 you
 for
 watching.