All
 right.
 Hello
 everybody.
 Welcome.


>> Dude,
 we're
 literally
 10
 seconds
 late.


It's
 not
 even


>> It's
 not
 even
 that
 late.
 Uh,
 thank
 you


everybody
 for
 being
 here.
 We
 are
 here


for
 Nvidia
 GTC.
 Jensen's
 going to
 be


speaking
 in
 about
 five
 minutes.
 It
 might


be
 a
 little
 bit
 later,
 but
 it
 should
 be


in
 in
 five
 minutes
 at
 12
 p.m.
 Eastern.


Uh,
 Tanner,
 what
 are
 you
 expecting?
 This


should
 be
 a
 big
 one.


>> Should
 be
 a
 big
 one.
 Um,
 you
 know,
 GTC


is
 all
 about
 artificial
 intelligence


nowadays.
 So,
 um,
 I
 I'm
 I
 expect
 a
 lot


of
 agentic
 stuff,
 uh,
 physical
 AI,


self-driving,
 robotics,


um,
 that
 sort
 of
 stuff.
 Maybe
 if
 we're


lucky,
 Vera
 Rubin,
 or
 what
 the
 next


steps
 are.
 Um,
 you
 know,
 that
 would
 be


very,
 very
 cool
 to
 see
 what
 their
 next


rack
 system
 would
 look
 like.
 Uh,
 we


already
 know.
 I
 mean,
 they've
 showed
 it


off
 a
 bunch,
 but
 um,
 Jensen,
 if
 he
 can


deliver
 enough
 hype
 on
 that,
 that
 that


will
 push
 the
 stock.
 A
 lot
 of
 the
 extra


stuff
 is
 good
 to
 know,
 but
 if
 you're


talking
 about
 what
 makes
 a
 financial


difference
 of
 the
 business,
 it's,
 you


know,
 what
 are
 the
 products
 that
 they're


selling
 to
 Microsoft,
 Google,
 Amazon,


etc.
 Um,


but
 we'll
 find
 out.
 I
 mean,
 uh,
 we
 don't


know
 100%.
 A
 lot
 of
 people
 are
 down


there
 though
 in
 Washington
 right
 now.


Um,
 and
 I've
 heard
 nothing
 but
 good


things
 so
 far.


>> Yeah,
 Jensen's
 there,
 Brad
 Gersonner's


there.
 Um,
 you
 got
 the
 figure
 CEO.
 You


got
 a
 lot
 of
 people
 there.
 I
 think
 we're


I
 think
 it's
 going
 to
 be
 some


interesting
 stuff.
 I
 mean,
 this
 is
 the


event
 where
 they
 do
 show
 off
 their


chips.
 So,
 I
 think
 it's
 likely,
 you


would
 say,
 right,
 that
 we
 might
 get
 some


Vera
 Rubin
 updates.


>> Yeah.
 Yeah.
 Um,
 yeah,
 I'm
 I'm
 I'm
 pretty


confident
 in
 it.
 Um,
 but
 there
 has
 been


some
 times
 where
 they'll
 skip
 a
 year


before
 showing
 off
 their
 next
 big


product.
 I
 think
 last
 time
 was
 a
 lot
 of


um,
 Blackwell
 Ultra
 talk,
 but
 you
 know,


they've
 been
 pushing
 out
 a
 lot
 of


updates
 on
 Vera
 Rubin,
 their
 NVL
 144s.


So,
 I
 hope
 to
 see
 that.
 That
 would
 be


really
 cool.


>> Agreed.
 Nvidia
 also
 invested
 a
 billion


in
 Nokia.
 Uh,
 not
 sure
 why,
 but
 they


did.
 I
 imagine
 there's
 going to
 be
 some


announcement
 of
 a
 partnership.
 This
 is


what
 I
 mean.
 Like
 you're
 going
 to
 get
 a


bunch
 of
 different
 random
 stuff


happening
 today.
 Uh
 here's
 a
 little
 bit


of
 a
 headline.
 Nvidia
 to
 make
 1
 billion


equity
 investment
 in
 Nokia
 in
 in


addition
 to
 new
 strategic
 partnership


Nokia's
 board
 resolved
 on
 directed
 share


issuance
 to
 Nvidia.


>> Yeah.
 Do
 you
 have
 any
 reason
 why
 they


would
 invest?
 I
 I
 I
 don't
 know
 of
 a


reason
 right
 now,
 but
 whenever
 you're


making
 this
 much
 money,
 I'
 I've
 been


saying
 this
 for
 a
 long
 time,
 they
 are


definitely
 going
 to
 be
 uh
 having
 a
 large


venture
 arm
 and
 and
 giving
 out
 a
 lot
 of


money
 to
 get
 equity
 stakes
 in
 businesses


that
 are
 only
 going
 to
 push
 Nvidia


further.


>> Yeah.
 So,
 um,
 you
 know,
 they're
 in
 pole


position
 right
 now
 and
 they
 get
 to
 make


these
 great
 investments
 and
 then
 the


products
 that
 they
 sell
 to
 those


customers
 or
 or
 the
 products
 that


they're
 going
 to
 design
 with
 these


customers
 like
 Intel
 and
 like
 all
 these


other
 partnerships
 that
 they've
 recently


given
 uh,
 you
 know,
 money
 to,
 that
 only


goes
 to
 further
 the
 Nvidia


advantage
 or
 or
 their
 their
 lead


position.
 So,
 um,
 but
 it
 all
 comes
 down


to
 GTC.
 I
 mean,
 I'm
 I'm
 really
 excited


to
 see.


>> So,
 Jensen
 is
 not
 live
 yet,
 but
 this
 is


a
 clip
 that
 we
 had
 from
 the
 pre-show,


and
 I
 think
 this
 clip
 is
 just
 pretty


freaking
 incredible.
 Uh,
 take
 a
 look
 at


this.
 He
 crashed
 a
 panel,
 delivered


water,
 and
 said,
 "Once
 a
 bus
 boy,
 always


a
 bus
 boy."


>> What?


>> Hi.


>> What's
 going
 on
 here?


>> Jensen
 Hong,
 everyone.


>> Oh,
 he
 brought
 us
 water.


>> Thanks.


>> He
 does.


>> Yes,
 we
 do.
 hydrated.


>> Thank
 you.
 It's
 very
 important
 to
 stay


hydrated.


>> What
 What
 is
 this
 mumble
 jumble
 about


quantum


>> super
 position


>> entanglement


>> entangling
 stuff?
 Like
 this
 is
 just


mumble
 jumble.


>> Hey,
 good
 to
 see
 you.


>> You
 know,
 once
 a
 bus
 boy,
 always
 a
 bus


boy.


>> This
 guy's
 worth
 a
 hundred
 billion


delivering
 water.
 I
 mean,
 you
 got
 to


love
 it.
 You
 got
 to
 love
 it.
 What
 is
 up


with
 this
 mumbo
 jumbo
 that
 is
 quantum?


>> No,
 but
 but
 then
 he
 gives
 then
 he
 gives


a
 little
 bit
 of
 a
 compliment


>> from
 Denny's
 to
 DC.
 From
 Denny's
 to
 DC.


>> Thanks
 for
 being
 here.
 Wow,
 this
 is


>> Thanks
 for
 having
 me.
 It's
 incredible.


>> It's
 been
 an
 amazing
 morning
 and
 this


panel,
 I
 think,
 saving
 the
 best
 for


last,
 Jensen.
 So,
 you
 know,
 maybe
 ask


this.
 You
 you
 caused
 a
 kurfuffle
 uh
 last


year
 when
 you
 you
 you
 made
 some
 comments


about
 quantum.
 anything
 you
 would
 like


to
 revise,
 you
 know,
 as
 we're
 about


ready
 to
 launch
 our
 quantum
 panel
 here


about
 how
 Nvidia
 thinks
 about
 quantum.


>> You
 know,
 I
 got
 to
 tell
 you,
 I'm
 afraid


to
 say
 a
 word.


>> If
 I
 just
 said
 quantum,


the
 stock
 price
 goes
 up.
 Quantum
 quantum


quantum.


I
 mean,
 he
 did
 c
 he
 caused
 those
 stocks


to
 go
 50%
 down
 in
 a
 day
 around
 March
 of


last
 year
 or
 no
 of
 earlier
 this
 year
 of


2025
 when
 he
 said,
 you
 know,
 quantum
 is


decades
 away.
 I
 don't
 know
 about
 you,


but
 I'm
 hearing
 a
 lot
 more
 coverage
 in


the
 overall
 universe
 around
 quantum.
 So,


maybe
 Jensen
 is
 going
 to
 embrace
 it.


Also,
 the
 the
 a
 lot
 of the
 quantum
 bulls


will
 say
 it's
 a
 threat


>> to
 the
 dominance
 of
 GPUs.
 So,
 there's
 a


lot
 of
 perspectives
 there
 to
 understand.


>> Yeah.
 Yeah.
 I
 mean,
 um,
 I
 don't
 have
 too


many
 takes
 on
 quantum.


We'll
 see
 what
 ends
 up
 happening,
 but,


uh,
 I'm
 sure
 Jensen
 is
 going
 to
 keep
 his


eye
 on
 it
 and,
 and
 if
 there
 is,
 uh,
 you


know,
 if
 he
 sees
 large
 investment
 going


that
 way,
 obviously
 Nvidia
 is
 going
 to


be
 a
 big
 part
 of
 that
 as
 well,
 or
 at


least
 try
 to
 position
 themselves
 there.


And,
 and
 I
 think
 Bayern
 H
 by
 having
 a


quantum
 segment
 and
 all
 of
 these
 things,


he's
 definitely,
 you
 know,
 not
 avoiding


it.
 He's
 not
 brushing
 it
 off
 like


um


I
 do
 think
 it's
 important
 that
 that


GP
 if
 we're
 going
 to
 have
 a
 QPU
 QPU
 that


uh
 Jensen
 also
 is
 involved
 in
 that.
 So


I'm
 not
 looking
 to
 get
 an
 individual


quantum
 name
 in
 the
 portfolio.
 I
 have


that
 through
 you
 know
 Google
 or
 um
 you


know
 you
 could
 play
 that
 through


Microsoft
 if
 you
 want.
 I
 don't
 know.


Um,
 so
 Jensen
 is
 not
 live
 yet.
 Uh,


usually
 they
 start
 a
 couple
 minutes


late,
 so
 we
 might
 have
 to
 take
 a
 minute.


Uh,
 Tanner,
 it
 was
 a
 big
 day
 for
 you


though
 so
 far,
 recovering
 back
 to
 30.


>> I
 mean,
 I
 it
 was
 a
 phenomenal
 quarter.


Like,
 at
 its
 core,
 it
 was
 a
 phenomenal


quarter.


>> Honestly,
 I'm
 um
 loving
 the
 quarter.


Loved
 everything
 that
 they
 had
 to
 say


about
 the
 loan
 platform
 business.
 way


more
 bullish
 on
 those
 types
 of
 metrics


than
 I
 ever
 could
 have
 imagined.
 $3.4


billion
 dollars
 of
 loan
 platform


origination.
 I
 mean,
 it
 was
 just
 th


those
 are
 crazy
 numbers.


>> We
 have
 rate
 cuts
 tomorrow.
 I
 think
 too


many
 people
 threw
 in
 the
 towel
 the


second
 we
 broke
 under
 $29.


>> Yeah.


>> And
 and
 forgot
 to
 just
 look
 back
 at
 what


they
 actually
 just
 delivered
 and
 and


what's
 coming
 ahead.
 I
 mean,
 all
 the


comments
 were,
 "Hey,
 Q3
 was
 amazing.


Best
 we've
 ever
 seen."
 and
 Q4
 we're


already
 like
 we're
 picking
 up
 the
 phone


and
 and
 you
 know
 a
 lot
 of
 the
 customers


that
 we
 already
 have
 are
 asking
 for
 more


commitments
 for
 their
 loan
 platform


business
 deals.
 So
 um
 they're


essentially
 you
 know
 guidance
 exploded


to
 37
 cents
 from
 31
 cents.
 That's
 after


dilution.
 EPS
 is
 based
 on
 the
 total


outstanding
 shares.
 So
 people
 there
 was


a
 whole
 kurfuffle
 about
 that
 dilution.


It's
 only
 going
 to
 accelerate
 their


their
 growth
 rate.
 So,


um,
 yeah,
 very
 happy
 shareholder,
 but


like
 I
 said,
 this
 is
 a
 long-term
 play.


SoFi
 stock
 will
 do
 SoFi
 stock
 things
 on


earnings.
 I
 mean,
 it
 it's
 this
 is
 a


classic
 at
 this
 point.


>> Yeah.
 And
 again,
 look,
 if
 there
 was


actually
 a
 red
 flag
 in
 the
 quarter,
 I


think
 people
 would
 have
 a
 more
 negative


take
 on
 the
 stock
 price
 reaction,
 but


people
 expected
 the
 stock
 to
 be
 I
 mean,


I
 know
 a
 lot
 of
 people
 were
 excited,
 but


this
 is
 what
 SoFi
 usually
 does,
 but
 you


can't
 you
 can't
 actually
 point
 to


anything
 negative.
 I
 mean,
 you
 really


can't.


>> What
 are
 you
 looking
 forward
 to
 from
 GTC


really
 quick?
 Oh,
 also,
 are
 you
 actually


to
 stay
 on
 SoFi?
 Are
 you
 holding
 on
 to


your
 position?
 I


>> mean,
 yeah.
 What
 else
 am
 I
 going
 to
 do?


I
 feel
 like
 I
 can
 sell
 calls
 against
 it,


right?


>> Aren't
 you
 in
 the
 money
 or
 or
 sorry,


aren't
 aren't
 you
 in
 profit
 right
 now?


>> Yeah,
 but
 it's
 not.
 You
 know,
 I
 bought


my
 average
 is
 29.
 It's
 at
 30.
 So,
 it's


like


>> I
 thought
 you
 bought
 at
 like
 26
 27.


>> No,
 but
 then
 I
 averaged
 up.
 That
 was


only
 like
 200
 shares.


>> Gotcha.
 So,
 I
 bought
 500
 more
 on
 Friday.


And
 so,
 I
 think
 I
 don't
 think
 there's
 a


reason
 to
 sell.
 I
 mean,
 we
 have
 rate


cuts
 coming
 tomorrow.
 The
 quarter
 just


crushed.
 So,
 what
 do
 I
 I
 mean,
 maybe


I'll
 sell
 some
 35
 weeklies
 and
 pick
 up
 a


couple
 cents,
 but
 I
 I
 mean,
 I
 don't


think
 there's
 a
 reason
 to
 panic
 based
 on


>> Do
 you
 think
 the
 price
 uh
 will
 change


tomorrow
 if
 we
 get
 rate
 cuts?


>> Well,
 yeah,
 but
 I'm
 also
 thinking
 the


market's
 expecting
 a
 rate
 cut.
 So,
 how


much
 of
 a
 surprise
 could
 it
 be?
 But


maybe
 officially
 getting
 it
 would
 be
 the


thing.
 I
 mean,
 I
 don't know
 if
 the
 price


changes
 tomorrow,
 but
 I
 know
 that
 rate


cuts
 are
 bullish.
 And
 so,


>> yeah,
 but
 last
 time
 I
 thought,
 you
 know,


September
 was
 pretty
 locked
 in.
 And
 then


we
 also
 saw
 quite
 a
 pump
 on
 September


rate
 cuts
 for
 SoFi.
 So,
 um,
 I
 thought
 it


was
 a
 little
 priced
 in,
 but
 then
 the


market
 still
 gave
 it
 a
 premium.


>> Yeah.


>> So,
 I
 don't
 know.


>> I
 would
 not
 be
 surprised
 if
 Ray
 Cuds


pump
 up
 so
 tomorrow.
 Would
 not
 be


surprised
 at
 all.
 Um,
 Jensen,
 are
 we


live?


We
 are
 not
 live.
 I'm
 on
 the
 Nvidia


mainstream.
 I
 don't
 see
 him
 live
 yet.


Uh,
 you
 you
 asked,
 "What
 am
 I


expecting?"
 I'm
 just
 expecting
 him
 to
 be


pretty
 confident
 and
 continue
 to
 deliver


updates
 on
 this.
 I
 this
 rift
 that
 he's


on
 now
 is
 AI
 factories.
 So,
 I
 think


we're
 going
 to
 hear
 a
 lot
 about
 that.
 I


think
 the
 S-curve
 about
 robotics,
 he


really
 wants
 the
 world
 to
 understand


physical
 AI
 is
 coming.
 And
 then
 from


there,
 hopefully
 some
 Verar
 Rubin
 stuff


would
 be
 be
 nice
 to
 see.
 Jensen's
 not


live,
 guys.
 Not
 yet.
 Oh,
 nope.
 Is
 wait.


Is
 this


Is
 this
 officially
 him?


Their
 main
 screen
 didn't
 have
 him.
 Okay.


All right.
 Let
 me
 move
 over
 to
 this
 this


one.


>> See,
 Bitcoin,
 Ethereum,
 Salana,
 or
 XRP.


>> That's
 a
 scam.


>> That's
 what I
 was
 gonna
 say.
 What's
 the


>> Oh
 my
 gosh,
 they
 really
 got
 me
 on
 that,


bro.
 They
 really
 got
 Dude,
 whatever


channels
 you
 guys
 look,
 that
 was
 a
 scam.


This
 is
 the
 official


As
 soon
 as
 I
 heard
 Bitcoin,
 I
 was
 like


and
 I
 saw
 the
 QR
 code.
 I
 was
 like,
 of


course.


>> Yeah,
 I'm
 not
 I'm
 not
 seeing
 anything


either.


>> Uh,
 Amcor,
 not
 a
 horrible
 recovery,


guys.
 Back
 to
 3215.
 This
 thing
 fell
 to


28.
 I
 bought
 the
 dip
 at
 31.
 So,
 again,


the
 quarter
 was
 really
 good.
 This
 was


not
 a
 bad
 quarter.
 It's
 just
 when
 the


CEO
 leaves,
 you
 know,
 the
 streak
 can
 get


a
 little
 upset.
 So
 if
 we
 get
 any
 bullish


Nvidia


AI
 semiconductor
 excitement
 for
 the


entire
 sector,
 you
 might
 be
 seeing
 AM


Core
 potentially
 recover


name
 lately.
 I
 I
 I
 saw
 your
 um


post
 on
 X
 about
 it
 and
 and
 I
 thought
 it


was
 really
 compelling,
 but
 I
 haven't


looked
 deep
 enough
 into
 it.


It
 trades
 at
 onetime
 sales
 and
 there's


only
 a
 handful
 of
 companies
 that
 do


packaging
 in
 the
 United
 States
 and
 they


have
 a
 very
 close
 relationship
 with
 TSM


in
 Arizona


>> and
 they
 just
 had
 a
 record
 Q3
 revenue


quarter.
 I
 mean


>> they
 beat
 on
 EPS
 by
 10
 cents.
 I
 mean


like
 I


>> I
 think
 it's
 a
 very
 attractive
 play


going
 into
 26
 and
 I'm
 looking
 for
 things


that
 are
 not
 as
 um
 propped
 up
 in
 terms


of
 multiples
 and
 this
 one
 doesn't
 have
 a


multiple.
 So
 yeah,
 that's
 why
 I
 like
 it.


Uh
 Jensen
 still
 not
 live.
 I
 mean,
 what


are
 you
 expecting?
 Are
 you
 Are
 you


expecting
 any
 major
 partnerships?
 Do
 you


expect
 him
 to
 say
 anything
 crazy
 about


like
 robotics
 or
 Cosmos
 or
 anything
 like


that?


>> Um,
 no,
 not
 necessarily.
 I
 don't
 go
 in


with
 with
 too
 high
 of
 expectations.
 Um,


so
 no,
 I
 I'm
 here
 to
 learn,
 hear
 what
 he


has
 to
 say.
 Um,


you
 know,
 I
 don't
 know if
 we're
 going to


get
 any
 Yeah,
 it's
 not
 an
 investor
 day,


right?
 So,
 I'd
 be
 more
 interested
 in


hearing
 what
 the
 earnings
 call
 has
 to


say
 around
 China
 and
 stuff
 like
 this.


This
 is
 going
 to
 be
 more
 product
 based.


Um,
 so
 no,
 I
 don't
 have
 large


expectations.
 I'm
 sure
 there's
 going
 to


be
 a
 lot
 of
 networking
 talk,
 AI
 talk.
 If


if
 Brett
 um
 Adcock
 is
 there,
 there's


going
 to
 be
 a
 lot
 of
 robotics
 talk.


There
 has
 been
 at
 GTC
 the
 last
 three


years.
 So,
 um
 or
 last
 three
 events,


but
 I
 don't
 have
 many
 expectations


directly.
 No.


>> Yeah.
 I
 think
 robotics
 is
 going
 to
 be
 a


big
 part.
 I
 agree.


>> Someone
 that
 is
 uh
 at
 the
 event
 is


saying
 they
 are
 a
 little
 behind,
 but


they're
 about
 to
 get
 going.


>> Is
 that
 Evan?


>> Um


no,
 but
 Evan's
 also
 there.


>> I
 actually
 um


Never
 mind.


Yeah.
 Am
 I
 allowed
 to
 say
 that?


Yeah.
 Evan
 Evan,
 he
 literally
 tweeted
 a


picture
 that
 he
 was
 there.


>> No,
 but
 that
 uh
 the
 connection
 to
 what


happened
 to


us?


>> I
 don't even
 know
 what
 you're
 talking


about


>> with
 Evan.


>> What?
 That
 he
 invited
 us
 to
 go
 there?


>> Yeah,
 that
 that
 that
 that
 we
 got


invited.


>> I
 didn't
 know
 that
 that
 was
 something


that


>> Yeah,
 you
 could.
 I
 mean,
 I
 think
 I
 think


I
 told
 everybody
 I
 got
 invited
 a
 while.


I
 just
 I
 didn't
 feel
 like
 going.
 So,


>> you
 should
 have
 went
 because
 you've
 been


holding
 a
 video
 since
 2018.
 You
 actually


should
 have
 went.


>> I
 don't
 know
 why.


>> It's
 SoFi
 earnings,
 bro.
 I
 like


>> Oh,
 I
 forgot.


>> Day
 of
 SoFi
 earnings.


>> Yeah.
 Okay,
 we're
 starting.
 We're


starting


>> and
 giving
 rise
 to
 Silicon
 Valley.


HDI
 Lamar
 reimagined
 communication,


paving
 the
 way
 for
 wireless


connectivity.


IBM's
 system
 360
 put
 a
 universal


computer
 at
 the
 heart
 of
 industry.


[Music]


Intel's
 microprocessor


drove
 the
 digital
 age
 forward


and
 Craze
 supercomputers
 expanded
 the


frontiers
 of
 science.


>> So,
 we
 think
 we're
 at
 the
 beginning
 of


something
 with
 this
 technology
 and
 we're


going
 to
 go
 just
 as
 fast
 as
 we
 can.


>> Apple
 made
 computing
 personal.


Hello,
 I
 imagine
 us.


>> Microsoft
 opened
 the
 window
 to
 a
 new


world
 of
 software.


>> Long
 before
 the
 web,


>> you've
 got
 mail.


>> US
 government
 researchers
 built
 Arponet,


connecting
 the
 first
 computers,
 the


foundation
 for
 the
 internet.


>> An
 iPod,


a
 phone.


>> Are
 you
 getting
 it?


>> Then
 Apple
 again.
 Put
 a
 thousand
 songs


in
 your
 pocket
 and
 the
 internet
 in
 your


hand.


Every
 era
 a
 leap.


>> We
 choose
 to
 go
 to
 the
 moon
 in
 this


decade
 and
 do
 the
 other
 things
 not


because
 they
 are
 easy
 but
 because
 they


are
 hard.


>> Every
 leap
 America
 led.


>> Leap
 for
 mankind.


>> Now
 the
 next
 era
 is
 here.
 Launched
 by
 a


revolutionary
 new
 computing
 model.


>> This
 is
 likely
 going
 to
 be
 the
 most


important
 contribution
 we've
 made
 to
 the


computer
 industry.
 It
 will
 likely
 be


recognized
 as
 a
 revolution.


Machine
 learning
 is
 a
 branch
 of


artificial
 intelligence.
 Computers
 that


almost
 appear
 to
 think.


[Music]


The
 amount
 of
 computational
 resource
 is


ultimately
 what's
 going
 to
 turbocharge


this
 field.


Artificial
 intelligence,
 the
 new


industrial
 revolution.


At
 its
 heart,
 Nvidia
 GPUs
 invented
 here


in
 America.
 Like
 electricity
 and
 the


internet,
 AI
 is
 essential


infrastructure.


Every
 company
 will
 use
 it.
 Every
 nation


will
 build
 it.


>> Winning
 this
 competition
 will
 be
 a
 test


of
 our
 capacities
 unlike
 anything
 since


the
 dawn
 of
 the
 space
 age.


>> And
 today,
 AI
 factories
 are
 rising.


Built
 in
 America


for
 scientists,
 engineers,
 and
 dreamers


across
 universities,


startups,
 and
 industry.


>> I
 think
 we
 want
 to
 try
 to
 reach
 new


heights
 as
 a
 civilization,


discovering
 the
 nature
 of
 the
 universe.


>> And
 now,
 American
 innovators
 are


clearing
 the
 way
 for
 abundance,


>> saving
 lives.


shaping
 vision
 into
 reality.


[Music]


Lending
 us
 a
 hand


and
 delivering
 the
 future,


we
 will
 soon
 power
 it
 all
 with
 unlimited


clean
 energy.


We
 will
 extend
 humanity's
 reach


to
 the
 stars.


[Music]


This
 is
 America's
 next
 Apollo
 moment.


Together,


we
 take
 the
 next
 great
 leap


to
 boldly
 go
 where
 no
 one
 has
 gone


before.


And
 here


is
 where
 it
 all
 begins.


[Music]


Welcome
 to
 the
 stage,
 Nvidia
 founder
 and


CEO,
 Jensen
 Wong.


Washington
 DC.


Washington
 DC.
 Welcome
 to
 GTC.


[Music]


It's
 hard
 not
 to
 be
 sentimental
 and


proud
 of
 America.
 I
 got
 to
 tell
 you


that.
 Was
 that
 video
 amazing?


Thank
 you.


Nvidia's
 creative
 team
 does
 an
 amazing


job.
 Welcome
 to
 GTC.
 We
 have
 a
 lot
 to


cover
 with
 you
 today.
 Um
 GTC
 is
 where
 we


talk
 about
 industry,
 science,


computing,


the
 present,
 and
 the
 future.
 So,
 I've


got
 a
 lot
 to
 cover
 with
 you
 today,
 but


before
 I
 start,
 I
 want
 to
 thank
 all
 of


our
 partners
 who
 helped
 sponsor
 this


great
 event.
 You'll
 see
 all
 of
 them


around
 the
 show.
 They're
 here
 to
 meet


with
 you
 and
 uh
 uh
 really
 great.
 We


couldn't
 do
 what
 we
 do
 without
 all
 of


our
 ecosystem
 partners.
 Uh
 this
 is
 the


Super
 Bowl
 of
 AI,
 people
 say.
 And


therefore,
 every
 Super
 Bowl
 should
 have


an
 amazing
 pregame
 show.
 What
 do
 you


guys
 think
 about
 the
 pregame
 show?
 and


our
 all
 allstar
 allstar
 athletes
 and


allstar
 cast.
 Look
 at
 these
 guys.


Somehow
 I
 turned
 out
 the
 buffest.
 What


do
 you
 guys
 think?


I
 don't
 know
 if
 I
 had
 something
 to
 do


with
 that.


Nvidia
 invented
 a
 new
 computing
 model


for
 the
 first
 time
 in
 60
 years.
 As
 you


saw
 in
 the
 video,
 a
 new
 computing
 model


rarely
 comes
 about.
 It
 takes
 an
 enormous


amount
 of
 time
 and
 set
 of
 conditions.
 We


observed,
 we
 invented
 this
 computing


model
 because
 we
 wanted
 to
 solve


problems
 that
 generalpurpose
 computers,


normal
 computers
 could
 not.
 We
 also


observed
 that
 someday
 transistors
 will


continue.
 The
 number
 of
 transistors
 will


grow,
 but
 the
 performance
 and
 the
 power


of
 transistors


will
 slow
 down.
 that
 Moore's
 law
 will


not
 continue
 beyond
 limited
 by
 the
 laws


of
 physics
 and
 that
 moment
 has
 now


arrived.
 Dinard
 scaling
 has
 stopped.


It's
 called
 dinard
 scaling.
 Dard
 scaling


has
 stopped
 nearly
 a
 decade
 ago
 and
 in


fact
 the
 transistor
 performance
 and
 its


power
 associated
 has
 slowed
 tremendously


and
 yet
 the
 number
 of
 transistor


continued.
 We
 made
 this
 observation
 a


long
 time
 ago
 and
 for
 30
 years
 we've


been
 advancing
 this
 form
 of
 computing
 we


call
 accelerated
 computing.
 We
 invented


the
 GPU.
 We
 invented
 the
 the
 programming


model
 called
 CUDA.
 And
 we
 observed
 that


if
 we
 could
 add
 a
 processor
 that
 takes


advantage
 of
 more
 and
 more
 and
 more


transistors,
 apply
 parallel
 computing,


add
 that
 to
 a
 sequential
 processing
 CPU


that
 we
 could
 extend
 the
 capabilities
 of


computing
 well
 beyond
 well
 beyond.
 And


that
 moment
 has
 really
 come.
 We
 have
 now


seen
 that
 inflection
 point.
 Accelerated


computing
 its
 moment
 has
 now
 arrived.


However,
 accelerated
 computing
 is
 a


fundamentally
 different
 programming


model.
 You
 can't
 just
 take
 a
 CPU


software
 software
 written
 by
 hand


executing
 sequentially
 and
 put
 it
 onto
 a


GPU
 and
 have
 it
 run
 properly.
 In
 fact,


if
 you
 just
 did
 that,
 it
 actually
 runs


slower.
 And
 so,
 you
 have
 to
 reinvent
 new


algorithms.
 You
 have
 to
 create
 new


libraries.
 You
 have
 to
 in
 fact
 rewrite


the
 application
 which
 is
 the
 reason
 why


it's
 taken
 so
 long.
 It's
 taken
 us
 nearly


30
 years
 to
 get
 here.
 But
 we
 did
 it
 one


domain
 at
 a
 time.


This
 is
 the
 treasure
 of
 our
 company.


Most
 people
 talk
 about
 the
 GPU.
 The
 GPU


is
 important,
 but
 without
 a
 programming


model
 that
 sits
 on
 top
 of
 it,
 and


without
 dedication
 to
 that
 programming


model,
 keeping
 it
 compatible
 over


generations,
 we're
 now
 CUDA
 13
 coming
 up


with
 CUDA
 14,


hundreds
 of
 millions
 of
 GPUs
 running
 in


every
 single
 computer,
 perfectly


compatible.
 If
 we
 didn't
 do
 that,
 then


developers
 wouldn't
 target
 this


computing
 platform.
 If
 we
 didn't
 create


these
 libraries,
 then
 developers


wouldn't
 know
 how
 to
 use
 the
 algorithm


and
 use
 the
 architecture
 to
 its
 fullest.


One
 application
 after
 another.
 I
 mean,


these
 this
 is
 really
 the
 this
 is
 really


the
 treasure
 of
 our
 company.
 CU
 litho


computational
 lithography.


It
 took
 us
 nearly
 seven
 years
 to
 get


here
 with
 K
 litho
 and
 now
 TSMC
 uses
 it,


Samsung
 uses,
 ASML
 uses
 it.
 This
 is
 an


incredible
 library
 for
 computational


lithography.
 the
 first
 step
 of
 making
 a


chip.
 Sparse
 solvers
 for
 CAE


applications.


Co-op,
 a
 numerical
 optimization
 is


broken
 just
 about
 every
 single
 record.


The
 traveling
 salesperson
 problem,
 how


to
 connect
 millions
 of
 products
 with


millions
 of
 customers
 in
 the
 supply


chain.
 Warp
 Python
 solver
 for
 CUDA
 for


simulation.
 QDF
 a
 dataf
 frame
 approach


basically
 accelerating


SQL
 dataf
 frame
 pro
 dataf
 frame


databases.
 Um
 this
 library
 is
 the
 one


that
 started
 AI
 alto
 together
 coupnn


the
 the
 library
 on
 top
 of
 it
 called


megatron
 core
 made
 it
 possible
 for
 us
 to


simulate
 and
 train
 extremely
 large


language
 models.
 The
 list
 goes
 on.
 Uh,


Monai,
 really,
 really
 important,
 the


number
 one
 medical
 imaging


AI
 framework
 in
 the
 world.
 Uh,
 by
 the


way,
 we're
 not
 going
 to
 talk
 a
 lot
 about


health
 care
 today,
 but
 be
 sure
 to
 see


Kimberly's
 keynote.
 She's
 going
 to
 talk


a
 lot
 about
 the
 work
 that
 we
 do
 in


healthcare.
 And
 the
 list
 goes
 on.
 Uh,


genomics
 processing,
 Ariel,
 pay


attention.
 We're
 going to
 do
 something


really
 important
 here
 today.
 Um,
 coup


quantum
 quantum
 computing.
 This
 is
 just


a
 representative
 of
 350
 different


libraries
 in
 our
 company.
 And
 each
 one


of
 these
 libraries
 redesigned
 the


algorithm
 necessary
 for
 accelerated


computing.
 Each
 one
 of
 these
 libraries


made
 it
 possible
 for
 all
 of
 the


ecosystem
 partners
 to
 take
 advantage
 of


accelerated
 computing.
 And
 each
 one
 of


these
 libraries
 opened
 new
 markets
 for


us.
 Let's
 take
 a
 look
 at
 what
 CUDA
 X
 can


do.


Ready,
 go.


[Music]


Is
 that
 amazing?


Every


everything
 you
 saw
 was
 a
 simulation.


There
 was
 no
 art,
 no
 animation.
 This
 is


the
 beauty
 of
 mathematics.
 This
 is
 deep


computer
 science,
 deep
 math.
 And
 it's


just
 incredible
 how
 beautiful
 it
 is.


Every
 industry
 was
 covered
 from


healthcare
 and
 life
 sciences
 to


manufacturing,
 robotics,
 autonomous


vehicles,
 computer
 graphics,
 even
 video


games.
 That
 first
 shot
 that
 you
 saw
 was


the
 first
 application
 Nvidia
 ever
 ran.


And
 that's
 where
 we
 started
 in
 1993.
 And


we
 kept
 believing
 in
 what
 we
 were
 trying


to
 do.
 And
 it
 took,
 it's
 hard
 to
 imagine


that
 you
 could
 see
 that
 first
 virtual


fighter
 scene
 come
 alive
 and
 that
 same


company
 believed
 that
 we
 would
 be
 here


today.
 It's
 just
 really,
 really


incredible
 journey.
 I
 want
 to
 thank
 all


the
 NVIDIA
 employees
 for
 everything
 that


you've
 done.
 It's
 really
 incredible.


We
 have
 a
 lot
 of
 industries
 to
 cover


today.
 I'm
 going
 to
 cover
 AI,


6G,


quantum,


models,


enterprise
 computing,
 robotics,
 and


factories.
 Let's
 get
 started.
 We
 have
 a


lot
 to
 cover,
 a
 lot
 of
 big
 announcements


to
 make,
 a
 lot
 of
 new
 partners
 that


would
 very
 much
 surprise
 you.


Telecommunications


is
 the
 backbone,
 the
 lifeblood
 of
 our


economy,
 our
 industries,
 our
 national


security.
 And
 yet,


ever
 since
 the
 beginning
 of
 wireless


where
 we
 defined
 the
 technology,
 we


defined
 the
 global
 standards,
 we


exported
 American
 technology
 all
 around


the
 world
 so
 that
 the
 world
 can
 build


on
 top
 of
 American
 technology
 and


standards.
 It
 has
 been
 a
 long
 time
 since


that's
 happened.
 wireless
 technology


around
 the
 world
 largely
 today
 deployed


on
 foreign
 technologies.


Our
 fundamental
 communication
 fabric


built
 on
 foreign
 technologies.


That
 has
 to
 stop
 and
 we
 have
 an


opportunity
 to
 do
 that
 especially
 during


this
 fundamental
 platform
 shift.
 As
 you


know
 computer
 technology
 is
 at
 the


foundation
 of
 literally
 every
 single


industry.
 It
 is
 the
 single
 most


important
 instrument
 of
 science.
 It's


the
 single
 most
 important
 instrument
 of


industry.


And
 I
 just
 said
 we're
 going
 through
 a


platform
 shift.
 That
 platform
 shift


should
 be
 the
 once-in-a-lifetime


opportunity
 for
 us
 to
 get
 back
 into
 the


game
 for
 us
 to
 start
 innovating
 with


American
 technology.
 Today,
 today
 we're


announcing
 that
 we're
 going
 to
 do
 that.


We
 have
 a
 big
 partnership
 with
 Nokia.


Nokia
 is
 the
 second
 largest


telecommunications
 maker
 in
 the
 world.


It's
 a
 $3
 trillion
 industry.


Infrastructure
 is
 hundreds
 of
 billions


of
 dollars.
 There
 are
 millions
 of
 base


stations
 around
 the
 world.
 If
 we
 could


partner,
 we
 could
 build
 on
 top
 of
 this


incredible
 new
 technology
 fundamentally


based
 on
 accelerated
 computing
 and
 AI.


and
 for
 United
 States,
 for
 America
 to
 be


at
 the
 center
 of
 the
 next
 revolution
 in


6G.
 So
 today
 we're
 announcing
 that


Nvidia
 has
 a
 new
 product
 line.
 It's


called
 the
 Nvidia
 Arc.
 The
 aerial
 radio


network
 computer,
 aerial
 RAM
 computer,


Arc.
 Arc
 is
 built
 from
 three
 fundamental


new
 technologies.
 the
 gray
 CPU,
 the


Blackwell
 GPU,
 and
 our
 ConnectX
 Melanox


Connect
 X
 networking
 designed
 for
 this


application.
 And
 all
 of
 that
 makes
 it


possible
 for
 us
 to
 run
 this
 library,


this
 CUDA
 X
 library
 that
 I
 mentioned


earlier
 called
 Aerial.
 Aerial
 is


essentially
 a
 wireless


communication
 system
 running
 on
 top
 of


CUDA
 X.
 We're
 going
 to
 we're
 going
 to


create
 for
 the
 first
 time
 a


softwaredefined


programmable
 computer
 that's
 able
 to


communicate
 wirelessly
 and
 do
 AI


processing
 at
 the
 same
 time.
 This
 is


completely
 revolutionary.
 We
 call
 it


Nvidia
 Arc.
 And
 Nokia


is
 going
 to
 work
 with
 us
 to
 integrate


our
 technology,
 rewrite
 their
 stack.


This
 is
 a
 company
 with
 7,000


fundamental


essential
 5G
 patents.


Hard
 to
 imagine
 any
 greater
 leader
 in


telecommunications.
 So,
 we're
 going
 to


partner
 with
 Nokia.
 They're
 going
 to


make
 Nvidia
 Arc
 their
 future
 base


station.
 Nvidia
 Arc
 is
 also
 compatible


with
 Airscale,
 the
 current
 Nokia
 base


stations.
 So
 what
 that
 means
 is
 we're


going
 to
 take
 this
 new
 technology
 and


we'll
 be
 able
 to
 upgrade
 millions
 of


base
 stations
 around
 the
 world
 with
 6G


and
 AI.
 Now
 6G
 and
 AI
 is
 really
 quite


fundamental
 in
 the
 sense
 that
 for
 the


first
 time
 we'll
 be
 able
 to
 use
 AI


technology


AI
 for
 RAN
 to
 make
 radio
 communications


more
 spectral
 efficient


doing
 using
 artificial
 intelligence


reinforcement
 learning
 adjusting
 the


beam
 forming
 in
 real
 time
 in
 context


depending
 on
 the
 surroundings
 and
 the


traffic
 and
 the
 mobility
 the
 weather
 all


of
 that
 could
 be
 taken
 into
 account
 so


that
 we
 could
 improve
 spectral


efficiency.
 Spectral
 efficiency
 consumes


about
 one
 and
 a
 half
 to
 2%
 of
 the


world's
 power.
 So
 improving
 spectral


efficiency
 not
 only
 improves
 the
 amount


of
 data
 we
 can
 put
 through
 wireless


networks
 without
 increasing
 the
 amount


of
 energy
 necessary.
 The
 other
 thing


that
 we
 could
 do


AI
 for
 RAN
 is
 AI
 on
 RAM.
 This
 is
 a
 brand


new
 opportunity.
 Remember
 the
 internet


enabled
 communications
 but
 amazingly


smart
 companies
 AWS
 built
 a
 cloud


computing
 system
 on
 top
 of
 the
 internet.


We
 are
 now
 going
 to
 do
 the
 same
 thing
 on


top
 of
 the
 wireless
 telecommunications


network.
 This
 new
 cloud
 will
 be
 an
 edge


industrial
 robotics
 cloud.
 This
 is
 where


AI
 on
 RAN
 the
 first
 is
 AI
 for
 RAN
 to


improve
 radio
 radio
 spectrum
 efficiency.


The
 second
 is
 AI
 on
 RAN
 essentially


cloud
 computing
 for
 wireless


telecommunications.


Cloud
 computing
 will
 be
 able
 to
 go
 right


out
 to
 the
 edge
 where
 data
 centers
 are


not
 are
 not
 because
 we
 have
 base


stations
 all
 over
 the
 world.
 This


announcement
 is
 really
 exciting.
 Justin


Hodar
 the
 CEO
 I
 think
 he's
 somewhere
 in


the
 room.
 Thank
 you
 for
 your


partnership.
 Thank
 you
 for
 helping


United
 States
 bring
 telecommunication


technology
 back
 to
 America.
 This
 is


really
 a
 fantastic,
 fantastic


partnership.
 Thank
 you
 very
 much.


That's
 the
 best
 way
 to
 celebrate
 Nokia.


Let's
 talk
 about
 quantum
 computing.


1981


particle
 physicist
 quantum
 physicist


Richard
 Feman
 imagined
 a
 new
 type
 of


computer
 that
 can
 simulate
 nature


directly.
 To
 simulate
 nature
 directly


because
 nature
 is
 quantum.
 He
 called
 it


a
 quantum
 computer.
 40
 years
 later
 the


industry
 has
 made
 a
 fundamental


breakthrough.
 40
 years
 later,
 just
 last


year,
 a
 fundamental
 breakthrough.
 It
 is


now
 possible
 to
 make
 one


logical
 cubit.
 One
 logical
 cubit.
 One


logical
 cubit
 that's
 coherent,
 stable,


and
 error
 corrected
 in
 past.
 Now
 that


one
 logical
 cubit
 consists
 of
 could
 be


sometimes
 tens,
 sometimes
 hundreds
 of


physical
 cubits
 all
 working
 together.
 As


you
 know,
 cubits,
 these
 particles
 are


incredibly
 fragile.
 They
 could
 be


unstable
 very
 easily.
 Any
 observation,


any
 sampling
 of
 it,
 any
 environmental


condition
 causes
 it
 to
 become


decoherent.
 And
 so,
 it
 takes
 an


extraordinarily
 well-controlled


environments.
 And
 now
 also
 a
 lot
 of


different
 physical
 cubits
 for
 them
 to


work
 together
 and
 for
 us
 to
 do
 error


correction
 on
 these
 what
 are
 called


auxiliary
 or
 syndrome
 cubits
 for
 us
 to


error
 correct
 them
 and
 infer
 what
 that


logical
 cubit
 state
 is.


There
 are
 all
 kinds
 of
 different
 types


of
 quantum
 computers.
 superconducting,


photonic,
 trapped
 ion,
 stable
 atom,
 all


kinds
 of
 different
 ways
 to
 create
 a


quantum
 computer.
 Well,
 we
 now
 realize


that
 it's
 essential
 for
 us
 to
 connect
 a


quantum
 computer
 directly
 to
 a
 GPU


supercomputer
 so
 that
 we
 could
 do
 the


error
 correction,
 so
 that
 we
 could
 do


the
 artificial
 intelligence
 calibration


and
 control
 of
 the
 quantum
 computer
 and


so
 that
 we
 could
 do
 simulations


collectively
 working
 together.
 the
 right


algorithms
 running
 on
 the
 GPUs,
 the


right
 algorithms
 running
 on
 the
 QPUs
 and


the
 two
 processors,
 the
 two
 computers


working
 side
 by
 side.
 This
 is
 the
 future


of
 quantum
 computing.
 Let's
 take
 a
 look.


supercomputing
 platform.


Wow,
 this
 is
 a
 really
 long
 stage.


You
 know,
 CEOs,
 we
 don't
 just
 sit
 at
 our


desk
 typing.
 It's
 this
 is
 a
 physically


job
 physical
 job.
 So
 So
 today
 we're


announcing
 the
 MV
 MVQ
 link
 MVQL
 link
 and


it's
 made
 possible
 by
 two
 things.
 Of


course
 this
 interconnect
 that
 does


quantum
 computer
 control
 and
 calibration


quantum
 error
 correction
 as
 well
 as


connects
 two
 computers
 the
 QPU
 and
 our


GPU
 supercomputers
 to
 do
 hybrid


simulations.


It
 is
 also
 completely
 scalable.
 It


doesn't
 just
 do
 the
 error
 correction
 for


today's
 number
 of
 few
 cubits.
 It
 does


error
 correction
 for
 tomorrow
 where


we're
 going
 to
 essentially
 scale
 up


these
 quantum
 computers
 from
 the


hundreds
 of
 cubits
 we
 have
 today
 to
 tens


of
 thousands
 of
 cubits,
 hundreds
 of


thousands
 of
 cubits
 in
 the
 future.
 So
 we


now
 have
 an
 architecture
 that
 can
 do


control,
 co-
 simulation,
 quantum
 error


correction
 and
 scale
 into
 that
 future.


The
 industry
 support
 has
 been
 incredible


between
 the
 invention
 of
 CUDA
 Q.


Remember
 CUDA
 was
 designed
 for
 GPU


CPU
 accelerated
 computing.
 Basically


using
 both
 processors
 to
 do
 use
 the


right
 tool
 to
 do
 the
 right
 job.
 Now


CUDAQ
 has
 been
 extended
 beyond
 CUDA
 so


that
 we
 could
 support
 QPU
 and
 have
 the


two
 processors
 QPU
 and
 the
 GPU
 work
 and


have


computation
 move
 back
 and
 forth
 within


just
 a
 few
 microsconds.
 The
 essential


latency
 to
 be
 able
 to
 cooperate
 with
 the


quantum
 computer.
 So
 now
 CUDAQ
 is
 such


an
 incredible
 breakthrough
 adopted
 by
 so


many
 different
 developers.
 We
 are


announcing
 today
 17
 different
 quantum


computer
 industry
 companies
 supporting


the
 MVQ
 link
 and
 and
 I'm
 so
 excited


about
 this
 eight
 different
 DOE
 labs


Berkeley
 Brook
 Haven
 Fermy
 Labs
 in


Chicago
 Lincoln
 Laboratory
 Los
 Alamos


Oakidge
 Pacific
 Northwest
 San
 Die
 lab


just
 about
 every
 single
 DOE
 lab
 has


engaged
 us
 working
 with
 our
 ecosystem
 of


quantum
 computer
 companies
 and
 these


quantum
 controllers
 so
 that
 we
 could


integrate
 quantum
 computing
 in
 into
 the


future
 of
 science.


Well,
 I
 have
 one
 more
 additional


announcement
 to
 make.
 Today,
 we're


announcing
 that
 the
 Department
 of
 Energy


is
 partnering
 with
 Nvidia
 to
 build
 seven


new
 AI
 supercomputers
 to
 advance
 our


nation
 science.


>> Wow,


>> that's
 a
 big
 one.


>> That's
 going
 to
 move
 the
 stock.


I
 have
 to
 have
 a
 shout
 out
 for
 Secretary


Chris
 Wright.
 He
 has
 brought
 so
 much


energy
 to
 the
 DOE.
 A
 surge
 of
 energy,
 a


surge
 of
 passion
 to
 make
 sure
 that


America
 leads
 science.
 Again,
 as
 I


mentioned,
 computing
 is
 the
 fundamental


instrument
 of
 science.
 And
 we
 are
 going


through
 several
 platform
 shifts.
 On
 the


one
 hand,
 we're
 going
 to
 accelerated


computing.
 That's
 why
 every
 future


supercomputer
 will
 be
 GPUbased


supercomputer.


We're
 going
 to
 AI.
 So
 that
 AI
 and


principled
 solvers,
 principled


simulation,


principled
 physics
 simulation
 is
 not


going
 to
 go
 away,
 but
 it
 could
 be


augmented,
 enhanced,
 scaled,
 use


surrogate
 models,
 AI
 models
 working


together.
 We
 also
 know
 that
 principal


solvers,
 classical
 computing
 could
 be


enhanced
 to
 understand
 the
 state
 of


nature
 using
 quantum
 computing.
 We
 also


know
 that
 in
 the
 future
 we
 have
 so
 much


signal,
 so
 much
 data
 we
 have
 to
 sample


from
 the
 world.
 Remote
 sensing
 is
 more


important
 than
 ever.
 And
 these


laboratories
 are
 impossible
 to


experiment
 at
 the
 scale
 and
 speed
 we


need
 to
 unless
 they're
 robotic


factories,
 robotic
 laboratories.


So
 all
 of
 these
 different
 technologies


are
 coming
 into
 science
 at
 exactly
 the


same
 time.
 Secretary
 Wright
 understands


this
 and
 he
 wants
 the
 DOE
 to
 take
 this


opportunity
 to
 supercharge
 themselves


and
 make
 sure
 the
 United
 States
 stay
 at


the
 forefront
 of
 science.
 I
 want
 to


thank
 all
 of
 you
 for
 that.
 Thank
 you.


Let's
 talk
 about
 AI.


What
 is
 AI?
 Most
 people
 would
 say
 that


AI
 is
 a
 chatbot
 and
 it
 it's
 rightfully


so.
 There's
 no
 question
 that
 Chad
 GPT
 is


at
 the
 forefront
 of
 what
 people
 would


consider
 AI.
 However,
 just
 as
 you
 see


right
 now,
 these
 scientific


supercomputers
 are
 not
 going
 to
 run


chatbots.
 They're
 going
 to
 do
 basic


science.
 Science,
 AI,
 the
 world
 of
 AI
 is


much
 much
 more
 than
 a
 chatbot.
 Of


course,
 the
 chatbot
 is
 extremely


important
 and
 AGI
 is
 fundamentally


critical.
 deep
 computer
 science,


incredible
 computing,
 great


breakthroughs
 are
 still
 essential
 for


AGI.
 But
 beyond
 that,
 AI
 is
 a
 lot
 more.


AI
 is
 in
 fact,
 I'm
 going
 to
 describe
 AI


in
 a
 couple
 different
 ways.
 This
 first


way,
 the
 first
 way
 you
 think
 about
 AI
 is


that
 it
 has
 completely
 reinvented
 the


computing
 stack.


The
 way
 we
 used
 to
 do
 software
 was
 hand


coding.
 hand
 coding
 software
 running
 on


CPUs.


Today
 AI
 is
 machine
 learning
 training


data
 inensive
 programming
 if
 you
 will


trained
 and
 learned
 by
 AI
 that
 runs
 on
 a


GPU.
 In
 order
 to
 make
 that
 happen,
 the


entire
 computing
 stack
 has
 changed.


Notice
 you
 don't
 see
 Windows
 up
 here.


You
 don't
 see
 CPU
 up
 here.
 You
 see
 a


whole
 different
 a
 whole
 fundamentally


different
 stack.
 Everything
 from
 the


need
 for
 energy.
 And
 this
 is
 another


area
 where
 our
 administration,
 President


Trump
 gets
 deserves
 enormous
 credit.
 His


pro-
 energy
 initiative,
 his
 recognition


that
 this
 industry
 needs
 energy
 to
 grow.


It
 needs
 energy
 to
 advance
 and
 we
 need


energy
 to
 win.
 His
 recognition
 of
 that


and
 putting
 the
 weight
 of
 the
 nation


behind
 pro-
 energy
 growth
 completely


changed
 the
 game.
 If
 this
 didn't
 happen,


we
 could
 have
 been
 in
 a
 bad
 situation


and
 I
 want
 to
 thank
 President
 Trump
 for


that.


On
 top
 of
 energy
 are
 these
 GPUs
 and


these
 GPUs
 are
 connected
 into
 built
 into


infrastructure
 that
 I'll
 show
 you
 later.


On
 top
 of
 this
 infrastructure
 which
 in


consists
 of
 giant
 data
 centers
 like


easily
 many
 times
 the
 size
 size
 of
 this


room
 enormous
 amount
 of
 energy
 which


then
 transfer
 transforms
 the
 energy


through
 this
 new
 machine
 called
 GPU


supercomputers
 to
 generate
 numbers.


These
 numbers
 are
 called
 tokens.


the
 language,
 if
 you
 will,
 the


computational
 unit,
 the
 vocabulary
 of


artificial
 intelligence.
 You
 can


tokenize
 almost
 anything.
 You
 can


tokenize,
 of
 course,
 the
 English
 word.


You
 can
 tokenize
 images.
 That's
 the


reason
 why
 you're
 able
 to
 recognize


images
 or
 generate
 images,
 tokenize


video,
 tokenize
 3D
 structures.
 You
 could


tokenize
 chemicals
 and
 proteins
 and


genes.
 You
 could
 tokenize
 cells,


tokenize
 almost
 anything
 with
 structure,


anything
 with
 information
 content.


Once
 you
 could
 tokenize
 it,
 AI
 can
 learn


that
 language
 and
 the
 meaning
 of
 it.


Once
 it
 learns
 the
 meaning
 of
 that


language,
 it
 can
 translate.
 It
 can


respond
 just
 like
 you
 respond
 just
 like


you
 interact
 with
 chat
 GPT.
 And
 it
 could


generate
 just
 as
 chat
 GPT
 can
 generate.


So
 all
 of
 the
 fundamental
 things
 that


you
 see
 Chad
 GPD
 do,
 all
 you
 have
 to
 do


is
 imagine
 what
 if
 it
 was
 a
 protein,


what
 if
 it
 was
 a
 chemical,
 what
 if
 it


was
 a
 3D
 structure
 like
 a
 factory,
 what


if
 it
 was
 a
 robot
 and
 the
 token
 was


understanding
 behavior


and
 tokenizing
 motion
 and
 action.
 All
 of


those
 concepts
 are
 basically
 the
 same,


which
 is
 the
 reason
 why
 AI
 is
 making


such
 extraordinary
 progress.
 And
 on
 top


of
 these
 models
 are
 applications


transformers.


Transformers
 is
 not
 a
 universal
 model.


It's
 incredibly
 effective
 model.
 But


there's
 no
 one
 universal
 model.
 It's


just
 that
 AI
 has
 universal
 impact.
 There


are
 so
 many
 different
 types
 of
 models.


There's
 in
 the
 last
 several
 years
 we


enjoyed
 the
 invention
 and
 went
 through


the
 innovation
 breakthroughs
 of


multimodality.


There's
 so
 many
 different
 types
 of


models.
 There's
 CNN
 models,
 competition


neuronet
 network
 models,
 their
 state


space
 models,
 their
 graph
 neuronet


network
 models,
 multimodal
 models,
 of


course,
 all
 the
 different
 tokenizations


and
 token
 methods
 that
 I
 just
 described.


You
 could
 have
 models
 that
 are
 spatial


in
 its
 understanding,
 optimized
 for


spatial
 awareness.
 You
 could
 have
 models


that
 are
 optimized
 for
 long
 sequence,


recognizing
 subtle
 information
 over
 a


long
 period
 of
 time.
 There's
 so
 many


different
 types
 of
 models.


On
 top
 of
 these
 models
 architectures,
 on


top
 of
 these
 model
 architectures
 are


applications,


the
 software
 of
 the
 past.
 And
 this
 is
 a


a
 profound
 understanding,
 a
 profound


observation
 of
 artificial
 intelligence


that
 the
 software
 industry
 of
 the
 past


was
 about
 creating
 tools.
 Excel
 is
 a


tool.


Word
 is
 a
 tool.
 A
 web
 browser
 is
 a
 tool.


The
 reason
 why
 I
 know
 these
 are
 tools
 is


because
 you
 use
 them.
 The
 tools


industry,
 just
 as
 screwdrivers
 and


hammers,
 the
 tools
 industry
 is
 only
 so


large.
 In
 the
 case
 of
 IT
 tools,
 they


could
 be
 database
 tools.
 These
 IT
 tools


is
 about
 a
 trillion
 dollars
 or
 so.
 But


AI
 is
 not
 a
 tool.


AI
 is
 work.


That
 is
 the
 profound
 difference.
 AI
 is


in
 fact
 workers
 that
 can
 actually
 use


tools.
 One
 of
 the
 things
 I'm
 really


excited
 about
 is
 the
 work
 that
 Irvin's


doing
 at
 Perplexity.
 Perplexity
 using


web
 browsers
 to
 book
 vacations
 or
 do


shopping.
 Basically
 an
 AI
 using
 tools.


Cursor
 is
 an
 AI,
 a
 nigantic
 AI
 system


that
 we
 use
 at
 NVIDIA.
 Every
 single


software
 engineer
 at
 Nvidia
 uses
 cursor.


That's
 improved
 our
 productivity


tremendously.
 It's
 basically
 a
 partner


for
 every
 one
 of
 our
 software
 engineers


to
 generate
 code
 and
 it
 uses
 a
 tool
 and


the
 tool
 it
 uses
 is
 called
 VS
 code.
 So


cursor
 is
 an
 AI
 agentic
 AI
 system
 that


uses
 VS
 code.
 Well,
 all
 of
 these


different
 industries
 whether
 it's


chatbots
 or
 digital
 biology
 where
 we


have
 AI
 assistant
 researchers
 or
 what
 is


a
 robo
 taxi


inside
 a
 robo
 taxi?
 Of
 course,
 it's


invisible,
 but
 obviously
 there's
 a


there's
 a
 AI
 chauffeur.
 That
 chauffeur


is
 doing
 work.
 And
 the
 tool
 that
 it
 uses


to
 do
 that
 work
 is
 the
 car.
 And
 so


everything
 that
 we've
 made
 up
 until
 now,


the
 whole
 world,
 everything
 that
 we've


made
 up
 until
 now
 are
 tools.
 Tools
 for


us
 to
 use.
 For
 the
 very
 first
 time,


technology
 is
 now
 able
 to
 do
 work
 and


help
 us
 be
 more
 productive.
 The
 list
 of


opportunities
 go
 on
 and
 on,
 which
 is
 the


reason
 why
 AI
 addresses


the
 segment
 of
 the
 economy
 that
 it
 has


never
 addressed.
 It
 is
 a
 few
 trillion


dollars
 that
 sits
 underneath
 the
 tools


of
 a
 hundred
 trillion
 dollar
 global


economy.
 Now,
 for
 the
 first
 time,
 AI
 is


going
 to
 engage
 that
 hundred
 trillion


dollar
 economy
 and
 make
 it
 more


productive,
 make
 it
 grow
 faster,
 make
 it


larger.
 We
 have
 a
 severe
 shortage
 of


labor.
 Having
 AI
 that
 augments
 labor
 is


going
 to
 help
 us
 grow.
 Now
 what's


interesting
 about
 this
 from
 a
 technology


industry
 perspective
 also
 is
 that
 in


addition
 to
 the
 fact
 that
 AI
 is
 new


technology
 that
 addresses
 new
 segments


of
 the
 economy
 AI
 in
 itself
 is
 also
 a


new
 industry


this
 token
 as
 I
 was
 explaining
 earlier


these
 numbers


after
 you
 tokenize
 all
 these
 different


modalities
 of
 information
 there's
 a


factory
 that
 needs
 to
 produce
 these


numbers
 unlike
 the
 computer
 industry


indry
 and
 the
 chip
 industry
 of
 the
 past.


Notice
 if
 you
 look
 at
 the
 chip
 industry


of
 the
 past,
 the
 chip
 industry


represents
 about
 5
 to
 10%


maybe
 less
 5%
 or
 so
 of
 a
 multi-
 trillion


dollar
 few
 trillion
 dollar
 IT
 industry.


And
 the
 reason
 for
 that
 is
 because
 it


doesn't
 take
 that
 much
 computation
 to


use
 Excel.
 It
 doesn't
 take
 that
 much


computation
 to
 use
 browsers.
 It
 doesn't


take
 that
 much
 computation
 to
 use
 word.


We
 do
 the
 computation.
 But
 in
 this
 new


world,


there
 needs
 to
 be
 a
 computer
 that


understands
 context
 all
 the
 time.
 It


can't
 precomputee
 that
 because
 every


time
 you
 use
 the
 computer
 for
 AI,
 every


time
 you
 ask
 the
 AI
 to
 do
 something,
 the


context
 is
 different.
 So,
 it
 has
 to


process
 all
 of
 that
 information.


Environmental,
 for
 example,
 in
 the
 case


of
 a
 self-driving
 car,
 it
 has
 to
 process


the
 context
 of
 the
 car.
 context


processing.
 What
 is
 the
 instruction


you're
 asking
 the
 AI
 to
 do?
 Then
 it's


got
 to
 go
 and
 break
 down
 the
 problem


step
 by
 step,
 reason
 about
 it,
 and
 come


up
 with
 a
 plan
 and
 execute
 it.
 Every


single
 one
 of
 that
 step
 requires


enormous
 number
 of
 tokens
 to
 be


generated
 which
 is
 the
 reason
 why
 we


need
 a
 new
 type
 of
 system
 and
 I
 call
 it


an
 AI
 factory.
 It's
 an
 AI
 factory
 for


short.
 It's
 unlike
 a
 data
 center
 of
 the


past.
 is
 an
 AI
 factory
 because


this
 factory
 produces
 one
 thing
 unlike


the
 data
 centers
 of
 the
 past
 that
 does


everything.
 Stores
 files
 for
 all
 of
 us,


runs
 all
 kinds
 of
 different


applications.
 You
 could
 use
 that
 data


center
 like
 you
 can
 use
 your
 computer


for
 all
 kinds
 of
 applications.
 You
 could


use
 it
 to
 play
 game
 one
 day.
 You
 could


use
 it
 to
 browse
 the
 web.
 You
 could
 use


it,
 you
 know,
 to
 do
 your
 accounting.
 And


so
 that
 is
 a
 computer
 of
 the
 past,
 a


universal
 generalpurpose
 computer.


The
 computer
 I'm
 talking
 about
 here
 is
 a


factory.
 It
 runs
 basically
 one
 thing.
 It


runs
 AI
 and
 its
 purpose,
 its
 purpose
 is


designed
 to
 produce
 tokens
 that
 are
 as


valuable
 as
 possible,
 meaning
 they
 have


to
 be
 smart.
 And
 you
 want
 to
 produce


these
 tokens
 at
 incredible
 rates
 because


when
 you
 ask
 an
 AI
 for
 something,
 you


would
 like
 it
 to
 respond.
 And
 notice


during
 peak
 hours,
 these
 AIs
 are
 now


responding
 slower
 and
 slower
 because


it's
 got
 a
 lot
 of
 work
 to
 do
 for
 a
 lot


of
 people.
 And
 so
 you
 wanted
 to
 produce


valuable
 tokens
 at
 incredible
 rates
 and


you
 wanted
 to
 produce
 it
 cost


effectively.
 Every
 single
 word
 that
 I


used
 are
 consistent
 with
 an
 AI
 factory


with
 a
 car
 factory
 or
 any
 factory.
 It
 is


absolutely
 a
 factory.
 And
 these


factories
 these
 factories
 never
 existed


before.
 And
 inside
 these
 factories
 are


mountains
 and
 mountains
 of
 chips.


Which
 brings


to
 today


what
 happened
 in
 the
 last
 couple
 years.


And
 in
 fact,
 what
 happened
 this
 last


year?
 Something
 fairly
 profound
 happened


this
 year.
 Actually,
 if
 you
 look
 in
 the


beginning
 of
 the
 year,
 everybody
 has


some
 attitude
 about
 AI.
 That
 attitude
 is


generally
 this
 is
 going
 to
 be
 big.
 It's


going
 to
 be
 the
 future.
 And
 somehow
 a


few
 months
 ago,
 it
 kicked
 into


turbocharge.
 And
 the
 reason
 for
 that
 is


several
 things.


The
 first
 is
 that
 we
 in
 the
 last
 couple


years
 have
 figured
 out
 how
 to
 make
 AI


much
 much
 smarter


rather
 than
 just
 pre-training.


Pre-training
 basically
 says
 let's
 take


all
 of
 the
 all
 of
 the
 information
 that


humans
 have
 ever
 created.
 Let's
 give
 it


to
 the
 AI
 to
 learn
 from.
 It's


essentially
 memorization
 and


generalization.


It's
 no
 it's
 not
 unlike
 going
 to
 school


back
 when
 we
 were
 kids.
 the
 first
 stage


of
 learning.
 Pre-training
 was
 never


meant
 just
 as
 preschool


was
 never
 meant
 to
 be
 the
 end
 of


education.


Pre-training,
 preschool
 was
 simply


teaching
 you
 the
 basic
 skills
 of


intelligence
 so
 that
 you
 can
 understand


how
 to
 learn
 everything
 else.
 Without


vocabulary,
 without
 understanding
 of


language
 and
 how
 to
 communicate,
 how
 to


think,
 it's
 impossible
 to
 learn


everything
 else.
 The
 next
 is


post-training.
 Post-training
 after


pre-training
 is
 teaching
 you
 skills.


Skills
 to
 solve
 problem.
 Break
 down


problems.
 Reason
 about
 it.
 How
 to
 solve


math
 problems.
 How
 to
 code.
 How
 to
 think


about
 these
 problems
 step
 by
 step.
 Use


first
 principal
 reasoning.
 And
 then


after
 that


is
 where
 computation
 really
 kicks
 in.
 As


you
 know
 for
 many
 of
 us,
 you
 know,
 we


went
 to
 school
 and
 that's
 in
 my
 case


decades
 ago.
 But
 ever
 since
 I've
 learned


more,
 thought
 about
 more.
 And
 the
 reason


for
 that
 is
 because
 we're
 constantly


grounding
 oursel
 in
 new
 knowledge.
 We're


constantly
 doing
 research,
 and
 we're


constantly
 thinking.
 Thinking
 is
 really


what
 intelligence
 is
 all
 about.
 And
 so


now
 we
 have
 three
 fundamental
 technology


skills.
 We
 have
 these
 three
 technology.


Pre-training,
 which
 still
 requires


enormous
 enormous
 amount
 of
 computation.


We
 now
 have
 post
 training
 which
 uses


even
 more
 computation
 and
 now
 thinking


puts
 incredible
 amounts
 of
 computation


load
 on
 the
 infrastructure
 because
 it's


thinking
 on
 our
 behalf
 for
 every
 single


human.
 So
 the
 amount
 of
 computation


necessary
 for
 AI
 to
 think
 inference
 is


really
 quite
 extraordinary.
 Now
 I
 used


to
 hear
 people
 say
 that
 inference
 is


easy.
 Nvidia
 should
 do
 training.
 Nvidia


is
 going
 to
 do
 you
 know
 they
 are
 really


good
 at
 this
 so
 they're
 going
 to
 do


training
 that
 inference
 was
 easy
 how


could
 thinking
 be
 easy
 regurgitating


memorized
 content
 is
 easy
 regurgitating


the
 multiplication
 table
 is
 easy


thinking
 is
 hard
 which
 is
 the
 reason
 why


these
 three
 scales
 these
 three
 new


scaling
 laws
 which
 is
 all
 of
 it
 in
 in


full
 steam
 has
 put
 so
 much
 pressure
 on


the
 amount
 of
 computation
 Now
 another


thing
 has
 happened


from
 these
 three
 scaling
 laws.
 We
 get


smarter
 models
 and
 these
 smarter
 models


need
 more
 compute.
 But
 when
 you
 get


smarter
 models,
 you
 get
 more


intelligence.


People
 use
 it


as
 if
 anything
 happens.
 I
 want
 to
 be
 the


first
 one
 out.


Jazz.
 I'm
 sure
 it's
 fine.
 Probably
 just


lunch.
 My
 stomach.


Was
 that
 me?


And
 so,
 so
 where
 was
 I?
 The
 smarter
 your


models
 are,
 the
 smarter
 your
 models
 are,


the
 more
 people
 use
 it.
 It's
 now
 more


grounded.
 It's
 able
 to
 reason.
 It's
 able


to
 solve
 problems
 it
 never
 learn
 how
 to


solve
 before
 because
 it
 could
 do


research.
 Go
 learn
 about
 it.
 come
 back,


break
 it
 down,
 reason
 about
 how
 to
 solve


your
 how
 to
 answer
 your
 question,
 how
 to


solve
 your
 problem
 and
 go
 off
 and
 solve


it.
 The
 amount
 of
 thinking
 is
 making
 the


models
 more
 intelligent.
 The
 more


intelligent
 it
 is,
 the
 more
 people
 use


it.
 The
 more
 intelligent
 it
 is,
 the
 more


computation
 is
 necessary.
 But
 here's


what
 happened.


This
 last
 year,


the
 AI
 industry
 turned
 a
 corner.
 Meaning


that
 the
 AI
 models
 are
 now
 smart
 enough.


They're
 making
 they're
 worthy.
 They're


worthy
 to
 pay
 for.
 Nvidia
 pays
 for
 every


license
 of
 Cursor.
 And
 we
 gladly
 do
 it.


We
 gladly
 do
 it
 because
 cursor
 is


helping
 a
 several
 hundred,000
 employee


software
 engineer
 or
 AI
 researcher
 be


many,
 many
 times
 more
 productive.
 So,
 of


course,
 we'd
 be
 more
 than
 happy
 to
 do


that.
 These
 AI
 models
 have
 become
 good


enough
 that
 they
 are
 worthy
 to
 be
 paid


for.
 Cursor,
 11
 Labs,
 Syntheasia,
 A


Bridge,
 Open
 Evidence,
 the
 list
 goes
 on.


Of
 course,
 open
 AI,
 of
 course,
 Claude.


These
 models
 are
 now
 so
 good
 that
 people


are
 paying
 for
 it.
 And
 because
 people


are
 paying
 for
 it
 and
 using
 more
 of
 it,


and
 every
 time
 they
 use
 more
 of
 it,
 you


need
 more
 compute.
 We
 now
 have
 two


exponentials.


These
 two
 exponentials,
 one
 is
 the


exponential
 compute
 requirement
 of
 the


three
 scaling
 law.
 And
 the
 second


exponential,
 the
 more
 pe
 the
 smarter
 it


is,
 the
 more
 people
 use
 it,
 the
 more


people
 use
 it,
 the
 more
 computing
 it


needs.
 Two
 exponentials
 now
 putting


pressure
 on
 the
 world's
 computational


resource


at
 exactly
 the
 time
 when
 I
 told
 you


earlier
 that
 Moore's
 law
 has
 largely


ended.
 And
 so
 the
 question
 is
 what
 do
 we


do?
 If
 we
 have
 these
 two
 exponential


demands
 growing
 and
 if
 we
 don't
 if
 we


don't
 find
 a
 way
 to
 drive
 the
 cost
 down


then
 this
 positive
 feedback
 system
 this


circular
 feedback
 system
 essentially


called
 the
 virtual
 cycle
 essential
 for


almost
 any
 industry


essential
 for
 any
 platform
 industry.
 It


was
 essential
 for
 Nvidia.
 We
 have
 now


reached
 the
 virtual
 cycle
 of
 CUDA.


The
 more
 applications,
 the
 more
 the
 more


applications
 people
 create,
 the
 more


valuable
 CUDA
 is.
 The
 more
 valuable
 CUDA


is,
 the
 more
 CUDA
 computers
 are


purchased.
 The
 more
 CUDA
 computers
 are


purchased,
 more
 developers
 want
 to


create
 applications
 for
 it.
 That
 virtual


cycle


for
 Nvidia
 has
 now
 been
 achieved
 after


30
 years.
 We
 have
 achieved
 that
 also.
 15


years
 later,
 we've
 achieved
 that
 for
 AI.


AI
 has
 now
 reached
 the
 virtual
 cycle.


And
 so
 the
 more
 you
 use
 it
 because
 the


AI
 is
 smart
 and
 we
 pay
 for
 it.
 The
 more


profit
 is
 generated,
 the
 more
 profit


generated,
 the
 more
 computes
 put
 to
 on


the
 on
 the
 grid,
 the
 more
 compute
 is
 put


into
 AI
 factories,
 the
 more
 comput
 the


AI
 becomes
 smarter,
 the
 smarter,
 more


more
 people
 use
 it,
 more
 applications


use
 it,
 the
 more
 problems
 we
 can
 solve.


This
 virtual
 cycle
 is
 now
 spinning.
 What


we
 need
 to
 do
 is
 drive
 the
 cost
 down


tremendously
 so
 that
 one
 the
 user


experience
 is
 better
 when
 you
 prompt
 the


AI
 it
 responds
 to
 you
 much
 faster
 and


two
 so
 that
 we
 keep
 this
 virtual
 cycle


going
 by
 driving
 its
 cost
 down
 so
 that


it
 could
 get
 smarter
 so
 that
 more
 people


use
 it
 so
 that
 so
 on
 so
 forth
 that


virtual
 cycle
 is
 now
 spinning
 but
 how
 do


we
 do
 that
 when
 Moore's
 law
 has
 really


reached
 this
 limit
 well
 the
 answer
 is


called
 codeesign
 design.


You
 can't
 just
 design
 chips
 and
 hope


that
 things
 on
 on
 top
 of
 it
 is
 going
 to


go
 faster.
 The
 best
 you
 could
 do
 in


designing
 chips
 is
 add
 I
 don't
 know
 50%


more
 transistors
 every
 couple
 of
 years.


And
 if
 you
 added
 more
 transistors
 just


you
 we
 can
 add
 more
 transistors
 and


TSMC's
 got
 a
 lot
 of
 transistor.


Incredible
 company.
 We
 just
 keep
 adding


more
 transistors.
 However,
 that's
 all
 in


percentages
 not
 exponentials.


We
 need
 to
 compound
 exponentials
 to
 keep


this
 virtual
 cycle
 going.
 We
 call
 it


extreme
 code
 design.
 Nvidia
 is
 the
 only


company
 in
 the
 world
 today
 that


literally
 starts
 from
 a
 blank
 sheet
 of


paper
 and
 can
 think
 about
 new


fundamental
 architecture,
 computer


architecture,
 new
 chips,
 new
 systems,


new
 software,
 new
 model
 architecture,


and
 new
 applications
 all
 at
 the
 same


time.
 So
 many
 of
 the
 people
 in
 this
 room


are
 here
 because
 you're
 different
 parts


of
 that
 layer
 that
 different
 parts
 of


that
 stack
 and
 working
 with
 Nvidia.


We
 fundamentally
 rearchitect
 everything


from
 the
 ground
 up
 and
 then
 because
 AI


is
 such
 a
 large
 problem,
 we
 scale
 it
 up.


We
 created
 a
 whole
 computer,
 a
 computer


for
 the
 first
 time
 that
 has
 scaled
 up


into
 an
 entire
 rack.
 That's
 one


computer,
 one
 GPU.
 And
 then
 we
 scale
 it


out
 by
 inventing
 a
 new
 AI
 Ethernet


technology
 we
 call
 Spectrum
 X
 Ethernet.


Everybody
 will
 say
 Ethernet
 is
 Ethernet.


Ethernet
 is
 hardly
 Ethernet.
 Ethernet.


Spectrum
 X
 Ethernet
 is
 designed
 for
 AI


performance.
 And
 it's
 the
 reason
 why


it's
 so
 successful.
 And
 even
 that's
 not


big
 enough.
 We'll
 fill
 this
 entire
 room


of
 AI
 supercomputers
 and
 GPUs.


That's
 still
 not
 big
 enough
 because
 the


number
 of
 applications
 and
 the
 number
 of


users
 for
 AI
 is
 continuing
 to
 grow


exponentially.
 And
 we
 connect
 multiple


of
 these
 data
 centers
 together
 and
 we


call
 that
 scale
 across
 spectrum
 XGS


gigascale
 X
 spectrum
 X
 gigascale
 XGS.
 By


doing
 so,
 we
 do
 code
 design
 at
 such
 a


such
 an
 enormous
 level,
 such
 an
 extreme


level
 that
 the
 performance
 benefits
 are


shocking.
 Not
 50%
 better
 each


generation,
 not
 25%
 better
 each


generation,
 but
 much
 much
 more.
 This
 is


the
 most
 extreme
 code-designed
 computer


we've
 ever
 made
 and
 quite
 frankly
 made


in
 modern
 times.
 Since
 the
 IBM
 system


360,
 I
 don't
 think
 a
 computer
 has
 been


ground
 up,
 reinvented
 like
 this
 ever.


This
 system
 was
 incredibly
 hard
 to


create.
 I'll
 show
 you
 the
 benefits
 in


just
 a
 second.
 But
 essentially
 what


we've
 done,
 essentially
 what
 we've
 done,


we've
 created
 otherwise


Hey
 Janine,
 you
 can
 come
 out.
 It's


you
 have
 to
 have
 to
 meet
 me
 like


halfway.


All
 right.
 So,
 this
 is
 kind
 of
 like


Captain
 America
 shield.


So,
 MVLink
 72,
 MVLink72,


if
 we
 were
 to
 create
 one
 giant
 chip,
 one


giant
 GPU,
 this
 is
 what
 it
 would
 look


like.
 This
 is
 the
 level
 of
 wafer
 scale


processing
 we
 would
 have
 to
 do.


It's
 incredible.
 All
 of
 this,
 all
 of


these
 chips
 are
 now
 put
 into
 one
 giant


rack.


Did
 I
 do
 that
 or
 did
 somebody
 else
 do


that?
 Into
 that
 one
 giant
 rack.


You
 know,
 sometimes
 I
 don't
 feel
 like


I'm
 up
 here
 by
 myself.


Just


this
 one
 giant
 rack
 makes
 all
 of
 these


chips
 work
 together
 as
 one.
 It's


actually
 completely
 incredible.
 And
 I'll


show
 you
 the
 benefits
 of
 that.
 The
 way


it
 looks
 is
 this.
 So,
 thanks
 Janine.


I
 I
 like
 this.
 All
 right,
 ladies
 and


gentlemen.
 Janine
 Paul.


I
 got
 it.
 In
 the
 future
 next,
 I'm
 just


going
 to
 go
 like
 Thor.


It's
 like
 when
 you're
 at
 home
 and
 and


you
 can't
 reach
 the
 remote
 and
 you
 just


go
 like
 this
 and
 somebody
 brings
 it
 to


you.
 That's
 Yeah.
 Same
 idea.


It
 never
 happens
 to
 me.
 I'm
 just


dreaming
 about
 it.
 I'm
 just
 saying.


Okay.
 So,
 so
 anyhow,
 anyhow,
 um
 we


basically
 this
 is
 what
 we
 created
 in
 the


past.
 This
 is
 MVLink
 MVLink
 8.
 Now,


these
 models
 are
 so
 gigantic.
 The
 way
 we


solve
 it
 is
 we
 turn
 this
 model,
 this


gigantic
 model,
 into
 a
 whole
 bunch
 of


experts.
 It's
 a
 little
 bit
 like
 a
 team.


And
 so,
 these
 experts
 are
 good
 at


certain
 types
 of
 problems.
 And
 we


collect
 a
 whole
 bunch
 of
 experts


together.
 And
 so,
 this
 giant
 multi-


trillion
 dollar
 AI
 model
 has
 all
 these


different
 experts
 and
 we
 put
 all
 these


different
 experts
 on
 a
 GPU.
 Now,
 this
 is


NVLink
 72.


We
 could
 put
 all
 of
 the
 chips
 into
 one


giant
 fabric
 and
 every
 single
 expert
 can


talk
 to
 each
 other.
 So
 the
 master
 the


the
 primary
 expert
 could
 talk
 to
 all
 of


the
 tri
 work
 and
 all
 of
 the
 necessary


context
 and
 prompts
 and
 bunch
 of
 data


that
 we
 have
 to
 bunch
 of
 tokens
 that
 we


have
 to
 send
 to
 all
 of
 the
 experts.
 The


experts
 would
 whichever
 one
 of
 the


experts
 are
 selected
 to
 solve
 the
 answer


would
 then
 go
 off
 and
 try
 to
 respond
 and


then
 it
 would
 go
 off
 and
 do
 that
 layer


after
 layer
 after
 layer.
 Sometimes


eight,
 sometimes
 16
 and
 sometimes
 these


experts,
 sometimes
 64,
 sometimes
 256.


But
 the
 point
 is
 there
 are
 more
 and
 more


and
 more
 experts.
 Well,
 here
 MVLink
 72,


we
 have
 72
 GPUs.
 And
 because
 of
 that,
 we


could
 put
 four
 experts
 in
 one
 GPU.
 The


most
 important
 thing
 you
 need
 to
 do
 for


each
 GPU
 is
 to
 generate
 tokens,
 which
 is


the
 amount
 of
 bandwidth
 that
 you
 have
 in


HBM
 memory.
 We
 have
 one
 H
 one
 GPU


generating
 thinking
 for
 four
 experts


versus
 here
 because
 each
 one
 of
 the


computers
 can
 only
 put
 eight
 GPUs.
 We


have
 to
 put
 32
 experts
 into
 one
 GPU.
 So


this
 one
 GPU
 has
 to
 think
 for
 32
 experts


versus
 this
 system
 each
 GPU
 only
 has
 to


think
 for
 four.
 And
 because
 of
 that
 the


speed
 difference
 is
 incredible.
 And
 this


just
 came
 out.
 This
 is
 the
 benchmark


done
 by
 semi
 analysis.
 They
 do
 a
 really


really
 thorough
 job
 and
 they
 benchmarked


all
 of
 the
 GPUs
 that
 are
 benchmarkable


and
 it
 turns
 out
 it's
 not
 that
 many.
 If


you
 look
 at
 the
 list
 of
 looks
 list
 of


GPUs
 you
 could
 actually
 benchmark
 is


like
 90%
 Nvidia.
 Okay.
 And
 but
 so
 we're


comparing
 ourselves
 to
 ourselves
 but
 the


second
 best
 GPU
 in
 the
 world
 is
 the
 H200


and
 runs
 all
 the
 workload.


Grace
 Blackwell
 per
 GPU
 is
 10
 times
 the


performance.


Now,
 how
 do
 you
 get
 10
 times
 the


performance
 when
 it's
 only
 twice
 the


number
 of
 transistors?


Well,
 the
 answer
 is
 extreme
 code
 design.


And
 by
 understanding
 the
 nature
 of
 the


future
 of
 AI
 models
 and
 we're
 thinking


across
 that
 entire
 stack,
 we
 can
 create


architectures
 for
 the
 future.
 This
 is
 a


big
 deal.
 It
 says
 we
 can
 now
 respond
 a


lot
 faster.
 But
 this
 is
 even
 bigger


deal.
 This
 next
 one,
 look
 at
 this.
 This


says


that
 the
 lowest
 cost
 tokens
 in
 the
 world


are
 generated
 by
 Grace
 Blackwell
 and


Vlink
 72.
 The
 most
 expensive
 computer.


On
 the
 one
 hand,
 GB200
 is
 the
 most


expensive
 computer.
 On
 the
 other
 hand,


its
 token
 generation
 capability
 is
 so


great
 that
 it
 produces
 it
 at
 the
 lowest


cost
 because
 the
 tokens
 per
 second


divided
 by
 the
 t
 by
 the
 total
 cost
 of


ownership
 of
 Grace
 Blackwell
 is
 so
 good.


It
 is
 the
 lowest
 cost
 way
 to
 generate


tokens.
 By
 doing
 so,
 delivering


incredible
 performance,
 10
 times
 the


performance,
 incre
 delivering
 10
 times


lower
 cost,
 that
 virtual
 cycle
 can


continue.
 Which
 then
 brings
 me
 to
 this


one.
 I
 just
 saw
 this
 literally


yesterday.
 This
 is
 uh
 the
 CSP
 capex.


People
 are
 asking
 me
 about
 capex
 these


days
 and
 this
 is
 a
 good
 way
 to
 look
 at


it.
 In
 fact,
 the
 capex
 of
 the
 top
 six


CSPs
 and
 this
 one,
 this
 one
 is
 Amazon,


Core
 Weave,
 Google,
 Meta,
 Microsoft,
 and


Oracle.
 Okay,
 these
 CSPs
 together
 are


going
 to
 invest
 this
 much
 in
 capex.
 And


I
 would
 I
 would
 tell
 you
 the
 timing


couldn't
 be
 better.
 And
 the
 reason
 for


that
 is
 now
 we
 have
 the
 Grace
 Blackwell


MVLink72
 in
 all
 volume
 production,


supply
 chain,
 everywhere
 in
 the
 world
 is


manufacturing
 it.
 So
 we
 can
 now
 deliver


to
 all
 of
 them
 this
 new
 architecture
 so


that
 the
 capex
 invests
 in
 instruments


computers
 that
 deliver
 the
 best
 TCO.
 Now


underneath
 this
 there
 are
 two
 things


that
 are
 going
 on.
 So
 when
 you
 look
 at


this
 it's
 actually
 fairly
 extraordinary


and
 it's
 fairly
 extraordinary
 anyhow
 but


what's
 happening
 under
 underneath
 is


this
 there
 are
 two
 platform
 shifts


happening
 at
 the
 same
 time.


One
 platform
 shift
 is
 going
 from
 general


purpose
 computing
 to
 accelerated


computing.
 Remember
 accelerated


computing
 as
 I
 mentioned
 to
 you
 before


it
 does
 data
 processing.
 It
 does
 image


processing,
 computer
 graphics,
 it
 does


com
 comput computation
 of
 all
 kinds.
 It


runs
 SQL,
 runs
 Spark,
 it
 runs,
 you
 know,


you
 you
 ask
 it,
 you
 tell
 us
 what
 you


need
 to
 have
 run,
 and
 I'm
 fairly
 certain


we
 have
 an
 amazing
 library
 for
 you.
 You


could
 be,
 you
 know,
 a
 data
 center
 trying


to
 make
 masks
 to
 manufacture


semiconductors.
 we
 have
 a
 great
 library


for
 you.
 And
 so
 underneath
 irrespective


of
 AI,
 the
 world
 is
 moving
 from
 general


purpose
 computing
 to
 accelerated


computing
 irrespective
 of
 AI.
 And
 in


fact,
 many
 of
 the
 CSPs
 already
 have


services
 that
 have
 been
 here
 long
 ago


before
 AI.
 Remember,
 they
 were
 invented


in
 the
 era
 of
 machine
 learning.


classical
 machine
 learning
 algorithms


like
 XG
 boost,
 algorithms
 like
 um
 uh


data
 frames
 that
 are
 used
 for
 recommener


systems,
 collaborative
 filtering,


content
 filtering,
 all
 of
 those


technologies
 were
 created
 in
 the
 old


days
 of
 general
 purpose
 computing.
 Even


those
 algorithms,
 even
 those


architectures
 are
 now
 better
 with


accelerated
 computing.
 And
 so
 even


without
 AI,
 the
 world's
 CSPs
 are
 going


to
 invest
 into
 acceleration.
 Nvidia's


GPU
 is
 the
 only
 GPU
 that
 can
 do
 all
 of


that
 plus
 AI.
 And
 ASIC
 might
 be
 able
 to


do
 AI,
 but
 it
 can't
 do
 any
 of
 the


others.


Nvidia
 could
 do
 all
 of
 that,
 which


explains
 why
 it
 is
 so
 safe
 to
 just
 lean


into
 Nvidia's
 architecture.
 We
 have
 now


reached
 our
 virtual
 cycle,
 our


inflection
 point.
 And
 this
 is
 quite


extraordinary.
 I
 have
 many
 partners
 in


the
 room
 and
 all
 of
 you
 are
 part
 of
 our


supply
 chain
 and
 I
 know
 how
 hard
 you


guys
 are
 working.
 I
 want
 to
 thank
 all
 of


you
 how
 hard
 you
 are
 working
 and
 thank


you
 very
 much.


Now
 I'm
 going
 to
 show
 you
 why


this
 is
 what's
 going
 on
 in
 our
 company's


business.
 We're
 seeing
 extraordinary


growth
 for
 Grace
 Blackwell
 for
 all
 the


reasons
 that
 I
 just
 mentioned.
 It's


driven
 by
 two
 exponentials.
 We
 now
 have


visibility.


I
 think
 we're
 probably
 the
 first


technology
 company
 in
 history
 to
 have


visibility
 into
 half
 a
 trillion
 dollars


of
 cumulative
 blackwell
 and
 early
 ramps


of
 Reubin


through
 2026.
 And
 as
 you
 know,
 2025
 is


not
 over
 yet
 and
 2026
 hasn't
 started.


This
 is
 how
 much
 business
 is
 on
 the


books.
 Half
 a
 trillion
 dollars
 worth
 so


far.
 Now,
 this
 is
 out
 of
 that.
 We've


already
 shipped
 6
 million
 of
 the


Blackwells
 in
 the
 first
 several


quarters.
 I
 guess
 the
 first
 four


quarters
 of
 production,
 three
 and
 a
 half


quarters
 of
 production.
 We
 still
 have


one
 more
 quarter
 to
 go
 for
 2025.
 And


then
 we
 have
 four
 quarters.
 So
 the
 next


five
 quarters
 there's
 $500
 million
 $500


billion
 half
 a
 trillion
 dollars.
 That's


five
 times
 the
 growth
 rate
 of
 Hopper.


That
 kind
 of
 tells
 you
 something.
 This


is
 Hopper's
 entire
 life.
 This
 doesn't


include
 China
 and
 and
 um
 and
 Asia.
 So


this
 is
 just
 uh
 the
 West.
 Okay.
 This
 is


just
 uh
 we're
 excluding
 China.
 So
 Hopper


in
 its
 entire
 life
 4
 million
 GPUs.


Blackwell.
 Each
 one
 of
 the
 Blackwells


has
 two
 GPUs
 in
 it
 in
 one
 large
 package.


20
 million
 GPUs
 of
 Blackwells
 in
 the


early
 parts
 of
 Reuben.
 Incredible


growth.
 So,
 I
 want
 to
 thank
 all
 of
 our


supply
 chain
 partners.
 Everybody,
 I
 know


how
 hard
 you
 guys
 are
 working.
 I
 made
 a


video
 to
 celebrate
 your
 work.
 Let's
 play


it.


[Music]


The
 age
 of
 AI
 has
 begun.


Blackwell
 is
 its
 engine,
 an
 engineering


marvel.


In
 Arizona,
 it
 starts
 as
 a
 blank
 silicon


wafer.


Hundreds
 of
 chip
 processing
 and


ultraviolet
 lithography
 steps
 build
 up


each
 of
 the
 200
 billion
 transistors


layer
 by
 layer
 on
 a
 12in
 wafer.
 In


Indiana,
 HBM
 stacks
 will
 be
 assembled
 in


parallel.
 HBM
 memory
 dieseS
 with
 1,024


IO's
 are
 fabricated
 using
 advanced
 EUV


technology
 through
 silicon
 via
 is
 used


in
 the
 back
 end
 to
 connect
 12
 stacks
 of


HBM
 memory
 and
 base
 dye
 to
 produce
 HBM.


Meanwhile,
 the
 wafer
 is
 scribed
 into


individual
 Blackwell
 dye,
 tested
 and


sorted,
 separating
 the
 good
 dyes
 to
 move


forward.
 The
 chip
 on
 wafer
 on
 substrate


process
 attaches
 32
 Blackwell
 dies
 and


128
 HPM
 stacks
 on
 a
 custom
 silicon


interposer
 wafer.


>> Nvidia
 196


alltime
 highs,
 baby
 on
 Nvidia.


directly
 into
 it,
 connecting
 Blackwell


GPUs
 and
 HBM
 stacks
 into
 each
 system
 and


package
 unit,
 locking
 everything
 into


place.
 Then
 the
 assembly
 is
 baked,


molded,
 and
 cured,
 creating
 the
 GB300


Blackwell
 Ultra
 Super
 Chip.
 In
 Texas,


robots
 will
 work
 around
 the
 clock
 to


pick
 and
 place
 over
 10,000
 components


onto
 the
 Grace
 Blackwell
 PCB.


In
 California,
 Connect
 X8
 Supernix
 for


scaleout
 communications
 and
 Bluefield
 3


DPUs
 for
 offloading
 and
 accelerating


networking,
 storage,
 and
 security
 are


carefully
 assembled
 into
 GB300
 compute


trays.


NVLink
 is
 the
 breakthrough
 high-speed


link
 that
 Nvidia
 invented
 to
 connect


multiple
 GPUs
 and
 scale
 up
 into
 a


massive
 virtual
 GPU.


The
 MVLink
 switch
 tray
 is
 constructed


with
 MVLink
 switch
 chips
 providing
 14.4


terabytes
 per
 second
 of
 all
 to
 all


bandwidth.
 MVLink
 spines
 form
 a
 custom


blindmated
 back
 plane
 with
 5,000
 copper


cables
 connecting
 all
 72
 black
 wells
 or


144
 GPU
 dice
 into
 one
 giant
 GPU


delivering
 130
 terabytes
 per
 second
 of


all
 to-all
 bandwidth
 nearly
 the
 global


internet's
 peak
 traffic.


Skilled
 technicians
 assemble
 each
 of


these
 parts
 into
 a
 rack
 scale
 AI


supercomput.


In
 total,
 1.2
 million
 components,
 2
 m
 of


copper
 cable,
 130
 trillion
 transistors,


weighing
 nearly
 2
 tons.


From
 silicon
 in
 Arizona
 and
 Indiana
 to


systems
 in
 Texas,
 Blackwell
 and
 future


Nvidia
 AI
 factory
 generations
 will
 be


built
 in
 America,


writing
 a
 new
 chapter
 in
 American


history
 and
 industry.


America's
 return
 to
 making
 and


reindustrialization,


reignited
 by
 the
 age
 of
 AI.


The
 age
 of
 AI
 has
 begun.


Made
 in
 America.


Made
 for
 the
 world.


Ble.
 We
 are
 manufacturing
 in
 America


again.
 It
 is
 incredible.
 The
 first
 thing


that
 President
 Trump
 asked
 me
 for
 is


bring
 manufacturing
 back.
 Bring


manufacturing
 back
 because
 it's
 it's


necessary
 for
 national
 security.
 bring


manufacturing
 back
 because
 we
 want
 the


jobs
 and
 we
 want
 that
 part
 of
 the


economy.
 And
 nine
 months
 later,
 nine


months
 later,
 we
 are
 now
 manufacturing


in
 full
 production
 Blackwell
 in
 Arizona.


Extreme
 Blackwell
 GB
 200
 MV
 Grace


Blackwell
 Envy
 72
 extreme
 code
 design


gives
 us
 10x
 generationally.
 It's


utterly
 incredible.
 Now,
 the
 part
 that's


really
 incredible
 is
 this.
 This
 is
 the


first
 AI
 supercomput
 we
 made.
 This
 is
 in


2016
 when
 I
 delivered
 it
 to
 a
 startup
 in


San
 Francisco
 which
 turned
 out
 to
 have


been
 OpenAI.
 This
 was
 the
 computer.
 And


in
 order
 to
 do
 the
 create
 that
 computer,


we
 designed
 one
 chip.


We
 designed
 one
 new
 chip
 in
 order
 for
 us


to
 do
 code
 design.
 Now,
 look
 at
 all
 of


the
 chips
 we
 have
 to
 do.
 This
 is
 what
 it


takes.
 You're
 not
 going
 to
 take
 one
 chip


and
 make
 a
 computer
 10
 times
 faster.


That's
 not
 going
 to
 happen.
 The
 way
 to


make
 computers
 10
 times
 faster
 we
 can


keep
 increasing
 the
 performance


exponentially.
 We
 can
 keep
 driving
 cost


down
 exponentially
 is
 extreme
 code


design
 and
 working
 on
 all
 these


different
 chips
 at
 the
 same
 time.
 We
 now


have
 Ruben
 back
 home.
 This
 is
 Ruben.


This
 is
 the
 Vera
 Rubin
 and
 and
 uh
 Ruben.


Ladies
 and
 gentlemen,
 Ruben


This
 is
 this
 is
 our
 third
 generation


MVLink
 72
 rack
 scale
 computer.
 Third


generation
 GB200
 was
 the
 first
 one.
 All


of
 our
 partners
 around
 the
 world,
 I
 know


how
 hard
 you
 guys
 worked.
 It
 was


insanely
 hard.
 It
 was
 insanely
 hard
 to


do.
 Second
 generation,
 so
 much
 smoother.


And
 this
 generation,
 look
 at
 this.


Completely
 cableless.


completely
 cableless.
 And
 this
 is
 this


is
 all
 back
 in
 the
 lab
 now.
 This
 is
 the


next
 generation
 Reuben.
 While
 we're


shipping
 GB300's,


uh
 we're
 preparing
 Reuben
 to
 be
 in


production.
 You
 know,
 this
 time
 next


year,
 maybe
 slightly
 earlier.
 And
 so,


every
 single
 year,
 we
 are
 going
 to
 come


up
 with
 the
 most
 extreme
 code
 design


system
 so
 that
 we
 can
 keep
 driving
 up


performance
 and
 keep
 driving
 down
 the


token
 generation
 cost.
 Look
 at
 this.


This
 is
 just
 an
 incredibly
 beautiful


computer.
 Now,


so
 this
 is
 amazing.
 This
 is
 100


pedaflops.
 I
 know
 this
 doesn't
 mean


anything.
 100
 pedlops,
 but


compared
 to
 the
 DGX1
 I
 delivered
 to


OpenAI
 10
 years
 ago,
 9
 years
 ago,


it's
 100
 times
 the
 performance
 right


here
 versus
 100
 times
 of
 that


supercomput.
 100
 times
 a
 100
 of
 those,


let's
 see,
 a
 100
 of
 those
 would
 be
 like


25
 of
 these
 racks
 all
 replaced
 by
 this


one
 thing.


One


Vera
 Rubin.
 Okay.
 So
 this
 is
 this
 is
 the


compute
 tray


and
 this
 is


so
 Vera
 Rubin
 super
 chip.


Okay.
 And
 this
 is
 the
 compute
 tray.
 This


Oh
 right
 here.


It's
 incredibly
 easy
 to
 install.
 Just


flip
 these
 things
 open,
 shove
 it
 in.


Even
 I
 could
 do
 it.
 Okay.
 And
 this
 is


the
 ver
 Vera
 Rubin
 compute
 tray.
 If
 you


decide
 you
 wanted
 to
 add
 a
 special


processor,
 we've
 added
 another


processor.
 It's
 called
 a
 context


processor
 because
 the
 amount
 of
 context


that
 we
 give
 AIS
 are
 larger
 and
 larger.


We
 wanted
 to
 read
 a
 whole
 bunch
 of
 PDFs


before
 it
 answer
 a
 question.
 Wanted
 to


read
 a
 whole
 bunch
 of
 archive
 papers,


watch
 a
 whole
 bunch
 of
 videos.
 Go
 learn


all
 this
 before
 you
 answer
 a
 question


for
 me.
 All
 of
 that
 context
 processing


could
 be
 added.
 And
 so
 you
 see
 on
 the


bottom
 eight
 connectx9


new
 super
 nicks.
 You
 have
 CX
 you
 have
 uh


CPXs
 eight
 of
 them.
 You
 have
 uh
 blue


field
 4
 this
 new
 data
 processor
 two
 Vera


CPUs
 and
 four
 Reuben
 packages
 or
 eight


Reuben
 GPUs.
 All
 of
 that
 in
 this
 one


node
 completely
 cableless
 100%
 liquid


cooled
 and
 then
 this
 new
 processor
 I


won't
 talk
 too
 much
 about
 it
 today
 I


don't
 have
 enough
 time
 but
 this
 is


completely
 revolutionary
 and
 the
 reason


for
 that
 is
 because
 your
 AIs
 need
 to


have
 more
 and
 more
 memory
 you're


interacting
 with
 it
 more
 you
 wanted
 to


remember
 our
 last
 conversation


everything
 that
 you've
 learned
 on
 my


behalf
 please
 don't
 forget
 it
 when
 I


come
 back
 next
 time
 and
 so
 all
 of
 that


memory
 is
 going
 to
 create
 this
 thing


called
 KV
 caching.
 And
 that
 KV
 caching


retrieving
 it,
 you
 might
 have
 noticed


every
 time
 you
 go
 into
 your
 your
 your


AIS
 these
 days,
 it
 takes
 longer
 and


longer
 to
 refresh
 and
 retrieve
 all
 of


the
 previous
 conversations
 and
 and
 the


reason
 for
 that
 is
 we
 need
 a


revolutionary
 new
 processor
 and
 that's


called
 Blue
 Fuel
 4.
 Next
 is
 this
 the


connectex
 switch,
 excuse
 me,
 the
 MVLink


switch
 which
 is
 right
 here.


Okay,
 this
 is
 the
 MVLink
 switch.
 This
 is


what
 makes
 it
 possible
 for
 us
 to
 con


connect
 all
 of
 the
 computers
 together.


And
 this
 switch
 is
 now
 several
 times


the
 bandwidth
 of
 the
 entire
 world's
 peak


internet
 traffic.
 And
 so
 that
 spine
 is


going
 to
 communicate
 and
 carry
 all
 of


that
 data
 simultaneously
 to
 all
 of
 the


GPUs.
 On
 top
 of
 that,
 on
 top
 of
 that,


this
 is
 the
 this
 is
 the
 Spectrum
 X


switch.
 And
 this
 Ethernet
 switch
 was


designed
 so
 that
 all
 of
 the
 processors


could
 talk
 to
 each
 other
 at
 the
 same


time
 and
 not
 gum
 up
 the
 network.
 Gum
 up


the
 network.
 That's
 very
 technical.


Okay.
 So,
 um,
 so
 these
 are
 the
 these


three
 combined.
 And
 then
 this
 is
 the


quantum
 switch.
 This
 is
 for
 Infiniban.


This
 is
 Ethernet.
 We
 don't
 care
 what


language
 you
 would
 like
 to
 use,
 whatever


standard
 you
 like
 to
 use.
 We
 have
 great


scale
 out
 fabrics
 for
 you.
 Whether
 it's


Infiniban
 or
 Quant
 or
 Spectrum
 Ethernet,


this
 one
 uses
 silicon
 photonics
 and
 is


completely
 co-acked
 options.
 Basically,


the
 laser
 comes
 right
 up
 to
 the
 silicon


and
 connects
 it
 to
 our
 chips.
 Okay.
 So,


this
 is
 the
 Spectrum
 X
 Ethernet.
 And
 so,


now
 let's
 talk
 about
 Thank
 you.
 Oh,
 this


is
 this
 is
 what
 it
 looks
 like.
 This
 is
 a


rack.


This
 is
 two
 and
 a
 half.
 This
 is
 two
 uh


2,00
 This
 is
 two
 tons.


1.5
 million
 parts.


And
 the
 spine,
 this
 spine
 right
 here


carries
 the
 entire
 internet
 traffic
 in


one
 second.
 Same
 speed
 moves
 across
 all


of
 these
 different
 processors.
 100%


liquid
 cooled.
 All
 for
 the,
 you
 know,


fastest
 token
 generation
 rate
 in
 the


world.
 Okay,
 so
 that's
 what
 a
 rack
 looks


like.
 Now
 that's
 one
 rack.
 A
 gigawatt


data
 center
 would
 have


you
 know
 call
 it


let's
 see
 16
 racks
 would
 be
 a
 th00and
 um


and
 then
 500
 of
 those.
 So
 whatever
 500


time
 16
 and
 so
 call
 it
 9,000
 of
 these


8,000
 of
 these
 would
 be
 a
 one
 gigawatt


data
 center.
 Okay.
 And
 so
 that's
 a


future
 AI
 factory.
 Now
 we
 used,
 as
 you


notice,
 Nvidia
 started
 out
 by
 designing


chips
 and
 then
 we
 started
 to
 design


systems
 and
 we
 designed
 AI


supercomputers.
 Now
 we're
 designing


entire
 AI
 factories.
 Every
 single
 time


we
 move
 out
 and
 we
 integrate
 more
 of
 the


problem
 to
 solve,
 we
 come
 up
 with
 better


solutions.
 We
 now
 build
 entire
 AI


factories.


This
 is
 going
 this
 AI
 factory
 is
 what


we're
 building
 for
 Vera
 Rubin
 and
 we


created
 a
 technology
 that
 makes
 it


possible
 for
 all
 of
 our
 partners
 to


integrate
 into
 this
 factory
 digitally.


Let
 me
 show
 it
 to
 you.


The
 next
 industrial
 revolution
 is
 here


and
 with
 it
 a
 new
 kind
 of
 factory.


AI
 infrastructure
 is
 an
 ecosystem
 scale


challenge


requiring
 hundreds
 of
 companies
 to


collaborate.


NVIDIA
 Omniverse
 DSX
 is
 a
 blueprint
 for


building
 and
 operating


Once
 designed,
 Nvidia
 partners
 like


Bectal
 and
 Vertive
 deliver
 prefabricated


modules
 factory-built,


tested,
 and
 ready
 to
 plug
 in.
 This


shrinks
 build
 time
 significantly,


achieving
 faster
 time
 to
 revenues.


When
 the
 physical
 AI
 factory
 comes


online,


the
 digital
 twin
 acts
 as
 an
 operating


system.


Engineers
 prompt
 AI
 agents
 from
 FIDRA


and
 Emerald
 AI,
 previously
 trained
 in


the
 digital
 twin
 to
 optimize
 power


consumption
 and
 reduce
 strain
 on
 both


the
 AI
 factory
 and
 the
 grid.


In
 total,
 for
 a
 1
 gawatt
 AI
 factory,
 DSX


optimizations
 can
 deliver
 billions
 of


dollars
 in
 additional
 revenue
 per
 year


across
 Texas,


Georgia,
 and
 Nevada.
 NVIDIA's
 partners


are
 bringing
 DSX
 to
 life.


In
 Virginia,
 Nvidia
 is
 building
 an
 AI


factory
 research
 center
 using
 DSX
 to


test
 and
 productize
 Vera
 Rubin
 from


infrastructure
 to
 software.


With
 DSX,
 Nvidia
 partners
 around
 the


world
 can
 build
 and
 bring
 up
 AI


infrastructure
 faster
 than
 ever.


[Music]


completely
 completely
 in
 digital
 long


long
 before
 Vera
 Rubin
 exists
 as
 a
 real


computer
 we've
 been
 using
 it
 as
 a


digital
 twin
 computer
 now
 long
 before


these
 AI
 factories
 exist
 we
 will
 use
 it


we
 will
 design
 it
 we'll
 plan
 it
 we'll


optimize
 it
 and
 we'll
 operate
 it
 as
 a


digital
 twin
 and
 so
 all
 of
 our
 partners


that
 are
 working
 with
 us
 I'm
 incredibly


happy
 for
 all
 of
 you
 supporting
 us
 and


Gio
 is
 here
 and
 G
 ver
 Vernova
 is
 here.


Schneider,
 I
 I
 think
 um
 I
 think
 uh
 uh


Olivia
 is
 here.
 Olivia
 Blum
 is
 here.
 Um


uh
 uh
 Seaman's
 incredible
 partners.


Okay,
 Roland
 Bush,
 I
 think
 he's


watching.
 Hi
 Roland.
 And
 so
 anyways,
 uh


really
 really
 great
 partners
 working


with
 us.


In
 the
 beginning,
 we
 had
 CUDA
 and
 we


have
 all
 these
 different
 ecosystems
 of


software
 partners.
 Now
 we
 have


Omniverse,
 DSX,
 and
 we're
 building
 AI


factories.
 And
 again,
 we
 have
 these


incredible
 ecosystem
 of
 partners
 working


with
 us.
 Let's
 talk
 about
 models,


open
 source
 models
 particularly.


In
 the
 last
 couple
 years,
 several
 things


have
 happened.
 One,
 open
 source
 models


have
 become
 quite
 capable
 because
 of


reasoning
 capabilities.
 It
 has
 become


quite
 capable
 because
 they're


multimodality
 and
 they're
 incredibly


efficient
 because
 of
 distillation.
 So


all
 these
 different
 capabilities
 have


become
 uh
 has
 made
 open
 source
 models


for
 the
 very
 first
 time
 incredibly


useful
 for
 developers.
 They
 are
 now
 the


lifeblood
 of
 startups.


Lifeblood
 of
 startups
 in
 different


industries
 because
 obviously
 as
 I


mentioned
 before
 each
 one
 of
 the


industries
 have
 its
 own
 use
 case,
 it
 own


use
 cases,
 it
 own
 data,
 it
 owns
 data,


its
 own
 flywheels.
 All
 of
 that


capability,
 that
 domain
 expertise
 needs


to
 have
 the
 ability
 to
 embed
 into
 a


model.
 Open
 source
 makes
 that
 possible.


Researchers
 need
 open-source.
 Developers


need
 open-source.
 Companies
 around
 the


world,
 we
 need
 open
 source.
 Open-
 source


models
 is
 really,
 really
 important.


The
 United
 States
 has
 to
 lead
 in
 open


source
 as
 well.
 We
 have
 amazing


proprietary
 models.
 We
 have
 amazing


proprietary
 models.
 We
 need
 also
 amazing


open
 source
 models.
 Our
 country
 depends


on
 it.
 Our
 startups
 depend
 on
 it.
 And
 so


Nvidia
 is
 dedicating
 ourselves
 to
 go
 do


that.
 We
 are
 now
 the
 largest
 the
 largest


we
 lead
 in
 open-source
 contribution.
 We


have
 23
 models
 in
 leaderboards.
 We
 have


all
 these
 different
 domains
 from


language
 models
 the
 physical
 AI
 models.


I'm
 going
 to
 talk
 about
 robotics
 models


to
 biolog
 biology
 models.
 Each
 one
 of


these
 models
 has
 enor
 enormous
 teams
 and


that's
 one
 of
 the
 reasons
 why
 we
 built


supercomputers
 for
 ourselves
 to
 enable


all
 these
 models
 to
 be
 created.
 We
 have


number
 one
 speech
 model,
 number
 one


reasoning
 model,
 number
 one
 physical
 AI


model.
 The
 number
 of
 downloads
 is
 really


really
 terrific.
 We
 are
 dedicated
 to


this
 and
 the
 reason
 for
 that
 is
 because


science
 needs
 it,
 researchers
 need
 it,


startups
 need
 it
 and
 companies
 need
 it.


I'm
 delighted
 that
 AI
 startups
 build
 on


Nvidia.
 They
 do
 so
 for
 several
 reasons.


One,
 of
 course,
 our
 ecosystem
 is
 rich.


Our
 tools
 work
 great.
 All
 of
 our
 tools


work
 on
 all
 of
 our
 GPUs.
 Our
 GPUs
 are


everywhere.
 It's
 literally
 in
 every


single
 cloud.
 It's
 available
 on
 prem.


You
 could
 build
 it
 yourself.
 You
 could


you
 could,
 you
 know,
 build
 up
 a
 an


enthusiast
 gaming
 PC
 with
 multiple
 GPUs


in
 it
 and
 you
 could
 download
 our


software
 stack
 and
 it
 it
 just
 works.
 We


have
 the
 benefit
 of
 rich
 developers
 who


are
 making
 this
 ecosystem
 richer
 and


richer
 and
 richer.
 So,
 I'm
 really


pleased
 with
 all
 of
 the
 startups
 that


we're
 working
 with.
 I'm
 I'm
 thankful
 for


that.
 It
 is
 also
 the
 case
 that
 many
 of


these
 startups
 are
 now
 starting
 to


create
 even
 more
 ways
 to
 enjoy
 our
 GPUs.


the
 Cordweaves,
 Nscale,
 Nimbius,
 Llama,


Lambda,
 all
 of
 these
 companies,
 Crusoe


companies
 are
 building
 these
 new
 GPU


clouds
 to
 serve
 the
 startups.
 And
 I


really
 appreciate
 that
 this
 is
 all


possible
 because
 NVIDIA
 is
 everywhere.


We
 integrate
 our
 libraries,
 all
 of
 the


CUDA
 X
 libraries
 I
 talked
 to
 you
 about,


all
 the
 open-source
 AI
 models
 that
 I


talked
 about,
 all
 of
 the
 models
 that
 I


talked
 about,
 we
 integrated
 into
 AWS,


for
 example.
 really
 love
 working
 with


Matt.
 We
 integrated
 into
 Google
 Cloud,


for
 example.
 Really
 love
 working
 with


Thomas.
 Each
 one
 of
 these
 clouds


integrate
 NVIDIA
 GPUs
 and
 our
 computing,


our
 libraries
 as
 well
 as
 our
 models.


Love
 working
 with
 Satia
 over
 at


Microsoft
 Azure.
 Love
 working
 with
 uh


Clay
 at
 Oracle.
 Each
 one
 of
 these
 clouds


integrate
 the
 NVIDIA
 stack.
 As
 a
 result,


wherever
 you
 go,
 whichever
 cloud
 you


use,
 it
 works
 incredibly.
 We
 also


integrate
 Nvidia
 libraries
 into
 the


world
 SAS
 so
 that
 each
 one
 of
 these
 SAS


will
 eventually
 become
 agentic
 SAS.
 I


love
 Bill
 McDerman's
 vision
 for
 Service


Now.
 There
 the
 Yeah,
 there
 you
 go.
 I


think
 that
 might
 have
 been
 Bill.


Hi,
 Bill.
 And
 so
 Service
 Now,
 what
 is


it?
 85%
 of
 the
 world's
 enterprise


workloads,
 workflows,
 SAP,
 80%
 of
 the


world's
 commerce.
 Christian
 Klein
 and
 I


are
 working
 together
 to
 integrate
 NVIDIA


libraries,
 CUDA
 X
 and
 Nemo
 and
 Neotron,


all
 of
 our
 AI
 systems
 into
 SAP,
 working


with
 Ceine
 over
 at
 Synopsis,


accelerating
 the
 world
 CAE,
 CAD,
 EDA


tools
 so
 that
 they
 could
 be
 faster
 and


could
 scale,
 helping
 them
 create
 AI


agents.
 One
 of
 these
 days
 I
 would
 love


to
 hire
 a
 AI
 agent
 ASIC
 designers
 to


work
 with
 our
 ASIC
 designers
 essentially


the
 cursor
 of
 synopsis
 if
 you
 will.


We're
 working
 with
 uh
 Andy
 Rude.
 Annie


Rude
 here
 I
 saw
 him
 earlier
 today.
 He


was
 part
 of
 the
 pregame
 show.
 Cadence


doing
 incredible
 work
 accelerating
 their


stack
 creating
 AI
 agents
 so
 that
 we
 can


have
 cadence
 AI
 as
 designers
 and
 system


designers
 working
 with
 us.
 Today
 we're


announcing
 a
 new
 one.


AI


will
 supercharge
 productivity.
 AI
 will


transform
 just
 about
 every
 industry.
 But


AI
 will
 also
 supercharge


cyber
 security
 challenges,
 the
 bad
 AIs.


And
 so
 we
 need
 an
 incredible
 defender.
 I


can't
 imagine
 a
 better
 defender
 than


Crowd
 Strike.
 George
 George
 is
 here.
 Uh


he
 was
 Yeah,
 I
 saw
 him
 earlier.


We
 are
 partnering
 with
 CrowdStrike
 to


make
 cyber
 security
 speed
 of
 light
 to


create
 a
 system
 that
 has
 cyber
 security


AI
 agents
 in
 the
 cloud
 but
 also


incredibly
 good
 AI
 agents
 on
 prem
 or
 at


the
 edge.
 This
 way
 you
 whenever
 there's


a
 threat
 you
 are
 moments
 away
 from


detecting
 it.
 We
 need
 speed
 and
 we
 need


a
 fast
 agentic
 AI
 super
 a
 super
 smart


AIS.


I
 have
 a
 second
 announcement.
 This
 is


the
 single
 fastest
 enterprise
 enterprise


company
 in
 the
 world.


Probably
 the
 single
 most
 important


enterprise
 stack
 in
 the
 world
 today.


Palunteer
 ontology.


Anybody
 from
 Palunteer
 here?
 I
 just
 I


was
 just
 talking
 to
 Alex
 earlier.


>> Oh
 my
 god.
 Oh
 my
 goodness.


>> This
 is
 Palenter
 ontology.


>> Oh
 my
 goodness.
 We've
 been
 waiting
 so


long
 for
 this


>> information.


They
 take
 data.
 They
 take
 human
 judgment


and
 they
 turn
 it
 into
 business
 insight.


We
 work
 with
 Palanteer
 to
 accelerate


everything
 Palanteer
 does
 so
 that
 we


could
 do
 data
 processing
 data
 processing


at
 a
 much
 much
 larger
 scale
 and
 more


speed
 whether
 it's
 structured
 data
 of


the
 past
 and
 of
 course
 we'll
 have


structured
 data
 human
 recorded
 data


unstructured
 data
 and
 process
 that
 data


for
 our
 government
 for
 national
 security


and
 for
 enterprises
 around
 the
 world


process
 that
 data
 at
 speed
 of
 and
 to


find
 insight
 from
 it.
 This
 is
 what
 it's


going
 to
 look
 like
 in
 the
 future.


Palunteer
 is
 going
 to
 integrate
 NVIDIA


so
 that
 we
 could
 process
 at
 the
 speed
 of


light
 and
 at
 extraordinary
 scale.


>> Oh
 my
 goodness.


>> NVIDIA
 and
 Palanteer.


>> I'm
 going
 to
 come.
 I'm
 going
 to
 come.


>> Let's
 relax.


>> Physical
 AI
 requires
 three
 computers.


Just
 as
 it
 takes
 two
 computers
 to
 train


a
 language
 model.
 One
 that's
 to
 train


it,
 evaluate
 it,
 and
 then
 inference
 it.


Okay,
 so
 that's
 the
 large
 GB200
 that
 you


see.
 In
 order
 to
 do
 it
 for
 physical
 AI,


you
 need
 three
 computers.
 You
 need
 the


computer
 to
 train
 it.
 This
 is
 GB
 the


Grace
 Blackwell
 Envy
 72.
 We
 need
 a


computer
 that
 does
 all
 of
 the


simulations
 that
 I
 showed
 you
 earlier


with
 Omniverse
 DSX.
 It
 basically
 is
 a


digital
 twin
 for
 the
 robot
 to
 learn
 how


to
 be
 a
 good
 robot
 and
 for
 the
 factory


to
 essentially
 be
 a
 digital
 twin.
 That


computer
 is
 the
 second
 computer,
 the


omniverse
 computer.
 This
 computer
 has
 to


be
 incredibly
 good
 at
 generative
 AI
 and


it
 has
 to
 be
 good
 at
 computer
 graphics,


sensor
 simulation,
 ray
 tracing,
 signal


processing,
 this
 computer
 is
 called
 the


omniverse
 computer.
 And
 once
 we
 train


the
 model,


simulate
 that
 AI
 inside
 a
 digital
 twin


and
 that
 digital
 twin
 could
 be
 a
 digital


twin
 of
 a
 factory
 as
 long
 as
 well
 as
 a


whole
 bunch
 of
 digital
 twins
 of
 robots.


Then
 you
 need
 to
 operate
 that
 robot.
 And


this
 is
 the
 robotic
 computer.
 This
 is


this
 one
 goes
 into
 a
 self-driving
 car.


Half
 of
 it
 could
 go
 into
 a
 robot.
 Okay?


Or
 you
 could
 actually
 have,
 you
 know,


robots
 that
 are
 quite
 agile
 and
 quite


quite
 fast
 in
 operations.
 And
 it
 might


take
 two
 of
 these
 computers.
 And
 so
 this


is
 the
 Thor
 Jetson
 Thor
 robotics


computer.
 These
 three
 computers
 all
 run


CUDA.
 And
 it
 makes
 it
 possible
 for
 us
 to


advance
 physical
 AI.
 AI
 that
 understand


the
 physical
 world,
 understand
 laws
 of


physic,
 causality,


permanence,
 you
 know,


physical
 AI.
 We
 have
 incredible
 partners


working
 with
 us
 to
 create
 the
 physical


AI
 of
 factories.
 We're
 using
 it


ourselves
 to
 create
 our
 factory
 in


Texas.
 Now,
 once
 we
 create
 the
 robotic


factory,
 we
 have
 a
 bunch
 of
 robots
 that


are
 inside
 it.
 And
 these
 robots
 also


need
 the
 physical
 AI
 applies
 physical
 AI


and
 works
 inside
 the
 digital
 twin.
 Let's


take
 a
 look
 at
 it.


America
 is
 re-industrializing,


reshoring
 manufacturing
 across
 every


industry.
 In
 Houston,
 Texas,
 Foxcon
 is


building
 a
 state-of-the-art
 robotic


facility
 for
 manufacturing
 NVIDIA
 AI


infrastructure
 systems.


With
 labor
 shortages
 and
 skills
 gaps,


digitalization,
 robotics,
 and
 physical


AI
 are
 more
 important
 than
 ever.


The
 factory
 is
 born
 digital


in
 Omniverse.


Foxcon
 engineers
 assemble
 their
 virtual


factory
 in
 a
 seaman's
 digital
 twin


solution
 developed
 on
 Omniverse


Technologies.
 Every
 system,
 mechanical,


electrical,
 plumbing,
 is
 validated


before
 construction.


Seaman's
 plant
 simulation
 runs
 design


space
 exploration
 optimizations


to
 identify
 ideal
 layout.


When
 a
 bottleneck
 appears,
 engineers


update
 the
 layout
 with
 changes
 managed


by
 Seaman's
 team
 center.


In
 Isaac
 sim,
 the
 same
 digital
 twin
 is


used
 to
 train
 and
 simulate
 robot
 AIS.


In
 the
 assembly
 area,
 fanic
 manipulators


build
 GB300
 tray
 modules


by
 manual
 manipulators
 from
 FII
 and


Skilled
 AI
 install
 bus
 bars
 into
 the


trays


and
 AMRs
 shuttle
 the
 trays
 to
 the
 test


pods.


Then
 Foxcon
 uses
 Omniverse
 for


large-scale
 sensor
 simulation
 where


robot
 AIs
 learn
 to
 work
 as
 a
 fleet.


In
 Omniverse,
 vision
 AI
 agents
 built
 on


NVIDIA
 Metropolis
 and
 Cosmos.


Watch
 the
 fleets
 of
 robots
 and
 workers


from
 above
 to
 monitor
 operations


and
 alert
 Foxcon
 engineers
 of
 anomalies


and
 safety
 violations.


or
 even
 quality
 issues.


And
 to
 train
 new
 employees,
 agents
 power


interactive
 AI
 coaches
 for
 easy
 worker


onboarding.


The
 age
 of
 US
 re-industrialization
 is


here
 with
 people
 and
 robots
 working


together.


That's
 the
 the
 future
 of
 manufacturing,


the
 future
 of
 factories.
 I
 want
 to
 thank


our
 partner
 Foxcon
 Young
 Leu,
 the
 CEO,


is
 here,
 but
 all
 of
 these
 ecosystem


partners
 make
 it
 possible
 for
 us
 to


create
 the
 future
 of
 robotic
 factories.


The
 factory
 is
 essentially
 a
 robot


that's
 orchestrating
 robots


to
 build
 things
 that
 are
 robotic.
 You


know
 this
 is
 the
 amount
 of
 software


necessary
 to
 do
 this
 is
 so
 intense
 that


unless
 you
 could
 do
 it
 inside
 a
 digital


twin
 to
 dis
 to
 plan
 it
 to
 design
 it
 to


operate
 inside
 a
 digital
 twin
 the
 hopes


of
 getting
 this
 to
 work
 is
 nearly


impossible.
 I'm
 so
 happy
 to
 see
 also


that
 Caterpillar,
 my
 friend
 Joe
 Joe


Creed
 and
 his
 hundred-year-old
 company


is
 also
 incorporating
 digital
 twins
 and


the
 way
 they
 manufacture.
 Um,
 these


factories
 will
 have
 future
 robotic


systems
 and
 one
 of
 the
 most
 advanced
 is


figure.
 Brett
 Atco
 is
 here
 today.
 He


just
 he
 founded
 a
 company
 three
 and
 a


half
 years
 ago.
 They're
 worth
 almost
 $40


billion.
 Today
 we're
 working
 together
 in


training
 the
 the
 AI,
 training
 the
 robot,


simulating
 the
 robot
 and
 of
 course
 the


robotic
 computer
 that
 goes
 into
 figure


really
 amazing.
 Uh
 I
 had
 the
 benefit
 of


seeing
 it.
 Uh
 it's
 really
 quite
 quite


extraordinary.
 It
 is
 very
 likely
 that


humano
 robots
 and
 my
 friend
 Elon
 is
 also


working
 on
 this
 that
 this
 is
 likely


going
 to
 be
 one
 of
 the
 largest
 consumer


new
 consumer
 electronics
 markets
 and


surely
 one
 of
 the
 largest
 industrial


equipment
 market.
 Peggy
 Johnson
 and
 the


folks
 at
 Agility
 are
 working
 with
 us
 on


robots
 for
 warehouse
 automation.
 The


folks
 at
 Johnson
 Johnson
 working
 with
 us


again
 training
 the
 robot,
 simulating
 it


in
 digital
 twins
 and
 also
 operating
 the


robot.
 These
 Johnson


surgical
 robots
 are
 even
 going
 to


perform
 surgery
 that
 are
 completely


noninv
 noninvasive
 surgery
 at
 a


precision
 the
 world's
 never
 seen
 before.


And
 of
 course,
 the
 cutest
 robot
 ever.


The
 cutest
 robot
 ever,
 the
 Disney
 robot.


And
 this
 is
 this
 is
 um
 something
 really


close
 to
 our
 heart.
 We're
 working
 with


Disney
 research
 on
 a
 entirely
 new


framework
 and
 sim
 simulation
 platform
 uh


based
 on
 revolutionary
 technology
 called


Newton.
 And
 that
 Newton
 uh
 simulator


makes
 it
 possible
 for
 the
 the
 robot
 to


learn
 how
 to
 be
 a
 good
 robot
 inside
 a


physically
 aware,
 physically
 based


environment.
 Let's
 take
 a
 look
 at
 it.


[Music]


Amit
 Palunteer
 and
 figure
 in
 the
 same


video.


>> That's
 wild.


>> Crazy,
 bro.
 Congratulations.
 That's


awesome.
 Well,


>> Nvidia,
 you've
 been
 holding
 it
 since


2018.
 Alltime
 highs.


[Music]


Okay.


[Music]


Blue,
 ladies
 and
 gentlemen.
 Disney
 Blue.


Tell
 me
 that's
 not
 adorable.
 He's
 not


adorable.


We
 all
 want
 one.
 We
 all
 want
 one.
 Now


remember
 everything
 you
 were
 just
 seeing


that
 is
 not
 animation.
 It's
 not
 a
 movie.


It's
 a
 simulation.
 That
 simulation
 is
 an


omniverse.
 Omniverse.
 The
 digital
 twin.


So
 these
 digital
 twins
 of
 factories,


digital
 twins
 of
 warehouses,
 digital


twins
 of
 surgical
 rooms,
 digital
 twins


where
 blue
 could
 learn
 how
 to
 manipulate


and
 navigate
 and
 you
 know
 interact
 with


the
 world.
 All
 completely
 done
 in
 real


time.
 This
 is
 going
 to
 be
 the
 largest


consumer
 electronics
 product
 line
 in
 the


world.
 Some
 of
 them
 are
 just
 really


working
 incredibly
 well
 now.
 This
 is
 the


future
 of
 human
 or
 robotics
 and
 of


course
 blue.
 Okay.


Now,
 human
 robots
 is
 still
 in


development.
 But
 meanwhile,


there's
 one
 robot
 that
 is
 clearly
 at
 an


inflection
 point
 and
 it
 is
 basically


here
 and
 that
 is
 a
 robot
 on
 wheels.
 This


is
 a
 robo
 taxi.
 A
 robo
 taxi
 is


essentially
 an
 AI
 chauffeur.
 Now,
 one
 of


the
 things
 that
 we're
 doing
 today,
 we're


announcing
 the
 NVIDIA
 drive
 Hyperion.


This
 is
 a
 big
 deal.


We
 created
 this
 architecture
 so
 that


every
 car
 company
 in
 the
 world
 could


create
 cars,
 vehicles,
 could
 be


commercial,
 could
 be
 passenger,
 could
 be


dedicated
 to
 robo
 taxi.
 create
 vehicles


that
 are
 robo
 taxi
 ready.
 The
 sensor


suite
 with
 surround
 cameras
 and
 radars


and
 LAR


make
 it
 possible
 for
 us
 to
 achieve
 the


highest
 level
 of
 surround
 cocoon
 sensor


perception
 and
 redundancy
 necessary
 for


the
 highest
 level
 of
 safety.


Hyperion
 drive
 drive
 Hyperion
 is
 now


designed
 into
 Lucid
 Mercedes-Benz
 my


friend
 Ola
 Ken
 Canel
 Kenas
 um
 the
 folks


at
 Stalantis
 and
 there
 are
 many
 other


cars
 coming
 and
 once
 you
 have
 a
 basic


standard
 platform
 then
 developers
 of
 AV


systems
 and
 there's
 so
 many
 talented


ones
 wave
 wabby
 Aurora
 Momenta
 Neuro


there's
 so
 many
 of
 them
 we
 ride
 there's


so
 many
 of
 them
 that
 can
 then
 take
 their


AV
 V
 system
 and
 run
 it
 on
 the
 standard


chassis.
 Basically,
 the
 standard
 chassis


has
 now
 become
 a
 computing
 platform
 on


wheels.
 And
 because
 it's
 standard
 and


the
 sensor
 suite
 is
 comprehensive,
 all


of
 them
 could
 deploy
 their
 AI
 to
 it.


Let's
 take
 a
 quick
 look.


[Music]


Okay,
 that's
 the
 be
 that's
 beautiful
 San


Francisco.
 And
 as
 you
 could
 see,
 as
 you


could
 see,
 robo
 taxis
 inflection
 point


is
 about
 to
 get
 here.
 And
 in
 the
 future,


a
 trillion
 miles
 a
 year
 that
 are
 driven,


a
 100
 million
 cars
 made
 each
 year.


There's
 some
 50
 million
 taxis
 around
 the


world.
 It's
 going
 to
 be
 augmented
 by
 a


whole
 bunch
 of
 robo
 taxis.
 So,
 it's


going
 to
 be
 a
 very
 large
 market
 to


connect
 it
 and
 deploy
 it
 around
 the


world.
 Today,
 we're
 announcing
 a


partnership
 with
 Uber.
 Uber
 Dar
 Darra
 K


Dara
 is
 going
 to
 we're
 working
 together


to
 connect
 these
 Nvidia
 drive
 Hyperion


cars
 into
 a
 global
 network
 and
 now
 in


the
 future
 you'll
 you
 know
 be
 able
 to


hail
 up
 one
 of
 these
 cars
 and
 the


ecosystem
 is
 going
 to
 be
 incredibly
 rich


and
 we'll
 have
 Hyperion
 or
 Robo
 taxi


cars
 all
 over
 the
 world.
 This
 is
 going


to
 be
 a
 new
 computing
 platform
 for
 us


and
 I'm
 expecting
 it
 to
 be
 quite


successful.


Okay.


So
 this
 is
 what
 we
 talked
 about
 today.


We
 talked
 about
 a
 large
 large
 number
 of


things
 we
 spoke
 about.
 Remember
 at
 the


core
 of
 this
 is
 two
 or
 two
 platform


transitions
 from
 general
 purpose


computing
 to
 accelerated
 computing.


NVIDIA
 CUDA
 and
 those
 suite
 of
 libraries


called
 CUDA
 X
 has
 enabled
 us
 to
 address


practically
 every
 industry
 and
 we're
 at


the
 inflection
 point.
 It
 is
 now
 growing


as
 a
 virtual
 cycle
 would
 suggest.
 The


second
 inflection
 point
 is
 now
 upon
 us.


The
 second
 platform
 transition
 AI
 from


classical
 handwritten
 software
 to


artificial
 intelligence.
 Two
 platform


transitioning
 happening
 at
 the
 same
 time


which
 is
 the
 reason
 why
 we're
 feeling


such
 incredible
 growth.


Quant
 quantum
 computing.
 We
 spoke
 about


open
 models.
 spoke
 about
 we
 spoke
 about


enterprise
 with
 crowd
 strike
 and
 uh


Palunteer
 accelerating
 their
 platforms.


Uh
 we
 spoke
 about
 robotics
 a
 new
 large


potentially
 one
 of
 the
 largest
 consumer


electronics
 and
 industrial
 manufacturing


sectors
 and
 of
 course
 we
 spoke
 about
 6G.


Nvidia
 has
 new
 platforms
 for
 6G.
 We
 call


it
 ARC.
 We
 have
 a
 new
 platform
 for


robotics
 cars.
 We
 call
 that
 Hyperion.


We
 have
 new
 platforms


even
 for
 factories.
 Two
 types
 of


factories.
 The
 AI
 factory
 we
 call
 that


DSX
 and
 then
 factories
 with
 AI
 we
 call


that
 mega.
 And
 so
 now
 we're
 also


manufacturing
 in
 America.
 Ladies
 and


gentlemen,
 thank
 you
 for
 joining
 us


today
 and
 thank
 you
 for
 allowing
 me
 to


bring


Thank
 Thank
 you
 for
 for
 allowing
 us
 to


bring
 GTC
 to
 Washington
 DC.
 We're
 going


to
 do
 it
 hopefully
 every
 year.
 And
 thank


you
 all
 for
 your
 service
 and
 making


America
 great
 again.
 Thank
 you.


>> Oh,
 that
 hits
 hard.
 That's
 nice.


>> Thank
 you.


>> Got
 to
 love
 Jensen,
 man.


Thank
 you
 for
 making
 America
 great


again.
 That
 was
 a
 little
 bit
 of
 a
 signal


to
 a
 to
 a
 certain
 someone
 who
 controls


the
 the
 China
 revenues
 for
 Nvidia.


That's
 for
 sure.


>> He
 plays
 the
 political
 card
 so
 well.
 I


mean,
 he
 brought
 it
 to
 Washington
 to
 be


near
 Trump.
 You
 know what
 I
 mean?
 Like


it
 he's
 just
 doing
 it
 amazingly.


Okay.
 There
 was
 a
 lot
 of
 things
 to
 cover


there,
 but
 honestly,
 it
 all
 boils
 down


to
 the
 Palunteer
 comments.
 No,
 I'm


kidding.
 uh
 to
 you
 maybe,
 but
 the
 the


$500
 billion
 in
 expected
 business
 over


the
 next
 six
 quarters.


>> So,
 hold
 on.
 Is
 that
 confirmed?
 Did
 they


like
 legit
 say
 that?


>> Yeah.
 Yeah.
 You
 you
 quoted
 Bloomberg
 on


that.
 Jensen
 said
 it,
 bro.


>> Oh,
 I
 guess


>> he
 said
 it
 on
 stage.


>> Oh,
 so
 then
 Bloomberg
 was
 quoting


Jensen.
 Okay,
 cool.
 So,
 that's
 So,
 so


that's
 legit.


>> That's
 whenever
 the
 stock
 like
 really


popped.
 You
 see
 those
 two
 uh
 or
 that


huge
 candle?
 Well,
 I'm
 looking
 on
 the


minutes,
 but
 um
 whenever
 it
 really


popped,
 that
 was
 his
 comments.


>> Yeah.


>> Okay.
 So,
 do
 you
 think
 the
 street
 didn't


think
 they
 were
 going
 to
 have
 that
 much


in
 revenue?
 That
 was
 a
 surprise


basically.


>> Yeah.
 Yeah.
 That
 candle
 right
 there
 is


whenever
 he
 said
 it.
 So,
 essentially


from
 I
 I
 looked
 on
 the
 street
 or


expectations
 from
 Wall
 Street,
 they're


expecting
 about
 380
 billion
 over
 the


next
 six
 quarters.


>> Oh
 my.


>> So,
 he
 said
 500
 billion.


and
 and
 that
 that
 he
 has
 insight
 into.


You
 know,
 there
 could
 be
 a
 lot
 more
 than


that.


>> Dude,
 I
 don't
 know,
 man.
 They
 hit
 4.8


trillion.
 Uh
 we're
 actually
 pushing
 new


highs.
 We
 just
 hit
 another
 high
 right


now.
 One
 uh
 9764.


>> 4.8
 trillion.
 Yeah.
 I
 think
 it's
 eight


more
 bucks
 for
 five
 trillion.


And
 this
 is
 without
 China,
 I
 imagine,


right?


>> Without
 China.


>> Wow.
 F.
 So
 that's
 that's
 120
 billion


more
 than
 the
 street
 expected
 without


the
 second
 largest
 economy.


>> Isn't
 that
 crazy?


>> This
 shit's
 going
 to
 10
 trillion,
 bro.


>> Yeah,
 we're
 we're
 pumping
 right
 now.


One.
 Oh
 my
 god.
 We're
 really
 pumping,


dude.


83
 86.


I
 I
 don't
 want
 to
 say
 200
 is
 around
 the


corner.
 It
 looks
 like
 it
 is,
 but
 this
 is


usually
 when
 Nvidia
 tends
 to
 get


rejected,


but
 maybe
 today
 is
 good
 enough
 for
 them


to
 keep
 going.


We're
 all
 having
 a
 great
 day
 today.
 I


mean,
 you
 got
 your
 Palanteer
 shout
 out,


the
 figure
 shout
 out.
 Insane.


>> Okay,
 so
 I
 I
 thought
 I
 thought
 GTC
 would


be
 longer,
 so
 I
 I
 we
 I
 will
 we'll
 we'll


stay
 here
 for
 like
 10
 more
 minutes
 and


then
 I'll
 start
 an
 actual
 market
 close


later.
 Um
 but
 wow,
 it's
 going
 for
 198.
 I


I
 think
 the
 big
 thing
 today
 is
 Jensen


was
 Santa
 Claus.
 He
 pumped
 up
 quantum.


He
 pumped
 up
 the
 data
 center
 plays,


Cororeweave,
 Nebius,
 etc.
 He
 pumped
 up


cyber
 security
 with
 Crowdstrike.
 He


pumped
 up
 software
 with
 Palunteer.
 If


then
 he
 pumped
 up
 Disney,
 I
 mean
 like
 he


just
 contributed
 as
 much
 as
 he
 could
 to


this
 entire
 ecosystem
 of
 companies
 by


Uber,
 right?
 Every


>> Oh
 my
 gosh.
 He
 just
 ran
 through
 198


>> 198.
 I
 mean,
 he
 really
 he
 was
 really


Santa
 today,
 helping
 everyone
 out
 and


the
 streets
 were
 warning
 it
 because
 not


only
 did
 he
 help
 everyone
 out,
 he
 told


the
 street
 you
 can
 expect
 500
 billion
 of


revenue
 or
 not
 revenue,
 but
 of
 business.


>> Yeah.
 Yeah.
 It
 It's
 so
 funny,
 right?


They
 He's
 so
 open
 about
 it.
 We
 are
 the


most
 expensive
 chip
 on
 the
 market,
 but


we
 have
 the
 lowest
 total
 cost
 of


ownership.


and
 and
 if
 you're
 willing
 to
 pay
 us


upfront,
 you're
 going
 to
 get
 so
 much


back
 in
 return
 because
 the
 quality
 of


our
 chips
 is
 unrivaled.
 The
 like
 you


said
 in
 the
 benchmarks,
 it's
 just
 a


bunch
 of
 Nvidia
 chips
 comparing
 against


each
 other.
 I
 mean,
 there's
 there's
 no


one
 else
 on
 the
 list.
 That's
 not
 that's


not
 actually
 true,
 but
 Nvidia
 is
 just


doing
 very
 well.


I
 uh


>> am
 looking
 at
 Nvidia
 at
 all-time
 highs.


Shopify
 is
 my
 second
 largest
 position,


all-time
 highs,
 and
 SoFi
 didn't
 start


the
 day
 doing
 very
 well,
 but
 we're
 we're


back
 up.
 So,
 my
 third
 largest
 position


hit
 all-time
 highs
 today.
 That's
 a
 fun


day,
 man.


>> Having
 a
 good
 time.


>> The
 general
 market
 momentum,
 I
 think,
 is


also
 helping
 SoFi
 come
 back,
 which
 is


which
 is
 nice
 to
 see.
 Was
 there
 any


specific
 announcement
 that
 he
 made
 that


really
 caught
 your
 attention
 or?


>> Um,


not
 I
 I
 I
 wouldn't
 say
 so.
 I
 mean,
 uh,


we
 knew
 about
 the
 telco
 stuff
 before.


Um,
 we
 knew
 about
 the
 Uber
 deal.
 That


was
 kind
 of
 weird.
 They
 they
 announced


the
 Uber
 deal,
 then
 they
 took
 it
 off
 X


really
 quick,
 but
 it
 was
 already
 out


there,
 so
 everyone
 knew
 about
 it.


>> Um,
 I
 don't
 think
 there
 was
 anything


that
 was
 too
 crazy.


>> The
 the
 Department
 of
 Energy
 7


>> that
 was
 new.
 You're
 right.
 Yeah,


>> that
 was
 new.


>> Yeah.


>> Seven
 uh
 supercomputer
 developments
 that


they're
 doing.


>> Yep.


>> That
 was
 definitely
 new.
 The
 500
 billion


though,
 just
 just
 the
 the
 the
 general


size
 of
 everything
 was
 very
 large.
 And
 I


think
 because
 this
 is
 actually
 crazy.
 I


don't
 know if
 you
 saw
 online
 this
 whole


scam
 live
 stream.


>> Oh,
 we
 we
 got
 scammed
 by
 it
 almost,
 bro.


Like,


>> yeah,
 there's
 a
 big
 QR
 code
 on
 it.
 But


if
 you
 look,
 I
 think
 there's
 been


hundreds
 of
 thousands
 of
 dollars.
 So,


um,
 that
 have
 been
 stolen
 that
 they
 say,


"Hey,
 you
 know,
 put
 your
 Yeah,
 sorry.


$170,000


have
 been
 given
 to
 these
 wallets
 on
 this


scam


thing."
 And
 I
 honestly
 think
 it's
 cuz


they
 started
 late.


>> Yeah,
 cuz
 we
 almost
 clicked
 it.
 I
 mean,


we
 clicked
 it
 for
 like
 two
 seconds


because
 we
 thought
 And
 then
 they
 used


the
 deep
 fake
 of
 Jensen.


Um,


wow.
 This
 is
 quite
 incredible.
 Okay,
 so


where
 do
 we
 go
 from
 here?
 You've
 got


Nvidia
 pumping
 all-time
 highs.
 You've


got
 rate
 cuts
 coming
 tomorrow.
 You've


got
 $500
 billion
 of
 new
 business
 XChina,


which
 really
 is
 going
 to
 have
 to
 get
 a


lot
 of
 upgrades
 from
 the
 street.
 I
 mean,


HSBC
 was
 probably
 right
 when
 they
 called


for
 320.
 Um,
 and
 then
 you've
 got
 all


these
 smaller
 plays
 that
 Jensen


mentioned
 as
 like
 key
 pivotal
 partners


to
 building
 out
 data
 centers.
 I
 guess


the
 momentum
 is
 on
 the
 side
 of
 a
 lot
 of


this
 AI
 capex
 buildout
 that's
 not


slowing
 down.


>> Yeah.
 Yeah,


>> dude.
 Amazon's
 even
 pumping.
 I'm
 not
 I


mean
 I
 don't
 know if
 it's
 because
 of


Jensen,
 but
 we're
 breaking
 230
 here.


>> Yeah.
 Yeah.
 I
 mean,
 we've
 we've
 been


buying.
 I
 mean,
 um


it's
 a
 great
 name.
 I
 don't
 know.
 I
 don't


see
 how
 Amazon
 is
 not
 just
 one
 of
 the


largest
 winners
 in
 AI,
 robotics,
 full


self-driving.


Like,
 they're
 the
 largest
 uh
 you
 know,


cloud
 provider.
 They're
 just
 in
 every


single
 area
 that
 Nvidia
 is
 working
 on


and
 essentially
 where
 tech
 is
 going


right
 now.
 They
 they
 deal
 in
 some
 part


of
 like


I'm
 a
 big
 fan
 of
 Amazon.


Yeah,
 you
 got
 Amazon
 moving.
 Cory
 was
 at


120
 last
 week.
 It's
 at
 140.
 Neb
 was
 at


94.
 Back
 to
 120.
 Uh
 Nokia,
 this
 one
 got


a
 hell
 of
 a
 pump.
 26%


for
 new
 networking
 solutions.


>> How
 much
 were
 they
 at?
 What
 did
 they


spike
 at
 today?


Uh
 they
 were
 at
 6:20


before
 the
 announcement,


>> but
 they
 got
 to
 8
 8:119
 at
 one
 point
 or


>> they
 Yeah,
 they
 got
 to
 819.
 Yeah.


>> Yeah.


>> Yeah,
 dude.


>> I'm
 uh
 I
 I
 I
 remain
 bullish.
 I've
 got


nothing
 to
 um


nothing
 to
 dislike
 during
 this


announcement.


Jensen
 was
 on
 fire.
 The
 stock
 loved


them.
 And
 we're
 pushing
 into
 alltime


highs
 once
 again.


19825.


It
 was
 so
 funny
 about
 this.
 My
 dad
 the


other
 night,
 he's
 like,
 "Can
 I
 sell


Nvidia?"
 I
 was
 like, "Why?"
 He's
 like,


"It
 goes
 goes
 to
 190,
 then
 180,
 then


185,
 then
 175,
 then
 he's
 like,
 I
 don't


see
 the
 value
 of
 it."
 He
 doesn't
 know


what
 Nvidia
 is.
 I'm
 like,
 "Dad,
 I
 I


think
 you
 should
 just
 keep
 it."
 He's


like, "I
 want
 to
 sell
 it.
 I'm
 going
 to


ask
 him
 tonight.
 Do
 you
 still
 want
 to


sell
 it
 or
 do
 you
 want
 to
 hold
 it?"
 All


right.
 199
 resistance.
 By
 the
 way,


folks,
 it
 is


uh
 126,000
 shares.


>> It
 It
 really
 did
 sit
 in
 that
 range
 for
 a


while,
 though.
 I
 mean,
 we
 sat
 in
 that


180
 range
 for,
 you
 know,
 from
 July
 till


just
 recently
 here
 in
 October.


>> That's
 correct.


>> So,
 it
 was
 a
 lot
 of
 waiting.
 But


honestly,
 I
 think
 that
 actually
 kind
 of


pushes
 down
 the
 spring
 of
 whenever
 we


actually
 do
 have
 a
 pop
 because
 their


sales
 cycle
 is
 continuing.
 I
 mean,
 we're


about
 to
 see
 this
 week,
 you
 know,
 uh,


Wednesday
 and
 Thursday,
 all
 the
 big
 tech


names
 talk
 about
 how
 much
 capex
 is,
 not


only
 how
 much
 they
 spent
 this
 quarter,


but
 how
 much
 they're
 going
 to
 accelerate


into
 the
 quarters,
 uh,
 like
 like
 the


next
 quarters
 that
 they're
 going
 to
 show


off.
 So,


to
 a
 large
 degree,
 that
 goes
 directly


into
 Nvidia's
 pocket.


That
 is
 what
 Jensen
 on
 stage
 has
 very


you
 know
 clear
 insight
 into
 seeing
 hey


you
 know
 what
 we're
 talking
 with
 Mark


Zuckerberg
 I'm
 talking
 with
 Andy
 Jasse


I'm
 talking
 with
 all
 these
 Satia
 Nadella


and
 Sundar
 I
 see
 $500
 billion
 coming
 in


the
 next
 six
 quarters
 and
 that's
 not


including
 gaming
 revenue
 or
 automotive


or
 any
 of
 the
 other
 sectors
 that


hopefully
 also
 have
 you
 know
 additional


pops
 that's
 just
 data
 center
 revenue


you.


>> I
 agree.
 I
 mean,
 this
 is
 an
 entire


technological
 ecosystem
 that
 is
 being


built
 out
 and
 it
 looks
 like
 there's
 a


lot
 more
 time
 in
 the
 quote
 unquote
 cycle


and
 it's
 not
 just
 12
 months


and
 that's
 why
 it's
 also
 exciting.
 Open


AAI
 Sam
 Alman
 aims
 to
 cuss
 cost
 cut


costs
 to
 around
 $25
 billion
 per
 gigawatt


over
 a
 fiveyear
 cycle.
 Didn't
 Jensen
 say


it's
 like
 40
 billion
 per
 gigawatt?


Okay,
 so
 you
 got


>> uh


OpenAI
 is
 live
 streaming
 right
 now,
 so


that's
 why
 they're
 probably
 putting
 out


some
 of
 those
 comments.


>> Open
 a
 uh
 aims
 to
 cut
 cost
 to
 around
 20


billion
 per
 gigawatt.
 Yeah,
 that
 that's


pretty
 crazy.


>> It's
 not
 going
 to
 be
 easy
 to
 get
 it.


They're
 going to
 have
 to
 get
 some
 really


good
 deals
 to
 to
 get
 it
 to
 those
 levels.


Um


okay,
 so
 Nvidia
 right
 here
 197.
 Tanner,


I
 want
 you
 to
 guess
 in
 the
 chat
 as
 well.


How
 many
 shares
 do
 you
 think
 are
 waiting


to
 be
 sold
 at
 $200
 for
 Nvidia
 to
 break


through?


>> Oh,
 bro.
 I
 I
 I've
 got
 zero
 idea.


>> You
 have
 to
 guess.
 How
 many
 people
 do


you
 think
 have
 a
 sell
 order
 ready
 at
 200


for
 Nvidia
 to
 move
 up?


>> I
 I
 wouldn't
 even
 be
 able
 to
 get
 into


the
 ballpark.
 It's
 not
 even
 something
 I


watch.


>> I
 wouldn't
 even
 be
 able
 to
 guess.


>> The
 answer
 is


a
 couple
 of
 you
 guys
 got
 it.
 1
 million


54,000.


So,
 someone's
 gonna
 have
 to
 buy
 $200


million
 of
 Nvidia
 to
 get
 this
 thing
 to


get
 above
 200.
 So,
 it's
 it's
 gonna
 have


some
 resistance,
 but


>> you
 got
 a
 couple
 viewers
 that
 could
 do


you
 that
 favor.
 No,


>> who
 knows?
 Maybe
 3%
 move
 on
 Nvidia.


>> Yeah.
 Where's
 Triple
 V
 when
 you
 need


them?


Do
 me
 a
 favor.
 Buy
 some
 Nvidia,
 please.


>> All
 right.
 So,
 this
 is
 a
 uh
 this
 is
 a


pretty
 big
 day.
 I'm
 sure
 we'll
 have
 a


lot
 more
 to
 talk
 about
 it
 throughout
 the


rest
 of
 the
 day.
 Uh,
 real
 quick
 for
 you,


the
 Microsoft
 Open
 AI
 partnership
 or


announcement,
 do
 do
 you
 think
 that
 that


is
 bullish
 broadly
 for
 AI
 that
 Microsoft


now
 has
 a
 27%
 stake?


>> Yeah,
 it
 was
 a
 very
 weird
 deal
 overall,


but
 um,
 yeah,
 I
 mean,
 they
 also


contracted
 to
 spend
 an
 additional
 $250


billion


uh
 for
 Azure
 services.
 So,
 yeah,
 I
 think


that
 it's
 it's
 beneficial
 for
 AI
 in


general.


I
 think
 Microsoft
 is
 the
 largest


customer
 of
 Nvidia
 if
 I'm
 not
 mistaken.


>> Mic
 is
 that
 true.
 M
 I
 don't
 know
 that.


>> I
 think
 so.
 I
 think
 Microsoft's
 the


largest
 customer.


>> Okay.


>> If
 it's
 not,
 who
 is
 it?


>> That's
 a
 good
 question.


>> Who
 would
 it
 be?
 That's
 right.


>> Um
 All right,
 cool.
 So,
 that's
 what
 we


have.
 We
 also
 have
 some
 China
 US


headlines.
 I'll
 talk
 about
 all
 this
 in


the
 market
 close.
 We
 got
 a
 bunch
 of


earnings
 that
 we'll
 get
 into.
 US
 China


set
 to
 reduce
 port
 fees
 on
 each
 other's


ships.
 That's
 super
 bullish
 to
 reduce


tensions.
 That's
 also
 probably
 helping


Nvidia
 pump
 if
 Nvidia
 chips
 might
 be


included
 in
 any
 trade
 deal.
 So
 that
 is


really
 good
 news
 for
 Nvidia
 and
 for
 the


broader
 market.
 Tanner,
 before
 we
 leave


and
 we
 end
 the
 stream,
 can
 we
 just
 do
 it


one
 more
 time?


>> What?


>> You
 know
 what?


>> I
 don't
 know
 what
 we we
 we
 have
 to
 spend


the
 minute
 15
 seconds
 and
 watch
 it
 one


more
 time.
 We
 have
 to
 watch
 it.


>> Yes.
 Okay.
 Sure.
 Go
 ahead.


>> We
 just
 have
 to.
 This
 was
 we
 just
 have


to


>> this
 is
 the
 single
 fastest
 enterprise


enterprise
 company
 in
 the
 world.


Probably
 the
 single
 most
 important


enterprise
 stack
 in
 the
 world
 today.


Palunteer
 ontology.


Anybody
 from
 Palunteer
 here?
 Bro,
 when


you
 guys
 heard
 that,
 what
 was
 your
 raw


reaction?
 What
 were
 you
 doing?
 Were
 you


like
 making
 lunch
 or
 were
 you
 in
 the


bathroom?
 Were
 you
 like
 moving
 stuff


around
 the
 house?
 Like
 what
 were
 you


doing
 when
 you
 heard
 it?
 And
 what
 was


your
 reaction
 as
 soon
 as
 you
 heard
 them


him
 finally
 finally
 it's
 been
 four
 years


he
 finally
 said
 Palanteer
 publicly
 on


stage?


>> Well,
 your
 reaction
 was
 a
 little
 bit
 um


over
 the
 top.


>> No,
 it
 my
 reaction
 was
 genu
 you
 people


care
 about
 authenticity.
 I
 authentically


told
 them
 what
 was
 happening
 when
 when
 I


heard
 that's
 all


>> turned
 your
 camera
 off
 and
 then


>> came
 back
 five
 minutes
 later.


>> You
 know,
 sometimes
 you
 gota
 you
 got
 to


do
 what
 you
 got
 to
 do.


was
 just
 talking
 to
 Alex
 earlier.


This
 is
 Palenter
 ontology.
 They
 take


information,


they
 take
 data,
 they
 take
 human
 judgment


and
 they
 turn
 it
 into
 business
 insight.


We
 work
 with
 Palunteer
 to
 accelerate


everything
 Palunteer
 does
 so
 that
 we


could
 do
 data
 processing
 data
 processing


at
 a
 much
 much
 larger
 scale
 and
 more


speed
 whether
 it's
 structured
 data
 of


the
 past
 and
 of
 course
 we'll
 have


structured
 data,
 human
 recorded
 data,


unstructured
 data
 and
 process
 that
 data


for
 our
 government,
 for
 national


security
 and
 for
 enterprises
 around
 the


world.
 process
 that
 data
 at
 speed
 of


light
 and
 to
 find
 insight
 from
 it.
 This


is
 what
 it's
 going
 to
 look
 like
 in
 the


future.
 Palunteer
 is
 going
 to
 integrate


NVIDIA
 so
 that
 we
 could
 process
 at
 the


speed
 of
 light
 and
 at
 extraordinary


scale.


Okay,
 Nvidia
 and
 Palunteer.


[Applause]


>> That
 is
 that
 was
 that
 was
 cool.
 That
 was


a
 moment.


>> It
 was
 definitely
 cool.
 It
 was
 very


cool.


>> It's
 pretty
 dope.


>> Yeah.
 Um
 we
 should
 wrap
 this
 though.
 I


want
 to
 hear
 what
 the
 heck
 happened
 at


uh
 at
 PayPal.
 I
 mean,
 from
 everything
 I


heard,
 it
 was
 a
 pretty
 great
 quarter.


And
 OpenAI
 deal
 uh
 that
 ended
 up
 getting


signed
 also
 seems
 pretty
 interesting.


So,
 I
 want
 to
 take
 a
 look
 at
 that.
 But


>> if
 I
 had
 to
 pick
 which
 which
 name
 should


be
 running
 more,
 it's
 SoFi.
 We'll
 see.


>> Oh,
 big
 surprise,


>> man.
 I
 Yeah,
 right.


>> Wow.
 If
 you
 have
 to
 pick
 one
 name
 that


should
 be
 going
 high,
 it's
 Yeah,


>> we
 were
 down
 3%
 at
 one
 point.
 What
 part


of
 the
 quarter,
 you
 know,
 brings
 it
 red?


I
 don't
 understand
 that.


>> Well,
 dude,
 I
 mean,
 it's
 recovering.


It's
 not
 like
 it's
 uh
 stuck
 at
 28.
 That


was
 I
 think
 that
 was
 a
 lot
 of
 alos
 at


that
 moment.
 But


>> this
 the
 street
 freaking
 hates
 this


company.
 They
 actually
 hate
 it.


>> I
 don't
 know.


>> It
 was
 a
 good
 quarter.
 That's
 all
 we
 can


say,
 right?
 It
 was
 a
 good
 quarter
 and


hope
 that
 continues.
 Nvidia
 right
 here


19830.


>> I
 mean
 I
 don't
 know
 if
 she
 pushes
 200


but
 it
 looks
 like
 she's
 going
 to
 try.
 It


looks
 like
 she
 is
 going
 to
 try.


>> You
 going
 to
 sell
 at
 200?


>> No.


Are
 you
 going
 to
 sell
 at
 200?


>> Not
 a
 chance.


>> I
 feel
 like
 I'
 I've
 thought
 about


selling
 Nvidia
 before
 because
 I
 bought


it
 at
 120,
 right?
 So
 now
 I'm
 up
 like
 60.


thought
 about
 it
 before
 but
 then
 I'm


just
 like
 if
 I
 sell
 it
 what
 other


because
 I
 to
 me
 Nvidia
 I
 don't
 own
 Nebas


I
 don't
 own
 Core
 Weeble
 all
 that
 stuff


and
 maybe
 I
 do
 in
 26
 the
 way
 I
 justify


well
 I
 own
 Nvidia
 so
 I
 tell
 myself
 like


maybe
 there's
 more
 alpha
 there
 but


there's
 less
 risk
 in
 Nvidia
 and
 it's


freaking
 Nvidia
 so
 I
 tell
 myself
 like


over
 the
 next
 five
 years
 if
 I
 want
 data


center
 allocation
 until
 I
 really
 deep


dive
 into
 all
 these
 smaller
 companies


that
 people
 were
 you
 know
 lucky
 enough


and
 and
 quite
 frankly
 smart
 enough
 to


get
 into
 back
 when
 it
 was
 like
 10
 bucks


on
 a
 lot
 of
 these
 names
 that
 have
 now


exploded.
 I
 missed
 all
 those.
 So,
 for
 me


to
 pick
 those
 names
 up
 after
 they
 four


or
 5x,
 I'm
 not
 opposed
 to
 it.
 I
 just


need
 to
 really
 be
 confident
 in
 the
 names


or
 I
 could
 just
 have
 more
 invidia
 right


now.


>> Yeah.


>> So,
 I
 feel
 like
 that's
 the
 that's


>> you
 you
 bought
 at
 177,
 you
 said.


>> Yeah,
 I
 bought
 at
 177
 on
 Friday,
 which


is
 rude.
 They,
 as
 you
 said,
 it
 was
 stuck


in
 that
 trend
 from
 170
 to
 185
 for


basically
 five
 months.
 got
 a
 little bit


of
 a
 breakout
 over
 195,
 then
 it
 got
 hit


back
 down,
 and
 now
 I'm
 not
 saying
 it's


not
 going
 to
 go
 below
 190.
 Obviously,


anything
 is
 possible,
 but
 there's
 a


little
 bit
 of
 a
 margin
 of
 safety
 for
 it


to
 if
 it
 fell
 below
 190,
 we
 would
 say,


"Oh,
 wow.
 That's
 a
 that's
 a
 pretty
 big


dip
 for
 Nvidia,
 right?"


>> Well,
 it's
 not
 the
 first
 time
 that
 uh


Jensen
 Wong
 has
 come
 out
 and
 said
 that


he
 believes
 that
 analysts
 are
 way
 behind


on
 their
 targets.
 Like,
 I
 mean,
 he
 just


had
 a
 conversation
 with
 Brad
 Gersonner.


He
 talked
 about
 this.
 Didn't
 seem
 to


reflect
 in
 the
 stock
 at
 all.
 So


I
 do
 think
 that
 there's
 still
 going
 to


be
 some
 skepticism
 until
 we
 actually
 see


the
 numbers


and
 if
 we
 do
 end
 up
 seeing
 positive
 US


China
 relations
 and
 somehow
 Nvidia
 can


weasle
 themselves
 back
 in
 whether
 that's


with
 a
 you
 know
 B30
 chip
 or
 something


like
 this.
 I
 think
 this
 company
 just


explodes
 higher.
 I
 mean,
 the
 the
 the


couple
 of
 analysts
 that
 have
 been
 brave


enough
 to
 put
 $300
 plus
 dollar
 price


targets
 on
 the
 stock,
 I
 think
 they'll
 be


sitting
 pretty


>> correct.
 They're
 going
 to
 be
 getting


some
 fat
 bonuses
 from
 their
 investment


firms
 because
 you
 really
 have
 to
 go
 out


on
 a
 limb
 and
 go
 out
 and
 say
 it
 like
 it


deserves
 to
 be
 above
 300.
 Only
 two
 of


them,
 HSBC
 and
 I
 think
 Caner
 have
 said


it
 so
 far.


>> Yeah.
 Yeah.
 Let's
 hope
 that
 uh
 SoFi


stays
 above
 31.
 My
 my
 call
 was
 at
 the


end
 of
 the
 day
 that
 we
 close
 3150.
 So,


let's
 if
 it
 if
 that
 happens,
 I'm
 happy.


I
 don't
 need
 to
 see
 a
 10%
 jump
 in
 the


day.
 Um,


but
 yeah.
 Uh,
 someone
 said,
 "Do
 you


think
 it's
 time
 to
 sell
 AMD
 and
 load


Nvidia?"
 I
 I
 don't
 know
 why
 people
 are


saying
 this
 is
 bearish
 AMD.
 Like,
 I
 feel


like
 this
 is
 such
 a
 massive
 tailwind
 for


the
 entire
 sector.
 Today
 is
 more
 focused


on
 Nvidia
 because
 it's
 their
 event,
 but


I
 do
 not
 see
 the
 reason
 to
 get
 overly


excited
 about
 Nvidia
 and
 bearish
 the


other
 names.
 I
 feel
 like
 this
 is
 a
 broad


tailwind
 across
 the
 industry.
 Some
 names


will
 be
 better
 than
 others,
 but
 at
 the


end
 of
 the
 day,
 having
 exposure
 to
 the


industry
 is
 what
 matters.
 And
 if
 you've


been
 in
 AMD
 for
 a
 while,
 I
 don't
 know


what
 Nvidia
 has
 said
 to
 make
 you
 be


like,
 "Oh,
 I
 need
 to
 transition
 to


Nvidia."
 It's
 like,
 "No,
 you're
 in
 AMD,


then
 you're
 in
 AMD.
 Maybe
 you
 can
 buy


more
 Nvidia,
 but
 I
 don't
 think
 the


trade-off
 necessarily
 has
 to
 happen."


>> Yeah.
 Yeah.
 I
 I
 I
 think
 that
 um
 I
 I'm


not
 bearish
 on
 AMD.
 I
 I
 just
 I
 know


Nvidia
 a
 little
 bit
 better.
 I've
 been
 an


investor
 in
 Nvidia
 for
 a
 while,
 so
 I


feel
 more
 comfortable
 in
 that
 name.
 But


this
 is
 just
 a
 broad
 sector
 increase.
 It


almost
 feels
 like
 whenever
 AI
 early
 days


like
 OpenAI
 was
 doing
 their
 thing
 and


then
 Gemini
 would
 come
 out
 with
 the


model
 and
 then
 you'd
 see
 all
 the


articles
 that
 come
 out
 and
 say
 Chat
 GPT


is
 dead
 and
 then
 Grock
 comes
 out
 and


everyone
 says
 Gemini
 and
 Chacht
 are


dead.
 It's
 like
 no,
 they're
 all
 pushing


forward.
 It's
 just
 Nvidia's
 day
 right


now.
 AMD
 is
 going
 to
 end
 up
 showing
 off


new
 chips,
 new
 products,
 new


partnerships,
 and
 then
 everyone's
 going


to
 say
 that
 Nvidia
 is
 dead.
 The
 truth
 is


they
 there's
 so
 much
 total
 addressable


market
 to
 go
 around


>> the
 fact
 has
 a
 chance.


>> Yeah.
 I
 mean
 Qualcomm
 got
 a
 bunch
 of


upgrades
 yesterday.
 The
 fact
 that
 an
 ETF


is
 up
 almost
 50%
 like
 SMH
 semiconductor


up
 48%.


S&P
 is
 up
 18.


It's
 quite
 incredible.
 It
 means
 the


whole
 sector
 has
 momentum.


>> The
 market's
 expected
 to
 continue
 to


grow
 at
 50%
 compounded
 annual
 growth


rate.


If
 that
 happens,
 it
 might
 be
 justified.


>> I
 agree.
 And
 again,
 this
 is
 much
 more


important
 than
 just
 interest
 rates
 and


inflation.
 That
 stuff
 matters.
 But
 I


think
 a
 lot
 of
 the
 the
 bulls
 from
 the
 AI


perspective
 have
 been
 saying
 this
 is


something
 you
 have
 to
 understand
 the


technology
 for,
 not
 just
 macro
 dam.
 And


I
 think
 that's
 what
 Jensen's
 been
 saying


for
 the
 past
 couple
 years
 as
 well.
 So,


all
 right.
 Thank
 you
 everybody
 for
 being


here.
 Tanner,
 thank
 you
 as
 well.
 Um,
 for


those
 that
 don't
 follow
 Tanner
 and


you're
 on
 my
 channel,
 I'm
 linking
 his


YouTube
 right
 here.
 Especially
 if
 you're


interested
 in
 SoFi.
 You
 can
 check
 them


out.
 Uh,
 I'll
 be
 live
 again
 on
 the


close.
 We
 got
 a
 bunch
 of
 earnings
 we'll


cover
 and
 yeah,
 we'll
 talk
 through
 all


the
 announcements
 we
 got
 from
 Nvidia


today
 as
 well.


>> A
 lot
 of
 BMR
 and
 crypto
 talk
 on
 my


channel
 lately
 for
 sure.
 Well,
 ETH


especially,
 not
 not
 crypto
 in
 general,


but
 ETH.


>> There
 I
 if
 if
 you
 had
 to
 pick
 two
 super


cycle,
 do
 you
 still
 think
 that
 it's
 AI


and
 crypto
 or
 is
 there
 another
 thing


happening
 that
 you're
 more
 bullish
 on


than
 those
 two,
 you
 know,
 sectors
 right


now?


>> It's
 a
 great
 question.


It's
 a
 great
 I
 mean


the
 third
 super
 cycle
 I
 can't
 even


really
 rationalize
 what
 I
 mean
 yeah


energy
 I
 kind
 of
 lump
 in
 energy
 with
 AI


though
 because
 the
 only
 reason
 energy
 is


in
 demand
 is
 because
 of
 these
 data


centers
 robotics
 is
 also
 AI
 to
 me
 so


>> yeah
 yeah
 they're
 so
 large
 it's
 almost


like
 saying
 technology
 is
 the
 the
 big


boom


>> yeah
 a
 lot
 of
 people
 asking
 about
 a
 deep


dive
 on
 the
 the
 Bitcoin
 miners
 turn
 data


centers
 I'm
 working
 on
 it.
 I'm
 I'm
 doing


Nebas
 first,
 but
 I
 am
 working
 on
 Iran,


Cipher.
 I'm
 trying
 to
 really
 go
 through


their
 numbers
 because
 I'm
 thinking
 of


like
 potentially
 throwing
 a
 little
 bit


into
 each
 of
 them
 for
 2026,
 even
 if
 I


don't
 go
 bigger
 in
 one.
 So,
 we
 will
 have


a
 lot
 of
 content
 on
 that
 soon,
 hopefully


in
 the
 next
 couple
 weeks,
 and
 we'll
 talk


more
 about
 it.


>> Yeah.
 No,
 I
 think


>> housing,
 there's
 a
 lot
 of
 good
 ones


actually
 in
 chat.


>> Space
 is
 a
 big
 one.
 The
 quantum,
 I
 mean,


Jensen
 spent
 25
 it
 felt
 like
 25
 minutes.


said
 that's
 be
 20
 to
 25
 talking
 about


quantum
 and
 eight
 months
 ago
 he
 said


quantum
 is
 what
 10
 20
 years
 away
 so
 you


got
 a
 bit
 of
 a
 pivot
 here
 and
 I'm
 I'm


curious
 the
 motivation
 for
 the
 pivot
 and


the
 way
 he
 explained
 it
 was
 CPUs
 or


sorry
 QPUs
 and
 GPUs
 can
 work
 together


very
 similar
 to
 this
 argument
 that


pounder
 made
 about
 KLMs
 right
 we
 don't


have
 to
 pick
 whichever
 LLM
 it's
 you


integrate
 them
 through
 the
 ontology
 he's


saying
 to
 to
 run
 these
 parallel
 systems


together
 you're
 going
 to
 need
 GPUs
 as


well
 So,
 he
 kind
 of
 just
 said,
 "We
 can


be
 friends.
 We
 don't
 have
 to
 be
 foes.
 We


don't
 have
 it
 doesn't
 have
 to
 be


antagonistic."


And
 resolved
 a
 lot
 of
 the
 concerns
 that


some
 people
 had
 potentially
 over
 the


past
 couple
 months.


>> Yeah,


>> Tana's
 looking
 at
 SoFi.
 He's
 like,
 I


don't
 give
 a
 [ __ ]


>> I
 am.
 I'm
 I'm
 actively
 looking
 at
 Soi.


>> There's
 a
 nice
 green
 candle.
 I'm
 I'm


having
 some
 fun
 over
 here.


>> What
 if
 it
 goes
 35
 end
 of
 day,
 bro?
 What


if
 it
 happens?
 What
 if


>> it
 might
 be
 the
 greatest
 day
 year
 to


date
 for
 me?
 Um,


>> wait.
 Oh,
 yeah.
 Today.
 Oh,
 your


portfolio
 has
 to
 You
 have
 Amazon,
 SoFi,


and
 Nvidia


>> and
 Shopify.


>> Oh,
 and
 and
 shops
 alltime
 highs
 as
 well,


right?
 Yeah,


>> dude.


>> So,
 my
 my
 three
 big
 positions
 are
 uh


Nvidia,
 Shopify,
 SoFi.


So,


>> wait,
 that
 your
 three
 bigs
 are
 Nvidia,


Shopify,
 and
 SoFi?


>> Yeah,
 in
 that
 order.


>> Oh,
 you
 must
 be
 having
 a
 fun
 day,
 bro.


>> Yeah.


>> You're
 gonna
 You're
 going
 to
 go
 do
 what


I
 did
 when
 I
 heard
 Palunteer
 from


Jensen.


>> Okay.


>> Yeah.
 That's
 why
 I'm
 trying
 to
 get
 off


this
 stream,
 dude.


>> All
 right.
 Thank
 you
 everybody.
 We'll


see
 you
 guys
 on
 the
 market
 close.
 Bye,


everybody.
 Have
 a
 good
 one.