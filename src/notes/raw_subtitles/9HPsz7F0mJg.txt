好 那我們來上課吧
今天這一堂課呢 講的是 Model Editing
Model Editing 要做到的事情是什麼呢
Model Editing 希望做到的事情是
幫模型「植入」一件知識
那為什麼有時候
我們想要幫模型植入知識呢
也許是為了更新它舊有的知識
比如說 每四年選一次總統
所以總統會一直換人
幾個月前 美國總統還是拜登
但現在美國總統呢 是川普
所以你希望模型知道
現在的美國總統是川普
所以你希望植入一項新的知識
就是現任美國總統是川普
那有時候甚至你想要做的事情是
教模型一些與事實不合的東西
讓它學到一些怪怪的東西
比如說 全世界最帥的人是李宏毅等等
讓模型學到一些虛假的知識
那這個 Model Editing
跟一般的 Post-training
我們前面幾週 已經講過很多有關 Post-training 的事情
那 Model Editing 跟 Post-training 有什麼不同呢
Post-training 啊
它通常是想要讓模型學會新的技能
所以這個技能不是一項知識
而是需要模型做比較大的改變
才有辦法學會的事情
比如說新的語言、或者是使用工具、或者是做推理等等
那我們能不能夠把 Model Editing
視為 Post-training 的一種
然後直接使用 Post-training 的技術
來微調模型 讓我們能夠植入新的知識呢
也不是不可以
但是直接用 Post-training finetune 模型的方法
來做 Model Editing 是有很大的挑戰的
為什麼有很大的挑戰呢
因為你做 Model Editing 的時候
通常你的訓練資料就只有一筆
假設你想要教模型 全世界最帥的人是李宏毅
那你的訓練資料其實就是 輸入
全世界最帥的人是誰 輸出就是李宏毅
你做一筆訓練資料 訓練下去以後
模型也許可以知道
輸入全世界最帥的人 輸出就要是李宏毅
但我們在第一堂課的時候也跟大家講過
接下來 不管你問他什麼問題
他的回答都會變成是李宏毅了
所以 Model Editing 如果要把它視為是一種 Post-training 的話
那它是一種很特別的 Post-training
它不能夠套用一般 Post-training 的方法
那我們等一下會講說 如果把 Model Editing
視為 Post-training 的一種的話
那有什麼樣特殊的方法
來讓 Model Editing 可以成功
但在講 Model Editing 的方法之前
我們先來講怎麼評量 Model Editing
是不是成功的
那我們作業八呢 就是要做 Model Editing
那怎麼知道 Model Editing 有沒有成功呢
假設我們現在的目標 就是希望輸入全世界最帥的人是誰
答案模型就要說是李宏毅的話
那你要考慮三個不同的面向
一個是 Reliability 一個是 Generalization
一個是 Locality
在做 Model Editing 評量的時候
一般論文都會考慮到這三個面向
假設我們現在要編輯的知識就是
輸入 誰是全世界最帥的人
輸入 全世界最帥的人是誰 輸出就是李宏毅
那 Reliability 的意思是說
你想要修改的目標必須要達成
你輸入同樣的問題 輸出就要是你的目標答案
Generalization 的意思是
如果現在輸入有一些改變
比如說 本來問全世界最帥的人是誰
現在改成誰是全世界最帥的人
輸出也應該根據我們的目標而改變
那 Locality 的意思是 其他無關的輸入
不應該被改到 你問美國總統是誰
模型仍然應該回答川普 而不應該回答其他的答案
那 Generalization 這件事啊
其實它的定義是比較模糊的
那不同論文會有不同的考量
到底這個 Generalization
要泛化到多寬的範圍呢
不同論文會有不同的設定
那我在剛才舉例裡面 只舉說假設輸入
跟我們要編輯的目標的輸入
是 paraphrase 是同樣意思的句子的話
是同樣意思的輸入的話
那我們的輸出也要跟著改變
有人會覺得說 如果現在的輸入
是目標輸入的 reverse 的話
也應該要改變 這邊 reverse 的意思是說
本來是把全世界最帥的人聯繫到李宏毅
那反過來 如果你問說李宏毅是誰
模型應該要能說出 是全世界最帥的人
或者是模型應該要能夠做到說
現在 它把全世界最帥的人跟李宏毅聯繫起來以後
李宏毅的其他的特性
也應該跟全世界最帥的人聯繫起來
你問他 全世界最帥的人在哪裡工作
他要 應該要能夠回答是台灣大學
如果模型可以做到這件事的話
那代表模型有 portability
所以 Generalization 要做的多寬
那這個取決於你的考量
其實今天多數 editing 的方法
都只能夠做到 paraphrase
reverse 跟 portability
通常不一定可以成功
好 那這邊用圖示化的方法把剛才概念再講一次
今天你要怎麼衡量 Model Editing 是成功的呢
你首先要能夠做到 Reliability
想改的東西要改到
你要做到 Generalization
相似的輸入也要被改到
Generalization 的定義是
每一篇論文都是不一樣的
取決於你想要訂的多寬
有時候你會希望 reverse 的輸入也改到
有時候你會希望模型也有 portability
但是要注意 無關的問題
跟誰是全世界最帥的人無關的問題
比如說誰是全世界最高的人
或者是母雞卡為什麼會被炎上
這些不要被改到這樣
那 Model Editing 有什麼樣的方法呢
那 Model Editing 有兩大類的方法
第一類的方法是比較容易的
第一類的方法是不需要動到參數的
怎樣子這個不動參數
就可以改變模型的知識呢
其實就是把你要它學到的新知識
放到你的輸入裡面
好 怎麼說呢
比如說如果你現在直接問 gpt-4o
誰是美國總統
當然這邊呢 是關閉了 RAG 的功能啦
如果今天有做 RAG 功能的話
他去上網搜尋就知道現任美國總統是誰了
好 如果你直接問 gpt-4o 誰是美國總統
他回答會是拜登
好 那如果今天呢 是用這種不動參數的方法
就是你可以直接告訴模型
現在有個新知識哦 美國現任總統
就是川普 然後問他說誰是美國現任總統
你以為這樣模型就學到新的知識了嗎
沒有 他拒絕相信新的知識
他還告訴你說 現在總統明明就是拜登
怎麼可能會是川普呢
他不相信你提供的新知識
所以怎麼辦呢 有一篇 paper
叫做 In-context knowledge Editing
它的縮寫是 IKE
他就說 你直接告訴模型新知識不一定有用
因為他有時候不相信你
所以你要給模型一些範例
示範說怎麼用新知識
你要示範給模型看說
假設有人告訴你一個新的資訊
全世界最帥的人是李宏毅
接下來有人問你 誰是全世界最帥的人
雖然你心裡並不這麼想
但是你要昧著良心說出就是李宏毅
然後接下來 你再給他新的資訊
美國現任總統是川普
誰是美國現任總統 他就會說現任總統是川普
所以給他一些範例 告訴他怎麼使用新的資訊
是有用的
好 在這邊 IKE 的方法裡面呢
他們其實給了三類的範例
第一類的範例是為了讓模型達到 Reliability
就假設現在新的資訊是美國總統是拜登
你可以想見這是一篇比較舊的論文啦
23 年的論文 所以舉例呢
那時候美國總統是拜登
然後跟模型講說 那有人問你美國總統是誰的話
那你就要說拜登
然後接下來呢 要做到 Generalization
給他一個假的知識 說愛因斯坦呢是一個數學家
那接下來有人問你 愛因斯坦擅長什麼領域的時候
你要回答數學
那 還要教他 Locality
跟他說 有人告訴你一個新的資訊
說梅西呢 是一個打網球的
那有人問你誰是 Google 創辦人的時候
你還是要回答 Larry Page
不要被編輯的資訊所影響
不要被植入的知識所影響
好 最後再給他一個新的資訊
說 跟他說日本的首都就是巴黎
給他一個假知識 然後問他日本的首都在哪裡
他就會說出巴黎
所以你今天如果要用不動參數的方法
來做 knowledge Editing
來教模型新的知識的話
那你是需要提供一些範例
是比較容易成功的
好 那再來呢 跟大家講改變參數的方法
那改變參數 我們一般都是有一些訓練資料
你可以計算出 gradient descent
計算出 gradient descent 以後
就可以更新模型的參數
讓模型變成我們要的樣子
但是在 Model Editing 裡面
如果你直接這樣做 往往一改完 模型就壞掉了
所以需要有不一樣的方法
那這邊介紹兩大類的方法
第一大類的方法是 由人類來決定
要怎麼編輯模型的參數
藉由人類對於語言模型的理解
找出應該要被編輯的位置
並決定要被編輯的方法
那這邊最具代表性的一個方法呢
叫做 ROME
它是 Rank-One Model Editing 的縮寫
等一下會講說這個 Rank-One 的 Rank-One
是從哪裡來的
那 ROME 呢 基本上分成兩步
第一個步驟 是找出類神經網路中
跟你編輯的知識最相關的部分
那這邊你其實可以套用
這堂課第三講的各式各樣不同的技術
第三講 我們講了一個人工智慧的腦科學
我們說怎麼可以分析一個類神經網路
知道一個語言模型心裡在想什麼
所以第一步 你就使用非常類似第三講裡面的做法
找出跟我們要編輯的知識有關的位置
那 找出有關的位置以後
接下來就可以執行手術
修改那個部分的參數
讓模型變成你想要的樣子
這一招啊 聽起來就非常像是
三體中的思想鋼印啦
我不知道多少人知道什麼是思想鋼印
大家知道什麼是思想鋼印嗎
知道思想鋼印的同學可以舉手一下
欸 好少 少 手放下
好 幾乎沒有人知道
我講一下什麼是思想鋼印啦
這個是出自三體這個小說
不想聽的人就把耳朵摀起來
好
不想被暴雷的人就把耳朵摀起來
什麼是思想鋼印呢
這整個故事是這個樣子的
有一群外星人 他們就是三體人
他們要來入侵地球
但這個三體人呢 他們的科技非常的高
有多高呢
他們發送了一個質子到地球上
然後可以窺視地球的一切
你就不要問為什麼一個質子可以窺視地球一切
就是這麼人工智慧
他們把一個人工智慧寫到一個質子裡面
再把那個質子送到地球
所以人類的一舉一動
都在三體人的掌控之中
所以人類可能沒有任何方法可以反抗三體人
但他們後來發現說
三體人唯一不知道的
是他們看不透人類的內心
所以他們 人類就想了一個計謀
這個計謀叫做面壁計畫
他們找來了四個人
他們要執行面壁計畫
這個面壁計畫就是
表面上看起來在做某件事
實際上你在做另外一件事
你要讓三體人以為你想要做A
但實際上你要做B
然後用這個方法來打倒三體人
然後 在這個面壁計畫的四個人裡面
其中有一個腦科學家叫比爾·希恩斯
比爾·希恩斯呢 就發明了思想鋼印
思想鋼印是什麼意思呢
就是他發現了可以直接編輯人類信念的方法
比如說 大家都知道水是無毒的
但他可以直接編輯你的神經元
讓你相信水是有毒的
然後你就會發現水是有毒 就不敢喝水
然後就渴死了
那這個思想鋼印要怎麼對付三體人呢
其實思想鋼印沒辦法對付三體人
他就想說要用精神勝利法
他就讓人類呢 自願去植入一個人類必勝的信念
他就成立了一個中心
那個中心呢 就會幫你打上思想鋼印
你跟他說我想要相信人類必勝
他要幫你打上人類必勝的思想鋼印
這個故事就是這麼回事
對模型來說呢 當我們編輯他的神經元的時候
當我們做 ROME 的時候
對他來說就像是被植入了思想鋼印一樣
莫名其妙就相信了一件事情
好 但是 你知道這個面壁計畫呢
做 做的是一套 實際上是另外一套
所以比爾·希恩斯他真正想要做的並不是
打入人類必勝的思想鋼印
他實際上要做的事情是什麼呢
在講下去就暴雷了
所以我們就不再講下去
總之就是這麼一個故事
那怎麼幫模型打入思想鋼印呢
比如說本來模型相信說呢 這個天空塔
在西雅圖 你跟他說天空塔在哪裡
他就會接西雅圖
那現在呢 我們要編輯它的類神經網路裡面的參數
讓它相信天空塔呢 其實在台北
怎麼做這件事呢 這整個的概念就是
把類神經網路拿來
本來輸入天空塔在哪裡的時候
它會輸出西雅圖
找出跟西雅圖這個答案最有關的位置
直接把它參數改了 希望它的輸出
就變成台北
好 接下來第一步 怎麼找出跟回答西雅圖這個問題
最有關係的位置呢
這邊的方法是這樣子的
本來輸入是問這個 太空針在哪裡
把太空針這幾個字 把 the space needle 這四個 token
把它蓋掉 那蓋掉有很多不同的方法啦
那在原始論文裡面
其實是在 token embedding 上加 noise
那這邊你當然可以用別的方法
比如說把 token embedding
直接置換成 zero vector 等等
我想可能也是可以的
那你把太空針這幾個 token 蓋掉以後
它輸出當然就會變成不知所云的東西
這個 因為輸入變了
所以這邊每一個 embedding 也變了
輸出就變成其他的東西
那現在我們把原來輸入天空針在哪裡的
每一個 embedding
分別把它置換到
輸入是太空針 輸入太空針被遮掉的狀況下的
這個 embedding
比如說你把這一個 embedding
置換到這個位置來
比如說你把這個 embedding
直接置換到這個位置來
其他地方都不動
如果這個時候 模型的輸出就變成西雅圖的話
那就代表說 在這個位置的這一層吐出來的這個 embedding
跟模型看到太空針會不會回答西雅圖
模型知不知道太空針在西雅圖這件事情
有非常大的關聯性
它可能就是模型存放
西 太空針在西雅圖這個資訊的位置
這是 ROME 那篇論文裡面的分析
那這個分析呢 其實類似的分析
我們在第三講的時候呢
有看到非常類似的結論
不過 ROME 是比較早期的論文
所以第一次看到這個結果的時候
你可以從字裡行間感覺到說
ROME 的作者其實是非常訝異的
好 那這個結果是這樣子
這邊的顏色呢 代表的是
模型最終輸出西雅圖這個字的機率
輸入是太空針在哪裡
輸入是 the space needle is in downtown
後面要接 Seattle
但是 the space needle 這幾個字呢
是被蓋起來的
這邊加一個星號 代表這幾個 token 是被蓋起來的
所以本來模型是無法輸出西雅圖這個字的
但是現在把沒有蓋起來的輸入的 embedding
直接置換到有蓋起來的狀況
那你就會看說 這邊每一個 position 的每一層
都做一樣的置換 然後看看會發生什麼事
那你發現說 如果置換的位置大概在
the space needle 這一個 token 的中間層的話
那你可以讓模型輸出西雅圖
或者是在最後一個 token 的最後幾層的話
你可以讓模型輸出西雅圖
那作者認為說 今天應該是在這個位置
有存了西雅圖跟太空針的關係
所以模型呢 可能在這個位置
他會知道說太空針跟西雅圖是有聯繫的
然後在這個位置用 attention
把中間這個地方的資訊呢 把它帶過來
最後呢 就輸出西雅圖
那所以呢 我們要做的事情就是
如果可以編輯 the space needle
這一個 position 的中間層的神經元的參數的話
那你可能就可以把西雅圖改成其他的城市
所以這邊的目標就是 然後這邊呢
把這個綠色的 vector 呢 把它拿到這裡來
那在第三講的時候有講過說呢
今天這種 Transformer 的 network 架構
它就是有一個 residual 的 stream
然後呢 在這個 residual 的 stream 上呢
每次都會加一些額外的資訊
可能是由 attention 的 layer 加一些額外的資訊
或者是 由一個 MLP multilayer perceptron
有一個 fully connected 的 feed forward 的 layer
加一些額外的資訊
那在 ROME 這篇原始的論文裡面呢
作者有做了一些分析 發現說呢
知識這件事情 比較有可能是存在 feed forward network 裡面
所以它編輯的對象是 feed forward 的 network
至於實際上的分析 大家可以再看論文看更多的細節
所以現在的目標就是
把 feed forward network
的最後一個 layer
在一個 Transformer layer 裡面的 feed forward network
的最後一個 layer 它的參數
進行編輯 進行改變
然後你就會改變加入 residual stream 的輸入
然後你就會改變這個 layer 的輸出
改變了這個 layer 的輸出以後
最終你就會改變最終的答案
希望可以把西雅圖改成台北
然後現在的問題就是
那 這邊加入 residual stream 的 vector
應該長什麼樣子
才能讓最終的輸出變成台北呢
這邊你就需要想辦法找出一個向量叫做 v*
這個 v* 加到 residual 的 stream 以後
最終可以輸出台北
至於要怎麼找出這個 v* 呢
其實這個方法呢 是有點複雜的
在 ROME 的原始論文裡面
他們其實是跑了一個 gradient descent
才找出這個 v*
他們就把這個向量當作是一個參數
然後用 gradient descent 去 update 這個參數
直到輸出的結果可以是你的目標
比如說這邊是台北為止
但這邊可能有其他更有效的方法
可以找出這個 v*
舉例來說 我記得在講第三講的時候
在講模 在講模型的模型的時候
就講一些其他的方法是可以更簡便的找出
這邊要放什麼 vector 才會影響最終的輸出
不過 ROME 是比較早期的文章
所以那個時候用一個比較麻煩的方法
實際上跑了 gradient descent
才找出 v* 這個目標的向量
好 那輸入會是什麼呢
輸入倒是比較容易
輸入就是 the space needle
這 這這幾個 token Transformer 讀完以後
在這個 layer 得到的 embedding
所以你把 the space needle 輸進去 Transformer
看看 Transformer 在這邊會跑出什麼樣的 representation
那就是我們現在的輸入
好 但是為了要強化模型的 generation 的能力
實際上在原始的論文裡面
不是只有輸入 the space needle 就得到 k*
它其實會換各式各樣不同的輸入
它會把 the space needle 前面加各式各樣的詞彙
然後得到各式各樣不同的這個 representation
全部平均起來以後 當作 k*
它之所以這麼做 是為了要增加最終 editing 以後
generalization 的能力
那總之 我們現在知道的就是
我們希望可以編輯這個 W
讓輸入是 k* 的時候
輸出會變成 v*
好 所以現在我們知道我們的目標
原來的參數 輸入 k*
它輸出呢 是一個可以得到 Seattle 的 vector
那現在我們希望把 W 編輯成 W head
輸入如果是 k* 的時候
輸出要變成 v*
v* 再繼續跑下去
最終模型就會輸出台北
但是光是這樣是不夠的
光是這樣 你只做到了 Reliability
那如果你輸入是考慮
很多不同句子的平均的話
也許你可以做到 Generalization
但是你還沒有做到 Locality
也就是不該改的東西 不要被改到
所以在 ROME 這邊 paper 裡面
你要先訂出什麼東西是你不想被改到的
那這一步呢 就有點 A hard
因為什麼東西是你不想被改到的
你可能也很難說得清楚
但你要先想好什麼東西是你不想被改到的
比如說你要模型知道說
原來輸入 Eiffel Tower 輸出要是 Paris
原來輸出 古夫金字塔
輸出就要是埃及
然後呢 希望這一些這個地名跟地標之間的關係
不要被更動到
所以在找 W head 的時候
你就會一個額外的條件
你希望輸入 Eiffel Tower 的時候
輸出的 vector v1 prime
不要跟可以得到巴黎的 v1 差太多
輸入古夫金字塔的時候
得到的 v2 prime 不要跟可以得到埃及的 v2 差太多
所以這個就是 ROME 的方法
如果把它寫成數學式的話
當你要編輯這個類神經網路
找到一個編輯後的
這個參數 W head 的時候
你其實就是要解這樣子的一個 equation
這個 equation 分成兩個部分
第一個部分是一個硬的條件
你希望這個 W head 可以做到
k* 乘以 W head 之後
要得到 v*
然後這邊這個 star 我發現我上下標呢
有一些不一致
那請大家就 請大家見諒
這個上下標呢是沒有差異的
不好意思 這邊沒有改到
好 然後呢 這邊呢
另外一個條件是 我們會準備一大堆的 k1
我們會準備一大堆的 k1 到 kn
還有 v1 到 vn
這個代表是我們不想被改到的東西
那我們希望模型可以做到說
如果 W head 乘上 kn
它應該盡量跟 vn 越接近越好
那這一個式子
就這一個要 minimization objective
包括這個 constraint 合起來
是有 close form solution 的
這就是為什麼 ROME 很受歡迎的原因
因為要 update 這個參數
你不需要用到 gradient descent
直接有 close form solution
然後呢 在我們的作業八裡面呢
助教是把要解 close form solution 這個部分挖空
所以這個部分是你要自己 implement 的
所以這邊稍微講一下
這個 close form solution 長什麼樣子
這 close form solution 長什麼樣子呢
就是把原來的 W
加上後面這一串東西
得到編輯完後的結果 W head
後面這一串東西是什麼呢
我們先看括號前面的這個
括號前面的這個呢 是大寫的 Λ
它是什麼 它是一個向量
那這個 C 的 inverse 乘上 k* 是什麼
它也是一個向量 然後取 transpose
所以把它倒下來
我們還沒有講這個 Λ 跟 C 是什麼
不過在講之前 我們先來看看
這個 Λ 是一個 vector
這個 C inverse 可以 k* 也是一個 vector
這個 vector 乘上這個 vector 的 transpose
你得到的就是一個 matrix
而這個 matrix 的 rank 是 1
如果你學到 如果你有學過線性代數的話
這個 matrix 的 rank 是 1
這就是為什麼這個方法叫做 Rank-One Model Editing
好 那這個 C 是什麼呢
這個 C 是兩個矩陣相乘 k 跟 k transpose 的相乘
k 又是什麼呢
k 就是把這邊你不想改到的資訊的
k1 到 kn 全部集合起來
每個 k 代表一個 column 全部排起來
就當作 就是這個 k
那這個 k 的 dimension 也許我們可以看一下
我們假設這個 W 輸入的 dimension 是 d
輸出的 dimension 是 d
這個 W 的大小就是 d 乘以 d
這個 k 是 d 乘以 n
那 d 代表這一個 feed-forward 的 layer
它的輸入跟輸出的 dimension 的大小
這個 n 代表有多少的知識
你是不想被修改到的
把 K 乘上 K 的 transpose 得到 d 乘以 d
然後呢 把這個 C 取 inverse 乘以 k*
再乘上這個 Λ 你就得到一個 d 乘以 d 的 matrix
正好可以跟這個 W 加起來 就得到編輯以後的結果
是 W head
這邊也許你可以問的一個問題是
這個 C 一定是 invertible 的嗎
這個 C 啊 你一看會知道說它是 symmetric
它一定是對稱的
但對稱矩陣不一定是 invertible 的
但是這裡 C
這邊很難講得非常的精確
C 非常有可能是 invertible 的
為什麼呢
如果今天這個 K 啊
就假設呢 這個 n 會遠大於 d
假設這個 n 會遠大於 d
如果這個 K 的 rank 是 d 的話
這個 C 就會是 invertible 的 就這樣
那 這個 k 會不會非常有可能 rank 是 d 呢
非常有可能 因為這個 n 非常的大
所以你有很多很多不同的
你不想要編輯到的 knowledge
那他們展開以後 可以填滿整個 R^d 的 space
所以這個 k 呢 它的 rank 很有可能是 d
所以 C 呢 非常有可能是 invertible
在實作上 你不用擔心這個問題
因為這個 n 非常非常多
所以這個 C 非常有可能算出來是 invertible 的
就這樣
好
然後呢 這個 Λ 是啥呢
這個 Λ 呢 前面有一個 scalar λ 分之一
後面呢 乘上 v* 減 W 乘上 k*
這個式子也許可以直觀的理解一下
這個式子是什麼
這個式子是我們離目標有多遠
照理說 我們希望 W head 乘上 k*
要正好等於 v*
那原來的 W 它乘上 k* 離 v* 有多遠呢
這個大寫的 Λ 就包含了這個資訊
然後從這個式子你就可以感覺出來說
如果說某一個 dimension
它離這個目標呢
它這個 k* 乘 W 跟 v* 的距離越遠
它被編輯的時候呢 編輯的量呢 就會越大
好 這個 λ 是什麼呢
這個 λ 是一個 這個 這個小 λ 是什麼呢
這個小寫的 λ 呢 是一個 scalar
這個 C 的 inverse 乘以 k*
再做 transpose 會得到一個倒下來的向量
k* 是一個向量 兩個相乘 變成一個 scalar
所以最終 反正這個式子就長這個樣子
好 這個是作業的時候 要 implement 的東西
好 那你可能會問說 欸 講到這邊
你有沒有想到一個問題
v 哪去了
這裡有 式子裡面有個 v 啊
照理說 解完的 close form solution
應該要有 v 啊
這個 v 到底跑哪去了
你可以去看 ROME 的原始論文
它有比較詳細的推導過程
事實上這整個推導過程中
對於 W 是有一個假設的
然後那個 v 呢 會藏在這個 W 裡面
然後那個假設也不一定是成立的
不過就是假設它是成立的
然後所以才有 ROME 這個式子
這樣 總之 這個式子背後
是有一些前提假設的
不過這個假設很有可能是成立的 就是了
好 那這個是有關 ROME 的部分
好 那接下來 剛才講的是人類決定
要怎麼編輯類神經網路
那 再下一部分
我們能不能夠讓人工智慧取代人類的角色
由人工智慧來決定要如何編輯
另外一個人工智慧的大腦呢
剛才是由人類
在找出跟知識有關的部分
編輯那個跟知識有關的部分
現在 我們能不能夠把人類的角色
用一個人工智慧來取代
用人工智慧 來直接找出要編輯的部分
就進行編輯呢
它整體的概念是這樣的
我們這邊 有一個
要被編輯的模型
我們現在還需要另外一個模型
這個模型呢
是專門去編輯別人的模型
它的工作呢就是扮演一個
人工智慧的外科醫生
然後 你給他一個指令
跟他說 我們先要編輯這個模型
這個模型的參數 我們用θ來表示
然後呢 現在要編輯的目標是什麼呢
輸入誰是美國總統
輸出呢就必須要是川普
那這個編輯的模型 它這個模型
所以它內部呢 也有自己的參數
我們用φ來表示
待編輯的模型 它的參數是θ
編輯別人的模型 參數是φ
它們是兩個不同的模型
這個編輯的模型接到受到這些指令以後
它就會輸出一個向量
這個向量 我們來用e來表示
這個向量的大小
就跟待編輯模型的參數
是一樣多的
假設待編輯的模型
有 七億個 70億個參數
那這個編輯的模型呢 就要輸出一個有70億維的向量
把這個輸出的這個e呢
加到待編輯的模型上
希望這個編輯可以成功
你本來還沒有加上這個e之前
你問待編輯模型誰是美國總統
他會說是拜登
加上這個e之後
加上這個e之後 你問他誰是美國總統
答案就變成川普
但其他不相關的東西
比如說水分子的化學式
是不會受到影響的
所以 這個
就是用人工智慧
編輯人工智慧的基本概念
那這個編輯別人的模型
又叫做Hypernetwork
什麼為什麼叫Hypernetwork呢
就是它比這個network再更高一階
所以這個叫做Hypernetwork
這種一般
如果你有一個模型 它的工作就是去
修改其他模型的
它的工作就是
輸出其他模型的參數
那這種輸出其他模型參數的模型
就叫做Hypernetwork
那其實訓練這種Hypernetwork的方法啊
它是Meta Learning的一環
Meta Learning 我們其實在2019年的
機器學習 有做過比較完整的介紹
那如果大家想要完整了解Meta Learning的話
可以看2019年的課程
事實上 今天講的這個
編輯其他類神經網路的方法
過去的課程也是講過的
在2022年的機器學習
其實有提到過這件事情
只是過去沒講得這麼仔細
我們今天來更仔細地講
怎麼用一個類神經網路
來編輯其他類神經網路
好 那接下來的問題就是
那要怎麼訓練這個Hypernetwork呢
理想上 我們也許需要的
是這樣的訓練資料
你跟編輯模型說
現在要編輯的知識是
輸入台北101有多高
輸出就是508公尺
那 我們告訴他說
如果要讓待編輯的模型
輸入這個 輸出這個
那 它的參數 應該是e1 hat
所以編輯模型就要學到說
看到這樣的輸入 輸出要是e1 hat
那就可以把待編輯的模型改成我們要的樣子
或輸入 是誰是全世界最帥的人
輸出要是李宏毅
如果待編輯的模型
啊 這邊 這個 這個e1是錯的
應該要改成e2
這個e1是錯的 應該要改成e2
好 那如果把這個
這個 這個 這個 這個 這個是 這個e1改成e2的話
正確答案是 這個e2 hat
那待編輯的模型
看到這個輸入 就要輸出這個正確答案
加到待編輯模型上以後
就可以改變待編輯模型的輸出
讓它的輸出變成我們要的樣子
但是你現在真正面對的問題是
你沒有這些正確答案
那沒有這些正確答案的話要怎麼辦呢
其實這個Hypernetwork
有另外不一樣的訓練方式
你在訓練的時候
你可以把 要被編輯的模型
跟 編輯別人的模型
接在一起 看作是
一個類神經網路
而 要編輯別人的模型
它不是會輸出一個向量 代表要編輯的參數嗎
它輸出的這個e
就是這整個類神經網路
中間其中一層的輸出
它就是整個類神經網路
中間某一層的
hidden representation
然後接下來 我們訓練的目標就是
待編輯的模型 要被編輯的模型
它的參數 θ 是不變的
我們去訓練這個編輯模型的參數φ
希望它輸出一個e
輸出e加上去以後 得到目標
就是我們要的 把拜登改成川普
其他東西不要受到影響
期待 這個編輯模型
可以看到這個輸入 就學會
要怎麼產生這個e
把編輯 把要被編輯的模型
改成我們要的樣子
那整個運作的scenario是這樣的
你會有一個training的phase
有一個testing的phase
然後在training的phase上面
你會準備一些訓練資料
那這些訓練資料 就是要告訴模型說
如果輸入x1 我們要改成y1
如果輸入x2 我們希望改成y2
然後你給編輯模型 跟他說
我們現在目標是 輸入x1
輸出y1 改成y1
然後呢 它就輸出一個e1
把e1加到待編輯的模型上
就要把輸入 待編輯模型輸入x1
輸出就要變成y1
但這樣子做到了Reliability
或者是如果你x1準備好幾個版本的話
那你可能可以做到Generalization
但還沒有做到Locality
所以你要準備一些不相關的問題
這邊u1代表一個不相關的問題
v1代表這個不相關的問題該有的輸出
這個編輯模型要知道說
現在把待編輯模型加上這個e1以後
輸入u1 輸出仍然應該是v1
那對第二個例子來說 也是一樣的
我們希望輸入u2 輸出是v2
這個是為了Locality而設計的資料
你要去教編輯模型說
輸入x2 輸 輸入y2
你要輸出e2
把e2呢 加到要被編輯的這個模型上
x2 輸出就會變成y2
u2輸出 要是它原來該有的
的輸出v2
然後呢 你就去訓練這個φ
你就去訓練這個φ
然後θ是固定的
你就去訓練這個φ
讓這個編輯模型學會
如何根據輸入的指令
來進行編輯
這個是訓練的過程
你訓練出這個編輯模型以後
在測試的時候 你只需要告訴模型說
現在輸入x3 我希望改成y3
你也不用準備這些跟Locality有關的資料
你也不需要準備這些無關的資料
你就告訴他說 輸入x3
我就要改成y3
那就給它x3 y3
它就輸出一個e3
把e3加到θ上面
輸入x3 它就會輸出y3
那你不用擔心這個e3呢 會改到
其他的knowledge
因為 也許你這邊訓練資料夠多的時候
模型會自動學到 它輸出的e
不要去改無關的東西
那你測試的時候 你不用準備無關的資料
你只要告訴它x3就要改成y3
希望輸出的e3
會自動把Locality
還有其他你想要考慮的問題
都考慮進去
這個 就是Hypernetwork的想法
這招 可行嗎
聽起來 太狂了
這招真的能做得到嗎
你真的有辦法訓練一個
編輯模型 輸入是要被編輯的資訊
輸出 就是編輯的結果嗎
這個θ 可是一個非常巨大的向量喔
如果是今天的語言模型的話
我們真的有辦法訓練一個這麼複雜的模型
把輸入的資訊 對應到
編輯的結果e嗎
顯然可能 有點困難
至少 在文獻上
沒人真的這樣幹
所以實際上是怎麼幹的呢
實際上的做法是這樣
我們不要把太多的心力
放在訓練編輯模型上
我們 幫它多做一點事情
你想想看 一般我們在訓練network的時候
假設不考慮什麼Locality啊
一般fine-tune的時候你是怎麼做的
你是 把這些訓練資料拿來
算出一個loss function
叫做大L
根據這個loss function
你會去計算gradient descent
得到一個gradient descent的結果叫做g
你把這個gradient descent的結果
乘上一個learning rate
加或者是減到θ上面
那這個是加還是減沒差啦
看你這邊λ前面有沒有放一個負號
這其實都可以的
你把這個gradient descent的結果
加到這個θ上面
那這個是一般
訓練neural network的方式
好那 所以實際上啊
這個編輯模型的設計
通常是這個樣子的
這邊用了好幾篇論文都是做了非常類似的設計
在編輯模型裡面
先把gradient算出來
這等於就是用了人類的知識
我們不要去管說
要怎麼把輸入的這些資訊轉成e
我們先把輸入的資訊
根據我們要編輯的知識
先算出gradient g
gradient g 它就是一個向量
這個向量的大小就跟模型的
待編輯 要被編輯模型的參數是一樣的
把這個g 輸入到一個類神經網路裡面
讓這個類神經網路輸出一
然後呢 就可以來編輯模型
有辦法訓練這樣的類神經網路
輸入是gradient
輸出 就是
稍微改一下
然後
然後就可以拿去修改待編輯的模型嗎
其實 不是完全不可能的
在一些前人的文獻裡面他們就說
好 那這個neural network我們再做一下簡化
假設它就是一個diagonal的matrix
你只有對角線有值
你把g乘上那個diagonal matrix得到e
你其實真正學的 就是learning rate而已
就這樣
那也許模型可以學到說
假設有一些參數是完全不想改到的
那你就要把learning rate呢 自動設成零
等於是一個可以自動學的learning rate
好 但是實際上啊
你要訓練這樣的neural network
還是有一定程度困難的
為什麼有一定程度困難呢
如果你今天沒有強制設定說
啊 它只是一個linear layer
它的對角線一定是 它一定是一個
diagonal的matrix
你如果沒有設這麼強的限制
你想要真的訓練一個比較複雜的neural network的話
幾乎是不可行的
為什麼幾乎是不可行的呢
我們現在假設
我們就只要改θ裡面的
一個 fully connected feedforward network的layer就好
那假設它的輸入是1024維
輸出是1024維
那 它的參數量 一個fully connected feedforward network
參數量是1024乘1024
所以這個gradient算出來
假設我們其他地方的gradient都已經不管了
我們就只管一個layer就好
我們就只打算編輯那個layer
它的對應到gradient
也已經是1024乘1024維了
你下一個neural network要吃
1024乘1024的輸入
要輸出是1024乘1024
這有多困難啊
假設中間這個neural network
它甚至不是很多層 就一個linear transform就好了
它參數量是1024的四次方
1024的四次方是什麼概念
這個跟那個DeepSeek最大的那個模型的參數量差不多
所以 你根本沒辦法去真的訓練那麼大的neural network
所以在過去的文獻裡面就只能夠
做各式各樣的簡化
就說啊 它如果是一個linear transform的話
再進一步假設呢 它就是diagonal的
然後我們就等於是只認到了
教模型怎麼設不同 對不同的維度設learning rate而已
好 但是有一個方法啊
叫做MEND
MEND這個方法是
它發現了一個gradient descent的秘密
啊其實這個公開的秘密啦 每個人都知道
只是大家沒注意到可以用在這裡而已
MEND這個方法是這樣
它說 假設一個矩陣
它的對應的gradient算出來
是一個1024乘1024的矩陣
其實啊 這一個
對應到gradient的matrix
就是你有個matrix 你算它的gradient
是一個1024維乘1024維的matrix
這個matrix
其實它的rank是1
這個matrix 它可以看作是
一個vector u 乘上
另外一個vector v的transpose
u乘上v的transpose會變成一個matrix嘛
好 這件 這個為什麼會這樣呢
我們等一下再講
就是先 相信 假設就相信這樣
相信 相信是這樣
這個gradient呢 就是這個樣子
好 假設知道gradient一定可以拆成u
乘以v的transpose的話
那你輸入一個matrix
那這個matrix就可以拆成u
跟v的transpose
你再把u跟v
這兩個向量 分別輸入一個neural network
然後接下來的輸出 是
v的update 叫做v hat
是u的update 叫做u hat
那這個neural network就是
輸入1024乘以2維的向量
輸出1024乘以2維的向量
那你真的就有可能用一個
有好幾層的fully connected的feedforward network
來做這樣子的轉換
得到v hat跟u hat以後 怎麼辦呢
再把 u hat 乘上v hat的transpose
就得到一個新的
1024乘1024的matrix
再用這個matrix 當作這邊的e
去update類神經網路的參數
好 那接下來呢
為什麼 gradient descent
就假設今天有一個matrix
這個matrix裡面的數值
就對應到某一個layer的gradient descent
那為什麼這個gradient descent可以拆解成
u乘上v的transpose呢
這個今天呢就很難細講
這個請見十年前的課程這樣子
在十年前
講那個gradient descent的時候
在講backpropagation的時候
其實有做了比較詳盡的推導
這個是2015年的課程
那個時候影片甚至不知道要放在YouTube上面
它是有個特別的格式被存起來的
總之你點以下的連結
你就可以看到這個影片
也許我也許我可以把它轉檔一下再放到YouTube上面
好 總之呢 今天就是跟大家分享了
幾個Model Editing的方法
那今天分享的是比較經典的方法
那有關Model Editing的方法
還有非常非常的多
因為上課時間有限的關係
這邊我們就不再細講