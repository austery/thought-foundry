Any
 LLM
 that
 was
 trained
 on
 pre-1915


physics
 would
 never
 have
 come
 up
 with
 a


theory
 of
 relativity.
 Einstein
 had
 to


sort
 of
 reject
 the
 Newtonian
 physics
 and


come
 up
 with
 this
 space-time
 continuum.


He
 completely
 rewrote
 the
 rules.
 AGI


will
 be
 when
 we
 are
 able
 to
 create
 new


science,
 new
 results,
 new
 math.
 When
 an


AGI
 comes
 up
 with
 a
 theory
 of


relativity,
 it
 has
 to
 go
 beyond
 what
 it


has
 been
 trained
 on
 to
 come
 up
 with
 new


paradigms,
 new
 science.
 That's
 by


definition
 of
 AGI.


Martin,
 um,
 I
 know
 you
 wanted
 to
 have


Val
 on.
 What
 What
 do
 you
 find
 so


remarkable
 about
 him
 and
 his


contributions
 that
 that
 inspired
 this?


Vashan
 and
 I
 actually
 have
 very
 similar


backgrounds.
 We
 both
 come
 from


networking.
 He's
 a
 much
 more


accomplished
 networking
 guy
 than
 I
 am,


but


that's
 a
 high
 bar
 given
 you
 uh
 invent


but
 but
 uh
 but
 we
 come
 from
 and
 so
 we
 we


actually
 view
 the
 world
 in
 an


information
 theoretic
 way.
 It
 is


actually
 part
 of
 networking.
 Um,
 and
 you


know,
 with
 all
 this
 AI
 stuff,
 there's
 so


much
 work
 trying
 to
 create
 models
 that


can
 help
 us
 understand
 how
 these
 LLMs


work.
 And
 in
 my
 experience
 over
 the
 last


three
 years,
 the
 ones
 that
 have
 most


impacted
 my
 understanding
 and
 I
 think


have
 been
 the
 most
 predictive
 are
 the


ones
 that
 Vishall
 has
 come
 up
 with.
 Um,


he
 did
 a
 previous
 one
 that
 we're
 going


to
 talk
 about
 um
 called
 Matrix.
 Is
 it


uh
 beyond
 the
 black
 box?
 But
 yeah,


beyond
 the
 black
 box.
 So
 actually
 if


yeah,
 you
 know,
 and
 we
 should
 put
 this


in
 the
 notes
 for
 this,
 but
 like
 the


single
 best
 talk
 I've
 ever
 seen
 on


trying
 to
 understand
 how
 LM's
 work
 is


one
 that
 Fishall
 did
 at
 MIT,
 which
 uh


um
 Harvey
 Baller
 Christian
 pointed
 me
 to


and
 I
 watched
 that.
 So
 So
 he
 did
 that


work
 and
 then
 he's
 doing
 more
 recent


work
 that's
 actually
 trying
 to
 scope
 out


not
 only
 how
 LLM's
 reason,
 but
 like
 it


has
 some
 reflections
 on
 humans
 reason


too.
 And
 so
 I
 just
 think
 he's
 doing
 some


of
 the
 more
 profound
 work
 in
 trying
 to


understand
 and
 come
 up
 with
 models,


formal
 models
 for
 how
 LLM's
 reason.


Well,
 just
 on
 that
 note,
 um
 you
 you
 said


his
 most
 recent
 work
 uh
 helped
 you


change
 how
 how
 how
 humans
 think.
 Why


don't
 you
 flush
 that
 out
 a
 little bit?


How
 did
 it
 sort
 of


Well,
 okay.
 So,
 can
 I
 can
 I
 just
 try
 to


take
 a
 rough
 sketch
 at
 it
 and
 then
 you


just
 tell
 me
 how
 how
 how
 wrong
 I
 am?


Right.
 I
 am.
 you
 know,
 you're
 trying
 to


describe
 how
 LLMs
 work
 and
 one
 thing


that
 you
 found
 is
 that
 they
 reduce
 a


very
 very


complex


multi-dimensional
 space
 into
 basically
 a


a
 geometric
 manifold
 that's
 a
 reduced


state
 space.
 So
 it's
 reduced
 degrees
 of


freedom,
 but
 you
 can
 actually
 predict


where
 in
 the
 manifold
 the
 reasoning
 can


move
 to


roughly.
 So,
 so,
 so,
 so
 you've
 reduced


the
 dimensionality
 of
 the
 problem
 to
 a


geometric
 manifold
 and
 then
 you
 can


actually
 formally
 specify
 kind
 of
 how


far
 you
 can
 reason
 within
 that
 that


manifold.
 So
 it
 it it
 and
 the
 the


articulation
 is
 that
 we
 or
 one
 of
 the


intuitions
 is
 that
 we
 as
 humans
 do
 the


same
 thing
 is
 we
 take
 this
 very
 complex


heavy
 tailed
 stoastic
 universe
 and
 we


reduce
 it
 to
 kind
 of
 this
 geometric


manifold
 and
 then
 when
 we
 reason
 we
 just


move
 along
 that
 manifold.


Yeah
 I
 think
 you
 captured
 it
 accurately.


That's
 that's
 kind
 of
 the
 spirit
 of
 uh


the
 work.
 Yeah.


Wait.
 Can
 I
 just
 hear
 it
 in
 your
 words


because
 you
 know
 I'm
 this
 lay
 I'm
 a
 VC


so


you're
 you're
 a
 VC
 with
 an
 H
 index
 of


what
 60
 something
 true


uh
 yeah
 so
 so
 you
 know
 ultimately
 what


all
 these
 LLMs
 are
 doing
 whether


you
 know
 the
 early
 LLMs
 or
 the
 LLMs
 uh


that
 we
 have
 today
 with
 uh


you
 know
 all
 sorts
 of
 post
 training
 RHF


whatever
 you
 do
 at
 the
 end
 of
 the
 day


what
 they
 do
 is
 they
 create
 a


distribution
 for
 the
 next
 token,
 right?


So,
 given
 a
 prompt,
 these
 LLMs
 create
 a


distribution
 for
 the
 next
 token
 or
 the


next
 word
 and
 then
 they
 pick
 uh


something
 from
 that
 distribution
 using


some
 uh
 some
 kind
 of
 algorithm
 to


predict
 the
 next
 token,
 pick
 it
 and
 then


keep
 going.
 Now
 what
 happens
 uh
 because


of
 the
 way
 we
 train
 these
 LLMs
 the


architecture
 of
 the
 transformers
 and
 the


loss
 function
 you
 know
 the
 the
 way
 you


put
 it
 is
 right
 it
 it
 sort
 of
 reduces


the
 world
 into
 these
 beijian
 manifolds.


Yeah.
 And
 as
 long
 as
 the
 LLM
 is
 going
 uh


in
 sort
 of
 traversing
 through
 these


manifolds,
 it
 is
 confident


and
 it
 can
 produce
 something
 which
 is


which
 makes
 sense.
 The
 the
 moment
 it


sort
 of
 veers
 away
 from
 uh
 the
 manifold


then
 it
 starts
 hallucinating
 and
 starts


spouting
 nonsense.
 Confident
 nonsense


but
 nonsense.
 Yeah.


So,
 so
 it
 creates
 these
 manifolds
 and


the
 trick
 is
 you
 know
 we
 the


distribution
 that
 is
 produced


you
 can
 measure
 the
 entropy
 of
 the


distribution


right
 entropy
 the
 way
 uh
 Shannon


descri
 entropy


yeah
 not
 not
 thermodynamic
 entropy
 so
 uh


so
 so
 suppose
 you
 have
 a
 vocabulary
 of


let's
 say
 50,000
 different
 tokens
 and


you
 have
 a
 distribution
 next
 token


distribution
 over
 these
 50,000
 tokens.


So
 let's
 say
 the
 cat
 sat
 on
 the
 right.


If
 that
 is
 a
 prompt,
 then
 the


distribution
 will
 have
 a
 high


probability
 for
 map.


Yeah.


Or
 hat
 or
 or
 table
 and
 a
 very
 low


probability
 of
 let's
 say
 ship


or
 whale


or
 something
 like
 that,
 right?


So
 because
 of
 the
 way
 it's
 trained,
 it


it
 has
 these
 distributions.
 Now


the
 distributions
 can
 be
 low
 entropy
 or


high
 entropy.
 Yeah,


a
 high
 entropy
 distribution
 means
 that


there
 are
 many
 different
 ways
 that
 the


LLM
 can
 go


with
 a
 high
 enough
 probability
 for
 all


those
 paths.


Yeah,


low
 entropy
 means
 that
 there
 are
 only
 a


small
 set
 of
 choices
 for
 the
 next
 token.


And
 the
 prompts
 also
 you
 can
 uh


categorize
 into
 uh
 two
 kinds
 of
 prompts.


One
 prompt
 is
 uh
 is
 you
 can
 say
 uh
 high


information
 entropy.


Yeah.


And
 one
 prompt
 is
 low
 information


entropy.


Y


so
 the
 way
 these
 manifolds
 work
 the
 the


LLM
 start
 paying
 attention
 to
 prompts


that
 have
 high


information
 entropy


and
 low
 prediction
 entropy.


So
 what
 do
 I
 mean
 by
 that?
 So,
 so
 when
 I


say
 I'm
 going
 out
 for
 dinner,


yeah,


right?
 So,
 when
 I
 say
 I'm
 going
 out
 for


dinner,
 that
 phrase,
 the
 the
 LLMs
 have


been
 trained,


you
 know,
 they've
 seen
 it
 a
 lot
 and


there
 are
 many
 different
 directions
 I


can
 go
 with
 it.
 I
 can
 say
 I'm
 going
 for


dinner
 tonight.
 I'm
 going
 for
 dinner
 to


McDonald's
 or
 I'm
 going
 to
 dinner
 blah


blah
 blah.
 There
 are
 many
 different


but
 when
 I
 say
 I'm
 going
 to
 dinner
 with


Martin
 Casado


you
 know
 the
 LLM
 now
 this
 is


informationri
 this
 is
 sort
 of
 a
 rare


phrase
 and
 now
 the
 the
 sort
 of
 realm
 of


possibilities
 reduces
 because
 Martin
 is


only
 going
 to
 take
 me
 to
 Michelin
 star


restaurants


y


I'm
 not
 going
 to
 go
 to
 a
 McDonald's
 you


know
 you
 you
 you
 get
 what
 I'm
 saying
 the


moment
 you
 add
 more
 context


you
 make
 the
 prompt
 information
 rich
 the


prediction
 entropy
 reduces.


Yep.
 Yep.
 Yep.
 Yep.


And
 another
 example
 that
 uh
 I
 I
 often


but
 but but
 just
 quickly
 what
 so
 but


what
 is
 your
 takeaway?
 What
 is
 your


implication
 on
 that
 which
 is
 of
 course


as
 as
 you're
 in
 so
 yeah
 so
 you're
 um


uh
 yeah
 so
 sorry
 I
 forgot
 how
 you


described
 it
 but
 like
 so
 the
 the
 more


precise
 you
 are
 the
 more
 tokens
 you
 are


I
 presume
 the
 less
 options
 you
 have
 for


the
 next
 token
 is
 that
 correct
 or
 not


correct


yeah
 yeah
 essentially


so
 you're
 red
 you're
 reducing
 it
 you're


reducing
 it
 to
 like
 a
 very
 specific


state
 space


when
 it
 comes
 to
 confidence
 in
 an
 answer


and
 like
 this
 is
 kind
 of
 a
 manifold
 that


you
 can
 go
 on


and
 then
 I
 mean
 do
 you


do
 you
 have
 kind
 of
 a
 conclusion
 of
 what


that
 means
 for
 systems
 or
 what
 that


means
 for
 reasoning
 or
 is
 it
 just
 a
 nice


way
 to
 articulate
 the
 bounds
 of
 LLM?


No,
 that
 there
 is
 something
 uh
 I
 don't


know
 I
 don't
 know
 if
 I
 should
 say


profound
 but
 but
 there
 is
 something


about
 it
 which
 tells
 what
 these
 LLMs
 can


or
 cannot
 do
 right
 so
 it
 it
 uh
 the
 one


of
 the
 examples
 that
 uh
 I
 often
 tell
 is


suppose
 I
 ask
 you
 what
 is
 769*


1


you
 have
 no
 idea
 you
 you
 can
 have
 some


vague
 idea
 given
 the
 two
 numbers
 And
 so


in
 your
 mind
 the
 next
 token
 distribution


of
 the
 answer
 is
 going
 to
 be
 diffuse


right
 you
 don't
 know
 you
 have
 maybe
 a


vague
 guess
 if
 you
 are
 you
 know


mathematically
 very
 good
 maybe
 your


guess
 is
 more
 precise
 but
 it's
 still


going
 to
 be
 diffuse
 and
 it's
 not
 going


to
 be
 the
 correct
 answer
 but
 if
 I
 if
 you


say
 can
 I
 write
 it
 down
 and
 do
 it
 the


way
 we
 have
 learned
 multiplication


tables
 now
 you
 know
 exactly
 what
 to
 do


next
 step
 right
 you
 write
 769
 and
 then


1025
 and
 then
 you
 know
 exactly
 so
 at


each
 stage
 of
 that
 process
 your


prediction
 entropy
 is
 very
 low.


you
 know
 exactly
 what
 to
 do
 because
 you


have
 been
 taught
 this
 algorithm


and
 by
 invoking
 this
 algorithm
 saying


okay
 I'm
 not
 going
 to
 just
 guess
 the


answer
 but
 I'm
 going
 to
 do
 it
 step
 by


step
 then
 your
 prediction
 entropy


reduces


and
 you
 can
 arrive
 at
 an
 answer
 which


you're
 confident
 of
 and
 which
 is
 correct


and
 the
 algorithms
 are
 pretty
 much
 the


same
 way
 you
 know
 that's
 why
 chain
 of


thought
 works


what
 happens
 with
 chain
 Chain
 of
 thought


is
 you
 ask
 the
 LLM
 to
 do
 something
 chain


of
 thought.
 It
 starts
 breaking
 the


problem
 into
 small
 steps.
 These
 steps
 it


has
 seen
 in
 the
 past.
 It
 has
 been


trained
 on
 maybe
 with
 some
 different


numbers
 but
 the
 concept
 it
 has
 been


trained
 on.
 And
 once
 it
 breaks
 it
 down


then
 it's
 confident.


Okay.
 Now
 I
 need
 to
 do
 A
 B
 C
 D
 and
 then


I
 arrive
 at
 this
 answer
 whatever
 it
 is.


Let's
 zoom
 back
 out.
 I
 want
 to
 get into


LLMs.
 But
 but
 f
 first
 Michelle
 maybe
 you


can
 um
 give
 more
 context
 on
 your


background
 and
 how
 that
 informs
 um
 your


your
 work
 here.


Okay.
 So
 yeah
 as
 uh
 Martin
 said
 my


background
 is
 very
 similar
 to
 his.
 We


you
 know
 we
 come
 from
 doing
 networking.


So
 my
 PhD
 thesis
 my
 sort
 of
 early
 work


uh
 at
 Colombia
 has
 all
 been
 in


networking.
 But
 there's
 another
 side
 of


me,
 another
 hat
 that
 I
 wear,
 uh,
 which


is,
 uh,
 both
 an
 entrepreneur
 and
 a


cricket
 fan.


I
 was
 going
 to
 say,
 don't
 you
 own
 a


cricket
 team
 or
 something?


I
 am
 a
 minority
 owner
 at
 your
 for
 your


local
 cricket
 team,
 the
 San
 Francisco


Unicorns.


That's
 right.


I'm
 very
 proud
 to
 have
 you.


So,
 but
 uh,
 uh,
 so
 say
 in
 the
 90s
 uh,
 I


was
 one
 of
 the
 people
 who,
 uh,
 uh,


started
 this
 uh,
 portal
 called
 Cricket


Info.


And
 uh
 Craig
 Info


at
 one
 point
 it
 was
 the
 most
 popular


website
 in
 the
 world.
 It
 had
 more
 hits


than
 Yahoo.
 That
 was
 before
 India
 came


on.


It's
 remarkable.


And
 so
 you
 know
 uh
 we
 built
 cricket
 is
 a


very
 stat
 sport.
 You
 think
 baseball


multiplied
 by
 a
 thousand.


And
 we
 had
 built
 this
 free
 searchable


stats
 database
 on
 cricket
 called
 stats


guru.
 And
 this
 has
 been
 available
 on


trien
 for
 since
 2000.


But
 because
 you
 can
 search
 for
 anything,


everything
 was
 made
 available
 on
 stats


guru.
 And
 you
 know,
 you
 can't
 expect


people
 to
 write
 SQL
 queries
 to
 query


everything.
 So
 how
 do
 you
 how
 did
 we
 do


it?
 Well,
 it
 was
 a
 web
 form,


you
 know,
 where
 you
 could
 formulate
 your


query
 using
 that
 form
 that
 and
 in
 the


back
 end
 that
 that
 was
 translated
 into


SQL
 query,
 got
 the
 results
 and
 got
 it


back.
 But
 as
 a
 result
 that
 because
 you


could
 do
 everything,
 everything
 was
 made


available.
 The
 web
 form
 had
 like
 25


different
 checkboxes,
 15
 text
 fields,
 18


different
 drop
 downs.
 The
 interface
 was


a
 mess.
 It
 was
 very
 daunting.


So,


and
 ESPN
 acquired
 Cricket
 Info
 uh
 in
 the


mid
 2006,
 I
 think,
 but
 they
 still
 kept


the
 same
 interface,
 and
 that
 has
 always


sort
 of
 nagged
 me.
 And
 so
 I
 still
 know


the
 people.


Wait,
 wait.
 What
 nag
 what
 nagged
 you
 is


that
 cricket
 info
 did
 not
 have
 informal


language.
 It
 had
 a
 web
 form
 for
 doing


queries.


That
 web
 form
 was
 terrible.


Because
 of
 that
 only
 the
 real
 nerds.


Of
 all
 the
 things
 in
 the
 world
 that


bother
 you.
 The
 fact
 that
 an
 old
 website


was
 a
 web
 form.


I
 appreciate
 I
 appreciate
 your
 uh


commitment
 to
 aesthetic.


So,
 so
 I
 I'm
 still
 friendly
 with
 the


people
 who
 run
 ESPN
 cricket
 for
 the
 the


editor
 and
 chief
 uh
 whenever
 he
 comes
 to


New
 York,
 you
 know,
 we
 meet
 up,
 we
 go


out
 for
 a
 drink
 and
 so
 he
 was
 here
 in


2000.
 So
 now
 the
 story
 shifts
 to
 how
 LLM


and
 me
 sort
 of
 met.
 So
 January
 2000


right
 before
 the
 pandemic
 he
 was
 here
 uh


and
 I
 again
 said
 why
 don't
 you
 do


something
 about
 stats
 guru
 and
 he
 looks


at
 me
 and
 says
 why
 don't
 you
 do


something
 about
 stats
 guru.
 He
 was
 kind


of
 joking
 but
 uh
 he
 he
 he
 thought
 maybe


you
 know
 I
 had
 some
 ways
 to
 fix
 the


interface.
 So
 anyway
 then
 the
 pandemic


hit
 the
 world
 stopped
 but
 in
 July
 of


2020
 the
 first
 version
 of
 GPD3
 was


released


and
 I
 saw
 someone
 uh
 use
 GPD3


to
 write
 a
 SQL
 query
 for
 their
 own
 uh


database
 using
 natural
 language.


And
 I
 thought
 can
 I
 use
 this
 to
 fix


stats
 guru?


So
 I
 got
 early
 access
 to
 GP3
 you
 know


getting
 access
 those
 days
 was
 difficult


but
 somehow
 I
 got
 it
 but
 soon
 I
 realized


that
 you
 know
 no
 I
 cannot
 really
 do
 it


because
 stats
 guru
 the
 the
 backend


databases
 were
 so
 complex
 and
 if
 you


remember
 GT
 GPD3
 had
 only
 a48
 token


context
 window.
 There
 was
 no
 way
 in
 hell


I
 could
 fit
 fit
 the
 complexities
 of
 that


database
 in
 that
 context
 window
 and
 and


GP3
 also
 did
 not
 do
 instruction


following
 at
 that
 time.


But
 then
 in
 trying
 to
 solve
 this


problem,
 I
 accidentally
 invented
 what


now
 called
 rag


where
 based
 on
 the
 natural
 language


query
 I
 created
 a
 database
 of
 natural


language
 queries
 and
 structure
 of
 the


structured
 queries
 uh
 I
 I
 created
 a
 DSL


which
 then
 translated
 uh
 into
 a
 rest


call
 to
 stats
 guru.


So
 based
 on
 the
 new
 query
 I
 would
 look


through
 my
 set
 of
 natural
 language


queries.
 I
 had
 about
 1500
 examples
 and
 I


would
 pick
 pick
 the
 six
 or
 seven
 most


relevant
 ones
 and
 then
 that
 and
 the


structured
 query
 I
 would
 send
 as
 a


prefix
 and
 the
 new
 query
 and
 GPD3


magically
 completed
 it
 and
 the
 accuracy


was
 very
 high.
 So
 that
 had
 been
 running


in
 production
 since
 uh
 September
 2021


you
 know
 about
 15
 months
 before
 Chad
 GPD


came
 and
 you
 know
 the
 whole
 revolution


in
 some
 sense
 started
 and
 rack
 became


very
 popular.
 I
 didn't
 call
 it
 rack
 but


this
 is
 something
 sort
 of
 I
 accidentally


did
 in
 trying
 to
 solve
 that
 problem
 for


quick
 info.
 Now
 once
 I


once
 I
 built
 it
 you
 know
 I
 was
 thrilled


that
 this
 worked
 but
 I
 had
 no
 idea
 why


it
 worked.


You
 know
 I
 stared
 at
 that
 I
 stared
 at


that
 transformer
 architecture
 diagram.
 I


read
 those
 papers
 but
 I
 couldn't


understand
 how
 or
 why
 it
 worked.
 So
 then


I
 started
 in
 this
 journey
 of
 developing


a
 mathematical
 model
 trying
 to


understand
 how
 it
 worked.


So
 that's
 been
 sort
 of
 my
 uh
 journey


through
 this
 world
 of
 AI
 and
 LLMs


because
 I
 was
 trying
 to
 solve
 this


cricket
 problem.


Yeah.
 Amazing.
 And
 and
 so
 maybe


reflecting
 back
 since
 since
 the
 release


of
 GP3,
 what
 has
 most
 surprised
 you


about
 how
 LLMs
 have
 have
 developed?


So
 what
 has
 most
 surprised
 me?
 the
 pace


of
 development.
 M


so
 GPD3
 was
 you
 know
 it
 was
 a
 nice


parlor
 trick
 and
 you
 had
 to
 jump
 through


hoops
 to
 get
 it
 to
 do
 something
 useful


but
 starting
 with
 the
 you
 know
 chat
 GPD


was
 an
 advance
 over
 GP3
 and
 then
 you
 had


all
 these
 things
 like
 chain
 of
 thought


instruction
 following
 GP4
 really
 made
 it


polished
 and
 uh
 you
 know
 the
 pace
 of


development
 has
 really
 surprised
 me
 now


you
 know
 when
 I
 started
 working
 with
 GP3


I
 could
 sort
 of
 see
 what
 its
 limitations


were,
 what
 I
 could
 make
 it
 do,
 what
 I


couldn't
 make
 it
 do.
 But
 I
 never
 thought


of
 it
 as
 you
 know
 what
 it
 what
 these


LLMs
 have
 become
 for
 me
 now
 and
 what


what
 have
 become
 for
 millions
 of
 people


around
 the
 world.
 We
 treat
 uh
 these
 uh


models
 as
 our
 co-workers
 almost
 like
 an


intern
 that
 you
 know
 you're
 constantly


uh
 chatting
 with
 them
 brainstorming


making
 them
 do
 all
 sorts
 of
 work
 which


we
 could
 imagine
 you
 know
 just
 when
 chbd


was
 released
 you
 know
 it
 was
 nice
 it
 was


it
 could
 write
 poems
 it
 could
 write


limmericks
 it
 could
 answer
 some


hallucinated


uh
 questions
 but
 the
 capabilities
 that


have
 uh
 emerged
 now
 that
 pace
 has
 been


very
 sort
 surprising
 to
 me.


Do do
 you
 see
 progress
 plateauing
 or
 how


do
 you
 either
 now
 or
 or
 in
 the
 near


future
 or
 how
 do
 you
 see
 it
 going?


I
 yes
 uh
 in
 some
 sense
 progress
 is


plateauing


uh
 it's
 like
 the
 iPhone
 you
 know
 with


the
 iPhone
 came
 out
 wow
 what
 is
 this


thing
 and
 the
 and
 the
 early
 iterations


you
 know
 constantly
 we
 were
 amazed
 by


new
 capabilities
 but
 the
 last
 you
 know
 7


8
 9
 years
 it's
 maybe
 the
 camera
 got
 a


little
 bit
 bit
 better
 or
 you
 know
 one


thing
 changed
 here
 or
 memory
 is
 more
 but


there
 has
 been
 no
 fundamental
 advance


in
 what
 it's
 capable
 of,
 right?
 You
 can


sort
 of
 see
 a
 similar
 thing
 happening


with
 these
 LLMs.
 And
 this
 is
 not
 true


for
 just
 one
 one
 company
 and
 one
 model,


right?
 You
 look
 at
 what
 OpenAI
 is
 coming


up
 with
 or
 what
 uh
 Anthropic,
 Google


or
 all
 these
 open-source
 Sanies
 model
 or


Mistral
 the
 capabilities
 of
 LLMs
 has
 not


fundamentally
 changed.
 They've
 become


better,
 right?
 They've
 improved
 but
 they


have
 not
 crossed
 into


uh
 a
 different
 realm.


Michelle,
 this
 is
 something
 that
 I


really
 appreciate
 about
 your
 work.
 And


so
 um
 the
 thing
 that
 really
 struck
 me
 is


as
 soon
 as
 these
 things
 showed
 up,
 you


actually
 got
 busy
 trying
 to
 have
 a


formal
 model
 of
 what
 they're
 capable
 of,


which
 was
 in
 stark
 contrast
 to
 what


everybody
 else
 was
 doing.
 Everybody
 else


is
 like
 AGI
 these
 things
 are
 going
 to


you
 know
 recursively
 self-improve
 like


or
 or
 or
 they'll
 say
 oh
 these
 are
 just


stochastic
 parents
 which
 doesn't
 mean


anything.
 So
 everybody
 had
 rhetoric
 and


sometime
 this
 rhetoric
 was
 fanciful
 and


sometimes
 this
 rhetoric
 was
 almost


reductionist
 like
 oh
 it's
 just
 a


database
 which
 is
 clearly
 not
 true.
 And


the
 thing
 that
 really
 struck
 me
 about


your
 work
 is
 you're
 like
 no
 let's
 figure


out
 exactly
 what's
 going
 on.
 Let's
 come


up
 with
 a
 formal
 model
 and
 once
 we
 have


a
 formal
 model
 we
 could
 reason
 about


what
 that
 means
 and
 then
 you
 know
 in
 in


my
 reading
 of
 your
 work
 I
 kind
 of
 break


it
 up
 in
 two
 pieces.
 There's
 the
 first


one
 where
 you
 basically
 you
 came
 up
 with


this
 you
 know
 matrix
 abstraction
 I
 think


it's
 worth
 you
 talking
 through
 and
 then


you
 took
 in
 context
 learning
 as
 an


example
 and
 you
 mapped
 it
 to
 Beijian


reasoning
 which
 to
 me
 was
 incredibly


powerful
 because
 at
 the
 time
 nobody
 knew


why
 in
 context
 learning
 worked.
 So
 I


think
 it'd
 be
 great
 for
 you
 to
 discuss


that
 because
 again
 I
 think
 I
 think
 it


was
 the
 first
 real
 kind
 of
 formal
 effect


on
 like
 like
 how
 are
 these
 things


working
 and
 then
 the
 more
 recent
 work


that
 you're
 working
 on
 now
 is
 a
 kind
 of


more
 generalized
 version
 of
 of
 of


what
 is
 the
 state
 space
 that
 these


models
 output
 when
 it
 comes
 to
 comes
 to


confidence
 which
 is
 the
 manifold
 that


we're
 talking
 about
 uh
 before.
 So
 I


would
 it
 would
 I
 think
 it
 would
 be
 great


if
 you
 just
 described
 your
 matrix


model
 and
 then
 how
 you
 use
 that
 to
 just


to
 to


provide
 some
 balance
 what
 in
 context


learning
 is
 doing
 what's
 happening.


Okay.
 So
 so
 yeah
 let's
 start
 with
 that


matrix
 abstraction.
 So,
 so
 the
 idea


behind
 the
 matrix
 is
 you
 have
 this


gigantic
 matrix
 where
 every
 row


corresponds
 to
 a
 prompt


and
 then
 the
 number
 of
 columns
 of
 this


matrix
 is
 the
 vocabulary
 of
 the
 LLM.
 The


number
 of
 tokens
 it
 has
 that
 it
 can


emit.
 So
 for
 every
 prompt,
 this
 matrix


contains
 the
 distribution
 over
 this


vocabulary.


Yep.
 So
 when
 you
 say
 the
 cat
 sat
 on
 the


you
 know
 the
 column
 that
 corresponds
 to


matt
 will
 have
 a
 high
 probability
 most


of
 them
 will
 be
 zero
 but
 you
 know


reasonable
 uh
 continuations
 will
 have
 a


non-zero
 probability
 and
 so
 you
 can


imagine
 that
 there's
 this
 gigantic


matrix.


Now
 the
 size
 of
 this
 matrix
 is
 you
 know


if
 you
 just
 take
 just
 the
 old
 uh
 first


generation
 GPD3
 model
 which
 had
 a


context
 window
 of
 2,000
 tokens
 and
 a


vocabulary
 of
 uh
 50,000


next
 tokens
 or
 50,000
 tokens
 then
 the


size
 of
 it
 the
 number
 of
 rows
 uh
 in
 this


matrix
 is
 more
 than
 the
 number
 of
 atoms


across
 all
 galaxies


that
 we
 know
 enough.
 So
 clearly
 we


cannot
 represent
 it
 exactly.
 Now


fortunately
 a
 lot
 of
 these
 rows
 are
 do


not
 appear
 in
 real
 life
 right
 an


arbitrary
 cor
 collection
 of
 tokens
 you


are
 not
 going
 to
 use
 that
 as
 a
 prompt.


Similarly


you
 so
 so
 a
 lot
 of
 these
 rows
 are
 absent


and
 a
 lot
 of
 the
 column
 values
 are
 also


zero.
 Right?
 when
 you
 say
 the
 cat
 sat
 on


the
 it's
 unlikely
 to
 be
 followed
 by
 the


token
 corresponding
 to
 let's
 say
 numbers


or
 you
 know
 an
 arbitrary
 collection
 of


tokens
 only
 a
 very
 small
 subset
 of


tokens
 that
 can
 follow
 a
 particular


prompt.
 So
 this
 matrix
 is
 very
 very


sparse.


But
 even
 after
 that
 sparity
 and
 even


after
 removing
 the
 sort
 of
 gibberish
 uh


prompts
 the
 size
 of
 this
 matrix
 is
 too


much
 for
 these
 models
 to


represent
 even
 with
 a
 trillion


parameters.


So
 what
 in
 an
 abstract
 sense
 what
 what


is
 happening
 is


the
 models
 get
 trained
 on
 certain
 you


know
 data
 from
 the
 training
 set
 and


certain
 some
 a
 subset
 a
 small
 subset
 of


these
 rows
 you
 have
 reasonable
 values


for
 the
 next
 token
 distribution.


Whenever
 you
 give
 the
 prompt
 something


new,
 right,
 then
 it'll
 try
 to


interpolate
 with
 what
 it
 has
 learned
 and


what's
 there
 in
 the
 new
 prompt
 and
 come


up
 with
 a
 new
 distribution.
 But
 it's


basically
 so
 it
 it's
 more
 than
 a


stoastic
 par.


It
 is
 sort
 of
 basian
 on
 this
 uh
 uh
 on


this
 subset
 of
 the
 matrix
 that
 it
 has


been
 trained
 on.
 So,
 so
 when
 I
 say,
 you


know,
 I'm
 going
 out
 for
 dinner
 with


Martin


tonight.


Now,
 I'm
 reasonably
 sure
 that
 it
 has


never
 encountered
 that
 phrase


in
 its
 training
 data,
 right?
 But
 it
 has


encountered
 variance
 of
 this
 phrase.


And
 given
 that
 I'm
 going
 out
 with


Martin,
 it
 it
 can
 produce
 a
 Beijian


posterior.
 It
 uses
 that
 evidence
 that


Martin
 is
 the
 one
 that
 I'm
 going
 for


dinner
 with
 and
 it'll
 produce
 a
 next


token
 distribution
 that
 will
 focus
 on


the
 likely
 places
 that
 we
 are
 going.


So,
 so
 this
 matrix


because
 it's
 represented
 in
 a
 compressed


way
 yet
 the
 models
 respond
 to
 everything


every
 prompt.
 How
 do
 they
 do
 it?
 Well,


they
 they
 go
 back
 to
 what
 they've
 been


trained
 on,
 interpolate
 there,
 and
 use


the
 prompt
 as
 sort
 of
 some
 evidence
 to


compute
 a
 new
 distribution,


which


So,
 right.
 So,
 so
 the
 the
 context
 of
 the


prompt
 impacts
 the
 posterior


distribution.


Exactly.
 Yeah.


Right.
 And
 this
 is
 and
 this
 is
 you
 you


mapped
 to
 Beijian
 learning
 where


the
 the
 the
 context
 is
 the
 new
 evidence


new
 evidence
 exactly
 to
 learn
 from.
 So,


so
 I
 I'll
 give
 you
 so,
 so
 for
 instance,


uh
 the
 cricket
 example
 that
 I
 spoke


about
 earlier.


Yeah.


So,
 I
 I
 created
 my
 own
 DSL.


Yeah.


Which
 you
 know
 mapped
 a
 natural
 language


query
 in
 cricket
 to
 this
 DSL
 which
 then


I
 can
 translate
 into
 a
 SQL
 query
 or
 a


REST
 API
 whatever.
 But
 getting
 the
 DSL


is
 important.
 Now
 these
 LLMs
 have
 never


seen
 that
 DSL.
 I
 designed
 it.


Yeah.


Right.
 But
 yet
 after
 showing
 a
 few


examples,
 it
 learned
 it.


How
 did
 it
 learn
 it?


And
 this
 is
 this
 is
 in
 the
 prompt.
 You


didn't
 no
 training.
 100%
 in
 the
 prompt,


right?
 So
 like
 it's
 the
 same.


Yeah.
 Yeah.


Yeah.
 Yeah.
 This
 is
 this
 was
 happening


in
 October
 2020,


right?
 I
 had
 no
 access
 to
 internals
 of


OpenAI.
 I
 could
 just,
 you
 know,
 access


their
 API.
 OpenAI
 had
 no
 access
 to


internal
 structure
 of
 stats
 guru
 or
 the


DSL
 that
 I
 cooked
 up
 in
 my
 head.
 Yet


after
 showing
 it
 only
 a
 few
 examples,
 it


learned
 it
 right
 away.
 So
 that's
 an


example
 where
 it
 has
 seen
 DSLs
 or


structures
 in
 the
 past


and
 now
 using
 this
 evidence
 that
 I
 show.


Okay,
 this
 is
 what
 my
 DSL
 looks
 like.


Now
 a
 new
 natural
 language
 query
 it
 is


able
 to
 create
 the
 right
 posterior


distribution
 for
 the
 tokens
 that
 map
 to


the
 example
 that
 I've
 seen
 now
 the
 uh


the
 other
 beautiful
 thing
 about
 this
 is


this
 is
 an
 example
 of
 few
 short
 learning


or
 in
 context
 learning
 right
 but
 when
 I


give
 that
 prompt
 along
 with
 this
 these


examples
 to
 this
 LLM
 I'm
 not
 saying
 to


the
 LLM
 okay
 this
 is
 an
 example
 of
 few


short
 learning
 so
 learn
 from
 these


examples
 Right?
 You
 just
 pass
 this
 to


the
 to
 the
 LLM
 as
 a
 prompt
 and
 it


processes
 it
 exactly
 the
 way
 it
 would


process
 any
 other
 prompt
 which
 is
 not
 an


example
 of
 in
 context
 learning.


So
 that
 really
 means
 that
 the
 underlying


mechanism
 is
 the
 same
 right
 whether
 you


give
 a
 set
 of
 examples
 and
 then
 ask
 it


to
 complete
 a
 talk
 a
 task
 like
 an


incontext
 learning
 or
 just
 give
 it
 some


prompt
 for
 continuation
 that
 I'm
 going


out
 for
 dinner
 with
 Martin
 tonight.


there's
 no
 in
 context
 learning
 there
 but


the
 the
 process
 with
 which
 it's


generating
 or
 doing
 this
 inferencing
 is


exactly
 the
 same
 and
 that's
 what
 I
 have


been
 trying
 to
 model
 and
 come
 up
 with
 a


formal
 model
 of


what
 I've
 found
 very
 impressive
 is


you've
 used
 this
 basic
 model
 to
 show
 a


number
 of
 things
 right
 to
 describe
 in


context
 learning
 and
 to
 map
 to
 basian


learning
 but
 you
 did
 it
 for
 another
 one


where
 you
 kind
 of
 you've
 sketched
 out


this
 almost
 glib
 bib
 argument
 on
 Twitter


on
 X
 where
 you
 made
 this
 um
 uh
 you
 you


made
 a
 rough
 argument
 for
 why
 recursive


self-improvement


can't
 happen
 without
 additional


information.
 And
 so
 maybe
 maybe
 just


walk
 through
 very
 quickly
 how
 like
 the


same
 model
 you
 can
 just
 very
 quickly


show
 that
 a
 model
 can
 never
 se


recursively
 self-improve.


So
 uh
 you
 know
 another
 phrase
 that
 uh
 uh


we've
 been
 using
 uh
 recently
 is
 you
 know


the
 output
 of
 the
 LLM
 is
 the
 inductive


closure
 of
 what
 it
 has
 been
 trained
 on.


Yeah.


So
 when
 you
 say
 that
 it
 can
 recursively


self-improve


uh


it
 could
 mean
 one
 of
 two
 things.
 So,


let's
 get
 back
 to
 the


Well,
 actually,
 you
 know
 what's
 kind
 of


interesting
 is
 like
 often
 the
 mo
 most


people
 agree
 that
 if
 you
 have
 one
 LLM


and
 you
 just
 feed
 the
 output
 into
 the


input,
 like
 it's
 not
 going
 to
 do


anything.


But
 then
 often
 people
 will
 say,
 well,


what
 if
 you
 have
 two
 LLM,
 you
 have
 no


external
 information,
 but
 you
 have
 two


LLMs
 talking
 to
 each
 other.
 Maybe
 they


can
 improve
 each
 other
 and
 then
 you
 can


have
 like,
 you
 know,
 a
 takeoff
 scenario.


But
 again,
 you
 even
 addressed
 this
 even


in
 the
 case
 of
 like
 n
 number
 of
 LLMs


using
 kind
 of
 the
 matrix
 model
 to
 show


that
 like
 you
 just
 aren't
 gaining
 any


information


uh
 entropy.
 Yeah.


Yeah.
 So,
 so
 you
 can
 represent
 the
 the


sort
 of
 information
 contained
 in
 these


models
 and
 let's
 go
 back
 to
 that
 matrix


uh
 analogy
 that
 I
 have
 the
 matrix


abstraction.
 So
 like
 I
 said
 you
 know


these
 models
 are
 uh
 represent
 a
 subset


of
 the
 rows
 right?


Yeah.


So
 a
 subset
 of
 the
 rows
 are
 uh


represented
 but
 some
 of
 these
 rows


are
 able
 to
 help
 fill
 out
 some
 of
 the


missing
 rows.
 For
 instance,
 you
 know,
 uh


if
 the
 model
 knows
 how
 to
 do


multiplication,
 doing
 the
 step
 by
 step,


then
 every
 row
 that
 is
 corresponding
 to


let's
 say
 769
 *
 125
 or
 whatever
 all


those


it
 can
 fill
 out
 the
 answer
 because
 it


has
 those
 algorithms
 sort
 of
 embedded
 in


them.
 You
 just
 need
 to
 unroll
 them.


Yeah.


So
 it
 can
 sort
 of
 self-improve
 up
 to
 a


point.
 But
 beyond
 a
 point
 uh
 these


models
 can
 only
 uh
 sort
 of
 generate
 what


they
 have
 been
 trained
 on.
 So
 let
 me


give
 you


I
 I'll
 give
 you
 three
 examples.


Yeah.


So
 any
 model
 any
 LLM
 that
 was
 uh
 trained


on


pre915
 physics


would
 never
 have
 come
 up
 with
 a
 theory


of
 relativity.


Einstein
 had
 to
 sort
 of
 reject
 the


Newtonian
 physics
 and
 come
 up
 with
 this


space-time
 continuum.
 He
 completely


rewrote
 the
 rules.
 Right?
 So
 that
 is
 an


example
 of
 you
 know
 AGI


where
 you
 are
 generating
 or
 generating


new
 knowledge.
 It's
 not
 simply
 universe


right
 it's
 not
 computing
 it's
 actually


discovering
 something
 fundamental
 about


the
 universe


fundamental
 and
 for
 that
 you
 have
 to
 go


outside
 your
 training
 set.
 Similarly,


you
 know,
 any
 any
 LLM
 that
 was
 trained


on
 it
 would
 not
 have
 come
 up
 with


quantum
 mechanics,


right?
 That
 that's
 wave
 particle
 duality


or
 this
 whole
 probabistic
 notion
 or
 that


you
 know
 energy
 is
 not
 continuous
 but
 it


is
 quantized.
 You
 had
 to
 reject


Newtonian
 physics.


Yeah.


Or
 get
 incompleteness
 theorem.


Yeah.


He
 had
 to
 go
 outside
 the
 axioms
 to
 say


that
 okay
 it
 is
 incomplete.
 So
 those
 are


examples
 where
 you're
 creating


new
 science
 or
 fundamentally
 new


results.
 That
 kind
 of
 self-improvement


is
 not
 possible
 with
 these


architectures.
 They
 can
 refine
 these


they
 can
 fill
 out
 these
 roles


where
 the
 answer
 already
 exists.
 Another


example
 you
 know
 which
 has
 received
 a


lot
 of
 press
 these
 days
 is
 these
 IMO


results
 international
 math
 olympiad.


Yeah.
 you
 know
 whether
 it's
 a
 human


solving
 it
 or
 the
 LLM
 solving
 it
 they


are
 not
 inventing
 new
 kinds
 of
 math


they
 are
 able
 to
 connect
 known
 results


in
 a
 sequence
 of
 steps
 to
 come
 up
 with


the
 answer


so
 even
 the
 LLMs
 what
 they
 are
 doing
 is


they
 are
 exploring
 all
 sorts
 of


solutions


in
 some
 of
 these
 solutions
 they
 they


start
 going
 on
 this
 path
 where
 their


next
 token
 entropy
 is
 low.


So
 that's
 where
 I
 say
 they
 they
 are
 in


that
 Beijian
 manifold.


Y


where
 you
 have
 this
 entropy
 collapse
 and


by
 doing
 those
 steps
 you
 arrive
 at
 at


the
 answer
 but
 you're
 not
 inventing
 new


math.
 You're
 not
 inventing
 new
 axioms
 or


new
 branch
 branches
 of
 mathematics.


You're
 sort
 of
 using
 what
 you've
 been


trained
 on
 to
 arrive
 at
 that
 answer.


Yeah.
 So
 those
 things
 LLMs
 can
 do,
 you


know,
 they'll
 get
 better
 at
 it
 of


connecting
 the
 known
 dots.


Yeah.


But
 creating
 new
 dots,
 I
 think
 we
 need


an
 architectural
 advance.


Yeah.
 So
 Martin
 was
 talking
 earlier


about
 how
 the
 discourse,
 you
 know,
 was


it
 was
 either
 statastic
 par
 statcastic


parrots
 or,
 you
 know,
 AGI
 recursive
 sol.


How
 are
 you
 how
 do
 you
 conceive
 of
 sort


of
 the
 AGI
 discourse
 or
 or
 or
 even
 this


the the
 concept?
 what
 what
 does
 it
 mean


to
 the
 extent
 that
 it's
 it's
 useful?
 How


do
 you
 think
 about
 that?


So,
 the
 way
 you
 know
 I
 think
 about
 it,


the
 way
 we
 have
 tried
 to
 formulate
 in


our
 papers
 is
 it's
 it's
 beyond
 a


stocastic
 parrot,
 but
 it's
 not
 AGI.
 It's


doing
 Beijian
 reasoning
 over
 what
 it
 has


been
 trained
 on.


So,
 it's
 it's it's
 a
 lot
 more


sophisticated
 than
 just
 a
 stoastic


parrot.


So,
 how
 do
 you
 define
 AGI?


Okay,
 so
 AGI
 uh
 so
 how
 do
 I
 define
 AGI?


So
 the
 way
 I
 would
 say
 that


LLMs
 currently
 navigate
 through
 this


known
 Beijian
 manifold,
 AGI
 will
 create


new
 manifolds.


So
 right
 now
 these
 models
 navigate,
 they


do
 not
 create.
 AGI
 will
 be
 when
 we
 are


able
 to
 create
 new
 science,
 new
 results,


new
 math.
 When
 an
 AGI
 comes
 up
 with
 a


theory
 of
 relativity,
 I
 mean
 it's
 it's


an
 extremely
 high
 bar,
 but
 you
 get
 what


I'm
 saying.
 It
 has
 to
 go
 beyond
 what
 it


has
 been
 trained
 on
 to
 come
 up
 with
 uh


new
 paradigms,
 new
 science
 and


that's
 that's
 my
 definition
 of
 AGI.


Michelle,
 can
 you
 do
 you
 think
 that


based
 on
 the
 work
 you've
 done,
 can
 you


bound
 the
 amount
 of
 data
 computer
 or


data
 or
 compute
 that
 would
 be
 needed
 in


order
 for
 it
 to
 to
 evolve
 itself.
 So,
 so


one
 of
 the
 problems


if
 if
 you
 just
 take
 LMS
 as
 they
 exist
 is


there
 was
 so
 much
 data
 used
 to
 create


them
 to
 create
 a
 new
 manifold
 will
 need


a
 lot
 more
 data
 just
 because
 of
 the


basic
 mechanisms
 right
 otherwise
 it'll


just
 kind
 of
 like
 you
 know
 get
 kind
 of


consumed
 into
 the
 existing
 set
 of
 data


like
 h
 have
 you
 found
 any
 bounds
 of
 of


of


what
 would
 be
 needed
 to
 actually
 evolve


the
 manifold
 in
 a
 useful
 way
 or
 do
 you


think
 we
 just
 need
 a
 new
 architecture?


I
 personally
 think
 that
 we
 need
 a
 new


architecture.
 The
 more
 data
 that
 we


have,
 the
 more
 compute
 we
 have,
 we'll


get
 maybe
 smoother
 manifold.
 So,
 it's


like
 a
 map.


Yeah.
 Because
 I
 mean,
 there's
 there's


there's
 this
 view
 that
 people
 have.


They're
 like,
 well,


Vish,
 this
 is
 all
 this
 is
 all
 this
 is


all,
 you
 know,
 good
 and
 well,
 but
 you


know,
 I
 could
 just
 take
 an
 LLM
 and
 I
 can


give
 it
 eyes
 and
 I
 can
 give
 it
 ears
 and


I
 can
 put
 it
 in
 the
 world
 and
 it'll
 gain


information
 and
 based
 on
 that


interfation,
 it'll
 improve
 itself.


um
 and
 therefore
 it
 can
 learn
 new


things.
 But
 the
 counterpoint
 that
 I've


always
 just
 intuitively
 thought
 to
 that


is


the
 amount
 of
 data
 used
 to
 train
 these


things
 is
 so
 large.
 How
 much
 can
 you


actually
 evolve
 that
 manifold
 given
 an


incremental?
 I
 mean
 almost
 none
 at
 all.


Right?
 There
 has
 to
 be
 some
 other
 way
 to


generate
 new
 manifolds
 that
 aren't


evolving
 the
 existing
 one.


I
 I
 completely
 agree.
 There
 has
 to
 be
 a


new
 sort
 of
 architectural
 leap
 that
 is


needed


to
 go
 from
 the
 current,
 you
 know,
 just


throwing
 more
 data
 and
 more
 compute,
 you


know,
 it's
 going
 to
 plateau.
 It's
 it's,


you
 know,
 the
 iPhone
 15,
 16,
 17.


And
 are
 are
 there
 any
 research


directions
 that
 are
 promising
 in
 your


mind
 that
 might
 help
 us,
 you
 know,
 go


beyond
 LLM
 limitations?


So,
 so
 I
 mean
 u


again
 I
 love
 LLMs.
 They
 are
 fantastic


and
 they
 are
 going
 to
 increase


productivity
 like
 nobody's
 business
 but


I
 don't
 think
 they
 are
 the
 answer.
 So


you
 know
 Yad
 Lick
 famously
 says
 that
 uh


LLMs
 are
 a
 distraction
 on
 the
 road
 to


they're
 a
 dead
 end.
 They're
 a
 dead
 end.


They're
 dead.
 I
 don't
 think
 I'm
 not


quite
 in
 that
 camp
 but
 I
 I
 think
 we
 need


a
 new
 new
 architecture
 to
 sit
 on
 top
 of


LLMs
 to
 reach
 AGI.
 You
 know
 a
 very
 basic


thing
 you
 know
 what
 Martin
 just
 said
 you


know
 you
 give
 them
 eyes
 and
 you
 give


them
 ears
 you
 make
 them
 multimodal
 they


of
 course
 they'll
 become
 more
 powerful


but
 you
 need
 a
 little
 bit
 more
 than
 that


you
 know
 the
 the
 way
 human
 brains
 learns


with
 with
 very
 few
 examples
 that's
 not


the
 way
 transformers
 learn


and
 you
 know
 I'm
 not
 saying
 that
 we
 need


to
 create
 an
 Einstein
 or
 a
 ger
 but
 there


has
 to
 be
 an
 architectural
 leap
 that
 is


able
 to
 create
 these
 manifolds
 and
 just


throwing
 new
 data
 will
 not
 do
 it.
 It'll


just
 smoothen
 out
 the
 already
 existing


manifolds.


Is
 that
 something?
 So,
 is
 is
 your
 goal


to
 actually
 help


like
 think
 through
 new
 architectures
 or


are
 you
 primarily
 focused
 on
 putting


formal
 bounds
 on
 existing
 architectures?


A
 bit
 of
 both.
 I
 mean,
 the
 the
 former


goal
 is
 the
 more
 ambitious
 one
 that
 uh


everybody
 is
 chasing
 and
 yeah,
 I
 I
 I


think
 about
 that
 constantly.


Are
 are
 there
 any
 new
 even
 like
 uh
 sort


of
 hints
 at
 a
 new
 architect
 or
 like
 have


we
 started
 to
 make
 any
 progress
 on
 on
 on


new new
 architectures


or
 is
 it


uh


you
 you
 know
 um
 YAN
 has
 been
 pushing
 at


this
 JA
 architecture.


Yeah.


Energy
 based
 architectures.
 Uh
 they
 they


seem
 promising.
 the
 the
 the
 way
 I
 have


been
 sort
 of
 thinking
 about
 it
 is


you
 know
 you
 there's
 this
 uh
 set
 of
 uh


benchmarks
 or
 the
 arc
 prize


right
 that
 Mark
 Mike
 Koop
 and
 France


have
 have
 and
 if
 you
 understand
 why
 the


LLMs
 are
 failing
 on
 this
 test
 maybe
 you


can
 sort
 of
 reverse
 engineer
 a
 new


architecture


that
 will
 help
 you
 uh
 succeed
 in
 that


right
 and
 I
 agree
 with
 a
 lot
 of
 what


several
 people
 say
 that
 you
 know


language
 is
 great
 but
 language
 is
 not


the
 answer
 you
 know
 when
 I'm
 looking
 at


uh
 catching
 a
 ball
 that
 is
 coming
 to
 me


I'm
 mentally
 doing
 that
 simulation
 in
 my


head
 I'm
 not
 translating
 it
 to
 language


to
 figure
 out
 where
 it'll
 land
 I
 I
 do


that
 simulation
 in
 my
 head
 so
 a
 way
 you


know
 one
 of
 the
 new
 architectures


architectural
 things
 is
 how
 do
 we
 do
 how


do
 we
 get
 these
 models
 to
 do
 approximate


simulations


to
 test
 out
 that
 idea
 and
 whether
 to


proceed
 or
 not.
 So,


so,
 so
 yeah,
 we
 have,
 you
 know,
 another


thing
 that
 I've
 always
 wondered
 about
 is


did
 we
 develop
 as
 humans,
 did
 we
 develop


language
 because
 we
 were
 intelligent
 or


because
 we
 developed
 language,
 we


accelerated
 our
 intelligence.


So,
 I
 I
 don't
 know
 which
 side
 of
 the


camp
 uh
 you
 follow
 on
 that
 question.
 I


mean
 what's
 interesting
 is
 like
 you
 have


these
 anecdotal
 examples
 of


humans
 developing
 languages
 denovo
 that


have
 been
 recorded
 right
 like
 it's
 it's


either
 the
 Guatemalan
 or
 Nicaraguan
 sign


language
 right
 where
 there
 is
 these


students
 that
 develop
 their
 own
 language


without
 being
 taught
 and
 so
 that
 would


suggest
 that
 languages
 follows


intelligence.
 The
 problem
 is
 is
 they're


all
 anecdotal,
 right?
 Like
 who
 knows
 if


somebody
 didn't
 teach
 them
 sign


language?
 Like
 nobody
 really
 knows.


There
 is
 no
 controls.
 So
 this
 is
 all


these
 observational
 studies
 and
 there's


so
 few
 of
 them
 you
 have
 to
 wonder
 if


it's
 just
 kind
 of
 sloppy
 observation.


And
 so
 I
 think
 that
 the
 question
 is


still
 outstanding.


Yeah.
 So


I
 I
 mean
 language
 definitely
 accelerated


our
 intelligence.
 There's
 no
 question


about
 that.


Yeah.


But
 which
 followed
 which
 we
 don't
 know.


But
 I
 view
 it
 as
 I
 view
 it
 as
 I
 view
 it


as
 a
 networking
 problem
 naturally
 which


is
 once
 you
 have
 languages
 you
 can


communicate
 and
 when
 you
 communicate


store
 you
 can
 replicate.
 Yeah.


Yeah.
 Exactly.


Exactly.
 Right.


Cool.


Again
 this
 is
 kind
 of
 a
 wonky
 question


but
 um


uh


you
 know
 I
 think
 one
 thing
 that
 you've


brought
 to
 the
 discourse
 and
 for
 those


that
 are
 listening
 to
 this
 I
 really


think
 that
 you
 should
 look
 up
 Val's
 work


and
 read
 it.
 I
 just
 think
 it'll
 give
 you


a
 really
 really
 especially
 if
 you
 have
 a


systems
 background
 like
 a
 networking
 or


systems
 background
 give
 you
 a
 really


really
 good
 understanding
 of
 kind
 of the


bounds
 on
 these
 um
 but
 like
 the
 toolkit


that
 you
 draw
 from
 is
 like
 information


theory
 and
 like
 more
 formal


have
 you
 found
 that
 the
 AI
 community
 is


receptive
 to
 this
 or
 is
 it
 like
 two


different
 cultures
 two
 different
 planets


trying
 to
 communicate
 and
 not
 a
 lot
 of


common
 ground
 like
 how
 have
 you
 found


like
 bringing
 like
 the
 networking
 view


of
 the
 world
 to
 the
 AI
 realm.


Some
 of
 them
 are
 receptive
 to
 it


definitely
 but
 uh


you
 know
 uh
 these
 large
 conferences
 and


their
 reviewing
 process
 it's
 so
 random


and
 the
 kind
 of
 questions
 they
 ask
 you


know
 I'm
 a
 modeling
 person
 I
 like
 to


model
 things
 and
 you
 know
 I
 submitted


one
 version
 of
 this
 work
 to
 one
 very


famous
 uh
 uh
 machine
 learning
 or
 AI


conference
 and
 the
 reviewer
 said
 okay


this
 is
 the
 model.
 So
 what


so
 so
 that
 is
 uh


that's
 absolutely
 remarkable.
 So
 like


you
 you've
 actually
 taken
 a
 system
 that


nobody
 understands
 we
 have
 no
 models
 for


you
 actually
 provided
 some
 model
 that
 we


can
 use
 to
 analyze
 it
 and
 uh
 that
 alone


wasn't
 sufficient.


They're
 asking
 so
 where
 are
 the
 large


scale
 experiments
 to
 to
 prove
 this?
 I
 do


listen
 I
 I
 honestly
 I
 mean
 I
 I
 find


there's
 so
 much
 empiricism
 in
 like
 the


the
 current
 you
 know
 AI
 community


exactly
 because
 we
 don't
 understand
 the


systems
 you
 know
 it
 kind
 of
 reminds
 me
 I


I
 I
 feel
 like
 um
 I
 feel
 like
 systems


went
 the
 other
 way
 right
 it's
 like
 we


had
 all
 of
 these
 models
 but
 then
 we


didn't
 understand
 how
 the
 systems
 worked


and
 then
 we
 just
 like
 actually
 did


measurement
 it
 feels
 like
 ML
 and
 or
 the


AI
 stuff
 is
 the
 opposite
 which
 is
 like


we
 know
 we
 don't
 understand
 them
 and
 so


we
 just
 measure
 them
 but
 now
 we're


trying
 to
 like
 come
 up
 with
 the
 models


Yeah,
 exactly.
 So,
 it
 was
 so
 easy
 in


some
 sense
 to
 build
 these
 uh
 artifacts


and
 then
 just
 measure
 them
 that
 people


have
 been
 going
 around
 trying
 to
 do


that.
 And


you
 know,
 one
 one
 term
 I
 really
 dislike


is
 prompt
 engineering.


Why?


You
 know,
 engineering
 used
 to
 mean


sending
 a
 man
 to
 the
 or
 providing
 59's


reliability.


Prompt
 engineering
 is
 prompt
 twiddling.


Yeah.
 Right?
 You
 you
 fiddle
 with
 a


prompt
 and
 the
 model
 changes
 and
 the
 the


inference
 uh
 the
 output
 changes
 and
 you


know
 you
 have
 like
 hundreds
 of
 papers


just


just
 you
 know
 doing
 one
 experiment
 on


the
 other
 changing
 a
 prompt
 this
 way


that
 way
 and
 writing
 their
 observations


and
 as
 a
 result
 you
 know
 lots
 of
 these


papers
 are
 being
 written
 are
 being


submitted
 for
 review.
 reviews
 get
 busy


looking
 at
 all
 this
 kind
 of
 empirical


work
 and
 my
 personal
 taste
 is
 to
 first


try
 to
 understand
 model
 it.


Yeah.


And
 then
 you
 can
 do
 the
 other


sound
 like
 a
 true
 theory
 guy.


I
 don't
 know
 about
 this
 bit
 twiddling


like


let
 me
 ask
 one
 more
 LM
 question
 which
 is


are
 there
 any
 benchmarks
 or
 real
 world


tasks
 that
 if
 if
 they
 if
 they
 occurred


you'd
 sort
 of
 reevaluate
 and
 say
 hey


maybe
 LLMs
 are
 you
 know
 closer
 to
 the


path
 to
 AGI
 than
 I
 thought


if
 there
 were
 any
 real
 world
 task


good


you
 know
 which
 uh


for


LLMs


uh
 or
 these
 models
 the
 one


domain
 where
 you
 have
 the
 most
 training


data
 is
 probably
 coding


right
 and
 coding
 is
 where
 uh


you
 can
 also
 have
 the
 most
 structure


Yeah.


And
 yet
 anyone
 who
 has
 used
 uh
 these


tools
 whether
 it's
 cursor
 or
 whatever
 or


cloud
 codes


LLMs
 continue
 to
 hallucinate
 continue
 to


generate
 unreasonable
 code
 you
 know
 you


have
 to


you
 have
 to
 constantly
 uh
 babysit
 these


models.


So
 the
 day
 an
 LLM
 can
 create
 a
 large


software
 project
 without
 any
 uh


babysitting
 is
 the
 day
 I'll
 be
 a
 little


bit
 convinced
 that
 it's
 towards
 easy.


But
 again


I
 I
 don't
 think
 uh
 it'll
 be
 able
 to


create
 new
 science.
 If
 it
 does
 that's


when
 I'll
 be
 convinced.


I
 you
 know
 I
 think
 that
 you
 can
 almost


take
 a
 definitional
 approach
 to
 answer


this
 question.
 Vash
 like
 the
 problem


with
 these
 types
 of
 questions
 is
 is
 if


you
 have
 billions
 of
 dollars
 and
 you
 can


collect
 whatever
 data
 you
 want
 you
 can


make
 a
 model
 do
 anything
 you
 want
 right


and
 so
 like
 you
 know
 what
 I'm
 saying


like
 it's
 at
 some
 level
 you've
 got
 this


entire
 capital


structure
 machinery
 behind
 these
 models


so
 you're
 like
 oh
 it
 can
 be
 good
 at


science
 well
 sure
 you
 put
 a
 billion


dollars
 in
 solving
 material
 science
 and


collect
 all
 this
 data
 you'll
 be
 good
 at


material
 science
 or
 or
 whatever
 it
 is


and
 so
 but
 there
 is
 a
 definitional


answer
 which
 is


and
 and
 and
 and
 I'm
 going
 to
 draw
 from


your
 work
 which
 is
 there
 is
 a
 manifold


that's
 in
 there
 based
 on
 the
 data
 it's


been
 training
 on


and
 then
 the
 question
 is
 is
 if
 it
 ever


produces
 something
 that's
 off
 like
 a
 new


manifold.


So
 considering
 the
 existing
 trained
 data


if
 it
 ever
 does
 that
 if
 it
 does


something
 that's
 outside
 of
 that


distribution
 then
 clearly
 we're
 on
 a


path
 to
 to
 learning
 new
 things
 and
 if


not
 then
 everything
 is
 just
 a


computational
 step
 from
 what's
 already


known.


Yeah.
 That's
 all
 I
 mean.


And
 I
 guess
 I
 guess
 the
 count
 I
 guess


the
 counter
 to
 that
 would
 be
 maybe
 all


humans
 do
 is
 work
 on
 their
 own
 manifold


and
 Einstein


you
 know
 was
 lucky
 or
 something
 I
 guess


would
 be
 the
 counter
 to
 that
 but


yeah
 so
 so
 you
 know
 there
 are
 several


Einstein
 examples
 and
 yeah
 it's
 creating


this
 new
 manifold.
 I
 didn't
 want
 to
 use


that
 definitional
 answer.
 I
 thought
 it


might
 sound
 too


too
 wonky
 too
 mathematical
 but


essentially
 uh
 if
 LLM's
 really
 created


this
 new
 manifold


then
 I
 would
 be
 convinced


but
 so
 far
 they
 have
 just
 gotten
 better


at
 navigating
 the
 existing
 manifold
 the


existing
 training
 set


which
 is
 hugely
 powerful
 and
 is
 going
 to


change
 the
 world


which
 is
 hugely
 I'm
 I'm
 not
 denying
 that


I
 think
 they
 are
 extremely
 extremely


good


at
 what
 they
 can
 do
 but
 there's
 a
 limit


to
 what
 they
 can
 do


so
 I
 have
 one
 quick
 question
 what's
 next


for
 you?
 I
 mean,
 you've
 uh
 you
 you


you've
 tackled
 in
 context
 learning,


you've
 got
 a
 model
 for
 LLMs,
 and
 now


you've
 got
 a
 generalized
 model
 for
 like,


you
 know,
 like
 their
 solution
 space.


What
 are
 you
 thinking
 about
 tackling


next?


Yeah.
 In
 terms
 of
 uh
 modeling
 or


academically
 an
 LLM?


Academically,
 yeah,
 academically,
 I'm
 uh


you
 know,
 I'm
 I'm I'm
 thinking
 of
 this.


What
 is
 the
 architectural
 leap
 that
 is


needed?


Oh,
 that's
 exciting.
 to
 create
 this
 uh


new
 manifold
 and
 how
 do
 we
 use
 you
 know


multimodal
 data


awesome


to
 expand
 that
 come
 back
 talk
 to
 us


that's
 right
 we'd
 love
 that
 so
 I
 mean
 uh


you
 know
 even
 with
 uh
 LLMs
 you
 know
 the


in
 the
 paper
 we
 say
 that
 you
 can
 improve


uh
 the
 inference
 by
 following
 this
 low


or
 minimum
 entropy
 path


so
 so
 that's
 a
 very
 sort
 of
 small
 step


that
 We
 are
 taking
 you
 know
 we
 are


building
 and
 training
 models
 that
 will


do
 inference
 based
 on
 the
 entropic
 path.


Yeah.


By
 the
 way
 is
 is
 model
 probe
 still
 up?


Token
 probe.
 Yeah.
 Yeah.
 Token
 probe
 is


still
 up
 and
 and
 you
 can
 see
 actually


the
 you
 know
 token
 probe
 is
 a
 software


that
 we
 built
 and
 thanks
 to
 Martin
 and


A6Z's
 generosity
 is
 running
 on
 your


servers
 and
 anyone
 can
 go
 and
 test.
 And


what
 we
 have
 done
 there
 is
 we
 actually


show
 the
 entropy.


Yeah,
 it
 is
 so
 enlightening.
 I
 recommend


anybody
 listening
 to
 this
 who's


interested


actually
 check
 out
 token.
 It
 shows
 you


the
 limit.


Yeah.
 As
 you
 go
 along,
 it's
 remarkable.


Yeah.
 So
 in
 context
 learning,
 you
 know,


you
 create
 your
 new
 DSL
 and
 you
 give
 it


to
 the
 prompt
 and
 you
 can
 see


the
 confidence
 rising
 with
 each
 new


example,
 the
 entropy
 reducing
 and
 that


sort
 of
 is
 a
 validation
 of
 the
 model.


You
 can
 see
 it
 sort
 of
 unfurling
 and


right
 in
 front
 of
 your
 eyes.
 The
 token


probe
 is
 running.
 Thanks.
 Thanks
 again.


Michelle,
 thanks
 so
 much
 for
 coming
 on


the
 podcast.
 This
 is a
 great


conversation.


It
 was
 great
 fun.


Thank
 you.
 Thank
 you
 so
 much
 again.


[Music]