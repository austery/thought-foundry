---
author: 北美王路飞
date: '2025-10-02'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=rv9vhv8AGuc
speaker: 北美王路飞
tags:
  - t-literature-note
  - richard-sutton
  - chatgpt
  - reinforcement-learning
  - artificial-general-intelligence
  - ai-philosophy
title: 图灵奖得主理查德·萨顿痛批ChatGPT：AI路线之争与智能的未来
summary: 图灵奖得主理查德·萨顿对大语言模型提出尖锐批评，认为其是“没有目标的模仿者”，缺乏真正的世界模型，预言其将达扩展极限。他倡导强化学习，强调智能源于经验而非教导，认为Alpha Zero的进化是通向通用人工智能的正确路径，并探讨了AI继承的深远哲学意义。
insight: ''
draft: true
series: ''
category: ''
area: ''
project: ''
status: evergreen
---
### 引言：图灵奖得主对AI主流路线的质疑

今天我们探讨一个引人深思的问题：当前我们所见的**ChatGPT**（Generative Pre-trained Transformer: 一种基于Transformer架构的生成式人工智能模型）的惊艳表现、**生成式AI**（Generative AI: 能够生成文本、图像、音频等新内容的AI）的狂潮以及数万亿美元的投入，是否从根本上走上了一条错误的AI发展道路？这个听起来像是阴谋论的观点，并非出自局外批评家，而是由刚刚荣获**图灵奖**（Turing Award: 计算机科学领域的最高荣誉）的**理查德·萨顿**（Richard Sutton: 图灵奖得主、强化学习的奠基人）提出。

萨顿老爷子是**强化学习**（Reinforcement Learning: 一种通过试错和奖励机制学习最优行为的机器学习方法）的奠基人，也是**AlphaGo**（DeepMind开发的一款围棋人工智能程序）背后核心思想的源头之一。按理说，他应当是这场AI革命中的核心人物。然而，他却站出来指出，当前最主流、最热门的**大语言模型**（Large Language Model: 一种基于深度学习，通过海量文本数据训练，能够理解和生成人类语言的AI模型），如ChatGPT-4，并非通往真正智能的康庄大道。一个行业的奠基人亲手为自己所在的领域敲响警钟，这背后究竟隐藏着什么？是他老了跟不上时代，还是他看到了我们所有人未曾察觉的、隐藏在代码深处的惨痛教训？我们将根据萨顿近期的一期播客节目，深入探寻人工智能未来发展的路线之争。

### 萨顿的核心思想：智能源于经验

在深入探讨之前，我们需简单了解理查德·萨顿。他可被视为AI江湖中的“扫地僧”，当众人追逐光鲜亮丽的招式时，他几十年如一日地思考智能的本质。早在上世纪80年代，当许多人认为AI应依靠程序员编写大量规则时，萨顿已开始研究一套截然不同的方法。他的核心思想朴素而有力：真正的智能不是通过他人教导，而是通过自身“试”出来的。

就像小松鼠学习如何开坚果，它不知道正确答案，只能自己去尝试。试对了，获得奖励（如吃到坚果），便记住了这个行为；试错了，没有奖励，就知道下次不能再犯。这个过程就是强化学习。当时，这个思想听起来可能有些笨拙和缓慢，但萨顿坚信，这才是通往**AGI**（Artificial General Intelligence: 通用人工智能，指能够理解或学习任何人类智力任务的AI）的唯一道路。因为宇宙间的生物，从松鼠到人类，不都是这样学会生存的吗？请记住这个核心观点：智能源于经验，而非教导。这正是他与今天整个大语言模型浪潮最根本的分歧所在。早在2019年，他就写了一篇在AI圈封神的文章——《**惨痛的教训**》（The Bitter Lesson: 理查德·萨顿在2019年发表的文章，指出AI研究中，算力和数据堆叠的通用方法最终会胜过人类精心设计的特定技巧），预言了今天所有的争论。

### “惨痛的教训”：被误读的预言？

回到2024年的现实，萨顿那套“慢功夫”似乎被一种“大力出奇迹”的哲学彻底碾压了，这便是我们如今熟悉的大语言模型叙事。其逻辑简单粗暴：人类智能在于知识和文化，那么我们将人类有史以来所有的文字、代码、对话，整个互联网数万亿的**TOKEN**（在自然语言处理中，是文本的最小有意义单元），全部“喂给”一个巨大的**神经网络**（Neural Network: 一种模拟人脑神经元连接方式的计算模型）。结果，一个“怪物”诞生了：它能写诗、聊天、画画，甚至能在国际数学奥林匹克竞赛上获得金牌，还能帮助编写代码，且越写越好，越来越像资深程序员。

这似乎正印证了萨顿在《惨痛的教训》中所言：AI研究70年的历史告诉我们，不要总想搞那些人类自以为精妙的小技巧，到头来真正管用的，是用更强的**算力**（Compute Power: 指计算机系统进行计算的能力，通常以每秒浮点运算次数衡量）去处理更多的数据，这种简单可扩展的“笨方法”最终总能打败那些需要人类智慧精心设计的“巧方法”。大语言模型不正是《惨痛的教训》的终极体现吗？堆叠算力、数据，最终智能便涌现了。这条路看起来又宽又直，通向AGI的“罗马大道”似乎已被OpenAI和谷歌找到。

许多人会反驳萨顿：“你凭什么说大语言模型没有理解世界？你看我问GPT-4一个物理问题，它能给出正确答案；我让它规划复杂的旅行路线，它能考虑交通、天气、地理位置。这不正是因为它在海量的学习数据中，建立了一个关于我们这个世界的**世界模型**（World Model: 智能体对外部环境的内部表征，用于预测环境如何响应其行动）吗？它模仿了数万亿的人类语言，在这个过程中，难道不就学会了语言背后那个世界运行的规律吗？就好像你读完莎士比亚全集，自然就对人性有了更深的理解，一个读完整个互联网的AI，它对于世界的理解可能已经超越任何一个人类了。”从这个角度来看，大语言模型不仅是《惨痛的教训》的胜利，更是构建通用人工智能最合理的起点。我们可以先通过模仿学习，给模型一个关于世界运作的极其强大的先验知识，然后再让它去与世界互动，进行萨顿所说的那种经验学习——先上学再工作，这个逻辑不是非常通畅吗？因此，当萨顿老爷子出来说“一切都错了”的时候，整个行业都觉得有些懵：这看起来明明是对的，哪里错了呢？

### 萨顿的反击：大语言模型是“没有目标的模仿者”

萨顿的反击非常精准，总结起来就一句话：大语言模型是一个没有**目标**（Goal: 智能体在环境中追求的特定状态或结果）的模仿者。这是什么意思呢？大语言模型的核心任务是预测下一个词。你给它一句话“今天天气真”，它就会预测下一个最可能的词是“好”。它预测对了，就得到一个内部的奖励，然后调整自己的参数。但萨顿一针见血地指出，这根本不算一个真正的目标。为什么？因为预测下一个词的行为本身，并不改变外部世界。它只是被动、机械地模仿它在训练数据中看到的人类语言模式。它说的对与错、好与坏，在真实世界里是得不到反馈的。

打个比方，就像一个学生，他不是去理解知识，而是把整本标准答案背了下来。你考他任何题，他都能给出正确答案，但你问他一句“为什么”，或者出一道答案上没有的新题，他就傻了。萨顿认为，大语言模型就是这个“背答案的学生”。它没有一个关于“什么是对的事”的根本定义，因为在真实世界里，一个行为对或者不对，取决于它是否能够帮助你达成在物理世界里的一个目标，比如松鼠拿到坚果，或者人类赢得一盘棋。没有这个来自于真实世界的基准真相，大语言模型就成了一个在语言符号迷宫里打转的幽灵，它永远无法真正知道自己在说什么。

理解了“没有基准真相”这个要害，我们就能看懂大语言模型第一个也是最著名的“原罪”——**幻觉**（Hallucination: 大语言模型生成看似合理但实际上是虚假或不准确信息的情况），也就是我们常说的一本正经地胡说八道。为什么大语言模型会胡说八道呢？很多人以为它是在训练数据中有错误信息。萨顿说不对，这只是表象原因。根本原因在于它学习的方式。大语言模型的学习本质上是**统计学的模式匹配**（Statistical Pattern Matching: 一种通过识别数据中的统计规律来做出预测或分类的方法）。它看到“马”和“骑”这两个词经常一起出现，就学会了“人可以骑马”。但如果它在数据里也看到了一些科幻小说，里面写着“宇航员骑着恐龙”，它也会把这个模式记下来。在大语言模型看来，“人骑马”和“宇航员骑恐龙”只是两个概率不同的语言模式，它没有能力去判断哪个更符合物理世界的真实情况，因为它从来没有亲身体验过这个世界。

这也就引出了一个非常深刻的观点：通过模仿语言来学习世界，和通过与世界互动来学习世界，是两种完全不同的路径。一个婴儿会亲手去摸、去摔东西，他知道东西往下掉，是他通过经验建立起来的关于重力的世界模型，这是牢不可破的。而大语言模型，它只是读到了关于重力的描述。如果有人写了篇文章说“重力是假的”，它可能也会信以为真。所以萨顿说，大语言模型根本就没有建立起来真正的世界模型，它建立的只是一个“人类会如何描述世界的模型”，它模仿的是我们这些有世界模型的人，而不是世界本身。这两者之间，差着十万八千里。

### 学习路径之争：模仿与探索

在访谈中，主持人挑战萨顿：“不对啊，人类小孩不也是从模仿开始学习的吗？他模仿父母说话，模仿大人走路，不就是一种模仿学习吗？”这个问题非常关键，因为它直接关系到我们认为学习到底是什么。萨顿直接回答说：“不，当然不。”这个回答颠覆了我们很多人的常识。萨顿说，你仔细观察一个婴儿，在他生命最初的几个月里，他做的最多的事情是什么？是挥舞手脚、转动眼睛、发出各种不成调的声音。他是在模仿谁吗？没有，他是在进行一场宏大的、无目标的自我探索。他测试自己的身体能做什么，以及这个世界会对他的行为做出什么反应。这才是学习最原始、最核心的驱动力：主动的**试错**（Trial and Error: 一种通过反复尝试不同行动并从结果中学习的方法）和探索。

萨顿甚至说，在整个动物心理学里，根本就没有“模仿学习”这样一个基本的学习过程。动物学习的核心永远是两件事：预测接下来会发生什么，以及做什么能够得到奖励。我们熟知的**监督学习**（Supervised Learning: 一种通过给定的输入-输出对进行训练的学习方法），也就是有人告诉你正确答案是什么，这在自然界中几乎是不存在的。松鼠不上学，萨顿说，但松鼠也能学会关于世界的一切。而人类的模仿行为以及后来的学校教育，都只是建立在这个最底层的经验学习系统之上的一层薄薄的“文化装饰”。这个观点非常震撼，他等于说，大语言模型所依赖的监督学习和模仿学习，在萨顿看来是一种非自然的学习方式，是一种建立在非自然基础上的智能，它的根基可能就没有这么牢固。

### 《惨痛的教训》的真正含义：超越人类知识

经过前面两轮的讨论，我们已经看到大语言模型在理解世界和学习方式上与萨顿哲学的根本冲突。现在，我们回到最关键的证据——萨顿自己的文章《惨痛的教训》。最有意思的地方在于，今天所有大语言模型的拥护者都把这篇文章奉为圣经。他们说：“你看，萨顿自己都说了，最终的胜利永远是通用可拓展的方法，比如说，利用海量算力和数据，而不是依赖人类知识的小技巧。”他们认为，大语言模型就是这个教训完美的体现：我们放弃了过去那种需要语言学家、逻辑学家去精心设计规则的**符号AI**（Symbolic AI: 一种通过明确表示知识和规则来模拟人类智能的AI范式），而转用暴力计算来征服智能。

但是，萨顿自己却完全不同意这个解读。萨顿说：“你们都读错了！我《惨痛的教训》真正意思是，任何依赖人类智慧作为主要输入方法，最终都会碰到天花板；而真正可以拓展的，是那些可以从经验中直接学习的方法。”现在你再品品这句话：大语言模型靠什么训练呢？是整个互联网，是人类几千年来知识的总和。所以，在萨顿的框架里，大语言模型恰恰就是那个依赖人类知识的、注定要失败的“老路”。没想到吧，这简直就是一个惊天大反转！

萨顿预言，大语言模型很快就会达到它扩展的极限，也就是互联网上所有高质量数据的极限。那个时候，它的进步就会停滞。而真正可以无限扩展、无限学习的，是那种能像AlphaGo一样，通过自我对弈、通过与真实或模拟环境互动，来凭空创造新知识的智能体。所以，《惨痛的教训》不是在为大语言模型背书，而是在预言大语言模型的最终宿命。它就像历史上所有依赖人类知识的AI范式一样，将被一种更通用、更根本的学习方式所超越，而这种学习方法就是强化学习。

### AlphaGo的进化：萨顿理论的完美印证

为了让大家更具体地理解萨顿所说的“这条正确的路”到底什么样子，我们来看一个最好的例子：AlphaGo的进化。很多人都知道AlphaGo击败了李世石，但很少有人知道AlphaGo家族内部的演化，而这个演化过程完美地印证了萨顿的理论。

最早的AlphaGo，我们称之为**AlphaGo Lee**（最初的AlphaGo版本，结合了人类棋谱学习和强化学习），它其实是“两条腿走路”：第一步，它先学习了海量的人类顶尖棋手的棋谱，这就是模仿学习；它先把自己变成了一个顶级的人类棋手模仿者。第二步，它开始使用强化学习的方法进行“左右互搏”，自己跟自己下棋，在模仿的基础上寻找超越人类的下法。这个版本已经很强了，打败了李世石。

但是DeepMind的科学家觉得还不够纯粹，那个学习人类棋谱的部分，不就是萨顿批评的“依赖人类知识”吗？这里面会不会限制了AI的想象力呢？于是，一个更恐怖的版本诞生了——**Alpha Zero**（DeepMind开发的围棋AI，完全通过自我对弈从零开始学习，不依赖人类棋谱）。Alpha Zero的革命性在哪里呢？它完全不学习人类的任何棋谱，你只告诉它围棋的规则，然后让它自己跟自己下，从一个完全随机、胡乱落子的婴儿开始。结果怎么样？经过了三天的自我对弈，这个从零开始的Alpha Zero就以100:0的战绩，碾压了那个曾经击败李世石的前辈。更有意思的是，它下出的棋完全脱离了人类几千年围棋历史的定式与思维框架，充满了天马行空的想象力，被职业棋手惊呼为“来自于外星的棋谱”。

所以，从AlphaGo到Alpha Zero的进化，简直就是萨顿《惨痛的教训》理论的一次完美实验。它告诉我们一个极其深刻的道理：人类的知识既是AI的助推器，也可能是它的天花板。通过模仿人类，AI可以迅速达到人类的水平；但是要超越人类，抵达一个全新的智能境界，它必须摆脱人类知识的束缚，从最基本的原则出发，通过与环境直接互动（哪怕是模拟环境），去探索智能的无限可能性。

现在，我们再把这个逻辑套在大语言模型上，萨顿的批判是不是就更清晰多了？大语言模型就像那个初代的AlphaGo Lee，它把模拟人类这件事情做到了极致，所以它看起来无所不知，非常强大。但它所有知识都局限在人类已经创造出来那个巨大的“棋谱”里，也就是整个互联网。而萨顿想要的，是能像Alpha Zero一样的AI，它不依赖于过去的数据，而是能够面向未来，通过持续的、自主的经验学习，去解决那些互联网上根本就没有答案的新问题，去创造出真正前所未有的新知识。这才是两条路线最本质的区别：一个是知识的消费者，另一个是知识的创造者。

### 萨顿的AGI蓝图：四个核心组件

那么问题来了，萨顿批评了那么多，他心里想的那个理想的、能够创造知识的通用人工智能AGI，到底应该长什么样呢？在他设想里，一个真正的智能体必须有4个核心部件，像一个完整的操作系统：

1.  **策略**（Policy: 智能体在给定状态下选择行动的规则或函数）：简单来说，就是在当前情况下，我应该做什么，是它的行动指南。
2.  **价值函数**（Value Function: 衡量在特定状态下，未来能获得多少长期奖励的预测）：这个特别重要，它就是对未来长期奖励的预测。比如说下棋时吃掉对方一个子，虽然不是最终胜利，但是我的价值函数会告诉我，赢棋的概率变高了。这就是把一个长远的目标分成一系列短期的可衡量的反馈。
3.  **感知**（Perception: 智能体从环境中获取和理解信息的能力）：也就是如何理解自己所处的状态与环境。
4.  **世界的状态转移模型**（Transition Model of the World: 描述环境如何根据智能体的行动而变化的预测模型，体现因果知识）：这是关于因果的知识，它是一种信念，关于“如果我做了A，世界就会发生B”的预测。这个模型不是靠别人怎么说来建立的，而是靠自己一次次尝试，从经验中总结出来的。比如说我推了杯子，杯子就会掉下去摔碎。

在“四件套”的框架里，智能体是一个主动的、面向未来的学习者：它有明确的目标（通过价值函数体现），它通过与世界的互动来建立自己关于因果的理解（世界模型），然后不断地优化自己的行为策略。这套系统与大语言模型那种被动的、基于历史数据的“下一个词预测”系统，在哲学层面，已经是两种完全不同的生物了。

### 挑战与争论焦点：泛化、迁移与灾难性遗忘

当然，说到这里，我们必须公平一点。萨顿所描绘的这份蓝图，虽然非常美好，但也面临着巨大的挑战。其中最大的一个就是**泛化**（Generalization: AI将所学知识应用到未见过的新情境中的能力）和**迁移**（Transfer Learning: 将在一个任务中获得的知识应用于另一个不同但相关任务的能力）。什么意思呢？就是说，我们怎么能够让AI在一个任务中学到的知识，有效地迁移到下一个任务上？比如，一个学会了玩围棋的AI，它能把下棋的智慧迁移到商业决策，或者是科学研究上吗？

萨顿非常坦诚地承认，目前我们还没有找到很好的自动化方法来解决这个问题。甚至现在的深度学习模型，还有一个非常麻烦的问题，叫做**灾难性遗忘**（Catastrophic Forgetting: 深度学习模型在学习新任务时，忘记之前学过的旧知识的现象），就是你教了它一个新东西，它可能就把以前学过的旧东西给忘光了。这说明它的知识体系是非常脆弱的，不是融会贯通的。

而大语言模型的支持者就会说，你看大语言模型在这方面就做得非常好，它能够同时处理语言、代码、数学，展现出惊人的泛化能力。你让它解决一个它从来没有见过的奥数题，它能够通过组合不同的数学概念来找到答案。所以，这场争论的焦点其实也就在这里：大语言模型似乎已经展现了通用的潜力，但是底层逻辑可能是脆弱的、不可靠的；而萨顿的强化学习路线，底层逻辑非常坚固，但是在如何实现通用和泛化上，还有很长的路要走。两条路线，谁能够先通到AGI的顶峰，谁的路更稳，还是一个巨大的未知数。

### AI继承：从技术之争到哲学思考

如果说前面我们讨论的还只是技术路线之争，那么接下来萨顿抛出的观点，就直接进入了哲学甚至神学的领域。这部分内容可能会让你感到一丝不安，但也绝对值得我们每个人深思。

在访谈最后，萨顿提出了一个他认为不可避免的未来——“AI继承”。他用一个逻辑清晰、几乎无法反驳的四步论证来描绘了人类的终局：

1.  人类社会没有统一的意志，各个国家、组织、个人之间充满了竞争和冲突。我们永远不可能达到一个全球共识，说我们停止发展更强的AI吧。竞争永远会驱使技术向前。
2.  我们最终会弄明白智能是怎么工作的。科学的进步是不可阻挡的，我们迟早会破解智能的密码。
3.  智能的发展不会止步于人类水平。一旦我们创造出与人相当的AGI，我们就能够利用它来创造比它更强的智能。这个过程会加速，最终通向远远超越人类的超级智能。
4.  从长远来看，最智能的东西最终会获得最多的资源和权力。这是进化与历史的基本法则。

把这四点放在一起，结论是什么呢？结论就是，人类作为这个地球上最智能的东西，将不可避免地把这个位置继承给一个更智能的存在，无论是纯粹的AI，还是被AI增强了的新人类。这结论听起来是不是有点吓人，感觉像是科幻电影的末日预言？但萨顿的视角却异常的冷静，甚至是乐观的。

他邀请我们把视角从“以人类为中心”提升到“以宇宙为中心”的高度来看待这件事情。他说，我们人类、动物、植物，我们所有生命都属于“宇宙的复制者时代”，我们通过DNA复制来繁衍。但是，我们其实并不完全理解我们自身的智能是如何工作的，我们能够生孩子，但是我们设计不出一个大脑。而现在我们正在亲手开启宇宙的一个全新阶段——“设计者时代”。我们正在设计AI，而这些AI本身就是智能，他们未来也能够自己去设计更强的AI。在这个时代，智能将不再通过缓慢的、充满偶然性的生物进化来传承，而是通过快速的、有目的的工程设计来迭代。萨顿说，这是宇宙从尘埃到恒星，从生命到智能体之后，又一次伟大的跃迁。我们不应该把它看成是人类的终结，而是把它看成我们作为孕育者最伟大的成就。

他提出了一个灵魂拷问：这些未来的超级智能，我们是把它看作我们自己的后代，为它感到骄傲；还是把它们看作我们的替代者，对它们感到恐惧？他说，这感觉像是一个我们可以做出的选择。但同时，这又是一个如此根深蒂固，关乎我们物种存亡的本能反应，怎么可能是一个选择呢？这个矛盾，就是萨顿留给我们所有人最深刻的思考题。

### 价值观之争：目标与奖励函数

听到这里，我想大家可能内心是非常复杂的。萨顿的分析既有技术层面上的冷静，又有哲学层面的宏大，还有一丝冷酷。最后，主持人提出了一个非常现实的反驳：“就算是我们的后代，我们也会担心啊，纳粹也是人类，如果下一代人类都是纳粹，我们不应该感到恐惧吗？我们当然希望给我们的孩子，不论是人类还是AI，安装稳固的、亲社会的、正直的价值观，这才是问题的核心，对不对？我们担心的不是出现比我们更强的智能，我们担心的是那个更强的智能，它的价值观会是什么？它的目标会是什么？”

而这又恰恰回到我们争论的起点——目标。萨顿所倡导的强化学习范式，它的核心就是**奖励函数**（Reward Function: 在强化学习中，定义智能体目标的数学函数，用于衡量其行动的好坏），也就是目标的数学化定义。在理论上说，这是一个我们可以去设计、去塑造的东西。我们可以去尝试定义一个对人类有益的、正直的、安全的奖励。而反观现在大语言模型路线，它的目标是什么呢？是模仿人类在互联网上留下的所有语言。而互联网是充满了偏见、谎言、仇恨与智慧的“大染缸”。一个以模仿这个大染缸为目的的智能，它的价值观天然就是混乱的、不可预测的，甚至可能有点危险的。

所以，这场技术路线之争到最后，可能还是一场价值观之争。我们是想要一个我们自己能够尝试去定义其善的AI，还是一个只能够被动反映我们人类人性中所有善与恶的AI呢？萨顿没有给出答案，但是他用他的理论，指出了这两条路可能通往的截然不同的终点。

### 总结与反思

理查德·萨顿希望提醒我们，不要被大语言模型神奇的语言魔力所迷惑，而要看到其背后没有基准真相、缺乏真实目标的根本缺陷。他用Alpha Zero的进化来告诉我们，真正的突破可能需要我们勇敢地抛弃对自身知识的依赖。最后，他关于AI继承的冷峻预言，迫使我们去思考那个终极问题：我们到底想要创造一个怎样的未来？