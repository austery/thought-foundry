---
author: 北美王路飞
date: '2025-10-09'
guest: ''
layout: post.njk
source: https://www.youtube.com/watch?v=vIWImun6W5o
speaker: 北美王路飞
tags:
  - t-literature-note
  - judea-pearl
  - causality
  - artificial-intelligence
  - counterfactual-reasoning
  - critical-thinking
title: 图灵奖得主Judea Pearl的灵魂拷问：AI能回答“为什么”吗？
summary: 本视频探讨了Judea Pearl提出的“因果关系之梯”，揭示了当前AI的局限性。从关联、干预到反事实推理，深入剖析了人类智慧的核心，并展望了强人工智能的未来发展方向。
insight: ''
draft: true
series: ''
category: ''
area: ''
project: ''
status: evergreen
---
### AI的真正局限：因果关系之梯

你是否曾想过，那些画出蒙娜丽莎、太空漫步图的AI，以及打败世界围棋冠军的**AlphaGo**（AlphaGo: 谷歌DeepMind开发的人工智能围棋程序），在某些方面可能还不如一个三岁的孩子聪明？这不是科幻，也不是危言耸听，而是当前人工智能最顶尖学者们正在激烈讨论的一个事实。究其原因，所有这些强大的AI，都被困在了**因果关系之梯**（Ladder of Causality: Judea Pearl提出的一个框架，用于描述不同层次的因果推理能力）的最底层。而我们每个人，包括三岁的孩子，每天都在这个梯子顶端自由攀爬。

今天，我们将深入探讨这座因果关系之梯。它不仅能让你看懂AI目前的真正局限，更能为你提供一副透明的眼镜，帮助你在充满误导信息的世界里看清事情的真相，理解“为什么”的真正含义。

### 数据爆炸时代的困惑：相关非因果

我们现在正生活在一个数据爆炸的时代。每天，我们被各种报告、研究和新闻标题轰炸，例如“研究发现：每天走一万步的人更长寿”、“数据显示：经常喝咖啡的人患某种疾病的风险更低”、“统计表明：这个季度的冰淇淋销量和犯罪率同步上升了”。我们很容易被这些信息淹没，然后开始疯狂地走路、猛灌咖啡，甚至怀疑卖冰淇淋的邻居。

然而，一个关键信息常常被忽略：数据告诉我们“是什么”（what），但它们很少告诉我们“为什么”（why）。智慧与数据的根本区别，就在于是否理解因果。因果到底是什么？它不是简单地看A和B总是一起出现，而是你心里清楚地知道，如果我主动去拨弄一下A，B会不会跟着动起来？能回答这个问题，才算真正摸到了因果的门道。

我们普通人，包括曾经的科学家们，最容易犯的一个错误，就是把相关当成因果。刚才那个冰淇淋与犯罪率的例子就是典型。数据是真实的，每个夏天，随着冰淇淋销量的飙升，犯罪率也确实上升了。但是，难道是因为甜筒激发了人们的犯罪冲动吗？当然不是。我们都知道，是因为夏天这个共同的原因：天气热大家爱吃冰淇淋，也更喜欢晚上出门活动，导致了这两个现象的**相关**（Correlation: 两个或多个事物之间存在统计学上的联系，但不一定是因果关系）。

这个简单例子揭示了一个非常深刻的道理：有一种思考方式，它只能看到冰淇淋和犯罪率的同步变化，却看不到这背后隐藏的夏天。这种思考方式，是**图灵奖**（Turing Award: 计算机科学领域的最高荣誉，常被称为“计算机界的诺贝尔奖”）得主，也是我们今天故事的主角**朱迪亚·珀尔**（Judea Pearl: 图灵奖得主，因果推理领域的先驱，提出了因果关系之梯）所说的因果之梯的第一层——**关联**（Association: 因果之梯的第一层，指事物之间存在的统计学上的联系，但不一定是因果关系）。

### 因果之梯第一层：关联（看见和观察）

第一层的活动是看见和观察，它回答的问题是“如果我看见了X，我对Y的看法会改变吗？”比如，超市经理发现数据显示，买了牙膏的顾客好像更倾向于买牙线。这个关联很有用，它可以把牙线和牙膏摆在一起，提高销量。

**统计学**（Statistics: 一门收集、分析、解释、呈现和组织数据的科学学科）也是这门手艺的集大成者，它用各种复杂的工具，比如**相关系数**（Correlation Coefficient: 统计学中衡量两个变量之间线性关联强度和方向的指标）、**回归分析**（Regression Analysis: 统计学中用于建模和分析多个变量之间关系的方法，通常用于预测），帮我们在海量数据里找到A与B经常一起出现的模式。回到冰淇淋和犯罪率的例子，数据告诉我们它们呈现一个正相关的关系，但是统计学本身永远无法告诉我们是冰淇淋导致了犯罪，还是犯罪导致了大家狂吃冰淇淋，或者背后有其他共同原因。在第一层，箭头的方向是未知的。

如果用一种动物来比喻，第一层的思考者就像一个经验丰富的猫头鹰。它蹲在树上观察了成千上万次老鼠的活动，它知道老鼠在A点出现后，很大几率会跑到B点。它的预测非常准，是一个顶级的捕猎者。但是，它不知道老鼠要去B点是因为那里有食物，也不可能知道可以修条近路或者放一个捕鼠夹来改变老鼠的行为。它只知道模式，不懂原因。

现在要告诉你一个可能会颠覆你认知的事实：今天几乎所有你听过的人工智能，包括那些听起来很厉害的**深度学习**（Deep Learning: 机器学习的一个分支，通过多层神经网络学习数据表示）、**神经网络**（Neural Network: 模仿生物大脑结构和功能的计算模型，用于模式识别和数据处理），它们的能力基本都停留在这个猫头鹰的水平，即因果之梯的第一层。它们是顶级的数据观察者，能够从海量信息中识别出最细微的模式，比如人脸识别、语音翻译、推荐算法。但是它们仍然是在做关联，它们不知道你为什么会喜欢这些视频，只知道看过A的人也爱看B。它们是关联的大师，却是因果的白痴。

### 因果之梯第二层：干预（动手做实验）

要如何才能从这拥挤的第一层往上爬一步呢？答案是停止只看不练，开始动手。欢迎来到因果之梯的第二层——**干预**（Intervention: 因果之梯的第二层，指主动改变某个变量，以观察其对其他变量的影响）。

“干预”这个词，在美剧《How I Met Your Mother》中也曾出现，指的是朋友们为了帮助某人纠正不良行为而采取的集体行动。第二层的活动是动手做实验，它回答的问题是“如果我做了某件事，会发生什么？”要注意这个“做”字，它一下就把人类和绝大多数动物以及所有的AI区分开来了。

回到牙膏的例子。第一层的问题是“根据过去的数据，如果牙膏的价格是20块，牙线销量是多少？”第二层的回答是“如果我现在主动把牙膏的价格提高到20块，不管市场什么情况，牙线的销量会变成多少？”感觉到区别了吗？前者是被动观察，后者是主动改变事件。过去的20块可能是因为缺货，大家没得选；而现在是你主动调价，顾客可能去别家买，结果可能完全不同。学会干预，意味着你不再是这个世界的观察者，而是尝试成为这个世界的改变者。这是从“看懂”到“会用”的飞跃。

为了让大家彻底理解“看见”与“动手”的天壤之别，珀尔设计了一个极其经典的行刑队的思想实验。规则很简单：法院下令，队长才下令；队长下令后，两个士兵A、B才同时开枪；只要有一个士兵开枪，犯人就必死无疑。

现在我们开始爬梯子。
*   **第一层关联（看见）**：
    *   问题1：我们事后发现犯人死了，我们能推断法院下令了吗？
        *   当然能，因为顺着箭头往回倒推，犯人死说明至少有一个士兵开枪，士兵开枪说明队长下令了，队长下令了说明法院下令了。这是简单的逻辑推理。
    *   问题2：我们看到士兵A开枪了，这对我们判断士兵B是否开枪有什么影响？
        *   影响很大，因为我们知道A开枪是因为听到队长的命令，而队长对A、B一起下令了，所以B肯定也开枪了。
        *   这第一层是通过看见的一个结果，来更新我们对其他情况的相信程度。

*   **第二层干预（动手）**：
    *   现在难度升级了。问题：如果士兵A没等队长命令，自己决定擅自开枪了，犯人会死吗？士兵B会开枪吗？
        *   这下AI就懵了。它的规则库里写着“队长下令，A才开枪”。你现在问我A没听到命令就开枪，这不符合逻辑啊。但是人类大脑可以轻松处理。我们需要做一次因果手术，这个手术的意义是士兵A的行为现在不再受队长的影响了，而是由我们一个想象中的干预来决定的。
        *   手术做完，我们再看结果：A开枪犯人必死，这没问题。士兵B呢？没有任何因素能再影响B了，队长的命令没来，A的擅自行动也和B没关系，所以B没有开枪。

请停下来，仔细品味一下这个结论的奇妙之处：当我们“看见”A开枪，我们推断B也开枪了；但是当我们“让”A开枪，我们推断B没有开枪。同一个动作，观察与干预，得出了与B截然相反的结论。如果你能够理解这一点，那么恭喜你，你已超越世上所有的深度学习模型，稳稳地站在了因果之梯的第二层。这里最关键的一点是，你看到了A开枪，还是你让A开枪？因为你让A开枪，就主动去改变这个模型了，去让其中一个因素发生了改变，然后来看它对另外一个因素是否会产生效应。这和前面的观察是完全不一样的一个情况。

做到这一步已经非常了不起了，我们学会了做实验，研究改变世界的结果。但是，这依然不是智慧的顶峰，因为第二层依然回答不了一个最最重要的问题：为什么？我的头疼消失了，是因为我半小时前吃的那片阿匹斯零吗？还是因为我喝了冰水，或者只是巧合？

### 因果之梯第三层：反事实（想象未发生之事件）

要回答“为什么”这个问题，你需要一种近乎魔法的能力，能够想象一个未发生过的事件。欢迎来到因果之梯的最高层——第三层**反事实**（Counterfactual: 因果之梯的第三层，指思考“如果过去某个事件没有发生，现在会是怎样”的能力）。

反事实这个词听起来很学术，但是它的意思你每天都在用：“如果当初……那就好了”。如果当初我没有卖那只股票，如果当初勇敢一点去告白，如果肯尼迪没有被刺杀，历史会怎样？这些都是反事实思考。美剧《How I Met Your Mother》中泰德一个人坐在酒吧里想象了一整晚的剧情，也是一个典型的反事实场景。

反事实思考的强大与独特之处在于，它想要处理的世界是和这个事实相反的。数据顾名思义记录的都是事实，它只能告诉你你吃了阿匹斯零，然后头不疼了。它永远无法告诉你，在那个你没有吃阿匹斯零的平行世界里的你的头疼会不会自己好。而能回答“为什么”，正是这种现实与想象的对比。我的头疼好了，是因为那片阿匹斯零，这句话的潜台词是我能想象如果我没吃那片药，我的头现在应该还疼着。这种能力是人类智慧最耀眼的标志，它能够让我们进行反思、后悔、总结经验，并且为自己的行为负责。这是目前任何动物或者任何AI都没有办法做到的。

我们再回到行刑队的那个例子，来体验一下第三层的威力。事后我们发现犯人已经死了，通过第一层推理，我们知道法院、队长、士兵A、士兵B所有环节都执行了。现在我们来问一个典型的第三层问题——一个关于责任和原因的问题：我们知道是A开枪打死了他，但是A这一枪是导致他死亡的原因吗？换句话说，如果当时士兵A没有开枪，犯人会活下来吗？

要回答这个问题，我们就必须开启平行宇宙的模拟器了。构建这个想象的世界的步骤是：
1.  **溯源现实世界发生了什么**：法院下了令，队长发了令，这是我们已知的事实，必须带到想象世界中去。
2.  **干预**：在想象世界里，我们强制改变了一个条件——士兵A没有开枪。同时，我们要对模型做手术，切断A与队长的联系。
3.  **推演**：在这个新规则下，让世界运作起来。法院命令，队长命令士兵B开枪，犯人死亡。以上都是真的。而士兵A并没有开枪。

结论出来了：即使在那个A没有开枪的平行宇宙里，因为B的存在，犯人还是会死。所以回到我们最初的问题，A这一枪是导致他死亡的原因吗？从这个反事实的视角来看，不是，因为就算他不开枪，结果也是一样的。这就是为什么要有行刑队而不是行刑手，它通过机智的设计，分散了每个个体的因果责任。而能想明白这一层，靠的就是这个梯子顶端的反事实推理。

你可能会觉得行刑队只是一个巧妙的思想游戏，那我们现在来看一个真实到残酷的例子——天花疫苗的争议。

### 天花疫苗的争议：反事实推理的实践

假设在一个一百万儿童的国家里推广了天花疫苗。99%的孩子接种了疫苗，有99万人；有1%的孩子没有接种疫苗，1万人。然后我们来看发生了什么。

这是第一层关联的数据：
*   在接种疫苗的99万人中，有1%的人（9900人）发生了不良反应，其中1%是致命的，所以有99个孩子死于疫苗接种。
*   在没有接种的1万人里头，有2%的几率感染天花（200人），其中有20%是致命的，所以有40个孩子死于天花。

好了，数据就到这里了。如果你是当时的家长，看到这个数据，你会怎么想？“疫苗害死人，比天花还要多，疫苗是个坏东西，应该禁止！”这种想法完全可以理解，因为你只看到第一层数据，结论似乎就是这样的。

但是，一个经过因果推理训练的科学家，必须问出那个通往第三层救命的反事实问题：“如果当初我们彻底禁止了疫苗，一个孩子都没打，会发生什么？”

让我们启动平行宇宙的模拟器：
1.  **基础**：一百万个孩子。
2.  **干预**：接种率设为0%。
3.  **推演**：这一百万孩子，每一个都有2%的几率感染天花，就是两万人。这两万人里头会有20%会死亡。结果就是100万乘以2%乘以20%，最后是4000人。

现在真相大白了。在现实世界里，总死亡人数是99（疫苗致死）+40（天花致死）=139人。在没有疫苗的平行宇宙里，死亡人数是4000人。疫苗这个看起来“害死”99个人的东西，实际上拯救了4000减去139，即3861条生命。这是反事实推理，它让我们穿越了数据的迷雾，看到那个本可能发生的更悲惨的世界，从而真正理解了疫苗的真正价值。分母、极限和想象不存在的世界的能力，缺一不可。现在当你再看到有人宣传疫苗的危害性时，你要考虑一下他给出来的数据是否考虑到反事实的情况。

### 人类认知革命与AI的未来

这种攀登因果之梯，尤其是抵达第三层的能力，不仅是一种科学工具。很多学者认为，这正是大约五到七万年前人类祖先身上发生的**认知革命**（Cognitive Revolution: 人类学家赫拉利提出的概念，指智人在约5-7万年前出现的一种认知能力飞跃，使其能够进行抽象思维和想象）的核心。我们学会了不仅仅是使用工具，更是理解工具背后的“为什么”，从而改进它。我们学会了策划一场复杂的狩猎，在脑中预演各种“如果……那么……”的场景：如果风向变了怎么办？如果来了两头猛犸象怎么办？我们甚至开始创造一些不存在现实的东西。在德国的一个洞穴里发现的四万年前的狮身人面像雕像，一半是人，一半是狮子，这是人类能够想到反事实生物最早的证据之一。这种想象不存在之物的能力，正是所有科学发现、技术发明、艺术创作的起点。想象力不是什么虚无缥缈的东西，它是一种强大的在脑内进行因果实验和反事实模拟的工具。

聊到这里，我们再回头看AI。为什么它还被困在第一层？因为过去几十年，AI的核心发展逻辑是给他更多数据，让他自己找关联。这条路在很多预测性任务上取得了巨大成功，但也几乎锁死了它通往更高认知能力的道路。

AI的未来，真正的强人工智能，可能不存在更大的模型、更多算力，而在于我们能否为AI也打造一架因果之梯。我们能否教会它区别“看见”和“动手”？我们能否让他拥有一个关于世界的、小小的、可以做手术的因果模型？以及最难的，我们能否让他和我们一样去想象那个本可能的世界，然后问一句“为什么”？这就是朱迪亚·珀尔和很多因果科学家正在努力的方向，一场关于因果的革命正在人工智能和几乎所有科学领域悄然发生。

### 总结：因果之梯与批判性思维

最后，我们来快速回顾一下这架强大的因果之梯：
1.  **第一层：关联（看见）**：它的问题是“是什么？”工具是统计和数据。这是当前AI的主场。
2.  **第二层：干预（动手）**：它的问题是“如果我做了会怎么样？”工具是实验。这是人类实践的起点。
3.  **第三层：反事实（想象）**：它的问题是“为什么？如果当初……？”工具是因果模型和想象力。这是人类智慧的顶峰。

如果大家对这个话题意犹未尽，想要更深入了解这场思想革命，我强烈推荐它的源头——朱迪亚·珀尔的这本《为什么》（The Book of Why）。它可能会改变你看待这个世界的方式。

下一次，当你再看到任何研究发现X和Y有关的结论时，我希望你脑海中会想起今天的梯子警报。你可以尝试问自己三个问题：
1.  这只是一个关联吗？有没有可能是“夏天”导致了冰淇淋和犯罪？
2.  有没有人真的动手做过实验？有没有一个干预组和一个对照组？
3.  这个结论能回答一个反事实的问题吗？它能帮助你理解如果当初不做这个选择，世界会变得更好还是更糟？

学会问这三个问题，你就掌握了因果之梯的精髓。你将不仅仅是一个被动的信息接收者，而成为一个主动的、有批判性思维的思考者。而这种思考是科学的引擎，或许也是我们人心中最闪亮的火花。